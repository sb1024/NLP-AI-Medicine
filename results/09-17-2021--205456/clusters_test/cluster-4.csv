text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Innovative Vaccine Approaches ABSTRACT Support is requested for a Keystone Symposia conference entitled Innovative Vaccine Approaches, organized by Drs. Mariagrazia Pizza, Galit Alter and Gordon Dougan. The conference will be held in Vancouver, Canada from June 27- July 1, 2021. Vaccines have the power to prevent and potentially eradicate a wide range of infectious diseases, representing one of the most effective life-saving measures at our disposal against global health threats. The recent coronavirus pandemic has brought the importance and urgency of vaccine development efforts into sharp focus. Moreover, the vaccinology field is evolving very rapidly, thanks to advances in our understanding of microbiology, immunology and genomics, as well as advances in structural analysis of antigens and antigen- antibody complexes and impacts of variation. Over the years, this field has also experienced an elucidation of mechanisms of immunity and protection, and identification of correlates. However, many questions are still unsolved and innovative approaches are needed to address new vaccine challenges like antimicrobial resistance, emerging infectious diseases, cancer and diseases associated with our aging population. This conference will cover the latest advances and novel approaches towards vaccine development, including: (1) novel antigen delivery systems; (2) in vitro and in vivo model systems for vaccine appraisal (3) the use of human challenge models; (4) the role of ‘systems biology’ in the comprehensive analysis of immune correlates, biomarker identification and safety; (5) machine-learning approaches to define correlations between antibody repertoires and protection; and (6) strategies for developing low cost vaccines for economically challenged populations. Together these topics will provide attendees with the new ideas and tools to continue to forge new frontiers in vaccine capabilities. PROJECT NARRATIVE Vaccines have the power to prevent and potentially eradicate a wide range of both infectious and non- infectious diseases. The field is evolving very rapidly due to improvements in our understanding of microbiology, immunology and genomics, as well as advances in structural analysis techniques. This conference will accelerate advances the field, bringing together public and private communities to ensure the end of the COVID-19 pandemic and other epidemics that afflict the population. This event provides a unique opportunity for discussion of the key challenges in making low cost vaccines for economically challenged populations and how to address burning topics such as pandemics, antimicrobial resistance, emerging infectious diseases, cancer and an aging population.",Innovative Vaccine Approaches,10237543,R13AI161938,"['Address', 'Antibody Repertoire', 'Antigen-Antibody Complex', 'Antigens', 'Antimicrobial Resistance', 'Biological Models', 'COVID-19 pandemic', 'Canada', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Coronavirus', 'Disease', 'Educational workshop', 'Emerging Communicable Diseases', 'Ensure', 'Epidemic', 'Event', 'Future', 'Genomics', 'Human', 'Immersion', 'Immune', 'Immunity', 'Immunology', 'In Vitro', 'Knowledge', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methodology', 'Microbiology', 'Modeling', 'Outcome', 'Population', 'Privatization', 'Research', 'Research Personnel', 'Role', 'Safety', 'Savings', 'Scientist', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Vaccines', 'Variant', 'aging population', 'biomarker identification', 'clinical practice', 'cost', 'experience', 'frontier', 'global health', 'in vivo Model', 'innovation', 'manufacturability', 'next generation', 'novel', 'novel strategies', 'novel vaccines', 'pandemic disease', 'posters', 'prevent', 'symposium', 'tool', 'vaccine development', 'vaccine discovery', 'vaccinology']",NIAID,KEYSTONE SYMPOSIA,R13,2021,8000
"Computational Analysis of Enzyme Catalysis and Regulation Project Summary: It is of great fundamental and biomedical importance to understand the physical princi- ples that govern the coupling between the chemical step in a biomolecule and other events, such as penetration of water molecules into the active site, recruitment of transient metal ions, or conformational rearrangements near and afar. This is a challenging task, however, due to the intrinsic multi-scale nature of the problem. As a result, our understanding in factors that dictate the efﬁciency and speciﬁcity of enzyme catalysis remains in- complete, especially regarding contributions beyond the active site; this knowledge gap has greatly limited our ability to design highly efﬁcient enzymes de novo. Motivated by these considerations, the overarching theme of our research is to develop and apply multi-scale computational methods to reveal the underlying mechanism of enzyme catalysis at an atomic level, with a particular emphasis on establishing to what degree the chem- ical step is coupled with other processes proximal or distal to the active site. Speciﬁcally, we aim to develop an efﬁcient QM/MM framework to compute free energy proﬁles of enzyme reactions with a good balance of computational speed and accuracy; further integration with enhanced sampling approaches, machine learning techniques and modern computational hardwares enables us to gain insights into the nature of coupling be- tween the chemical step and other events during the functional cycle. Accordingly, we are in a unique position to pursue several lines of exciting applications, which include the mechanism and impact of transient metal ion recruiting in nucleic acid processing enzymes, the catalytic and regulatory mechanism of peripheral membrane enzymes, and systemic analysis of allosteric coupling in a transcription factor; an emerging research direction is to explore the interplay of stability, catalytic activity, and allostery during continuous directed evolution. Our project integrates computational method developments with applications inspired by recent experimental ad- vances, such as time-resolved crystallography, deep mutational scanning and continuous directed evolution. The research efforts will lead to novel computational tools and mechanistic insights into the regulatory mech- anisms of enzymes by processes either near or remote from the active site. Thus the project will have both fundamental impacts and implications for better design strategies for catalysis and allostery in biomolecules. Narrative: The computational methodologies we develop will be applicable to a broad set of metalloen- zymes and proteins of biomedical relevance. In particular, we target fundamental mechanistic problems in enzymes that catalyze nucleic acids synthesis/modiﬁcation and lipid metabolism, since mutations in these en- zymes are implicated in numerous human diseases such as cancer, insulin resistance and diabetes. Although our project does not focus on design of drugs, the mechanistic insights into enzyme catalysis and allosteric regulation will broaden strategies that can be used to target various enzymes of biomedical signiﬁcance.",Computational Analysis of Enzyme Catalysis and Regulation,10206585,R35GM141930,"['Active Sites', 'Allosteric Regulation', 'Catalysis', 'Chemicals', 'Computer Analysis', 'Computing Methodologies', 'Coupled', 'Coupling', 'Crystallography', 'Diabetes Mellitus', 'Directed Molecular Evolution', 'Distal', 'Drug Design', 'Enzymes', 'Equilibrium', 'Event', 'Free Energy', 'Insulin Resistance', 'Ions', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Membrane', 'Metals', 'Modernization', 'Modification', 'Molecular Conformation', 'Mutation', 'Nature', 'Nucleic Acids', 'Penetration', 'Peripheral', 'Positioning Attribute', 'Process', 'Proteins', 'Reaction', 'Regulation', 'Research', 'Sampling', 'Specificity', 'Speed', 'Techniques', 'Time', 'Tweens', 'Water', 'computerized tools', 'design', 'enzyme mechanism', 'human disease', 'insight', 'lipid metabolism', 'method development', 'mutation screening', 'novel', 'recruit', 'transcription factor']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2021,295591
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,10111538,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2021,1354555
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,10113656,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2021,397125
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,10144435,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science Core', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2021,1471985
"Maximizing Investigators' Research Award (R35) PROJECT SUMMARY The “Tools for Transmission of Agents and Conditions (TRAC)” program will synergize statistical and mathematical modeling work in three areas of application: 1) Tuberculosis (TB) incidence and transmission; 2) monitoring substance use disorder (SUD) patterns; and 3) SARS CoV-2 transmission modeling. These three conditions are major public health problems, with TB being the leading cause of infectious disease death globally, SUD causing more deaths in the United States than HIV/AIDS in its peak, and SARS CoV-2 causing a pandemic with societal disruption and mortality exceeding anything we have experienced in the last century. We need improved analytical tools that leverage existing data to monitor these diseases, infer transmission hot spots, determine the efficacy of interventions, and understand the burden of these conditions. This program will bring together an expert group of quantitative researchers with skills that are readily applied to these problems. We also leverage our strong collaborations with clinician researchers and public health officials to ensure that the methods we develop are addressing important questions and consistent with our current understanding of these diseases. By creating a program to facilitate communication between these experts, we will enable greater innovation in modeling key aspects of these diseases and create exciting methodological synergies across diseases. Our team is well positioned to incorporate data from emerging technologies, including high throughput sequencing data to determine TB risk signatures and inform transmission links for TB and SARS CoV-2. Our expertise in machine learning, a broad range of statistical methodologies, and mathematical modeling will enable us to leverage the rich information in large databases that are emerging to better understand SUD patterns and identify risk signatures. We will also build infrastructure with our partners to make the analytical tools that we develop more accessible to public health practitioners and other researchers. The impact of this work is to develop a suite of analytical tools that leverage rapidly emerging rich data sets to improve our understanding of disease transmission patterns, monitor changing dynamics of these conditions, and understand intervention strategies that are most effective. This work will inform public health practice for these diseases and create reproducible tools that can be used in an ongoing way. PROJECT NARRATIVE This project will generate analytical tools to address three major public health challenges: 1) Tuberculosis, 2) Substance use disorders, and 3) SARS CoV-2. These tools will improve our understanding of the transmission patterns of these diseases, advance our ability to monitor them, and inform intervention strategies. We will build infrastructure to make these methods available to public health practitioners and other researchers.",Maximizing Investigators' Research Award (R35),10205596,R35GM141821,"['2019-nCoV', 'AIDS/HIV problem', 'Address', 'Area', 'Award', 'Cessation of life', 'Collaborations', 'Communicable Diseases', 'Communication', 'Data', 'Data Set', 'Databases', 'Disease', 'Emerging Technologies', 'Ensure', 'High-Throughput Nucleotide Sequencing', 'Hot Spot', 'Incidence', 'Infrastructure', 'Intervention', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Pattern', 'Positioning Attribute', 'Public Health', 'Public Health Practice', 'Reproducibility', 'Research', 'Research Personnel', 'Risk', 'SARS-CoV-2 transmission', 'Statistical Models', 'Substance Use Disorder', 'Treatment Efficacy', 'Tuberculosis', 'United States', 'Work', 'analytical tool', 'disease transmission', 'experience', 'improved', 'innovation', 'mathematical model', 'mortality', 'pandemic disease', 'programs', 'skills', 'synergism', 'tool', 'transmission process']",NIGMS,BOSTON UNIVERSITY MEDICAL CAMPUS,R35,2021,171876
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,10213202,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computational pipelines', 'computerized tools', 'dark matter', 'dietary', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'in silico', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2021,981634
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10250553,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2021,660324
"Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center The “clinical high risk” (CHR) for psychosis syndrome is an antecedent period characterized by attenuated psychotic symptoms that are marked by subtle deviations from normal development in thinking, motivation, affect, behavior, and a decline in functioning. Early intervention in this CHR population is critical to prevent psychosis onset as well as other adverse outcomes. However, the presentation of symptoms and subsequent course is highly variable, and there is a paucity of biomarkers to guide treatment development. Thus, to improve predictive models that are clinically relevant, several issues need to be addressed: 1) focusing on outcomes beyond psychosis; 2) taking into account heterogeneity in samples and outcomes; and 3) integrating data sets with a broad array of variables using innovative algorithms to overcome variability across studies. To address these challenges, the proposed “Psychosis Risk Evaluation Data Integration and Computational Technologies: Data Processing, Analysis, and Coordination Center” (PREDICT-DPACC) brings together a multidisciplinary team of highly experienced researchers with proven capabilities in all aspects of large-scale studies, CHR studies, as well as computational expertise. The ultimate goal is to identify new CHR biomarkers, and CHR subtypes that will enhance future clinical trials. To do so, the PREDICT-DPACC will 1) aggregate extant CHR- related data sets from legacy datasets; 2) provide collaborative management, direction, data processing and coordination for new U01 multisite network(s); and 3) develop and apply advanced algorithms to identify biomarkers that predict outcomes, and to stratify CHR into subtypes based on outcome trajectories, first from the extant data and then refined and applied to the new data. The PREDICT-DPACC team has the broad, comprehensive, and robust infrastructure that is sufficiently flexible to accommodate the inclusion of multiple data types and to optimally address the needs of the CHR U01 network(s). Carefully selected extant data will be rapidly obtained, processed, and uploaded to the NIMH Data Archive (NDA). Proposed analysis methods are powerful and robust, leveraging the expertise and experience of computer scientist developers, and experienced clinical researchers. The U01 network(s) will be coordinated by a team that is experienced in managing large studies, familiar with the needs of such studies, flexible, and is knowledgeable in all aspects of CHR studies, including measures, outcomes, biomarkers, and cohorts. Upon meeting the goals of this U24, and the supported U01 network(s), the expected outcomes of the PREDICT-DPACC will be new predictive biomarkers for CHR outcomes, new definitions of CHR subtypes that are clinically useful, and new curated and comprehensive CHR datasets (extant and new) as well as processing tools and prediction algorithms that are shared with the research community through the NIMH Data Archive. NARRATIVE The “Clinical High Risk” (CHR) for psychosis syndrome in young people represents an opportune window for early intervention to prevent the onset of psychosis and other disorders, and to forestall disability; however, clinical heterogeneity and the paucity of biomarkers have hampered the development of effective intervention. To address these challenges, working with NIMH and key stakeholders, we will harmonize and aggregate existing “legacy” CHR data, and guide and coordinate the collection of new data across a network of sites, to develop biomarker algorithms that can predict individual trajectories for diverse outcomes. This proposal leverages a multidisciplinary team with broad and CHR-specific experience in large-scale multisite and multimodal studies (including clinical trials), along with expertise in data type-specific processing, coordination, analysis, and computational analyses (e.g., machine and deep learning tools from artificial intelligence, and advanced statistical approaches), ethics, community outreach, and data dissemination, all of which will ensure the success of this project.","Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center",10256796,U24MH124629,"['Address', 'Adolescent', 'Affect', 'Algorithms', 'Anxiety Disorders', 'Artificial Intelligence', 'Attenuated', 'Behavior', 'Big Data', 'Biological Markers', 'Child', 'Clinical', 'Clinical Trials', 'Collection', 'Common Data Element', 'Communities', 'Community Outreach', 'Computer Analysis', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease remission', 'Early Intervention', 'Early identification', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'FAIR principles', 'Follow-Up Studies', 'Funding', 'Future', 'Goals', 'Heterogeneity', 'Human Resources', 'Impaired cognition', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Intervention', 'Lead', 'Leadership', 'Longterm Follow-up', 'Machine Learning', 'Measures', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Monitor', 'Moods', 'Motivation', 'National Institute of Mental Health', 'Online Systems', 'Outcome', 'Output', 'Perception', 'Procedures', 'Process', 'Protocols documentation', 'Psychotic Disorders', 'Quality Control', 'Recovery', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Sampling', 'Scientist', 'Secure', 'Site', 'Social Functioning', 'Standardization', 'Substance Use Disorder', 'Suggestion', 'Symptoms', 'Technology', 'Thinking', 'Time', 'Training', 'Transact', 'United States', 'Validation', 'Visualization software', 'adverse outcome', 'analytical tool', 'attenuated psychosis syndrome', 'base', 'bioinformatics infrastructure', 'candidate marker', 'clinical heterogeneity', 'clinical risk', 'clinical subtypes', 'clinically relevant', 'cloud based', 'cohort', 'computerized data processing', 'data acquisition', 'data archive', 'data dictionary', 'data dissemination', 'data harmonization', 'data infrastructure', 'data integration', 'data tools', 'deep learning', 'demographics', 'design', 'disability', 'effective intervention', 'experience', 'flexibility', 'functional decline', 'functional disability', 'high risk', 'high risk population', 'improved', 'inclusion criteria', 'innovation', 'meetings', 'member', 'multidisciplinary', 'multimodal data', 'multimodality', 'multiple data types', 'outcome prediction', 'persistent symptom', 'prediction algorithm', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'psychotic symptoms', 'quality assurance', 'recruit', 'research study', 'resilience', 'response', 'risk prediction', 'risk stratification', 'success', 'therapy development', 'tool', 'working group']",NIMH,BRIGHAM AND WOMEN'S HOSPITAL,U24,2021,3917810
"Non-target analysis of maternal and cord blood samples: Advancing computational tools and discovering novel chemicals PROJECT SUMMARY/ABSTRACT  Non-targeted analysis (NTA) provides a comprehensive approach to analyze environmental and biological samples for nearly all chemicals present. Despite the recent advancements in NTA, the number of confirmed chemicals with analytical standards remains fairly small compared to the number of detected features. There is, thus, a need to further develop computational tools to derive more chemical structures and leverage the full potential of HRMS. Enhancing our ability to derive more chemical structures will enable the discovery of new industrial chemicals that humans are exposed to, especially in critical windows of development, such as pregnancy. It will also enable the discovery of endogenously produced metabolites that may be related to biological outcomes of importance, such as preterm birth. The objective of my proposal is to develop novel computational methods to significantly advance our ability to analyze and interpret non-targeted analysis data from high-resolution mass spectrometry (HRMS) and apply them to study prenatal exposures to industrial chemicals and endogenous metabolites in a large cohort of pregnant women from Northern California. My proposal builds on my expertise in analytical and environmental chemistry and my current postdoctoral experience in computational chemistry and applications in human exposure. I seek additional training to develop and apply innovative computational methods to better characterize the human exposome and in particular the exposome of preterm birth. The contribution of my proposal will be two-fold: (1) developing novel computational structure-prediction algorithms for HRMS datasets based on MS data and physicochemical properties (equilibrium partition ratios between organic solvents and water, e.g., octanol/water, chlorobenzene/water, diethyl ether/water etc.) (Aim 1) and apply them to derive potential structures for chemical features detected in a HRMS dataset from 340 maternal and 340 matched cord blood samples to complement the limited number of chemicals identified through MS/MS and analytical standards (Aim 2); and (2) study the interplay between the exposome and the metabolome in preterm birth using molecular interaction networks to visualize and compare how molecular interactions between industrial chemicals and endogenous metabolites differ between preterm and full-term birth (Aim 3). The K99 training will expand my prior research experience through coursework, research apprenticeship, and mentored reading, with specific training in: (1) advanced analytical skills including -omics data analysis, machine learning, and biostatistics; (2) epidemiology, risk assessment, human exposure to chemical stressors; and (3) human pregnancy and development. The skills acquired during this award are critical to my long-term goal to advance computational methods to better analyze and interpret non-targeted analysis data to support efforts to better characterize the human exposome. This work will produce new scientific knowledge to greatly advance the understanding of the influence of environmental exposures in the development of adverse health outcomes and in particular, preterm birth. PROJECT NARRATIVE  Advances in high-resolution mass spectrometry (HRMS) offer unique opportunities to study the human exposome and the development of adverse health outcomes. Despite advances in non-targeted analysis, the number of confirmed chemicals remains fairly limited compared to the number of detected chemical features. The goal of this proposal is to develop novel computational methods to derive chemical structures and apply them to a large dataset of HRMS data from blood samples of pregnant women to study the influence of the exposome on pregnancy outcomes with a focus on preterm birth.",Non-target analysis of maternal and cord blood samples: Advancing computational tools and discovering novel chemicals,10191991,K99ES032892,"['Algorithms', 'Analytical Chemistry', 'Area', 'Award', 'Bile fluid', 'Binding', 'Bioinformatics', 'Biological', 'Biological Monitoring', 'Biometry', 'Birth', 'Blood specimen', 'California', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Chlorobenzene', 'Clinical Data', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Endocrine Disruptors', 'Ensure', 'Environmental Exposure', 'Epidemiology', 'Equilibrium', 'Ethyl Ether', 'Exposure to', 'Flame Retardants', 'Goals', 'Health', 'Human', 'Human Development', 'Industrialization', 'Knowledge', 'Liquid Chromatography', 'Low Birth Weight Infant', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mentors', 'Metabolic Diseases', 'Methods', 'Neurodevelopmental Disorder', 'Octanols', 'Organic solvent product', 'Outcome', 'Pathway interactions', 'Pesticides', 'Plasticizers', 'Poly-fluoroalkyl substances', 'Pregnancy', 'Pregnancy Outcome', 'Pregnant Women', 'Premature Birth', 'Property', 'Reading', 'Research', 'Resolution', 'Risk Assessment', 'Sampling', 'Science', 'Stimulus', 'Structure', 'Sum', 'Term Birth', 'Time', 'Toxicity Tests', 'Training', 'Umbilical Cord Blood', 'Water', 'Work', 'advanced analytics', 'apprenticeship', 'base', 'case control', 'cohort', 'computational chemistry', 'computer framework', 'computerized tools', 'data integration', 'developmental toxicity', 'environmental chemistry', 'experience', 'exposed human population', 'human disease', 'in silico', 'innovation', 'insight', 'large datasets', 'machine learning algorithm', 'metabolome', 'metabolomics', 'molecular mass', 'novel', 'prediction algorithm', 'prenatal', 'prenatal exposure', 'psychologic', 'reproductive toxicity', 'response', 'skills', 'small molecule libraries', 'stressor', 'success', 'time of flight mass spectrometry', 'tool']",NIEHS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K99,2021,99848
"ARAGORN: Autonomous Relay Agent for Generation Of Ranked Networks We propose an Autonomous Relay Agent for Generation of Ranked Networks (ARAGORN), which will query Knowledge Providers (KPs) and synthesize answers relevant to user-specified questions, building upon algorithms and components developed as part of the ROBOKOP [1,2] application during the feasibility phase of Translator. The ARAGORN services represent the next generation of ROBOKOP component services, iterating and innovating in response to challenges exposed in the Translator feasibility phase. Based on that work, we have identified overarching issues that must be addressed to truly unleash the power of Translator. 1. ARAs must be able to operate in a federated knowledge environment effectively and efficiently. First-generation Translator tools assembled full data sets from which to extract answers, which were subsequently ranked. Second-generation tools must be able to efficiently operate on massive, distributed data, demanding a new approach. ARAGORN will act asynchronously, interleaving KP queries with partial scoring of answers, prioritizing search directions on-the-fly, and delivering early results that are updated over time in response to newly explored paths 2. ARAs must bridge the precision mismatch between data representations and algorithms that require specificity, and users who pose questions and prefer answers at a more abstract level. Biomedical scientists do not pose questions as database queries. Furthermore, even expert users of current biomedical databases such as ROBOKOP KG or RTX require exploration and experimentation to craft queries to express their intent. ARAGORN will employ multiple strategies to remove this barrier to asking questions effectively, from basic maintenance of a question library, to node generalization, query rewriting, and machine learning techniques such as capsule graph networks. ARAGORN will further use elements of specific answers to create gestalt explanations, clustering, and combining answers with similar content, revealing the commonalities and contradictions in answers. 3. ARAs must be able to generalize answer ranking to address a broader range of question formulations and data types, and to account for counterevidence. In the Translator implementation phase, we anticipate having access to many varied KPs and ARAs that provide diverse quantitative metadata regarding the confidence in assertions or strength of associations. There will be a pressing need to synthesize such data into scores for arbitrarily-shaped answer graphs, in order to filter and prioritize answers for further analysis or user digestion. ARAGORN will address this need by providing a novel scoring algorithm capable of (a) scoring arbitrary directed multi-hypergraphs, (b) accounting for heterogeneous quantitative metadata; and (c) leveraging relationship polarity to incorporate counterevidence. ARAGORN will provide access to this functionality, and connect to KPs using community-defined APIs and data models. The ARAGORN team has contributed to these community efforts during the Translator feasibility phase, and if funded will continue to work with the Standards and Reference Implementations (SRI) group, NCATS staff, and other awardees to continue to define and refine methods and models for data sharing and collaboration. The ARAGORN services will be created with collaboration in mind, such that they can be plugged into larger pipelining and architectural efforts. Most of the risks to the ARAGORN strategy are shared by the entire program; as standardization evolves, the ARAGORN team and other members of the Translator consortium will be required to spend effort updating components. ARAGORN will require access to ontology and similarity tools that we anticipate will be provided by KPs or shared infrastructure; if these do not materialize, the ARAGORN team will create the tools that it needs to accomplish its goals. Additionally, we are assuming the existence of fully translator-compliant KPs from which to draw data; if the program collectively decides that compliance is enforced in ARAs instead, we will draw on our work in ROBOKOP to implement the necessary normalization components in ARAGORN. n/a",ARAGORN: Autonomous Relay Agent for Generation Of Ranked Networks,10332268,OT2TR003441,"['Accounting', 'Address', 'Algorithms', 'Architecture', 'Collaborations', 'Communities', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Digestion', 'Elements', 'Environment', 'Formulation', 'Funding', 'Generations', 'Goals', 'Graph', 'Infrastructure', 'Knowledge', 'Libraries', 'Machine Learning', 'Maintenance', 'Metadata', 'Methods', 'Mind', 'Modeling', 'Ontology', 'Phase', 'Provider', 'Risk', 'Services', 'Specific qualifier value', 'Specificity', 'Standardization', 'Techniques', 'Time', 'Update', 'Work', 'arbitrary spin', 'base', 'biomedical scientist', 'capsule', 'data modeling', 'data sharing', 'database query', 'distributed data', 'innovation', 'member', 'next generation', 'novel', 'novel strategies', 'programs', 'response', 'tool']",NCATS,UNIV OF NORTH CAROLINA CHAPEL HILL,OT2,2021,1052712
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,10240955,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Hi-C', 'Human', 'Human Biology', 'Human Genome', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data infrastructure', 'data integration', 'data standards', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'large scale data', 'member', 'mouse genome', 'multiple data types', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2021,1975273
"Detection and characterization of critical under-immunized hotspots Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots,10197938,R01GM109718,"['Affect', 'Bayesian Method', 'Behavioral Model', 'California', 'Characteristics', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Disease model', 'Economic Burden', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Funding', 'Geography', 'Growth', 'Health', 'Health Personnel', 'Herd Immunity', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Measles-Mumps-Rubella Vaccine', 'Medical', 'Methodology', 'Methods', 'Minnesota', 'Modeling', 'New Jersey', 'New York', 'Oregon', 'Outcome', 'Pathway interactions', 'Policies', 'Policy Maker', 'Population', 'Population Analysis', 'Privatization', 'Public Health', 'Records', 'Registries', 'Resolution', 'Resource Allocation', 'Resources', 'Risk', 'Scanning', 'Schools', 'Science', 'Source', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Uncertainty', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Washington', 'Work', 'base', 'data mining', 'demographics', 'diverse data', 'economic cost', 'economic outcome', 'health care delivery', 'health disparity', 'health organization', 'improved', 'interest', 'novel', 'novel strategies', 'population based', 'provider networks', 'public health intervention', 'social', 'social media', 'spatiotemporal', 'statistics', 'tool', 'transmission process', 'vaccine hesitancy']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2021,321062
"Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders Project Summary/Abstract  Disorders of mood and psychosis such as schizophrenia, bipolar disorder, and unipolar depression are  incredibly complex, influenced by both genetic and environmental factors, and the clinical characterizations are primarily based on symptoms rather than biological information. Current diagnostic approaches are based on symptoms, which overlap extensively in some cases, and there is growing consensus that we should approach mental illness as a continuum, rather than as a categorical entity. Since both genetic and environmental factors play a large role in mental illness, the combination of brain imaging and genomic data are poised to play an important role is clarifying our understanding of mental illness. However, both imaging and genomic data are high dimensional and include complex relationships that are poorly understood. To characterize the available information, we are in need of approaches that can deal with high-dimensional data exhibiting interactions at multiple levels (i.e., data fusion), while providing interpretable solutions (i.e., a focus on brain and genomic  networks). An additional challenge exists because the available data has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. To address these challenges, we introduce a new unified framework called flexible subspace analysis (FSA) that can automatically identify subspaces (groupings of unimodal or multimodal  components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus  leveraging the power of deep learning. We will apply the developed models to a large (N>60,000) dataset of  individuals along the mood and psychosis spectrum to evaluate the important question of disease categorization. We will compute fully cross-validated genomic-neuro-behavioral profiles of individuals including a comparison of the predictive accuracy of 1) standard categories from the diagnostic and statistical manual of mental disorders (DSM), 2) data-driven subgroups, and 3) dimensional relationships. We will also evaluate the single subject predictive power of these profiles in independent data to maximize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data  promises to advance our understanding of the nosology of mood and psychosis disorders in addition to providing new tools that can be widely applied to other studies of complex disease. Project Narrative  It is clear that mood and psychosis disorders, largely diagnosed without biological criteria, include a multitude of inter-related genetic and environmental factors. We propose to develop new flexible models to capture  multiscale (dynamic) brain imaging and genomics data, which we will use to study individuals along the mood and psychosis spectrum using a large aggregated dataset including a comparison of the predictive accuracy of two dichotomous approaches (standard diagnostic categories and unsupervised/data-driven) as well as a  dimensional approach to diagnosis.",Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders,10112311,R01MH118695,"['3-Dimensional', 'Address', 'Algorithms', 'Behavior', 'Behavioral', 'Benchmarking', 'Biological', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Brain region', 'Categories', 'Clinical', 'Communities', 'Complex', 'Consensus', 'Data', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Environmental Risk Factor', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genetic Risk', 'Genomics', 'Goals', 'Grouping', 'Image', 'Individual', 'Joints', 'Lead', 'Link', 'Major Depressive Disorder', 'Maps', 'Mental disorders', 'Methods', 'Modeling', 'Mood Disorders', 'Moods', 'Noise', 'Pathway interactions', 'Patients', 'Pattern', 'Play', 'Property', 'Psychotic Disorders', 'Research Personnel', 'Role', 'Sampling', 'Schizoaffective Disorders', 'Schizophrenia', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Source', 'Structure', 'Subgroup', 'Supervision', 'Symptoms', 'Syndrome', 'Time', 'Unipolar Depression', 'Work', 'base', 'bipolar patients', 'blind', 'connectome', 'data anonymization', 'data fusion', 'data repository', 'deep learning', 'disease classification', 'flexibility', 'genomic data', 'genomic locus', 'independent component analysis', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'profiles in patients', 'psychiatric genomics', 'psychotic symptoms', 'statistics', 'tool', 'user friendly software']",NIMH,GEORGIA STATE UNIVERSITY,R01,2021,705172
"ICEES+ Knowledge Provider: Leveraging Open Clinical and Environmental Data to Accelerate and Drive Innovation in Translational Research and Clinical Care. As part of the feasibility phase of the Translator program, we have developed a disease-agnostic framework and approach for openly exposing clinical data that have been integrated at the patient- and visit-level with environmental exposures data: the Integrated Clinical and Environmental Exposures Service (ICEES). We have validated ICEES and demonstrated the service’s ability to replicate and extend published findings on asthma, while also supporting open team science, accelerated translational discovery, and integration with the broader Translator ecosystem. This proposal aims to move ICEES from prototype to development via creation of an ICEES+ Knowledge Provider (KP). Specifically, we aim to address three major challenges that we have identified through research and development (R&D) of the prototype ICEES in an effort to improve the quality, value, and impact of query answers and assertions. Specific Aim 1. Advance the rigor of insights and assertions that ICEES provides. Our prototype ICEES currently provides the ability to dynamically define cohorts and conduct simple statistical associations to examine bivariate relationships between feature variables. Recently, we have identified an approach to extend the bivariate functionalities to support multivariate analysis of the data. For the proposed work, we will apply multivariate analyses, including traditional statistical methods (e.g., regression models) and machine learning methods (e.g., bayesian neural network models, variational autoencoder models), and systematically quantify the extent of data loss and analytic bounds when algorithms are imposed on the ICEES+ KP open application programming interface (API) versus the Institutional Review Board (IRB)– protected, fully identified, pre-binned, underlying integrated feature tables. The overall goal is to provide users with more rigorous insights and estimates of the robustness, validity, accuracy, and specificity of knowledge and assertions generated via the ICEES+ KP OpenAPI. Specific Aim 2. Address issues related to space–time and causality. Clinical and environmental data are inherently spatiotemporal, with observations or events that are contingent on space and time and may be causally related. For the proposed work, we will evaluate and implement technical approaches (e.g., ICEES+ design modifications), spatiotemporal statistical algorithms (e.g., conditional auto-regression), recurrent neural network models, and causal inference models. As part of this effort, we will derive insights from and contribute real-world evidence to support Causal Activity Models and Adverse Outcome Pathways. We also will explore approaches for incorporating into ICEES+ nationwide public data on school exposures—data that will allow us to begin to address patient mobility. Specific Aim 3. Evaluate the security of the ICEES+ KP to ensure that patient privacy is preserved as new capabilities are enabled. ImPACT is an NSF-funded package of tools and services that provides end-to-end infrastructure and support for privacy-assured research and computation on sensitive data. Over the award period, we will implement and evaluate ImPACT security protocols, focusing initially on application of the ImPACT secure multiparty computation (SMC) algorithm as a method to support secure multi-institutional sharing of data on rare diseases and events—a functionality that is not currently supported by ICEES. In addition, we will evaluate other ImPACT security protocols, working under the guidance of a security advisor and in the context of driving use cases and capabilities developed under Specific Aims1 and 2. Importantly, the project aims will be driven by three use cases and associated high-value queries designed to complement and extend our asthma-focused work on the prototype ICEES: (1) an asthma cohort from the Environmental Polymorphism Registry (EPR) at the National Institute for Environmental Health Sciences (NIEHS); (2) a primary ciliary disease cohort (PCD) from the UNC PCD Registry; and (3) a drug-induced liver injury (DILI) cohort from the National DILI Network. These use cases will invoke new diseases, new data types, new organ systems, new institutions, and new queries, thereby stress-testing the ICEES framework and approach and moving it from prototype to development as the ICEES+ KP. n/a",ICEES+ Knowledge Provider: Leveraging Open Clinical and Environmental Data to Accelerate and Drive Innovation in Translational Research and Clinical Care.,10333478,OT2TR003430,"['Address', 'Algorithms', 'Asthma', 'Automobile Driving', 'Award', 'Bayesian neural network', 'Clinical', 'Clinical Data', 'Complement', 'Computational algorithm', 'Data', 'Data Analyses', 'Development', 'Disease', 'Ecosystem', 'Ensure', 'Environmental Exposure', 'Etiology', 'Event', 'Funding', 'Genetic Polymorphism', 'Goals', 'Infrastructure', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Methods', 'Modeling', 'Modification', 'Multivariate Analysis', 'National Institute of Environmental Health Sciences', 'Neural Network Simulation', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Privacy', 'Protocols documentation', 'Provider', 'Publishing', 'Rare Diseases', 'Registries', 'Research', 'Schools', 'Science', 'Secure', 'Security', 'Services', 'Specificity', 'Statistical Algorithm', 'Statistical Methods', 'Stress Tests', 'Time', 'Translational Research', 'Variant', 'Visit', 'Work', 'adverse outcome', 'application programming interface', 'autoencoder', 'body system', 'clinical care', 'cohort', 'data sharing', 'design', 'improved', 'innovation', 'insight', 'liver injury', 'machine learning method', 'patient mobility', 'patient privacy', 'preservation', 'programs', 'prototype', 'recurrent neural network', 'research and development', 'spatiotemporal', 'tool']",NCATS,UNIV OF NORTH CAROLINA CHAPEL HILL,OT2,2021,982187
"The Network for Investigation of Delirium: Unifying Scientists (NIDUS)'s 9th-13th Annual Delirium Boot Camps: A Foundation for Future Exploration Delirium is a serious cognitive disorder associated with Alzheimer’s disease and related dementias (ADRD) that affects ~2.6 million older adults yearly. It is a frequent complication of acute illness, surgery and, now, of COVID-19 infection in older adults. Recognizing the relative dearth of delirium research, the National Institute for Aging (NIA) supported the establishment of the Network for Investigation of Delirium: Unifying Scientists (NIDUS), a collaborative interdisciplinary group of 28 investigators, from 27 institutions, to advance delirium research and develop network infrastructure. This included the creation of an annual “NIDUS bootcamp” conference, to bring together the growing national- and international delirium research community for networking and education. The bootcamp aims are to advance the science of the field and to provide junior investigators with intensive mentorship, through mock NIH application reviews, clinical and research lectures, breakout sessions, and post-bootcamp networking. Bootcamp alumni are provided guidance on: 1) using the NIDUS Delirium Research Hub, Measurement resources and Bibliography, 2) submitting proposals to the NIDUS Pilot Program (13 one-year $50,000 grants awarded), NIA GEMSSTAR/CLINSTAR, the Alzheimer’s Association, and other foundations, 3) attending Mentoring webinars, 4) participating in Junior Faculty Working Groups, and 5) submitting research abstracts to the American Delirium Society (ADS) Annual Meeting. As PIs, 94 alumni have received 46 grants, of which 18 (40%) were NIH-funded, and published 265 original peer- reviewed articles. NIDUS has jumpstarted the careers of many young investigators, particularly bootcamp alumni, enabling them to launch independent programs in delirium research. The goal of this application is to support continuation of a yearly, themed Delirium Bootcamp Conference (DBC), to ensure that the progress of this active research community is sustained. The first-year theme will be the inter-relationship between delirium and ADRD. The Specific Aims are to: (1) Engage and support junior investigators in delirium research through mentorship and access to the NIDUS resources/network (2) Boost the researchers’ funding success (3) Facilitate publication of delirium research and provide ongoing mentorship, and (4) Facilitate networking among junior, mid-career, and senior researchers during and after DBC. As the pool of delirium investigators expands, there is a critical need for a conference focused on addressing cutting-edge research methods in all areas of delirium research, including the relationship with ADRD, “-Omics” research, machine learning and big data, innovations in randomized trials, animal models and mechanistic research, and clinical practice improvement. The DBC will provide an unparalleled opportunity to advance cutting-edge delirium research through interactive didactic sessions and in-depth guidance on complex and nuanced research methods essential for the highest caliber and most impactful delirium research. Delirium is a serious, yet understudied, cognitive disorder that affects millions of elder Americans, and is closely related to Alzheimer’s disease and related dementias (ADRD). The NIA-supported (2015-2020) Network for Investigation of Delirium Unifying Scientists (NIDUS), a collaborative international network of delirium investigators, developed a successful, annual “NIDUS Bootcamp” conference, laying the foundation for this proposal. The new Delirium Bootcamp (DBC) will convene junior investigators and senior faculty in an annual conference with innovations including: 1) a new thematic focus each year, with the 2021 theme highlighting ADRD; 2) focus on high-impact, state-of-the-art methodologies to advance the field in new directions; 3) finding optimal methods to address the unique challenges of delirium research, and 4) developing collaborative interdisciplinary papers to be initiated at the DBC and completed in ongoing groups; thus, fostering the training, career development and success of the next generation of delirium investigators.",The Network for Investigation of Delirium: Unifying Scientists (NIDUS)'s 9th-13th Annual Delirium Boot Camps: A Foundation for Future Exploration,10237513,R13AG072860,"['Acute', 'Acute Disease', 'Address', 'Affect', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'American', 'Animal Model', 'Area', 'Attention', 'Award', 'Bibliography', 'Big Data', 'COVID-19', 'Caliber', 'Clinical', 'Clinical Research', 'Cognition', 'Cognition Disorders', 'Collaborations', 'Communities', 'Community Networks', 'Complex', 'Complication', 'Delirium', 'Discipline', 'Education', 'Elderly', 'Ensure', 'Epidemiology', 'Faculty', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Grant', 'Health Expenditures', 'Infrastructure', 'Institution', 'International', 'Investigation', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurement', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Monoclonal Antibody R24', 'National Institute on Aging', 'Network Infrastructure', 'Operative Surgical Procedures', 'Paper', 'Participant', 'Peer Review', 'Pilot Projects', 'Public Health', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'SARS-CoV-2 infection', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Series', 'Societies', 'Study Section', 'Training', 'United States National Institutes of Health', 'Writing', 'career', 'career development', 'clinical practice', 'cost', 'innovation', 'interdisciplinary collaboration', 'lectures', 'meetings', 'multidisciplinary', 'neglect', 'next generation', 'patient population', 'peer', 'programs', 'randomized trial', 'response', 'secondary analysis', 'senior faculty', 'skill acquisition', 'success', 'support network', 'symposium', 'systematic review', 'webinar', 'working group']",NIA,UNIV OF NORTH CAROLINA CHAPEL HILL,R13,2021,50000
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10216259,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data repository', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2021,886029
"Meta-analysis in human brain mapping This is the competing renewal of the R01 (MH074457-14) which sustains the BrainMap Project (www.brainmap.org). BrainMap is a neuroimaging research resource facilitating cognitive neuroscience and disease-biomarker discovery via coordinate-based meta-analysis (CBMA). BrainMap provides its end-user community with: curated 3-D coordinate data and experimental metadata from peer-reviewed publications; extensively validated computational tools for CBMA; CBMA-derived tools for data interpretation (e.g., functional property and disease loadings by location) and data analysis (e.g., via CBMA-derived disease models); instructional materials and on-site and on-line venues for learning CBMA methods; and, on-going end-user support. At present, BrainMap.org hosts two coordinate-based databases: task-activation (TA DB) and voxel- based morphometry (VBM DB). A voxel-based physiology database (VBP DB) is in the planning and piloting phase. BrainMap maintains an integrated pipeline of cross-platform (Java) tools for data coding (Scribe), filtered retrieval (Sleuth), activation-likelihood estimation (ALE) CBMA (GingerALE), data visualization (Mango), and data interpretation (CBMA-derived Mango plugins). Multiple network-modeling approaches have been successfully applied to BrainMap data – independent components analysis (ICA), author-topic modeling (ATM), graph-theory modeling (GTM), structural equation modeling (SEM), connectivity-based parcellation (CBP), and meta-analytic connectivity modeling (MACM) – but none are yet optimized and “pipelined” for general use. Utilization of BrainMap resources is substantial: our software, data and meta-data have been used in >1,000 peer-reviewed articles. Of these, > 500 were published in the current funding cycle (2015- 2020). Four aims are proposed, to maintain and extend this high-impact research resource.  Aim 1. Voxel-based Physiology DataBase (VBP DB) with Analysis Exemplars. Aim 2. BrainMap Community Portal for Multivariate Modeling with Applications & Exemplars. Aim 3. Large-scale Parameter Estimations. Aim 4. BrainMap Pipeline Enhancements and Community Support. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data.",Meta-analysis in human brain mapping,10157292,R01MH074457,"['3-Dimensional', 'Address', 'Brain Mapping', 'Categories', 'Code', 'Cognition Disorders', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Disease model', 'Educational workshop', 'Equation', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Java', 'Learning', 'Location', 'Machine Learning', 'Mango - dietary', 'Manuals', 'Mental disorders', 'Meta-Analysis', 'Metabolic', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Multivariate Analysis', 'Output', 'Peer Review', 'Phase', 'Physiology', 'Property', 'Publications', 'Publishing', 'Research Domain Criteria', 'Resources', 'Retrieval', 'Site', 'Software Framework', 'Structure', 'Surface', 'Symptoms', 'Taxonomy', 'Texas', 'Training', 'Uncertainty', 'Validation', 'base', 'biomarker discovery', 'case control', 'cognitive neuroscience', 'cohort', 'computerized tools', 'connectome', 'data submission', 'data tools', 'data visualization', 'design', 'experimental study', 'graph theory', 'hemodynamics', 'independent component analysis', 'learning materials', 'lectures', 'morphometry', 'network models', 'neuroimaging', 'simulation', 'webinar']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R01,2021,637306
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,10160982,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2021,575125
"PheMAP: Measured, Automated Profile to Facilitate High Throughput Phenotyping Electronic health records (EHRs) are a powerful and efficient tool for biological discovery globally. However, a vital step for EHR-based research is valid, accurate, and reliable phenotyping (i.e., correctly identifying individuals with a particular trait of interest). Conventional approaches to phenotyping are ad hoc, domain expert dependent, rule-based, and usually specific to EHR environments. However, each requires an extensive investment of time and resources to develop due to the heterogeneity, complexity, inaccuracy, and frequent fragmentation of EHRs. The lack of general, automatic, and portable approaches to enable accurate high- throughput phenotyping is a critical barrier that hampers our ability to leverage valuable clinical data in EHRs for better healthcare. We propose a new generalizable high-throughput approach: Phenotyping by Measured, Automated Profile (PheMAP) that we have developed from public resources and will further refine and implement across various EHRs. We recognize that mass information about phenotypes is often described in significant detail and continuedly accumulated within publicly available resources (e.g., MedlinePlus and Wikipedia). We hypothesize this information can be retrieved, filtered, organized, measured, and formalized into standard EHR phenotype profiles. Indeed, we have used such an ensemble approach to integrate four generalizable online medication resources (e.g., SIDER and RxNorm) to create MEDI--a resource linking 2,136 medications and 13,304 indications. In preliminary studies, we extended this strategy to phenotyping and created a prototype PheMAP. For each phenotype, we identified relevant clinical concepts and weighted each based on its importance to the phenotype. We then mapped all associated concepts to commonly-used clinical terminologies. Our preliminary studies showed an average consistency of 98.6%±0.8% between our early-stage PheMAP and three validated eMERGE algorithms (Type 2 Diabetes, dementia, and hypothyroidism). We seek support to refine and optimize PheMAP and develop tools to allow researchers to implement PheMAP efficiently in different EHRs. This will allow researchers to rapidly and accurately determine the status of thousands of phenotypes for millions of individuals with minimal human intervention. Since PheMAP is created using independent resources that are more generalizable than a local clinical dataset, the implementation will generate more consistent outcomes in different EHRs for large-scale analyses.The work we propose is a necessary step toward being able to conduct high-throughput genome-wide and phenome-wide association analyses (GWASs and PheWASs). We will use data from multiple biobanks to accomplish these tasks. Specifically, we will achieve the following goals in this grant: 1.refine PheMAP and conduct large-scale validation, 2. implement PheMAP and perform representative GWASs and PheWASs, 3. Use PheMAP to conduct GWASs for unstudied or understudied diseases and phenotypes, and 4. Share PheMAP to facilitate research using EHRs. Electronic health records (EHRs) are a powerful and efficient tool for biological discovery globally while a vital step for EHR-based research is valid, accurate, and reliable phenotyping. Conventional approaches to phenotyping are ad hoc, domain expert dependent, rule-based, and usually specific to EHR environments. We propose to refine, validate, and share a new generalizable high-throughput approach: Phenotyping by Measured, Automated Profile (PheMAP) that allows researchers to rapidly and accurately determine the status of thousands of phenotypes for millions of individuals with minimal human intervention.","PheMAP: Measured, Automated Profile to Facilitate High Throughput Phenotyping",10095131,R01GM139891,"['Algorithms', 'Benchmarking', 'Biological', 'Catalogs', 'Clinical', 'Clinical Data', 'Data', 'Data Set', 'Dementia', 'Disease', 'Effectiveness', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Environment', 'Evaluation', 'Genes', 'Genotype', 'Goals', 'Grant', 'Healthcare', 'Heritability', 'Heterogeneity', 'Human', 'Hypothyroidism', 'Individual', 'Institution', 'Intervention', 'Investments', 'Knowledge', 'Left', 'Link', 'Machine Learning', 'Maps', 'Measures', 'Medical center', 'MedlinePlus', 'Modeling', 'Non-Insulin-Dependent Diabetes Mellitus', 'Ontology', 'Outcome', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Physicians', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Sensorineural Hearing Loss', 'Signal Transduction', 'Site', 'Statistical Models', 'Terminology', 'Time', 'Validation', 'Work', 'base', 'biobank', 'biomedical ontology', 'clinically relevant', 'cost', 'data modeling', 'disease phenotype', 'experience', 'genome wide association study', 'genome-wide', 'implementation tool', 'interest', 'novel', 'off-label drug', 'off-label use', 'phenome', 'phenotyping algorithm', 'portability', 'prototype', 'tool', 'trait']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2021,432500
"Integrating genomic and clinical data to predict disease phenotypes using heterogeneous ensembles PROJECT SUMMARY Genomic and other “omic” profiles hold immense potential for advancing personalize/precision medicine by enabling the accurate prediction of disease phenotypes or outcomes for individual patients, which can be used by a clinician to design an appropriate plan of care. However, despite this potential, the actual impact of these omic profiles on disease phenotype prediction may be limited by the fact that even large cohorts collecting these data do not cover large enough numbers of individuals. In contrast, a variety of clinical data types, such as laboratory tests and physician notes, are routinely collected and studied for a much larger number of patients undergoing treatment for such diseases at medical centers. The abundance of these clinical data, and their complementarity with multi-omic data, offer an opportunity to advance personalized medicine by integrating these disparate types of data. However, this disparity in data formats, namely several omic profiles being structured, and several clinical data types, such as physician notes, being unstructured, poses challenges for this integration. An associated challenge due to this disparity is that different classes of computational methods are likely to be the most effective for predicting disease phenotypes from these clinical and omics datasets. These challenges pose barriers for current data integration methods to address this problem. Here, we propose an innovative approach to this integration by assimilating diverse base phenotype predictors inferred from individual clinical and omics datasets into heterogeneous ensembles. These ensembles, which have shown promise for several other computational genomics problems, can aggregate an unrestricted number and variety of base predictors, which is ideal for this integration problem. Specifically, we describe how existing heterogeneous ensemble methods for single datasets can be transformed and advanced to address the multiple clinical and omic dataset integration problem. In particular, we detail novel algorithms for improving these integrative ensembles by modeling and incorporating the inherent patient and dataset heterogeneity in these datasets. We also propose novel algorithms for leveraging the inherent complementarity among clinical and omic datasets, as well as an innovative approach for handling expected missing data, both with the goal of making ensemble phenotype predictors more accurate and applicable to patient cohorts. To assess the performance of this novel suite of data integration-oriented heterogeneous ensembles, we will validate their effectiveness for predicting asthma and Inflammatory Bowel Disease phenotypes in substantial patient cohorts with diverse omics and clinical datasets. We will publicly release efficient software implementations of the methods developed in this project to enable others to carry out similar analyses with other diverse data collections. Successful accomplishment of the proposed work will contribute to the advancement of personalized medicine through accurate individualized prediction of disease phenotypes. Predictive modeling is expected to become a cornerstone on the path to achieving precision/personalized medicine, as one of the key tasks here will be making individualized predictions of disease characteristics/phenotypes like subtype and risk of progression and/or recurrence. We propose several innovative computational algorithms for developing accurate predictive models that integrate diverse clinical and omic data, as well as several rigorous validation exercises that will demonstrate the capabilities of these models. Successful accomplishment of the proposed work will contribute to the advancement of personalized/precision medicine through more accurate individualized prediction of disease characteristics.",Integrating genomic and clinical data to predict disease phenotypes using heterogeneous ensembles,10218766,R01HG011407,"['Address', 'Algorithms', 'Asthma', 'Automobile Driving', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Disease', 'Disease Outcome', 'Docking', 'Effectiveness', 'Electronic Health Record', 'Encapsulated', 'Exercise', 'Genomics', 'Goals', 'Health', 'Individual', 'Inflammatory Bowel Diseases', 'Institution', 'Laboratories', 'Learning', 'Malignant Neoplasms', 'Medical', 'Medical Imaging', 'Medical center', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Molecular Profiling', 'Multiomic Data', 'Patients', 'Performance', 'Phenotype', 'Physicians', 'Population', 'Recurrence', 'Research Personnel', 'Risk', 'Sampling', 'Structure', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Validation', 'Variant', 'Work', 'advanced disease', 'base', 'clinical phenotype', 'cohort', 'data format', 'data integration', 'deep learning', 'design', 'disease phenotype', 'diverse data', 'feature selection', 'flexibility', 'genomic data', 'heterogenous data', 'improved', 'individual patient', 'innovation', 'insight', 'member', 'multiple datasets', 'multiple omics', 'multitask', 'novel', 'novel strategies', 'outreach', 'patient population', 'personalized medicine', 'personalized predictions', 'precision medicine', 'predictive modeling', 'programs', 'rapid growth', 'repository', 'scale up', 'transcriptomics', 'vector']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2021,539951
"Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences Abstract A major approach in causal inference literature aimed at mitigating bias due to unmeasured confounding is the so- called instrumental variable (IV) design which relies on identifying a variable which (i) influences the treatment process, (ii) has no direct effect on the outcome other than through the treatment, and (iii) is independent of any unmeasured confounder. IV methods are very well developed and widely used in social and health science, although validity of IV inferences may not be reliable if any of required assumptions (i)-(iii) is violated. This proposal aims to develop (a) new IV methods robust to violation of any of (i)-(iii); (b) New negative control methods that can be used to detect and sometimes to nonparametrically account for unmeasured confounding bias; (c) New bracketing methods for partial inference about causal effects in comparative interrupted time series studies. The proposed methods will be used to address current scientific queries in three major substantive public health areas:(1) to understand the health effects of air pollution; (2) to quantify the causal effects of modifiable risk factors for Alzheimer's disease and related disorders; (3) To uncover the mechanism by which a randomized package of interventions produced a substantial reduction of HIV incidence in a recent major cluster randomized trial of treatment as prevention in Botswana, Africa. Our proposal will provide the best available analytical methods to date to resolve confounding concerns in these high impact public health applications and more broadly in observational studies in the health sciences. Summary This proposal aims to develop new causal inference methods to tame bias due to hidden confounding factors in obser- vational studies as well as in randomized experiments subject to non-adherence. The proposed methods are firmly grounded in modern semiparametric theory which will be used to obtain more robust and efficient inferences about causal effects in a broad range of public health applications including in Epidemiology of Aging, Environmental Health Epidemiology and HIV/AIDS Prevention.",Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences,10159821,R01AG065276,"['AIDS prevention', 'Address', 'Adherence', 'Africa', 'Aging', 'Air Pollution', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Area', 'Blood Pressure', 'Body mass index', 'Botswana', 'Cluster randomized trial', 'Data', 'Diabetes Mellitus', 'Disease', 'Environmental Health', 'Epidemiology', 'Genetic', 'HIV', 'Health', 'Health Sciences', 'Incidence', 'Interruption', 'Intervention', 'Learning', 'Linkage Disequilibrium', 'Literature', 'Machine Learning', 'Masks', 'Mendelian randomization', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Observational Study', 'Outcome', 'Participant', 'Process', 'Public Health', 'Public Health Applications Research', 'Randomized', 'Research Design', 'Research Personnel', 'Risk Factors', 'Series', 'Social Sciences', 'Testing', 'Thromboplastin', 'Time', 'ambient air pollution', 'analytical method', 'c new', 'comparative', 'design', 'experimental study', 'genetic variant', 'high dimensionality', 'intervention effect', 'modifiable risk', 'mortality', 'novel', 'pleiotropism', 'semiparametric', 'simulation', 'theories', 'treatment as prevention', 'treatment effect', 'uptake', 'user friendly software']",NIA,UNIVERSITY OF PENNSYLVANIA,R01,2021,468961
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,10158446,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Models', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2021,7065330
"The plasticity of well-being:  A research network to define, measure and promote human flourishing PROJECT SUMMARY/ABSTRACT This U24 application is written in response to RFA-AT-20-003 to establish a high-priority research network on emotional well-being (EWB). While psychological research on well-being has dramatically increased over the past 15 years, virtually all of this work has been descriptive and has not emphasized the “how” of well-being: How might well-being be cultivated? In addition, virtually all of the extant work on the correlates of individual differences in well-being has used responses on retrospective questionnaires as the primary tool to assess well-being. While there have been exciting findings, particularly relating individual differences in well-being to various indices of physical health, many questions remain and methodological limitations plague the validity of this work. This U24 network will assemble a highly multi-disciplinary group of 10 investigators across 3 (or more in the future) institutions to significantly advance our understanding of the “how” of EWB, identify the core plastic constituents of EWB, specify and/or develop robust measures of these constituents at biological, behavioral and experiential levels of analysis and characterize the plasticity of these constituents. The measurement strategy will ultimately focus on the development of technology-based passive measures of EWB that require no explicit user input and are highly scalable. The network will also focus its efforts on the development and evaluation of programs to train EWB and will assess whether such programs might serve as prevention strategies. The network will consist of scientists and scholars from a broad range of fields including psychology, neuroscience, electrical and computer engineering, population health and biology, computer science and the humanities. These scientists and scholars will focus on the following major aims: Aim 1: To arrive at a core consensus of the minimal set of constituents that can be described and measured at biological, behavioral and experiential levels that constitute the plastic elements of EWB and to specify already existing measures and /or develop novel measures of each of these constructs at each level of analysis. Aim 2: Using the active measures described in Aim 1, to develop passive measures using digital technologies of at least two of the core constituents of well-being. Aim 3: To develop pilot projects specifically focusing on prevention strategies for learning well-being in various samples. The network will train new investigators and bring established investigators into this new field, disseminate a framework for understanding the plasticity of well- being, a toolbox of measures for assessing the plasticity of components of well-being, and several pilot datasets that showcase the novel passive and field-friendly biological measures. In these ways, the network will dramatically accelerate progress in the nascent field of EWB. PROJECT NARRATIVE This U24 network on emotional well-being (EWB) will catalyze the emerging field of the plasticity of well-being and will showcase how well-being can be learned and the consequences of such skill development on physical and emotional health and on prevention of disease. A framework for understanding how well-being can be learned along with measures of the core components of well-being that can be learned will be developed and disseminated. The network will also train new investigators in this area and will engage established investigators to contribute to this field.","The plasticity of well-being:  A research network to define, measure and promote human flourishing",10151850,U24AT011289,"['Area', 'Attention', 'Awareness', 'Behavior', 'Behavioral', 'Biological', 'Cellular Phone', 'Communities', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Distal', 'Drug abuse', 'Elements', 'Emotional', 'Engineering', 'Face', 'Future', 'Gold', 'Grant', 'Health', 'Human', 'Humanities', 'Individual Differences', 'Institution', 'Interruption', 'Literature', 'Measurement', 'Measures', 'Mental Depression', 'Methodology', 'Mind', 'Modernization', 'Neurosciences', 'Outcome', 'Patient Self-Report', 'Personal Satisfaction', 'Pilot Projects', 'Plague', 'Population Biology', 'Prevention strategy', 'Program Evaluation', 'Psychology', 'Publications', 'Questionnaires', 'Randomized Controlled Trials', 'Regulation', 'Research', 'Research Personnel', 'Research Priority', 'Risk', 'Sampling', 'Scientist', 'Signal Transduction', 'Specific qualifier value', 'Subgroup', 'Techniques', 'Technology', 'To specify', 'Training', 'Well in self', 'Work', 'base', 'computer science', 'cost', 'digital', 'disorder prevention', 'indexing', 'learning strategy', 'mHealth', 'machine learning algorithm', 'meetings', 'member', 'mindfulness meditation', 'multidisciplinary', 'novel', 'physical conditioning', 'population health', 'prevent', 'programs', 'psychologic', 'response', 'skill acquisition', 'social', 'standard measure', 'technology development', 'tool', 'virtual', 'web site']",NCCIH,UNIVERSITY OF WISCONSIN-MADISON,U24,2021,30000
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,10168611,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2021,315727
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,10168488,R37DA009757,"['Address', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'substance use treatment', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2021,360584
"Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents PROJECT ABSTRACT  Antimicrobial resistance is a critical public health issue. Infections with drug resistant pathogens are estimated to cause an additional eight million hospitalization days annually over the hospitalizations that would be seen for infections with susceptible agents. The use of antibiotics (in both clinical and agricultural settings) is being viewed as precursor for these infections and thus, is a major public health concern—particularly as outbreaks become more frequent and severe. However, scientiﬁc evidence describing the hazards associated with antibiotic use is lacking due to inability to quantify the risk of these practices. One promising avenue to elucidate this risk is to use shotgun metagenomics to identify the AMR genes in samples taken through systematic spatiotemporal surveillance. The goal of this proposed work is to develop algorithms that will provide such a means for analysis. The algorithms need to be scalable to very large datasets and thus, will require the development and use succinct data structures.  In order to achieve this goal, the investigative team will develop the theoretical foundations and applied meth- ods needed to study AMR through the use of shotgun metagenomics. A major focus of the proposed work is developing algorithms that can handle very large datasets. To achieve this scalability, we will create novel means to create, compress, reconstruct and update very large de Bruijn graphs that metagenomics data in a manner needed to study AMR. In addition, we will pioneer the study of AMR through long read data by proposing new algorithmic problems and solutions that use data. For example, identifying the location of speciﬁc genes in a metagenomics sample using long read data has not been proposed or studied. Thus, the algorithmic ideas and techniques developed in this project will not only advance the study of AMR, but contribute to the growing domain of big data analysis and pan-genomics.  Lastly, we plan to apply our methods to samples collected from both agricultural and clinical settings in Florida. Analysis of preliminary and new data will allow us to conclude about (1) the public risk associated with antimicro- bial use in agriculture; (2) the effectiveness of interventions used to reduce resistant bacteria, and lastly, (3) the factors that allow resistant bacteria to grow, thrive and evolve. A–1 PROJECT NARRATIVE  Antibiotic use in agriculture is a major public health concern that is receiving a lot of media attention, par- ticularly as antibiotic-resistant infections in become more frequent and severe. This research will build a novel bioinformatics framework for determining how antimicrobial resistant genes evolve, grow, and persist in a system that has been affected by antibiotic use. This will, in turn, facilitate the development of effective intervention methods that reduce resistant pathogens in clinical and agricultural settings. N–1",Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents,10053321,R01AI141810,"['Affect', 'Agriculture', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Attention', 'Bacteria', 'Base Pairing', 'Big Data', 'Bioinformatics', 'Clinical', 'Collaborations', 'Combating Antibiotic Resistant Bacteria', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Compression', 'Data Set', 'Development', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Florida', 'Food production', 'Foundations', 'Genes', 'Genomics', 'Goals', 'Graph', 'Hospitalization', 'Infection', 'International', 'Investigation', 'Length', 'Location', 'Measures', 'Memory', 'Metagenomics', 'Methods', 'Monitor', 'Noise', 'Organism', 'Pathogenicity', 'Plasmids', 'Prevention', 'Public Health', 'Research', 'Resistance', 'Risk', 'Sampling', 'Shotguns', 'Structure', 'Surveillance Methods', 'System', 'Techniques', 'Time', 'Translating', 'Update', 'Work', 'antibiotic resistant infections', 'bacterial resistance', 'base', 'combinatorial', 'drug resistant pathogen', 'effective intervention', 'foodborne outbreak', 'genetic variant', 'hazard', 'improved', 'large datasets', 'machine learning algorithm', 'method development', 'microbial', 'microbiome analysis', 'microbiome research', 'multiple datasets', 'novel', 'pathogen', 'petabyte', 'reconstruction', 'research and development', 'resistance gene', 'spatiotemporal', 'standard care']",NIAID,UNIVERSITY OF FLORIDA,R01,2021,422334
"Reproducible and FAIR Bioinformatics Analysis of Omics Data PROJECT SUMMARY/ABSTRACT Modern biomedical research is increasingly quantitative. The next generation of researchers will need an entirely new set of quantitative skills to fully take advantage of the data they create. In response to this need, the goal of the current R25 proposal is to transform an existing, 5-day bioinformatics techniques course into a new, two-week short course, Reproducible and FAIR Bioinformatics Analysis of Omics Data, to provide a unique educational opportunity for biomedical research scientists-in-training to begin to develop core competencies in bioinformatics and biostatistical analyses of large datasets. The new course will also address NIH priorities including rigor and reproducibility and Findable, Accessible, Interoperable and Reusable (FAIR) data principles. During the last five years, the Dartmouth faculty who are serving as Principal Investigators have taught a 5-day course at the MDI Biological Laboratory on bioinformatics and biostatistics to ~40 trainees/year (202 total). Based on overwhelmingly positive feedback, the current project will extend this 5-day course into a longer, two-week short course at the MDI Biological Laboratory that will feature a low student-to- instructor ratio (5:1), more hands-on experiential learning, and exceptional faculty who are highly experienced in teaching and performing big-data analyses. The new course is designed to accommodate ~35 trainees per year (175 total over the five-year R25 project). It will incorporate modules on biostatistics, scientific rigor and reproducibility, and FAIR data principles. The course design will also involve a short conceptual presentation, followed by an exercise in which students will gain confidence by applying a new skill. Each active-learning session will have three levels of difficulty (beginner, intermediate, and advanced) to allow each student to progress at their own pace. The low student-to-faculty ratio will allow course facilitators to guide participants through realistic challenges without causing frustration. The specific aims of the proposed course include: Specific Aim 1. Develop a two-week short course primarily for postdoctoral fellows and graduate students that improves their ability to design and analyze omics experiments such as RNA-seq, 16S (microbiome), metagenomics, and sc-RNA-seq data; Specific Aim 2. Enhance the impact of research by biomedical scientists by teaching them the Responsible Conduct of Research, the secure and ethical use of data, as well as rigor and reproducibility and FAIR data principles; Specific Aim 3. Disseminate the training curriculum to a broad audience; and Specific Aim 4. Evaluate the short- and long-term impacts of the course on students, including a long-term follow-up to determine students’ confidence in and actual integration of bioinformatics, biostatistics, and FAIR data principles into their research, and the reported impact of this course on their career trajectory and competitiveness in the job market. In summary, the proposed course will provide a unique cross- training, educational opportunity for biomedical research scientists-in-training to begin to develop core competency in bioinformatics and biostatistical analyses of large data sets. PROJECT NARRATIVE The goal of our two-week short course, Reproducible and FAIR Bioinformatics Analysis of Omics Data, is to provide a unique cross-training, experiential, educational opportunity for biomedical research scientists-in- training to begin to develop core competency in bioinformatics and biostatistical analyses of large data sets.",Reproducible and FAIR Bioinformatics Analysis of Omics Data,10087570,R25HG011447,"['Active Learning', 'Address', 'Advisory Committees', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Cancer Education Grant Program', 'Competence', 'Data', 'Data Analyses', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Environment', 'Ethics', 'Euclidean Space', 'Evaluation', 'Exercise', 'Experimental Designs', 'FAIR principles', 'Faculty', 'Feedback', 'Frustration', 'Goals', 'Guidelines', 'High Performance Computing', 'Laboratories', 'Learning', 'Linear Models', 'Long-Term Effects', 'Longterm Follow-up', 'Machine Learning', 'Measurement', 'Measures', 'Metagenomics', 'Modernization', 'Occupations', 'Outcome', 'Participant', 'Postdoctoral Fellow', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Secure', 'Students', 'Surveys', 'Techniques', 'Testing', 'Training', 'United States National Institutes of Health', 'Visualization', 'base', 'biomedical scientist', 'career', 'data reuse', 'design', 'expectation', 'experience', 'experimental study', 'follow-up', 'graduate student', 'improved', 'instructor', 'interest', 'job market', 'large datasets', 'microbiome', 'next generation', 'open source', 'programs', 'response', 'responsible research conduct', 'skills', 'transcriptome sequencing']",NHGRI,MOUNT DESERT ISLAND BIOLOGICAL LAB,R25,2021,79877
"Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation Project Summary/Abstract: With the surge of large genomics data, there is an immense increase in the breadth and depth of different omics datasets and an increasing importance in the topic of privacy of individuals in genomic data science. Detailed genetic and environmental characterization of diseases and conditions relies on the large-scale mining of functional genomics data; hence, there is great desire to share data as broadly as possible. However, there is a scarcity of privacy studies focused on such data. A key first step in reducing private information leakage is to measure the amount of information leakage in functional genomics data, particularly in different data file types. To this end, we propose to to derive information-theoretic measures for private information leakage in different data types from functional genomics data. We will also develop various file formats to reduce this leakage during sharing. We will approach the privacy analysis under three aims. First, we will develop statistical metrics that can be used to quantify the sensitive information leakage from raw reads. We will systematically analyze how linking attacks can be instantiated using various genotyping methods such as single nucleotide variant and structural variant calling from raw reads, signal profiles, Hi-C interaction matrices, and gene expression matrices. Second, we will study different algorithms to implement privacy-preserving transformations to the functional genomics data in various forms. Particularly, we will create privacy-preserving file formats for raw sequence alignment maps, signal track files, three-dimensional interaction matrices, and gene expression quantification matrices that contain information from multiple individuals. This will allow us to study the sources of sensitive information leakages other than raw reads, for example signal profiles, splicing and isoform transcription, and abnormal three-dimensional genomic interactions. Third, we will investigate the reads that can be mapped to the microbiome in the raw human functional genomics datasets. We will use inferred microbial information to characterize private information about individuals, and then combine the microbial information with the information from human mapped reads to increase the re-identification accuracy in the linking attacks described in the second aim. We will use the tools to quantify the sensitive information and privacy-preserving file formats in the available datasets from large sequencing projects, such as the ENCODE, The Cancer Genome Atlas, 1,000 Genomes, gEUVADIS, and Genotype-Tissue Expression projects. Project Narrative: Sharing large-scale functional genomics data is critical for scientific discovery, but comes with important privacy concerns related to the possible misuse of such data. This proposal will quantify and manage the rieslkasted to releasing functional genomics datasets, based on integrating inferred genotypes from the raw sequence files, signal tracks, and microbiome mapped sequences. Finally, we will develop file formats, statistical methodologies, and related software for anonymization of functional genomics data that enable open sharing.",Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation,10251876,R01HG010749,"['3-Dimensional', 'Address', 'Algorithms', 'Assessment tool', 'Biology', 'ChIP-seq', 'Code', 'Computer software', 'Consent', 'DNA sequencing', 'Data', 'Data Files', 'Data Science', 'Data Set', 'Databases', 'Diet', 'Disease', 'Environment', 'Equilibrium', 'Extravasation', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Genotype-Tissue Expression Project', 'Glean', 'Hi-C', 'Human', 'Individual', 'Institutes', 'Laws', 'Learning', 'Letters', 'Life Style', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Mining', 'Motivation', 'Participant', 'Patients', 'Phenotype', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Protein Isoforms', 'Protocols documentation', 'Provider', 'Pythons', 'Quantitative Trait Loci', 'RNA Splicing', 'Research Personnel', 'Risk', 'Risk Assessment', 'Sampling', 'Sequence Alignment', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Smoker', 'Source', 'Structure', 'Techniques', 'The Cancer Genome Atlas', 'Tissues', 'Variant', 'base', 'clinically relevant', 'computerized data processing', 'data mining', 'data sharing', 'experimental study', 'file format', 'functional genomics', 'genome sequencing', 'genomic data', 'human tissue', 'interest', 'large datasets', 'microbial', 'microbiome', 'open data', 'privacy preservation', 'social', 'tool', 'transcriptome sequencing']",NHGRI,YALE UNIVERSITY,R01,2021,526482
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",10083209,U24DK112331,"['ATAC-seq', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Cost efficiency', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Infrastructure', 'Institutes', 'Knowledge', 'Lead', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'analysis pipeline', 'base', 'bisulfite sequencing', 'data exchange', 'data resource', 'epigenomics', 'exercise intervention', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'individual response', 'insight', 'machine learning algorithm', 'medical schools', 'methylome', 'mortality risk', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2021,3913893
"High Throughput Screen and High Information Follow-Up Tests for Genotoxicants Project Summary  Current batteries of genetic toxicology assays exhibit several critical deficiencies. First, the throughput capacity of in vitro genotoxicity tests is low, and does not meet current needs, especially for early, high volume screening environments that need to prioritize chemicals for further testing and/or development. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no information provided about genotoxic mode of action. This is severely limiting, as it does not generate key information necessary for prioritizing chemicals for further testing, guiding subsequent assays’ endpoints/experimental designs, or conducting risk assessments. Finally, most current assays do not place requisite emphasis on dose response relationships, and therefore do not contextualize the results in terms of potency. These deficiencies prevent genotoxicity data from optimally contributing to modern risk assessments, where all of these capabilities and high information content are essential. We will solve these issues by developing, optimizing, and validating a two-tiered testing strategy based on multiplexed DNA damage responsive biomarkers and high-speed flow cytometric analysis. The first-tier focuses on throughput and is used to prioritize likely genotoxicants for more comprehensive analysis in second tier testing. Specifically, it involves a collection of several multiplexed biomarkers that will be used to identify likely genotoxic agents and provide a preliminary assessment of genotoxic mode of action. The gH2AX biomarker detects DNA double strand breaks, phospho-histone H3 identifies mitotic cells, nuclear p53 content reports on p53 activation in response to DNA damage, the frequency of 8n+ cells measure polyploidization, and the ratio of nuclei to microsphere counts provides information about treatment-related cytotoxicity. The second tier focuses on information content and considers many more concentrations as well as additional biomarkers, including micronucleus formation. Collectively, the tier two results provide definitive predictions about test chemicals’ genotoxic potential, mode of action, and potency. Over the course of this project we will study more than 3,000 diverse chemicals in order to understand the performance characteristics and generalizability of the two-tiered testing strategy. An interlaboratory trial will be conducted with prototype assay kits to assess the transferability of the methods, with the ultimate goal of providing the Nation with commercially available kits and testing services. Project Narrative Some chemicals in commercial use and in the environment can cause DNA damage and this damage can contribute to the development of cancer and other severe diseases. We will develop, optimize, and validate an improved testing strategy based on highly automated processes tracking several DNA damage biomarkers that can be analyzed without the need for animal testing. These methods will be configured into commercially available kits and testing services.",High Throughput Screen and High Information Follow-Up Tests for Genotoxicants,10255405,R44ES033138,"['Address', 'Animal Testing', 'Biological Assay', 'Biological Markers', 'Buffers', 'Canada', 'Cell Line', 'Cell Nucleus', 'Cells', 'Characteristics', 'Chemicals', 'Code', 'Collection', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Elements', 'End Point Assay', 'Environment', 'Exhibits', 'Experimental Designs', 'Flow Cytometry', 'Formulation', 'Frequencies', 'Goals', 'Health', 'Histone H3', 'Human', 'In Vitro', 'Industry', 'Logistics', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metabolic Activation', 'Methods', 'Microspheres', 'Miniaturization', 'Mitotic', 'Modeling', 'Modernization', 'Mutagenicity Tests', 'Mutagens', 'National Toxicology Program', 'Nuclear', 'Performance', 'Phase', 'Process', 'Protease Inhibitor', 'Reagent', 'Recommendation', 'Reporting', 'Risk Assessment', 'Sampling', 'Scheme', 'Sensitivity and Specificity', 'Speed', 'Statistical Data Interpretation', 'System', 'TP53 gene', 'Techniques', 'Temperature', 'Testing', 'Time', 'Toxicogenetics', 'Toxicology', 'Training', 'Validation', 'Work', 'base', 'blind', 'cell type', 'climate change', 'computerized tools', 'cytotoxicity', 'design', 'experimental study', 'follow-up', 'genotoxicity', 'high throughput screening', 'improved', 'innovation', 'instrumentation', 'micronucleus', 'phosphatase inhibitor', 'prevent', 'programs', 'prototype', 'response', 'response biomarker', 'screening', 'testing services']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2021,204743
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,10085664,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'detection limit', 'disease phenotype', 'driver mutation', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'machine learning method', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistical learning', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2021,305167
"Development of Technologies for Efficient In Vivo Prime Editing PROJECT SUMMARY  Genome editing is revolutionizing biomedicine and biotechnology by enabling the precise modification of genomic DNA in living cells. While various genome-editing tools have been developed over the past decade, the CRISPR-Cas9 system has emerged as a particularly versatile and efficient technology for editing DNA. Nonetheless, limitations derived from its reliance on DNA double strand breaks (DSB), which can lead to unpredictable editing outcomes and even chromosomal translocations, could limit its applications.  Base editors (BEs) and prime editors (PEs) are two novel classes of genome-editing tools capable of introducing precise single-base conversion in DNA without the requirement of a DSB. PEs, in particular, provide greater flexibility than BEs, owing to their ability to introduce any type of base conversion and even programable small insertions and deletions. This expanded set of capabilities compared to other technologies makes PEs a particularly promising platform for applications in biomedicine; however, the large size of PEs precludes their in vivo delivery by AAV, a promising and effective gene delivery vehicle that is currently under evaluation in multiple clinical trials.  To overcome these obstacles, we have created a split-PE platform that is compatible with AAV delivery and have demonstrated the functionality of this approach in cultured cells. Despite this progress, there still remain several critical challenges, which we here propose to overcome in order to optimize this technology for effective and specific in vivo prime editing.  To accomplish this objective, we have assembled a multidisciplinary team with collective expertise in genome editing (Dr. Perez-Pinera), AAV gene delivery (Dr. Gaj) and computational biology (Dr. Song). Our collaborative efforts will yield an integrated and comprehensive PE toolset that will blend strategies for target identification and editing optimization, with methods for reducing off-target effects and immune responses, thus priming this technology for future in vivo applications.  Given that the flexibility of PEs has significantly expanded the number of actionable target sites that can be genetically modified, we anticipate that the integrated technologies we develop will have large, direct and long- lasting impact in biomedicine by enabling not only novel gene therapies, but also basic research. In particular, our technology will provide investigators with biological tools that are uniquely capable of introducing mutations within post-mitotic cells in vivo, which could be used to dissect functional elements or even determine the role of pathogenic mutations in a cell- and tissue-specific manner. The technologies created by this application will thus broadly impact biotechnology and biomedicine. NARRATIVE Genome editing technologies provide novel opportunities to advance basic research and treat human diseases but approaches that use active nucleases to edit DNA can induce undesired and deleterious effects in the genome, which can reduce their therapeutic potential. Prime editors are a recently emerged gene-editing variant capable of introducing precise modifications to DNA, with minimal damage to the genome. This proposal is focused on creating an integrative prime editing toolkit that will provide investigators with technologies that will enable their use in vivo, improve their specificity and streamline their design and implementation, thereby advancing the applications of this methodology in biotechnology and biomedicine.",Development of Technologies for Efficient In Vivo Prime Editing,10184207,R01GM141296,"['Amyotrophic Lateral Sclerosis', 'Basic Science', 'Biological', 'Biotechnology', 'CRISPR/Cas technology', 'Carrying Capacities', 'Cells', 'Chromosomal translocation', 'Clinical Research', 'Clinical Trials', 'Computational Biology', 'Cultured Cells', 'DNA', 'DNA Double Strand Break', 'DNA Sequence', 'Data', 'Development', 'Elements', 'Engineering', 'Evaluation', 'Experimental Designs', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Gene Delivery', 'Genes', 'Genome', 'Genomic DNA', 'Genomic approach', 'Immune response', 'Knowledge', 'Lead', 'Machine Learning', 'Mediating', 'Methodology', 'Methods', 'Mitotic', 'Modification', 'Mutation', 'Nonsense Mutation', 'Open Reading Frames', 'Outcome', 'Pathogenicity', 'Peptide Signal Sequences', 'Poly A', 'Positioning Attribute', 'Regulatory Element', 'Research', 'Research Personnel', 'Role', 'Site', 'Site-Directed Mutagenesis', 'Specificity', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Trans-Splicing', 'Variant', 'adeno-associated viral vector', 'base', 'biophysical techniques', 'cell type', 'computational suite', 'design', 'experience', 'experimental study', 'flexibility', 'gene therapy', 'genome editing', 'human disease', 'immunogenic', 'improved', 'in vivo', 'innovation', 'insertion/deletion mutation', 'intein', 'miniaturize', 'multidisciplinary', 'novel', 'nuclease', 'particle', 'predictive modeling', 'prevent', 'programs', 'promoter', 'reconstitution', 'response', 'success', 'technology development', 'technology research and development', 'tool']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R01,2021,535871
"lntegration and Visualization of Diverse Biological Data PROJECT SUMMARY The onset of most human disease involves numerous molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway, and treatment contexts. This system is probed by thousands of functional genomics and quantitative genetic studies, and integrative analysis of these data can generate testable hypotheses identifying causal genetic variants and linking them to network level changes in cells to disease phenotypes. This can enable deeper molecular-level understanding of pathophysiology, paving the way to genome-based precision medicine.  The long term goal of this project is to enable such discoveries through integrative analysis of high- throughput biological data in a disease context. In the previous funding periods, we developed accurate data integration methods, created algorithms for the prediction of disease genes through context-specific and mechanistic network models and analysis of quantitative genetics data, and made novel insights into important biological processes and diseases. We further enabled experimental biological discovery by building public interactive systems capable of real-time user-driven integration that are popular among experimental biologists.  We now propose to connect these gene-level functional network approaches with the underlying genomic variation by deciphering how genomic variants lead to specific transcriptional and posttranscriptional effects. We propose to develop ab initio sequence-level models capable of predicting biochemical effects of any genomic variant (including rare or never observed) on chromatin state and RNA regulation, then link these effects with gene-level regulatory consequences (including tissue-specific transcription and RNA splicing), and finally put genomic sequence directly into the network context via a statistical approach for detecting genes and network neighborhoods with a significantly elevated mutational burden in disease. Our key deliverable will be a user- friendly, interactive web-based framework enabling systems-level variant impact analysis in a network context and an open source library for computational scientists. In addition to systematic analysis across contexts and diseases, we will collaborate with experimentalists to apply our methods to Alzheimer’s, autism spectrum disorders, chronic kidney disease, immune diseases, and congenital heart defects as case studies for the iterative improvement of our methods and to directly contribute to better understanding of these diseases. PROJECT NARRATIVE To pave the way for mechanistic interpretation of disease in the genomic context and eventually, precision medicine, we will develop algorithms for de novo prediction of functional biochemical effects of noncoding variants at the DNA regulation and RNA processing levels and then build frameworks for sequence-based prediction of tissue-specific transcription and post-transcriptional RNA processes (starting with splicing). To facilitate discovery of disease mechanisms, we will develop approaches for analyzing these variant effects in a network context, including those developed in the previous grant period (mechanistic and functional networks) and novel network models that integrate exon usage information or enhancer-gene interactions. In addition to verifying top predictions experimentally in our group or by our collaborators in case study areas of neurodegenerative disease, chronic kidney disease, ASD, and congenital heart disease, we will make our methods available to the broader biomedical community through public, interactive user interfaces and open source libraries.",lntegration and Visualization of Diverse Biological Data,10192732,R01GM071966,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Base Sequence', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Case Study', 'Cell Lineage', 'Cells', 'Chromatin', 'Chronic Kidney Failure', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Congenital Heart Defects', 'DNA', 'Data', 'Data Analyses', 'Deoxyribonucleases', 'Disease', 'Enhancers', 'Exons', 'Feedback', 'Functional disorder', 'Funding', 'Genes', 'Genetic Transcription', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Grant', 'Histones', 'Hypersensitivity', 'Immune System Diseases', 'Immunology', 'Knowledge', 'Laboratories', 'Lead', 'Letters', 'Libraries', 'Link', 'Measurement', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Neighborhoods', 'Nephrology', 'Network-based', 'Neurobiology', 'Neurodegenerative Disorders', 'Online Systems', 'Pathway Analysis', 'Pathway interactions', 'Post-Transcriptional Regulation', 'Process', 'Proteins', 'Quantitative Genetics', 'RNA', 'RNA Processing', 'RNA Splicing', 'RNA-Binding Proteins', 'Regulation', 'Research', 'Research Personnel', 'Scientist', 'System', 'Time', 'Tissue-Specific Gene Expression', 'Tissues', 'Untranslated RNA', 'Variant', 'Visualization', 'autism spectrum disorder', 'base', 'biomedical scientist', 'causal variant', 'cell type', 'congenital heart disorder', 'crosslinking and immunoprecipitation sequencing', 'data integration', 'deep learning', 'disease phenotype', 'epigenomics', 'functional genomics', 'gene interaction', 'genetic variant', 'genome wide association study', 'genomic variation', 'high throughput analysis', 'human disease', 'improved', 'in vivo', 'insight', 'network models', 'novel', 'open source', 'precision medicine', 'prediction algorithm', 'predictive modeling', 'transcription factor', 'user-friendly']",NIGMS,PRINCETON UNIVERSITY,R01,2021,448294
"Computational Methods for Designing Optimal Genomics-guided Viral Diagnostics Project Summary/Abstract Viral genome sequencing is growing exponentially and cutting-edge molecular technologies, guided by genomic data, show great promise in detecting and responding to viruses. Yet we lack a computational framework that efficiently leverages viral data to design the nucleic or amino acid sequences applied by these technologies. The proposal provides a career development plan to (i) build computational techniques — algorithms, models, and software — that yield highly accurate diagnostic assays, with potential to outperform existing ones, and (ii) use the techniques to proactively design assays for detecting 1,000s of viruses. The project will first develop methods for designing optimal viral genome-informed diagnostics. The study will formulate objective functions that evaluate an assay’s performance across a distribution of anticipated viral targets. Combinatorial optimization algorithms and generative models, constructed in the study, will optimize the functions. The project will also develop datasets for training predictive models of assay performance, which are used in the objective functions, focusing on CRISPR-, amplification-, and antigen-based diagnostics. Preliminary experimental results suggest such models can render assays with exquisite sensitivity and specificity. The study will compare the algorithmically-designed assays to state-of-the-art tests for four viruses. With these methods, the project will design diagnostic assays that are species-specific and broadly effective across genomic diversity for all viruses known to infect vertebrates. The study will build a system to monitor the assays’ effectiveness against emerging viral genomic diversity and to continually update them as needed. To enable the broad adoption of these methods, the project will implement them efficiently in accessible software. The proposal aligns with a NIAID goal of improving diagnostics via data science. The methods developed here may also aid therapy and vaccine design, and will leave the world better prepared to combat viral outbreaks. The career development award will provide training for the candidate in applied areas of long-term interest to his career. The candidate has previous experience in developing computational methods and analyzing viral genomes. Through the award, he will gain new knowledge and skills in diagnostic applications, alongside formal and informal training in immunology, bioengineering, and related laboratory techniques. This training will help the candidate progress toward therapy and vaccine applications that could benefit from advanced computational methods. The Broad Institute provides a supportive environment for the candidate’s development, including career development workshops, research seminars aligned with the proposed plan, and opportunities to initiate collaborations with scientists having expertise complementary to the candidate’s. The research and training will help him form an independent research group focused on developing and applying computational methods to enable more effective microbial surveillance and response. Project Narrative Viral genomic data is reshaping how we prepare for and respond to viral threats, but there is a scarcity of computational techniques that harness this vast, ever-growing data for designing diagnostic assays. The project will develop and test algorithms, machine learning models, and software systems to efficiently design highly accurate diagnostic assays by optimizing well-defined objective functions, applied to multiple diagnostic technologies, and will build a resource of broadly effective diagnostic assays for 1,000s of viral species. The resource and software developed in the project will advance capabilities for detecting viruses, and the new methods may accommodate challenges in designing more effective viral therapies and vaccines.",Computational Methods for Designing Optimal Genomics-guided Viral Diagnostics,10284445,K01AI163498,"['2019-nCoV', 'Adoption', 'Algorithm Design', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Award', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Models', 'Biology', 'Biomedical Engineering', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Combinatorial Optimization', 'Computational Technique', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Data', 'Data Science', 'Data Set', 'Dengue', 'Detection', 'Development', 'Development Plans', 'Diagnostic', 'Disease Outbreaks', 'Educational workshop', 'Effectiveness', 'Ensure', 'Failure', 'Focus Groups', 'Genome', 'Genomics', 'Goals', 'Growth', 'Immunology', 'Influenza', 'Institutes', 'K-Series Research Career Programs', 'Knowledge', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Molecular', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nucleic Acids', 'Performance', 'Research', 'Research Training', 'Resolution', 'Resources', 'Scientist', 'Sensitivity and Specificity', 'Software Tools', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vaccine Design', 'Vaccines', 'Validation', 'Variant', 'Vertebrates', 'Viral', 'Viral Genome', 'Virus', 'ZIKA', 'accurate diagnostics', 'advanced analytics', 'antigen diagnostic', 'base', 'career', 'career development', 'combat', 'computer framework', 'design', 'detection assay', 'diagnostic assay', 'diagnostic technologies', 'enzyme activity', 'experience', 'genome sequencing', 'genomic data', 'improved', 'insight', 'interest', 'microbial', 'model design', 'pathogen', 'predictive modeling', 'predictive test', 'prevent', 'response', 'skills', 'software development', 'software systems', 'spatiotemporal', 'success', 'supportive environment', 'therapy design', 'viral genomics']",NIAID,"BROAD INSTITUTE, INC.",K01,2021,129165
"OpenMM: Scalable biomolecular modeling, simulation, and machine learning PROJECT SUMMARY / ABSTRACT OpenMM [http://openmm.org] is the most widely-used open source GPU-accelerated framework for biomolecular modeling and simulation (>1300 citations, >270,000 downloads, >1M deployed instances). Its Python API makes it widely popular as both an application (for modelers) and a library (for developers), while its C/C++/Fortran bindings enable major legacy simulation packages to use OpenMM to provide high performance on modern hardware. OpenMM has been used for probing biological questions that leverage the $14B global investment in structural data from the PDB at multiple scales, from detailed studies of single disease proteins to superfamily-wide modeling studies and large-scale drug development efforts in industry and academia. Originally developed with NIH funding by the Pande lab at Stanford, we aim to fully transition toward a community governance and sustainable development model and extend its capabilities to ensure OpenMM can power the next decade of biomolecular research. To fully exploit the revolution in QM-level accuracy with machine-learning (ML) potentials, we will add plug-in support for ML models augmented by GPU-accelerated kernels, enabling transformative science with QM-level accuracy. To enable high-productivity development of new ML models with training dataset sizes approaching 100 million molecules, we will develop a Python framework to enable OpenMM to be easily used within modern ML frameworks such as TensorFlow and PyTorch. Together with continued optimizations to exploit inexpensive GPUs, these advances will power a transformation within biomolecular modeling and simulation, much as deep learning has transformed computer vision. PROJECT NARRATIVE Biomolecular modeling and simulation is a key technology for leveraging the $14B global investment in biomolec- ular structure data in the protein databank to understand the basic molecular mechanisms underlying biology and disease and the development of new therapies. In this proposal, we aim to expand the development of OpenMM, a free and open source biomolecular modeling and simulation package that can exploit a wide range of consumer-grade and high-end graphics processing units (GPUs) to enable researchers and applications built on OpenMM to achieve high performance with extreme ﬂexibility. A key aspect of this proposal is to accelerate research in the emerging ﬁeld of biomolecular machine learning by tightly integrating OpenMM with modern ma- chine learning frameworks, enabling researchers to build, use, and deploy machine learning potentials, collective variables, and integrators to advance the state of biomolecular modeling.","OpenMM: Scalable biomolecular modeling, simulation, and machine learning",10100573,R01GM140090,"['Academia', 'Architecture', 'Automobile Driving', 'Binding', 'Biological', 'Biological Process', 'Biological Response Modifier Therapy', 'Biology', 'Chemical Models', 'Chemicals', 'Chemistry', 'Code', 'Communities', 'Computer Vision Systems', 'Custom', 'Data', 'Data Set', 'Development', 'Disease', 'Ecosystem', 'Ensure', 'Event', 'Free Energy', 'Funding', 'Future', 'Goals', 'Home environment', 'Hybrids', 'Industry', 'Investigation', 'Investments', 'Laboratories', 'Learning', 'Libraries', 'Ligands', 'Machine Learning', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Conformation', 'Performance', 'Plug-in', 'Productivity', 'Proteins', 'Pythons', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Speed', 'Standardization', 'Structure', 'Study models', 'Sustainable Development', 'System', 'Technology', 'TensorFlow', 'Training', 'United States National Institutes of Health', 'Update', 'Work', 'cluster computing', 'deep learning', 'deep neural network', 'drug development', 'enzyme mechanism', 'flexibility', 'insight', 'interoperability', 'model development', 'models and simulation', 'molecular mechanics', 'next generation', 'novel therapeutics', 'open source', 'operation', 'physical model', 'predictive modeling', 'protein data bank', 'quantum', 'repository', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2021,426294
"Center for Machine Learning in Urology PROJECT SUMMARY We propose to establish an Exploratory Center for Interdisciplinary Research in Benign Urology at the Children’s Hospital of Philadelphia (CHOP) and the University of Pennsylvania (Penn), the central mission of which is to apply machine learning to improve the understanding of the pathophysiology, diagnosis, risk stratification, and prediction of treatment responses of benign urological disease among children and adults. The proposed CHOP/Penn Center for Machine Learning in Urology (CMLU) addresses critical structural and scientific barriers that impede the development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. Structurally, urologic research occurs in silos, with little interaction among investigators that study different diseases or different populations (e.g. pediatric and adult). Scientifically, analysis of imaging and other types complex data is limited by inter-observer variability, and incomplete utilization of available information. This proposal overcomes these barriers by applying cutting-edge approaches in machine learning to analyze CT images that are routinely obtained for evaluation of individuals with kidney stone disease. Central to the CHOP/Penn CMLU is the partnership of urologists and experts in machine learning, which will bring a new approach to generating knowledge that advances research and clinical care. In addition, the CMLU will expand the urologic research community by providing a research platform and standalone machine learning executables that could be applied to other datasets. The Center’s mission will be achieved through the following Aims, with progress assessed through systematic evaluation: Aim 1. To expand the research base investigating benign urological disease. We will establish a community with the research base, particularly with the KURe, UroEpi programs, other P20 Centers, and O’Brien Centers. We will build this community by providing mini-coaching clinics to facilitate application of machine learning to individual projects, developing an educational hub for synchronous and asynchronous engagement with the research base, and making freely available all source codes and standalone executables for all machine learning tools. Aim 2. To improve prediction of ureteral stone passage using machine learning of CT images. The CMLU has developed deep learning methods that segment and automate measurement of urinary stones and adjacent renal anatomy. In the Research Project, we will compare these methods to existing segmentation methods and the current gold standard of manual measurement. We will then extract informative features from thousands of CT scans to predict the probability of spontaneous passage of ureteral stones for children and adults evaluated in the CHOP and Penn healthcare systems. Aim 3. To foster collaboration in benign urological disease research across levels of training and centers through an Educational Enrichment Program. We will amplify interactions across institutions and engage investigators locally and nationally by providing summer research internships, and interinstitutional exchange program, and an annual research symposium. PROJECT NARRATIVE The proposed CHOP/Penn O’Brien Center for Machine Learning in Urology addresses critical structural and scientific barriers that impede development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. This application overcomes these barriers by applying cutting- edge approaches in machine learning to analyze complex imaging data for individuals with kidney stone disease.The Center’s strategic vision of using machine learning to generate knowledge that improves diagnosis, risk stratification strategies, and prediction of outcomes among children and adults will be achieved through the implementation of a Educational Enrichment Program and a Research Project.",Center for Machine Learning in Urology,10260577,P20DK127488,"['Address', 'Adult', 'Algorithms', 'Anatomy', 'Area', 'Benign', 'Characteristics', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Investigator', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Educational Status', 'Evaluation', 'Fostering', 'Functional disorder', 'Funding', 'Future', 'Gold', 'Healthcare Systems', 'Image', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Internships', 'Interobserver Variability', 'Investigation', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Lead', 'Longevity', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Mission', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pediatric Hospitals', 'Pennsylvania', 'Philadelphia', 'Population', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Probability', 'Publishing', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Site', 'Source Code', 'Structure', 'Students', 'Techniques', 'United States National Institutes of Health', 'Universities', 'Urinary Calculi', 'Urologic Diseases', 'Urologist', 'Urology', 'Vision', 'Visit', 'X-Ray Computed Tomography', 'base', 'clinical care', 'complex data', 'deep learning', 'deep neural network', 'design', 'experience', 'feature selection', 'human error', 'improved', 'interdisciplinary collaboration', 'interest', 'learning strategy', 'novel strategies', 'outcome prediction', 'peer', 'programs', 'risk stratification', 'routine imaging', 'senior faculty', 'skills', 'summer research', 'symposium', 'tool', 'urologic', 'web page']",NIDDK,CHILDREN'S HOSP OF PHILADELPHIA,P20,2021,332101
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,10224845,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'feature selection', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2021,388750
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,10093160,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'data fusion', 'deep learning', 'design', 'feature extraction', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neural network classifier', 'neurophysiology', 'novel', 'post-trauma', 'predictive modeling', 'prevent', 'random forest', 'support vector machine', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2021,243545
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,10116306,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'antibody test', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'machine learning method', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2021,591130
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",10188360,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Alzheimer&apos', 's disease pathology', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Models', 'Computer software', 'Data', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genetic testing', 'genome-wide', 'genomic data', 'genomic locus', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'large scale data', 'machine learning algorithm', 'machine learning method', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2021,410000
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10269008,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data repository', 'data reuse', 'data sharing', 'data visualization', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2021,617911
"Adaptive evolutionary inference frameworks for understudied populations using generative neural networks PROJECT SUMMARY In the field of population genetics, machine learning methods are emerging as promising frameworks for understanding evolution. However, these algorithms rely heavily on simulated datasets, which currently fail to recapitulate the features of diverse natural genomes. Deep neural networks in particular are disconnected from evolutionary modeling, and their results are difficult to interpret in a biological context. In this project, we propose to develop simulation frameworks that automatically adapt to any population or species. The resulting customized synthetic datasets will be used to train neural networks that quantify the unique evolutionary histories of understudied human groups. By including genealogical and epigenetic information as auxiliary input, we will be able to link predictions back to genomic features. Our results will enable us to estimate the interactions between local phenomena such as natural selection, mutation patterns, and recombination hotspots. Taken together, outcomes from our work will allow us to create a detailed model evolutionary of processes, both along the genome and across human populations. PROJECT NARRATIVE In population genetics, machine learning methods are emerging as promising frameworks for understanding evolution. However, it is difficult to apply these algorithms to understudied populations, as they are reliant on custom simulations, difficult to interpret, and disconnected from evolutionary modeling. The goals of this project are to develop simulation frameworks that automatically adapt to diverse datasets, allowing us to study evolutionary forces along the genome and across human populations.",Adaptive evolutionary inference frameworks for understudied populations using generative neural networks,10114449,R15HG011528,"['Admixture', 'African', 'Algorithms', 'Area', 'Back', 'Biological', 'Biological Process', 'Chromatin', 'Classification', 'Custom', 'Data', 'Data Set', 'Decision Trees', 'Epigenetic Process', 'European', 'Event', 'Evolution', 'Exposure to', 'Genealogy', 'Genes', 'Genetic Recombination', 'Genome', 'Genomic Segment', 'Genomics', 'Geography', 'Goals', 'Graph', 'Human', 'Human Genetics', 'Image', 'Individual', 'Industry', 'Internships', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Natural Selections', 'Outcome', 'Pattern', 'Population', 'Population Genetics', 'Population Sizes', 'Process', 'Recording of previous events', 'Research', 'Signal Transduction', 'Students', 'Training', 'Trees', 'Validation', 'Visualization', 'Work', 'automated algorithm', 'base', 'biobank', 'computer science', 'convolutional neural network', 'deep neural network', 'epigenetic marker', 'flexibility', 'health care settings', 'machine learning algorithm', 'machine learning method', 'methylation pattern', 'migration', 'neural network', 'simulation', 'single cell sequencing', 'statistics', 'theories', 'undergraduate student']",NHGRI,HAVERFORD COLLEGE,R15,2021,432494
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,10085652,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'metadata standards', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2021,489919
"Administrative Supplement: Using machine learning to predict odor characteristics from molecular structure PROJECT SUMMARY/ABSTRACT We cannot yet look at a chemical structure and predict if the molecule will have an odor, much less what character it will have. The goal of the proposed research is to apply machine learning to predict perceptual characteristics from chemical features of molecules. The specific aims of the proposal will determine (1) which molecules are odorous , and (2) what data are needed to model odor character. Building a highly predictive model requires two key ingredients: high-quality data and a sound modeling approach. High-quality data must be accurate (ratings are consistent and describe true odor properties) and detailed (ratings describe even small differences in odor properties). We have collected human psychophysical data on a diverse set of molecules and have trained a model to predict if a molecule has an odor, but pilot data identified odorous contaminants that limit model training and measurement of model accuracy. In Aim 1, I will apply my background in analytical chemistry to evaluate the accuracy of the data, using gas chromatography to identify and correct errors caused by chemical contaminants. In Aim 2, I will apply my experience in human sensory evaluation to measure and compare the consistency and the degree of detail in ratings that can be achieved with different sensory methods and subject training procedures. By executing my training plan, I will develop the skills in statistical programming and machine learning needed to employ a sound modeling approach to these problems. The model constructed in Aim 1 will enable prediction of odor classification (odor/odorless) for any molecule and thus define which molecules are perceptually relevant. Predicting odor character is a far more complex challenge – while a molecule can have only one of two odor classifications (odor or odorless) it may elicit any number of diverse odor character attributes (fruity, floral, musky, sweet, etc.). Descriptive Analysis (DA) is the gold standard method for generating accurate and detailed sensory profiles, but this method is time-consuming. We estimate that an odor character dataset will be large enough (“model-ready”) to predict odor character with approximately 10,000 molecules and that it would require more than 30,000 hours of human subject evaluation, or approximately 6 years for the typical trained panel, to produce this dataset using DA. Before we invest the time and resources, it is responsible to evaluate the relative data quality of more rapid sensory methods. The results of Aim 2 are expected to determine the best approach for generating a model-ready dataset by quantifying trade-offs in degree of detail (data resolution), rating consistency, and method speed of five candidate sensory methods. Together, these aims represent a significant step forward in linking chemical recipe to human odor perception, an advancement that supports the NIDCD goal of understanding normal olfactory function (how stimulus relates to percept) and has many potential applications in foods (what composition of molecules should be present to produce a target aroma percept). PROJECT NARRATIVE Currently, scientists cannot predict whether a molecule will have an odor and, if so, what odor characteristics it will have based on its chemical structure. The goal of this project is to develop predictive models linking chemical composition to odor characteristics. These models will advance our understanding of the human olfactory system and help design strategies for improving the aroma and palatability of healthy foods.",Administrative Supplement: Using machine learning to predict odor characteristics from molecular structure,10405294,F32DC019030,"['Address', 'Administrative Supplement', 'Analytical Chemistry', 'Characteristics', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Classification', 'Collection', 'Complex', 'Consumption', 'Data', 'Data Set', 'Descriptor', 'Development', 'Evaluation', 'Food', 'Fruit', 'Gas Chromatography', 'Goals', 'Gold', 'Health Food', 'Hour', 'Human', 'Human Resources', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Mass Fragmentography', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Structure', 'National Institute on Deafness and Other Communication Disorders', 'Odors', 'Olfactory Pathways', 'Palate', 'Perception', 'Positioning Attribute', 'Procedures', 'Programmed Learning', 'Property', 'Protocols documentation', 'Psychophysics', 'Quality Control', 'Recipe', 'Research', 'Research Technics', 'Resolution', 'Resources', 'Sampling', 'Science', 'Scientist', 'Sensory', 'Smell Perception', 'Speed', 'Stimulus', 'Structure', 'Testing', 'Time', 'Training', 'Work', 'base', 'data quality', 'design', 'experience', 'food science', 'human subject', 'improved', 'machine learning algorithm', 'model building', 'predictive modeling', 'prevent', 'rapid technique', 'skills', 'sound']",NIDCD,MONELL CHEMICAL SENSES CENTER,F32,2021,2500
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",10265769,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'detection method', 'detection platform', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'underserved community', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2021,482268
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",10113533,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'detection method', 'detection platform', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'underserved community', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2021,596017
"Unified Computation Tools for Natural Products Research Summary The overarching goal for this proposed renewal application will be to further advance tools that are in development and to effectively integrate several types of analytical data with biological assay data and genomic information. This will create a powerful set of tools for faster and even more accurate identification of new molecules, dereplication of known ones, and to directly infer biological activities from spectroscopic information. In the current period of support, we have made substantial progress in developing highly useful tools for automatic annotations and identifications of organic molecules, specifically focused on natural products. The Global Natural Products Social (GNPS) Molecular Networking analysis and knowledge dissemination ecosystem has processed almost 160,000 jobs in nearly 160 countries worldwide, has 4-6,000 new job submissions per month and is accessed over 200,000 times a month (majority accessions are for reference library access, inspection of public data and previous jobs that the community shares as hyperlinks in papers), and has become a mainstream tool for the annotation of organic molecules deriving from diverse sources, especially in metabolomics workflows. The public website for Small Molecule Accurate Recognition Technology (SMART), a deep learning model for providing candidate structures based on 1H-13C HSQC NMR data, went live in December 2019 and already has over 3000 jobs in 50 countries. All tools developed in this proposal will become part of this analysis ecosystem. The four laboratories contributing to this proposed research activity have created an open and integrated team that is continuing to creatively innovate new informatic tools to enhance small molecule structure annotations and inference of their chemical and biological properties. We have four specific aims: 1) To complete the development and evaluation of a set of new and innovative tools for natural products analysis, and deploy these as freely available resources for the worldwide community. 2) To refine the structural characterization of molecules through leveraging repository scale mass spectral information along with NMR data and genomic inputs. 3) To create a new SMART-based tool that integrates mass spectrometry and HSQC NMR data as the input for a new deep learning system with the goal of achieving more accurate predictions of structure. 4) To use deep learning to enhance SMART with bioactivity data so as to enable SMART to predict activities of molecules based on spectroscopic features. The data will also augment the GNPS database with biological assay binding data. An additional consequence of these goals will be the further digitization of natural products analytical data so that they can be used in the computational tools planned herein, as well as other tools in the future. Completion of these four specific aims will create new integrated tools for the precise identification of new natural product structures, and enable inference of their structural relatedness to other classes of organic molecules and their biological properties. Thus, these new informatic tools will have the potential to greatly enhance the small molecule drug discovery process.         Unified Computational Tools for Natural Products Research Approximately 50% of all FDA approved drugs trace their origins to natural products, either directly or through indirect routes of development. To accelerate and make more efficient the study of natural products, we are developing innovative computational tools for the rapid annotation of natural product structures and their associated chemical and biological properties.",Unified Computation Tools for Natural Products Research,10211176,R01GM107550,"['Address', 'Binding', 'Biological', 'Biological Assay', 'Chemicals', 'Classification', 'Communities', 'Country', 'Cyanobacterium', 'Data', 'Data Analytics', 'Databases', 'Development', 'Ecosystem', 'Evaluation', 'FDA approved', 'Future', 'Genomics', 'Goals', 'Grant', 'Knowledge', 'Laboratories', 'Legal patent', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Natural Products', 'Noise', 'Nuclear Magnetic Resonance', 'Occupations', 'Organism', 'Paper', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Property', 'Publications', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Route', 'Source', 'Spectrometry', 'Structure', 'Students', 'System', 'Technology', 'Time', 'annotation  system', 'base', 'computerized tools', 'deep learning', 'drug discovery', 'genome sequencing', 'genomic data', 'informatics tool', 'innovation', 'marine natural product', 'metabolomics', 'quantum', 'repository', 'small molecule', 'social', 'tool', 'web site']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2021,547104
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,10214625,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2021,3125
"Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1 PROJECT SUMMARY Previous studies showed discrepancies of health and behavior prevalence between American Indians (AI) population and other racial or ethnic groups. Most health surveys have certain limitations when studying AI population due to the small sample sizes for AI population. Data collected by AI Tribal Epidemiology Centers (TECs) provides an excellent opportunity to conduct research for AI population due to sufficient sample size and extensive information. However, most surveys conducted by TECs used non-probability sampling design (e.g. convenient sample) due to its lower cost and increased time efficiency. Non-probability sample may suffer from sampling, coverage and nonresponse errors without further proper adjustments. Such difficulties greatly hampers the analysis of AI population in health and behavior research. Our general hypothesis is that data integration by combining information from non-probability and probability samples can reduce sampling, coverage and nonresponse errors in original non-probability sample. The Goal of this project is to develop an accurate and robust data integration methodology for AI population analysis specifically tailored to health and behavior research. During the past years, we have 1) studied data integration using calibration and parametric modeling approaches; 2) investigated machine learning and propensity score modeling methods in survey sampling and other fields; and 3) assembled an experienced team of multi-disciplinary team of experts. In this project, we propose to capitalize on our expertise and fulfill the following Specific Aims: Aim 1. Develop a data integration approach using machine learning and propensity score modeling We will develop machine learning and propensity score based data integration approaches to combine information from non-probability and probability samples. Compared to existing methods (i.e., Calibration, Parametric approach), our proposed approaches are more robust against the failure of underlying model assumptions. The inference is more general and multi-purpose (e.g. one can estimate most parameters such as means, totals and percentiles). Simulation studies will be performed to compare our proposed methods with other existing methods. A computing package will be built to implement the method in other settings. Aim 2. Evaluate the accuracy and robustness of the proposed method in AI health and behavior research We will use real data to validate the proposed methods in terms of accuracy and robustness to the various data types. The performance will also be assessed by comparing with results from existing data integration methods such as calibration and parametric modeling approaches. The planned study takes advantage of a unique data source and expands the impact of the Indian Health Service (IHS)-funded research. We expect this novel integration method will vertically advance the field by facilitating the analysis based on non-probability sample, which can provide in-depth understanding regarding the AI population health and behavior studies. Project Narrative The overall goal of this R21 project is to develop an accurate, robust and multi-purpose data integration methodology for AI population (non-probability sample) analysis specifically tailored to health and behavior research such as diabetes and smoking. The code implementing the proposed method will be released and is general enough to be applied to AI population studies of other fileds. The success of this study will vertically advance the field by facilitating the AI population analysis, which can provide a better guidance and new insights on the future precision personalized prevention and treatment of certain diseases.",Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1,10271402,R21MD014658,"['Adult', 'Age', 'American', 'American Indians', 'Behavioral', 'Behavioral Risk Factor Surveillance System', 'Calibration', 'Censuses', 'Code', 'Communities', 'Community Surveys', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Disease', 'Epidemiology', 'Ethnic group', 'Event', 'Failure', 'Funding', 'Future', 'General Population', 'Geographic state', 'Goals', 'Health', 'Health Fairs', 'Health Surveys', 'Health behavior', 'High Prevalence', 'Kansas', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Not Hispanic or Latino', 'Oklahoma', 'Performance', 'Population', 'Population Analysis', 'Population Study', 'Prevalence', 'Probability', 'Probability Samples', 'Publishing', 'Race', 'Research', 'Research Personnel', 'Respondent', 'Risk Factors', 'Sample Size', 'Sampling', 'Smoking', 'Surveys', 'Target Populations', 'Testing', 'Texas', 'Time', 'Tobacco', 'Training', 'United States Indian Health Service', 'Weight', 'Work', 'Youth', 'base', 'behavioral study', 'cigarette smoking', 'cluster computing', 'cost', 'data integration', 'data quality', 'design', 'experience', 'improved', 'individualized prevention', 'innovation', 'insight', 'multidisciplinary', 'novel', 'personalized medicine', 'population health', 'simulation', 'smoking prevalence', 'success', 'therapy development', 'tribal health']",NIMHD,UNIVERSITY OF OKLAHOMA HLTH SCIENCES CTR,R21,2021,109613
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,10063532,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2021,360227
"Merging machine learning and mechanistic models to improve prediction and inference in emerging epidemics PROJECT SUMMARY When an outbreak of an established or emerging infectious disease occurs we ask a standard set of questions that are critical to a lifesaving public health response: Where will future incidence occur? How many cases will there be? And where can we most effectively intervene? The proposed research is motivated by real world instances where answering these questions was critical to making practical public health decisions, and current methods came up short: from deciding if and where to build additional Ebola Treatment Units in the 2014-15 West African Ebola epidemic, to identifying priority districts where oral cholera vaccine should be used in the 2016-17 cholera outbreak in Yemen, to picking locations where sufficient cases might occur to selecting and prioritizing interventions to slow the spread of COVID-19 worldwide. Forecasts informing such decisions are typically generated either using an epidemic model that relies on knowledge of the disease transmission mechanism and epidemic theory or using a statistical model to project the expected number of cases based on the relationship between covariates and observed counts. However, both approaches are subject to limitations, particularly early in an epidemic when few cases are observed. This project is based on the overarching scientific premise that inferences that combine the strengths of mechanistic epidemic models and statistical covariate models will substantially outperform either approach alone in forecasting and making decisions to confront emerging infectious disease threats. Specifically, this project aims to (1) Develop a framework to forecast incidence in ongoing outbreaks that merges mechanistic and machine learning approaches; (2) Validate the framework using retrospective data and apply the framework to inform decision making in emerging epidemics; (3) Integrate this inferential forecasting framework into causal decision theory to optimize critical actions in the public health response to emerging epidemics; and (4) Develop accessible and extensible tools for forecasting and decision analysis in infectious disease epidemics. We will validate these approaches using rigorous simulation studies and by applying the proposed approaches to retrospective data from important recent epidemics (e.g., Ebola, Cholera and COVID-19, as mentioned above). We will prospectively apply our approach to inform the response to emerging disease threats that occur during the project period, including the ongoing COVID-19 pandemic. To ensure that the tools developed are useful, efficient, and user friendly, we will work with international humanitarian organizations responding to epidemics. Successful completion of these aims will provide a flexible and validated framework for forecasting and decision making during ongoing epidemics, while allowing for innovation in mechanistic and statistical approaches. In doing so it will provide tools to optimize responses and reduce morbidity and mortality during public health crises. PROJECT NARRATIVE The purpose of the proposed project is to improve inference, forecasting and decision making in response to emerging infectious diseases by developing a framework to integrate mechanistic and statistical approaches to epidemic modeling and causal inference. Approaches developed will be validated using simulations and retrospective data and applied prospectively to reduce morbidity and mortality in emerging public health crises. Further, they will be incorporated into publically available tools for use in epidemic response.",Merging machine learning and mechanistic models to improve prediction and inference in emerging epidemics,10142638,R01GM140564,"['African', 'Algorithms', 'Area', 'COVID-19', 'COVID-19 pandemic', 'Cholera', 'Cholera Vaccine', 'Communicable Diseases', 'Community Health', 'Cost utility', 'Data', 'Data Set', 'Decision Analysis', 'Decision Making', 'Decision Theory', 'Disease', 'Disease Outbreaks', 'Ebola', 'Emerging Communicable Diseases', 'Ensure', 'Epidemic', 'Evaluation', 'Fogs', 'Future', 'Geographic Locations', 'Incidence', 'International', 'Intervention', 'Knowledge', 'Liberia', 'Link', 'Location', 'Machine Learning', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Online Systems', 'Oral', 'Policies', 'Public Health', 'Research', 'Research Personnel', 'Series', 'Shapes', 'Statistical Algorithm', 'Statistical Methods', 'Statistical Models', 'System', 'Time', 'Translating', 'Update', 'War', 'Work', 'Yemen', 'base', 'case-based', 'curve fitting', 'dashboard', 'disease transmission', 'experience', 'flexibility', 'improved', 'innovation', 'mortality', 'multidimensional data', 'programs', 'prospective', 'response', 'simulation', 'sound', 'surveillance data', 'theories', 'tool', 'transmission process', 'user-friendly']",NIGMS,JOHNS HOPKINS UNIVERSITY,R01,2021,429701
"Center for Critical Assessment of Structure Prediction (CASP) PROJECT SUMMARY  Experimental determination of protein structure often provides atomic accuracy models, but is inherently time- consuming, often costly, and not always possible. Computational modeling is currently less accurate, but offers an alternative approach when experimental results are not available. The goal of CASP (Critical Assessment of Structure Prediction) is to advance the protein structure modeling field by conducting community-wide experiments that objectively determine the strengths and weaknesses of current methods and so foster progress. Approximately 100 research groups world-wide participate. In the most recent experiment (2018), there were 57,000 submissions in nine modeling categories, including over 35,000 tertiary structure models. The Center provides the infrastructure for CASP and Aim 1 is the continued development and operation of this resource. Principal tasks include registration and communication with participants; solicitation, characterization, and management of modeling targets; collection and validation of models; and extensive numerical analysis of submissions. These operations are supported by a secure and robust data infrastructure. The Center also develops evaluation, analysis, and display software, and provides access to models and evaluation results. CASP relies on independent assessors, experts in modeling or a related experimental field, to interpret the results. The Center coordinates this process, providing evaluation data and, when necessary, implementing new evaluation methods.  Recent CASPs have shown dramatic improvements in model accuracy, especially for the most difficult cases where homology modeling cannot be used. A major factor driving progress is the use of new machine learning approaches, particularly convolutional neural networks. These and related methods appear poised to make further major advances in a number of key modeling areas. The plan for the next period of the project is designed to capitalize on these and other opportunities for progress. Greater success with modeling small proteins and domains dictates a shift in emphasis to the still challenging area of large multi-domain proteins and complexes (Aim 2), where progress is expected both from the machine learning developments and from the incorporation of sparse experimental data. Although accuracy of models has improved, it is still seldom competitive with experiment. Aim 3 is to pursue strategies that will make models more accurate and useful, by nurturing further progress in refining initial models, better methods for estimating model accuracy, and assessment of the utility of models. Aim 4 introduces new ways of strengthening interactions between CASP and the broader research community, providing models that directly address contemporary problems (for example, for CoV-2 protein structures) and boosting communications through meetings, webinars, publications and other means. PROJECT NARRATIVE Knowledge of macromolecular structure plays a crucial role in biology and medicine, allowing for detailed studies and understanding of biological processes and disease mechanisms. Yet, relatively few structures are obtained experimentally - the rest must be modeled. The Critical Assessment of Structure Prediction program (CASP), provides the primary means of evaluating performance of the methods dedicated to this task.",Center for Critical Assessment of Structure Prediction (CASP),10220601,R01GM100482,"['2019-nCoV', 'Address', 'Area', 'Automobile Driving', 'Benchmarking', 'Biological', 'Biological Process', 'Biology', 'Categories', 'Chemicals', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Community Developments', 'Computer Models', 'Computer software', 'Consumption', 'Cryoelectron Microscopy', 'Crystallography', 'Data', 'Data Set', 'Development', 'Disease', 'Double-Blind Method', 'Drug Design', 'Epitopes', 'Evaluation', 'Fostering', 'Future', 'Goals', 'Gold', 'Homology Modeling', 'Information Dissemination', 'Infrastructure', 'International', 'Journals', 'Knowledge', 'Laboratories', 'Lead', 'Life', 'Machine Learning', 'Medicine', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Structure', 'Paper', 'Participant', 'Performance', 'Play', 'Procedures', 'Process', 'Proteins', 'Provider', 'Publications', 'Reporting', 'Research', 'Resources', 'Rest', 'Role', 'Sampling', 'Seasons', 'Secure', 'Sequence Alignment', 'Series', 'Structural Models', 'Structure', 'Techniques', 'Tertiary Protein Structure', 'Time', 'Validation', 'Visualization', 'Work', 'base', 'biological research', 'convolutional neural network', 'cost', 'crosslink', 'data acquisition', 'data infrastructure', 'deep learning', 'design', 'experimental study', 'improved', 'learning strategy', 'meetings', 'member', 'method development', 'online resource', 'operation', 'programs', 'protein complex', 'protein structure', 'structural biology', 'success', 'symposium', 'vaccine development', 'webinar']",NIGMS,UNIVERSITY OF CALIFORNIA AT DAVIS,R01,2021,638990
"Modeling the influence of translation-elongation kinetics on protein structure and function Project Summary mRNA degradation is an essential process in post-translational gene regulation, and influences protein expression levels in cells. In S. cerevisea the lifetime of mRNA ranges from 43 sec to 39 min, with a median half-life of 3.6 min. The molecular factors governing these differential degradation rates has long been an area of active research. Recently though, clear evidence has emerged that the codon optimality correlates with half- lives. At a mechanistic level, the emerging perspective is that some transcripts are translated quickly, and some slowly, and that transcripts in which ribosomes end up forming queues, much like a traffic jam of cars on a highway, are recognized by ubiquitin ligases such as Hel2 that trigger the RQC pathway to promote mRNA degradation. There are two major gaps in this field. The first is the capability to predict mRNA half-lives accurately from mRNA sequence features. The second is understanding at the molecular level how the distribution of codon translation speeds along a transcript’s coding sequence promote ribosome queues and hence degradation. In this proposal, a graduate student will combine the PI’s labs expertise in modeling the kinetics of translation and ribosome traffic with interpretable machine learning techniques to address these two gaps. In achieving this, the field will be advanced by having both predictive and explanatory models for how translation speed and codon usage differentially impacts the degradation rates of different mRNAs. Specifically, our first aim is to build an interpretable machine learning model to identify robust and predictive features governing mRNA degradation. Our second aim is to explain at the molecular level why these features influence degradation rates. We will do this in two ways. First, we will use the essential and predictive features resulting from the interpretable machine learning model to identify potential underlying mechanisms contributing to degradation. Second, we will simulate the movement of ribosomes on each transcript based on reported initiation and elongation rates to detect ribosome queues and provide an explanation for differential degradation rates. Finally, our third aim is to test the predictions coming from the models. For example, do the models from Aim 1 accurately predict mRNA half-lives when synonymous mutations are introduced? There is sufficient published data on transcriptome-wide mRNA half-lives on S. cerevisiae to train and test the machine learning models in Aim 1. Further, we have arranged for a machine learning expert to co-advise the graduate student on the second aim. This co-advisor is already a collaborator of the PI on other machine learning projects. Finally, a collaborator who has measured mRNA half-lives will further advise the student on the third aim. In summary, this training supplement will address cutting edge questions in the molecular biology and biophysics of mRNA lifetimes and provide the student the opportunity to get advanced training and expertise in machine learning, molecular modeling, and experimental techniques. Project Narrative Messenger RNA (mRNA) half-lives are influenced by the rate of protein synthesis and the ribosome traffic jams that can form on transcripts when slow-translating codons are encountered by ribosomes. The complex distribution of codon usage across transcripts, and the interplay of initiation and elongation rates that can create ribosome queues make it difficult to predict an mRNA's half-life based on its sequence. Here, we will apply machine learning to accurately predict mRNA half-lives from sequence, and combine it with biophysical modeling to understand the molecular events regulating mRNA degradation.",Modeling the influence of translation-elongation kinetics on protein structure and function,10307359,R35GM124818,"['Address', 'Area', 'Biophysics', 'Cells', 'Code', 'Codon Nucleotides', 'Complex', 'Coupling', 'Data', 'Event', 'Gene Expression Regulation', 'Half-Life', 'Kinetics', 'Lead', 'Machine Learning', 'Measures', 'Messenger RNA', 'Modeling', 'Molecular', 'Molecular Biology', 'Movement', 'Mutation', 'Pathway interactions', 'Process', 'Property', 'Protein Biosynthesis', 'Proteins', 'Publishing', 'Reporter', 'Reporting', 'Research', 'Ribosomes', 'Saccharomyces cerevisiae', 'Speed', 'Students', 'Techniques', 'Testing', 'Training', 'Transcript', 'Translating', 'Translations', 'base', 'biophysical model', 'graduate student', 'insight', 'kinetic model', 'mRNA Transcript Degradation', 'models and simulation', 'molecular modeling', 'protein expression', 'protein structure function', 'ribosome profiling', 'simulation', 'transcriptome', 'ubiquitin ligase']",NIGMS,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R35,2021,31246
"Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity PROJECT SUMMARY/ABSTRACT  Experimental animal and clinical testing to evaluate hepatotoxicity demands extensive resources and long turnaround times. Utilization of computational models to directly predict the toxicity of new compounds is a promising strategy to reduce the cost of drug development and to screen the multitude of industrial chemicals and environmental contaminants currently lacking safety assessments. However, the current computational models for complex toxicity endpoints, such as hepatotoxicity, are not reliable for screening new compounds and face numerous challenges. Our recent studies have shown that traditional Quantitative Structure-Activity Relationship modeling is applicable for relatively simple properties or toxicity endpoints with a clear mechanism, but fails to address complex bioactivities such as hepatotoxicity. The primary objective of this proposal is to develop novel mechanism-driven Virtual Adverse Outcome Pathway (vAOP) models for the fast and accurate assessment of hepatotoxicity in a high-throughput manner The resulting vAOP models will be experimentally validated using a complement of in vitro and ex vivo testing. We have generated a preliminary vAOP model based on the antioxidant response element (ARE) pathway that has undergone initial validation and refinement using in vitro testing. To this end, our project will generate novel predictive models for hepatotoxicity by applying 1) a virtual cellular stress pathway model to mechanism profiling and assessment of new compounds; 2) computational predictions to fill in the missing data for specific targets within the pathway; 3) in vitro experimental validation with three complementary bioassays; and 4) ex vivo experimental validation with pooled primary human hepatocytes capable of biochemical transformation. The scientific approach of this study is to develop a universal modeling workflow that can take advantage of all available short-term testing information, obtained from both computational predictions using novel machine learning approaches and in vitro experiments, for target compounds of interest. We will validate and use our modeling workflow to directly evaluate the hepatotoxicity of new compounds and prioritize candidates for validation in pooled primary human hepatocytes. The resulting workflow will be disseminated via a web portal for public users around the world with internet access. Importantly, this study will pave the way for the next generation of chemical toxicity assessment by reconstructing the modeling process through a combination of big data, computational modeling, and low cost in vitro experiments. To the best of our knowledge, the implementation of this project will lead to the first publicly available mechanisms-driven modeling and web- based prediction framework for complex chemical toxicity based on publicly-accessible big data. These deliverables will have a significant public health impact by not only prioritizing compounds for safety testing or new chemical development, but also revealing toxicity mechanisms. PROJECT NARRATIVE Hepatotoxicity is a leading safety concern in the development of new chemicals. We will create virtual “Adverse Outcome Pathway” models that will directly evaluate the hepatotoxicity potentials of chemicals using massive public toxicity data. The primary deliverable of this project will be a publically-accessible, web-based search engine to evaluate new chemicals for risk of hepatotoxicity.",Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity,10166848,R01ES031080,"['Address', 'Animal Model', 'Animal Testing', 'Antioxidants', 'Big Data', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Cellular Stress', 'Chemical Injury', 'Chemical Structure', 'Chemicals', 'Clinical', 'Complement', 'Complex', 'Computer Models', 'Computer software', 'Computers', 'Cryopreservation', 'Custom', 'Data', 'Data Pooling', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Drug Costs', 'Ensure', 'Environment', 'Environmental Pollution', 'Evaluation', 'Face', 'Generations', 'Hepatocyte', 'Hepatotoxicity', 'Human', 'In Vitro', 'Industrialization', 'Injury', 'Internet', 'Libraries', 'Liver', 'Luciferases', 'Machine Learning', 'Marketing', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Nutraceutical', 'Online Systems', 'Pathway interactions', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Population', 'Process', 'Property', 'Proteomics', 'PubChem', 'Public Health', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Personnel', 'Resources', 'Response Elements', 'Risk', 'Safety', 'Signal Transduction', 'Source', 'Statutes and Laws', 'System', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Toxicology', 'Translating', 'Validation', 'Vertebrates', 'adverse outcome', 'base', 'candidate validation', 'cell injury', 'combat', 'computational toxicology', 'computer framework', 'computerized tools', 'cost', 'data mining', 'deep neural network', 'design', 'developmental toxicity', 'drug development', 'endoplasmic reticulum stress', 'experimental study', 'hepatocellular injury', 'improved', 'in vitro Assay', 'in vitro testing', 'in vivo', 'interest', 'knowledge base', 'large datasets', 'liver injury', 'next generation', 'novel', 'pre-clinical', 'predictive modeling', 'reproductive toxicity', 'research clinical testing', 'safety assessment', 'safety testing', 'screening', 'search engine', 'tool', 'toxicant', 'transcriptomics', 'virtual', 'web portal']",NIEHS,RUTGERS THE STATE UNIV OF NJ CAMDEN,R01,2021,457521
"An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics Immune-repertoire sequence, which consists of an individual's millions of unique antibody and T-cell receptor (TCR) genes, encodes a dynamic and highly personalized record of an individual's state of health. Our long- term goal is to develop the computational models and tools necessary to read this record, to one day be able diagnose diverse infections, autoimmune diseases, cancers, and other conditions directly from repertoire se- quence. The key problem is how to find patterns of specific diseases in repertoire sequence, when repertoires are so complex. Our hypothesis is that a combination of bottom-up (sequence-level) and top-down (systems- level) modeling can reveal these patterns, by encoding repertoires as simple but highly informative models that can be used to build highly sensitive and specific disease classifiers. In preliminary studies, we introduced two new modeling approaches for this purpose: (i) statistical biophysics (bottom-up) and (ii) functional diversity (top-down), and showed their ability to elucidate patterns related to vaccination status (97% accuracy), viral infection, and aging. Building on these studies, we will test our hypothesis through two specific aims: (1) We will develop models and classifiers based on the bottom-up approach, statistical biophysics; and (2) we will de- velop the top-down approach, functional diversity, to improve these classifiers. To achieve these aims, we will use our extensive collection of public immune-repertoire datasets, beginning with 391 antibody and TCR da- tasets we have characterized previously. Our team has deep and complementary expertise in developing computational tools for finding patterns in immune repertoires (Dr. Arnaout) and in the mathematics that under- lie these tools (Dr. Altschul), with additional advice available as needed regarding machine learning (Dr. AlQuraishi). This proposal is highly innovative for how our two new approaches address previous issues in the field. (i) Statistical biophysics uses a powerful machine-learning method called maximum-entropy modeling (MaxEnt), improving on past work by tailoring MaxEnt to learn patterns encoded in the biophysical properties (e.g. size and charge) of the amino acids that make up antibodies/TCRs; these properties ultimately determine what targets antibodies/TCRs can bind, and therefore which sequences are present in different diseases. (ii) Functional diversity fills a key gap in how immunological diversity has been measured thus far, by factoring in whether different antibodies/TCRs are likely to bind the same target. This proposal is highly significant for (i) developing an efficient, accurate, generative, and interpretable machine-learning method for finding diagnostic patterns in repertoire sequence; (ii) applying a robust mathematical framework to the measurement of immuno- logical diversity; (iii) impacting clinical diagnostics; and (iv) adding a valuable new tool for integrative/big-data medicine. The expected outcome of this proposal is an integrated pair of robust and well validated new tools/models for classifying specific disease exposures directly from repertoire sequence. This proposal in- cludes plans to make these tools widely available, to maximize their positive impact across medicine. The proposed research is relevant to public health because B cells/antibodies and T cells play vital roles across such a vast range of health conditions, from infection, to autoimmunity, to cancer, that the ability to de- code what they are doing would be an important step forward for diagnosing these conditions. The proposed research is relevant to the NIH's mission of fostering fundamental creative discoveries, innovative research strategies, and their applications as a basis for ultimately protecting and improving health, specifically relating to the diagnosis of human diseases.",An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics,10165490,R01AI148747,"['Address', 'Affect', 'Aging', 'Amino Acid Motifs', 'Amino Acids', 'Antibodies', 'Autoimmune Diseases', 'Autoimmunity', 'B-Lymphocytes', 'Base Sequence', 'Big Data', 'Binding', 'Biophysics', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Code', 'Collection', 'Complex', 'Computer Models', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Entropy', 'Fostering', 'Gene Frequency', 'Genes', 'Goals', 'Health', 'Human', 'Immune', 'Immunology', 'Individual', 'Infection', 'Influenza vaccination', 'Intuition', 'Learning', 'Letters', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Physics', 'Play', 'Population Heterogeneity', 'Privatization', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Role', 'Sample Size', 'Sampling', 'Sampling Errors', 'Signs and Symptoms', 'Speed', 'Statistical Study', 'System', 'T-Cell Receptor', 'T-Cell Receptor Genes', 'T-Lymphocyte', 'Testing', 'United States National Institutes of Health', 'Vaccination', 'Virus Diseases', 'Work', 'base', 'biophysical properties', 'clinical diagnostics', 'computerized tools', 'diagnostic accuracy', 'human disease', 'immunological diversity', 'improved', 'information model', 'innovation', 'machine learning method', 'multidisciplinary', 'multilevel analysis', 'novel', 'novel strategies', 'tool']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2021,528873
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,10085244,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated algorithm', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'machine learning method', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2021,356625
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,10372268,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated algorithm', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'machine learning method', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2021,267499
"CSHL Network Biology Conference PROJECT SUMMARY This proposal seeks support for the conference on Network Biology, to take place March/April 2021 to 2025, at the Cold Spring Harbor Laboratory (CSHL). This meeting, held in biannual rotation on the CSHL campus in New York, brings together senior and junior scientists from both experimental and computational laboratories with common interests in network biology. The meeting will emphasize new discoveries and provide an open forum for the presentation of the latest research and results on molecular networks and their relevance to normal and abnormal cellular physiology. It will be essential for advancing knowledge in all aspects of the network modeling process, from data generation in experimental cell biology to data analysis and computer simulation and from the development and validation of network models describing these data to biological inferences made from the models. The conference will include platform sessions on interaction networks, signaling and network dynamics, regulatory networks, computational tools, artificial intelligence and big data, multi-scale networks, networks and disease, networks in differentiation, microbiome networks, network evolution, synthetic networks, network engineering and networks beyond biology though the exact program for the meeting will be assembled after the abstract submission deadline in February 2021 and adapted to ongoing developments in the field in subsequent years. This conference will include significant components designed to facilitate the active participation of younger scientists such as selection of platform speakers on the basis of the scientific merit of their submitted abstracts as well as poster presentations, round tables, lightning talks and poster prizes. Distinguished speakers will also be invited to give platform talks and interact with more junior scientists. The intimate environment at CSHL fosters social interactions and active participation by all. The majority of participants to the previous CSHL Network Biology meeting expressed that they were “very satisfied”. Speakers' panels have consisted of at least 50% women; the gender balance will be maintained in future meetings. In the 2019 iteration of the meeting, a panel discussion was established to address the challenges of Women in Network Science. We will continue to address big community challenges though panel discussions in this meeting. In 2021, we will discuss Applicability and Translatability of Network Biology with panelists including network biologists whose work is deeply influential throughout the ongoing covid-19 pandemic. PROJECT NARRATIVE Biological systems are functionally and structurally formed by complex networks of interacting molecular components, many of which are encoded in the genome. Elucidating the structure and function of these networks and understanding how their dysregulation causes disease are critical steps toward improving human health. This application seeks support for the conference on Network Biology to be held every two years in March/April 2021-25 at the Cold Spring Harbor Laboratory, to bring together experimental and computational biologists and discuss emerging trends and latest results in the field.",CSHL Network Biology Conference,10137390,R13HG011550,"['Address', 'Animals', 'Artificial Intelligence', 'Attention', 'Awareness', 'Big Data', 'Biochemistry', 'Biological', 'Biological Process', 'Biology', 'COVID-19 pandemic', 'Cell physiology', 'Cellular biology', 'Chalk', 'Collaborations', 'Communities', 'Complex', 'Computer Simulation', 'Data', 'Data Analyses', 'Development', 'Discipline', 'Disease', 'Engineering', 'Ensure', 'Environment', 'Equilibrium', 'Event', 'Evolution', 'Faculty', 'Fostering', 'Future', 'Gender', 'Gene Proteins', 'Generations', 'Genetic', 'Genome', 'Geography', 'Health', 'Human', 'Industrialization', 'Influentials', 'International', 'Intervention', 'Knowledge', 'Laboratories', 'Length of Stay', 'Lightning', 'Methodology', 'Microbe', 'Modeling', 'Molecular', 'Nationalities', 'New York', 'Normal Cell', 'Organism', 'Participant', 'Plants', 'Postdoctoral Fellow', 'Prize', 'Process', 'Property', 'Published Comment', 'RNA', 'Reagent', 'Reproducibility', 'Research', 'Research Institute', 'Research Personnel', 'Rotation', 'Schedule', 'Science', 'Scientist', 'Signal Transduction', 'Social Interaction', 'Structure', 'Technology', 'Validation', 'Woman', 'Work', 'base', 'biological systems', 'career', 'computer science', 'computerized tools', 'data exchange', 'data reuse', 'design', 'graduate student', 'host-microbe interactions', 'improved', 'innovation', 'interdisciplinary collaboration', 'interest', 'meetings', 'microbiome', 'multidisciplinary', 'network models', 'posters', 'precision medicine', 'programs', 'senior faculty', 'social', 'symposium', 'trend', 'unpublished works']",NHGRI,COLD SPRING HARBOR LABORATORY,R13,2021,3000
"Fast and flexible Bayesian phylogenetics via modern machine learning Project Abstract/Summary The SARS-CoV-2 pandemic underlines both our susceptibility to and the toll of a global pathogen outbreak. Phylogenetic analysis of viral genomes provides key insight into disease pathophysiology, spread and po- tential control. However, if these methods are to be used in a viral control strategy they must reliably account for uncertainty and be able to perform inference on 1,000s of genomes in actionable time. Scaling Bayesian phylogenet- ics to meet this need is a grand challenge that is unlikely to be met by optimizing existing algorithms.  We will meet this challenge with a radically new approach: Bayesian variational inference for phylogenet- ics (VIP) using ﬂexible distributions on phylogenetic trees that are ﬁt using gradient-based methods analogous to how one efﬁciently trains massive neural networks. By taking a variational approach we will also be able to integrate phylogenetic analysis into very powerful open-source modeling frameworks such as TensorFlow and PyTorch. This will open up new classes of models, such as neural network models, to integrate data such as sampling location and migration patterns with phylogenetic inference. These ﬂexible models will inform strategies for viral control.  In Aim 1 we will develop the theory necessary for scalable and reliable VIP, including subtree marginal- ization, local gradient updates needed for online algorithms, convergence diagnostics, and parameter support estimates. We will implement these algorithms in our C++ foundation library for VIP. In Aim 2 we will develop a ﬂexible TensorFlow-based modeling platform for phylogenetics, enabling a whole new realm of phylogenetic models based on neural networks to learn phylodynamic heterogeneity with minimal program- ming effort. We will provide efﬁcient gradients to this implementation via our C++ library. In Aim 3 we will use the fact that VIP posteriors are durable and extensible descriptions of the full data posterior to enable dynamic online computation of variational posteriors, including divide-and-conquer Bayesian phylogenetics. This work will enable a cloud-based viral phylogenetics solution to rapidly update our current estimate of the posterior distribution when new data arrive or the model is modiﬁed. 1 Project Narrative We have seen in the current SARS-CoV-2 pandemic, as for all major pathogen outbreaks in the last decade, how phylogenetic (i.e. evolutionary tree) methods are required to use viral genomic information to under- stand large-scale transmission patterns. However, current phylogenetic methods have two major limitations as a tool for viral control: ﬁrst, rigorous Bayesian probabilistic methods cannot scale to 1,000s of genomes, and second, models incorporating phylogenetic trees must be expressed in specialized phylogenetics pack- ages, making modern machine-learning approaches impossible. In this proposal, we develop variational ap- proaches to phylogenetics, which will allow fast inference and procedures to rapidly update inferences when new data arrives, as well as making phylogenetic trees a ﬁrst-class inferential object in major machine-learning packages. 1",Fast and flexible Bayesian phylogenetics via modern machine learning,10266670,R01AI162611,"['Age', 'Algorithms', 'Back', 'Bayesian Method', 'COVID-19 pandemic', 'Code', 'Collection', 'Complex', 'Computational Biology', 'Custom', 'Data', 'Data Set', 'Diagnostic', 'Discipline', 'Disease', 'Disease Outbreaks', 'Epidemic', 'Foundations', 'Functional disorder', 'Genome', 'Graph', 'Heterogeneity', 'Learning', 'Libraries', 'Location', 'Machine Learning', 'Markov chain Monte Carlo methodology', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Nature', 'Neural Network Simulation', 'Pattern', 'Phylogenetic Analysis', 'Predisposition', 'Procedures', 'Public Health', 'Research Personnel', 'Sampling', 'Statistical Models', 'Structural Models', 'Structure', 'Technology', 'TensorFlow', 'Time', 'Training', 'Trees', 'Uncertainty', 'Update', 'Variant', 'Viral', 'Viral Genome', 'Work', 'base', 'cloud based', 'data modeling', 'epidemiologic data', 'flexibility', 'genomic data', 'high dimensionality', 'insight', 'knowledge base', 'mathematical algorithm', 'mathematical methods', 'migration', 'neural network', 'novel strategies', 'open source', 'pathogen', 'prevent', 'scale up', 'social exclusion', 'theories', 'tool', 'transmission process', 'user-friendly', 'viral genomics', 'viral transmission']",NIAID,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2021,797370
"Integrating Biology into In Silico Methodologies: Modern approaches for incorporating biological reasoning and understanding into computational methods. 1 This proposal is for PA-18-648, NIH Support for Conferences and Scientific Meetings –  2 funding intended to help finance a two-day standalone “SOT CCT” workshop titled Integrating  3 Biology into In Silico Methodologies: Modern Approaches for incorporating biological  4 reasoning and understanding into computational methods. As a “Contemporary Concepts  5 in Toxicology” meeting, this workshop has the full backing, including being financially  6 underwritten, by the Society of Toxicology.  7 Computational modeling is an important tool for assessing the safety and use of  8 chemicals across many industries, including chemical, pharmaceutical, and consumer products.  9 Moreover, in silico methodologies offer academia and regulatory a fast and cheap method of 10 prioritizing its efforts to maintain compliance and safety in the market and environment. 11 This conference is designed to promote the development of actionable insights and 12 methodologies for increasing the biological relevance of in silico solutions. Specifically, this 13 conference will focus on solving the “black box effect”. There are many ways to validate a 14 model’s accuracy and domain – however if the model cannot explain what is happening 15 biologically, its use is severely diminished. This workshop will bring together regulatory, 16 academia, industry, and service providers to discuss current solutions and efforts, as well as 17 ongoing and future research. One goal of this conference will be to develop a roadmap for the 18 incorporation of AOPs (and similar biological reasonings) for computational tools. 19 This workshop has great appeal for multiple stakeholders within toxicology, namely 20 industry, academia, regulators, as well as service providers. The use of machine-learning to 21 replace laboratory toxicological tests is paramount to the future of the industry (3Rs). The use 22 of in silico models are explicitly referenced by NICEATM’s U.S. Strategic Roadmap, as well as 23 TSCA. Moreover, many industries and regulatory entities are taking significant steps away from 24 animal testing. Most recently, the US EPA stated that it will eliminate animal testing by 2035. 25 This workshop will bring together different stakeholders to discuss the current state of 26 AOPs and in silico methodologies, and to work towards a unified approach for their 27 incorporation. The final outcome of the workshop will be a white-paper that not only reviews the 28 current landscape but discusses concretes steps, as outlined in the breakout session, needed 29 for the regulatory acceptance of machine learning technologies – specifically a roadmap for the 30 inclusion of AOPs into computational tools and explanations. This proposal is for PA-18-648, NIH Support for Conferences and Scientific Meetings – funding intended to help finance a two-day standalone “SOT CCT” workshop titled Integrating Biology into In Silico Methodologies: Modern Approaches for incorporating biological reasoning and understanding into computational methods. This workshop will bring together different stakeholders to discuss the current state of AOPs and in silico methodologies, and to work towards a unified approach for their incorporation. The final outcome of the workshop will be a white- paper that not only reviews the current landscape but discusses concretes steps, as outlined in the breakout session, needed for the regulatory acceptance of machine learning technologies – specifically a roadmap for the inclusion of AOPs into computational tools and explanations.",Integrating Biology into In Silico Methodologies: Modern approaches for incorporating biological reasoning and understanding into computational methods.,10144727,R13ES032662,"['Academia', 'Address', 'Adoption', 'Animal Testing', 'Animals', 'Back', 'Biological', 'Biology', 'Budgets', 'Chemicals', 'Chemistry', 'Communities', 'Computer Models', 'Computing Methodologies', 'Decision Making', 'Development', 'Educational workshop', 'Environment', 'Event', 'Funding', 'Future', 'Goals', 'In Vitro', 'Individual', 'Industry', 'Laboratories', 'Laws', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Institute of Environmental Health Sciences', 'Nonprofit Organizations', 'Outcome', 'Paper', 'Pathway interactions', 'Pharmacologic Substance', 'Policies', 'Process', 'Publishing', 'Safety', 'Societies', 'System', 'Technology', 'Testing', 'Toxicology', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'cheminformatics', 'computer framework', 'computerized tools', 'consumer product', 'cost', 'design', 'improved', 'in silico', 'in vivo', 'insight', 'meetings', 'predictive modeling', 'research and development', 'service providers', 'symposium', 'tool', 'web site']",NIEHS,"TOXTRACK, LLC",R13,2021,4000
"Clinical Research Education in Genome Science (CREiGS) Project Summary/Abstract  The sensitivity and availability of omic technologies have enabled the genomic, transcriptomic and proteomic characterization of disease phenotypes, at the tissue and even the single cell level. This has allowed development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. Patients of providers who have participated in these educational initiatives also benefit as it allows for more rapid integration of genomic study findings into the clinical care setting. Thus, in response to PAR-19-185, we propose to develop and implement the Clinical Research Education in Genome Science (CREiGS) program that will not only focus on the analysis of genomic data, but also on gene-expression data, the integration of these two data types, as well as introductory theory and application of statistical and machine learning methods. Specifically we propose to accomplish the following specific aims: 1. Develop and successfully implement the online and in-person phases of CREiGS to increase the methodologic ingenuity by which researchers tackle important genomics-related clinical problems. 2. Establish a Diversity Recruitment External Advisory Board to ensure that the most effective strategies are employed to recruit URM doctoral students, postdoctoral fellows, and faculty from academic institutions nationwide into CREiGS. 3. Enhance the dissemination phase of CREiGS by packaging and uploading the asynchronous lectures and the online critical thinking/problem solving assessments with solutions for publicly available, online teaching resources. 4. Implement effective methods to evaluate the efficacy of CREiGS by examining:1) the participants' grasp of the CREiGS core competencies, 2) the clarity and quality of the curriculum, 3) program logistics and operation, and 4) the participants' short-term and long-term success attributed to participation in CREiGS. In summary, we posit that CREiGS will provide participants with a solid foundation in genomics science to answer complex, clinical questions. We believe that CREiGS supports the mission of the NHGRI by providing researchers with rigorous training to “accelerate medical breakthroughs that improve human health.” Project Narrative The sensitivity and availability of omic technologies have allowed for the development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. The overall goal of the Clinical Research Education in Genome Science program is to increase the methodologic ingenuity of students, postdoctoral fellows, and faculty from academic institutions nationwide through a solid foundation in genomics science to answer complex, clinical research questions and improve patient care.",Clinical Research Education in Genome Science (CREiGS),10147746,R25HG011021,"['Area', 'Biomedical Research', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Critical Thinking', 'Data', 'Data Analyses', 'Development', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Hour', 'Human', 'Hybrids', 'Institution', 'Knowledge', 'Logistics', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Human Genome Research Institute', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Persons', 'Phase', 'Phenotype', 'Play', 'Postdoctoral Fellow', 'Problem Solving', 'Proteomics', 'Provider', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Role', 'Single Nucleotide Polymorphism', 'Solid', 'Statistical Methods', 'Students', 'Technology', 'Tissues', 'Training', 'Translational Research', 'Treatment outcome', 'Underrepresented Minority', 'Underserved Population', 'Update', 'cancer therapy', 'clinical care', 'computerized tools', 'data integration', 'data management', 'disease phenotype', 'disorder subtype', 'doctoral student', 'education research', 'efficacy evaluation', 'genetic analysis', 'genome sciences', 'genomic data', 'grasp', 'health disparity', 'improved', 'individual patient', 'innovation', 'lectures', 'machine learning method', 'operation', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'response', 'statistical and machine learning', 'success', 'theories', 'therapy development', 'tool', 'transcriptomics', 'treatment optimization', 'virtual']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R25,2021,162000
"An Agent-Based Modeling Platform for Environmental Biotechnology Hazardous pollutants in the environment continue to threaten public health and environmental  safety. Human exposure to major contaminant classes, such as polyfluorinated compounds  (PFCs), hazardous organic compounds (HOCs), and heavy metals, has been linked to a variety of  diseases and is subject to stringent State and Federal environmental regulations.  Bioremediation is a low-cost and environmentally friendly approach with many successful  use-cases; however, conventional bioremediation technologies can suffer from unreliability, low  degradation rates, and incomplete degradation. As stakeholders to Superfund sites and other sites  with water or soil pollution urgently demand more efficient, less costly and more reliable  remediation technologies, it is critical to look to advancements in computational  modeling to develop next-generation, precision-engineered bioremediation technologies. The proposed project builds on successful outcomes from Phase I in which a new computational  platform was designed and validated to accurately predict the bioremediation kinetics of  a multi-organism microcosm degrading a combination of HOCs in groundwater. The basis of  this platform is an approach called agent-based modeling (ABM), where the functions of  individual components (e.g. microorganisms) within complex ecosystems are used to predict and  optimize system-level properties (e.g. bioremediation kinetics). In this Phase II project, the novel computational platform developed in Phase I is  further improved with a machine learning component that leverages bioinformatics  databases to develop rationally tailored microbiomes for degrading complex pollutant  mixtures. Iterative experimental validation of model outputs is conducted using an innovative  materials science platform that maintains the relative concentration of different species in the  microbiome constant within the multi-zone treatment barrier (in-situ) or multi-zone bioreactor  (ex-situ). The project includes focused development of a prototype for one bioremediation use-case,  which is directly compared to a conventional (non-precision) bioremediation system treating   actual contaminated groundwater. This will be performed in order to assess and quantify  the expected technical and economic benefits of harnessing the project's novel computational  platform in biotechnology development. The broad, long-term impact of the proposed project will be to transform the development and  implementation of bioremediation by integrating advancements in computational modeling, machine  learning, bioinformatics, and materials science. By leveraging novel tools across disciplines, the  project will accelerate the development of more precise, reliable and inexpensive technologies for  environmental remediation. The successful outcome of the proposed project will also provide new  collaborative opportunities for industry and academia to more rapidly address the remediation of  high-priority pollutants in the environment, and ultimately help mitigate the effects of hazardous  pollutants on communities impacted by the presence of environmental contamination. PROJECT NARRATIVE Contaminated soils and waters continue to threaten public health and safety. This project builds on the development of a novel computational platform for predicting the complex, dynamic interactions between microbial ecosystems and hazardous contaminants-of-concern in the environment, and to utilize this information to develop improved engineered remediation biotechnologies.",An Agent-Based Modeling Platform for Environmental Biotechnology,10158243,R44ES026541,"['Academia', 'Address', 'Biodegradation', 'Bioinformatics', 'Bioreactors', 'Bioremediations', 'Biotechnology', 'Chemicals', 'Classification', 'Colorado', 'Communities', 'Complex', 'Computer Models', 'Data', 'Databases', 'Development', 'Discipline', 'Disease', 'Economics', 'Ecosystem', 'Engineering', 'Environment', 'Environmental Monitoring', 'Environmental Pollution', 'Enzymes', 'Exposure to', 'Ginkgo biloba', 'Goals', 'Growth', 'Heavy Metals', 'In Situ', 'Indiana', 'Individual', 'Industry', 'Kinetics', 'Laboratories', 'Learning Module', 'Letters', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Molecular', 'Municipalities', 'Organism', 'Outcome', 'Output', 'Phase', 'Polymers', 'Process', 'Property', 'Public Health', 'Regulation', 'Research', 'Safety', 'Side', 'Site', 'Soil', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Water', 'Water Pollution', 'base', 'computational platform', 'cost', 'design', 'economic evaluation', 'enzyme pathway', 'exposed human population', 'ground water', 'improved', 'innovation', 'laboratory experiment', 'materials science', 'microbial', 'microbiome', 'microorganism', 'next generation', 'novel', 'pollutant', 'prototype', 'remediation', 'research and development', 'soil pollution', 'success', 'superfund site', 'tool']",NIEHS,"MICROVI BIOTECH, INC.",R44,2021,630992
"Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility Traumatic brain injury (TBI) is a leading cause of neurological disorders and affects over 2.5 million people each year, yet no treatment has successfully translated from bench to clinic. TBI is a broad term and encompasses an extremely heterogeneous set of injuries differing by cause, severity, biomechanics, and the varied, complex secondary injury responses that collectively result in chronic disabilities. Current preclinical research circumvents the issue of TBI heterogeneity by relying on specific preclinical animal models that mimic subpopulations of patients and particular secondary injury mechanisms with each study focusing on limited, individual pathways. This proposal instead aims to tackle TBI heterogeneity by approaching TBI as a “big data” problem and aggregating and analyzing the multidimensional data collectively. A framework for data harmonization and curation will be developed, and datasets from a consortium of preclinical labs employing a variety of preclinical TBI models will be collected and curated into an open data commons (ODC-TBI). Utilizing machine learning and multidimensional analytics, the proposed research will directly leverage TBI heterogeneity in the merged dataset to identify persistent features of TBI to empower translational research. By creating a preclinical TBI ODC and applying machine learning to integrate the heterogeneity of preclinical TBI models, the project will reveal multidimensional features of TBI across heterogeneous injuries and characterize how diverse secondary injury mechanisms interact and ultimately affect injury outcome. Throughout the project's timeline, new datasets will continue to be harmonized into the ODC-TBI according to the established framework. The ODC-TBI will be the first open multicenter, multi-model repository of preclinical TBI data and will enable the application of data science to the field of TBI. Furthermore, the ODC-TBI and the methods implemented throughout the project will be openly shared to improve reproducibility of TBI research. Together with the multidimensional analysis that will provide quantitative and qualitative understanding of TBI heterogeneity, the project aims to ultimately accelerate data- driven discovery and precision medicine for TBI. Reflecting the complexities of clinical traumatic brain injury (TBI), preclinical TBI research is confounded by the extreme heterogeneity prevalent across possible injury models and resulting biological responses. The proposed research will aggregate and curate an extensive open data commons (ODC) of preclinical TBI research with multiple TBI models and utilize machine learning to tackle TBI heterogeneity directly. The project will create an ODC for preclinical TBI research to improve data sharing and scientific reproducibility, and will empower translational TBI research by identifying multidimensional features of TBI that best predict functional outcome.",Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility,10212363,F32NS117728,"['Address', 'Affect', 'Animal Model', 'Big Data', 'Biological', 'Biological Markers', 'Biomechanics', 'Brain region', 'Chronic', 'Clinic', 'Clinical', 'Closed head injuries', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Element', 'Data Science', 'Data Set', 'Development', 'Foundations', 'Goals', 'Heterogeneity', 'Incidence', 'Individual', 'Inflammation', 'Informatics', 'Injury', 'Institutes', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Multivariate Analysis', 'National Institute of Neurological Disorders and Stroke', 'Outcome', 'Pathway interactions', 'Pattern', 'Pharmacologic Substance', 'Population', 'Positioning Attribute', 'Pre-Clinical Model', 'Principal Component Analysis', 'Publishing', 'Reproducibility', 'Research', 'Severities', 'Standardization', 'Synaptic plasticity', 'Therapeutic', 'TimeLine', 'Translating', 'Translational Research', 'Translations', 'Traumatic Brain Injury', 'behavioral outcome', 'bench to bedside', 'biomarker discovery', 'controlled cortical impact', 'data curation', 'data framework', 'data harmonization', 'data sharing', 'disability', 'experimental study', 'functional outcomes', 'genetic manipulation', 'improved', 'insight', 'multidimensional data', 'multiple datasets', 'nerve injury', 'nervous system disorder', 'neuroinflammation', 'open data', 'patient subsets', 'pre-clinical', 'pre-clinical research', 'precision medicine', 'repository', 'response', 'response to injury']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2021,71224
"Large-scale data integration and harmonization to accurately predict sites facing future health-based drinking water crises Project summary: Up to 45 million people per year in the U.S. are directly impacted by health-based drinking water problems. This leads to at least 16 million cases of acute gastroenteritis directly linked to pollution at community water systems, with tens of millions more directly impacted by chemical and organic pollutants. Impacts are further exacerbated in locations dealing with water scarcity, in under-served populations, and within other vulnerable populations already suffering from health disparities. Many of these water problems are the direct result of managerial negligence, inconsistent monitoring, and a lack of the ability to anticipate where problems may arise next. While the reasons for drinking water problems are complex, if we could anticipate where health-based drinking water problems were to occur in the future, it could have an immediate and positive impact on tens of millions of Americans annually. Interestingly, extensive data about water quality and the performance of municipal water systems already exists in large, disparate databases. These databases are largely ignored and, when used, are typically used only anecdotally and retroactively. Preliminary evidence suggests that these existing databases, which contain histories of administrative violations and sub-threshold water-quality results, can be mined to accurately predict future drinking water crises. The Superior Statistical Research R&D team is an internationally recognized group of water experts with cross-cutting expertise in statistics/data analysis/modelling/computing, water-quality monitoring of biological and chemical contaminants, and the ability to clearly and compellingly translate water-quality and health information to actionable steps for individuals, organizations and communities. In this Phase I project, we will show that it is possible to predict water-related, health-based problem areas utilizing already collected, historical data on water quality and municipal water system performance. We will begin by harmonizing the disparate water quality and municipal water system performance in two different states (Michigan and Iowa). We will then utilize machine-learning techniques to predict health-based violation histories and will evaluate our methods by comparing predicted violations to actual health-based violations in the previous 5 years. Finally, we will identify at least 10 municipalities determined by our algorithm to be at the highest risk for future health- based water problems and will do systematic sampling to confirm our model-based predictions. We will then demonstrate how making these predictions can be leveraged to profitability by exploring how our model-based predictions can be presented to customers in an economical, usable form. Proof of our concept and profitability models in two states (Phase I) will set us up for widespread (multi-state) database harmonization and improvement of the proposed machine-learning/modelling effort in Phase II. With multi-state harmonized datasets, identification of key data gaps in particular states/areas, and proven financial models, our technology will ultimately lead to dramatic reductions in the number of health-based drinking water problems annually. Project Narrative Up to 45 million people per year in the U.S. are directly impacted by health-based drinking water problems, but predicting where and when these health-based drinking water problems will occur remains a large and complex obstacle. Current approaches focus on a reactive approach to health-based water-quality violations in community water systems, rather than a proactive one that seeks to anticipate where problems will occur in the future. The overall goal of this project is to leverage large and disparate historical datasets of water quality to accurately predict locations of future health-based water-quality violations, validate the predictions, and commercialize our proprietary predictions as a practical and cost-saving approach to anticipating and heading off future health-based water problems.",Large-scale data integration and harmonization to accurately predict sites facing future health-based drinking water crises,10253600,R43ES033134,"['Acute', 'Address', 'Algorithms', 'American', 'Area', 'Biological Monitoring', 'Chemicals', 'Cities', 'Coal', 'Communities', 'Community Surveys', 'Complex', 'Cost Savings', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Ensure', 'Exposure to', 'Filtration', 'Focus Groups', 'Future', 'Gastroenteritis', 'Goals', 'Government', 'Health', 'Human', 'Individual', 'International', 'Iowa', 'Lead', 'Lead levels', 'Link', 'Location', 'Machine Learning', 'Methods', 'Michigan', 'Modeling', 'Monitor', 'Municipalities', 'Negligence', 'Pathway interactions', 'Performance', 'Persons', 'Phase', 'Pollution', 'Price', 'Provider', 'Public Health', 'ROC Curve', 'Recording of previous events', 'Records', 'Research', 'Safety', 'Sampling', 'Serinus', 'Site', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Translating', 'Trust', 'Underserved Population', 'Vulnerable Populations', 'Water', 'advocacy organizations', 'base', 'commercialization', 'data harmonization', 'data integration', 'drinking water', 'economic impact', 'health disparity', 'high risk', 'improved', 'inner city', 'innovation', 'large scale data', 'member', 'pollutant', 'predictive modeling', 'research and development', 'rural area', 'statistics', 'water quality', 'water sampling', 'water testing', 'willingness to pay']",NIEHS,"SUPERIOR STATISTICAL RESEARCH, LLC",R43,2021,256579
"Methods for Evolutionary Genomics Analysis Summary/Abstract Continuing advances in nucleotide sequencing have resulted in the assembly of datasets containing large numbers of species, genes, and genomic segments. Phylogenomic analyses of these data are essential to progress in understanding evolutionary patterns across the tree of life, and are finding increasing numbers of applications in practical analyses that require understanding of how patterns change over time. The sheer size of phylogenomic datasets limits the practical utility of available methods due to excessive time and memory requirements. We have developed many high impact methods and tools for comparative analysis of molecular sequences, a tradition we propose to continue through this MIRA project by developing innovative methods that address new challenges in phylogenomics. We will focus on pattern-based approaches of machine learning with sparsity constraint (SL) applied to phylogenomics, as a complement to traditional model-based methods in molecular evolution and phylogenetics. In the proposed SL in Phylogenomics (SLiP) framework, we will build models that best explain the biological trait or evolutionary hypothesis of interest, with genomic loci, such as genes, proteins, and genomic segments, serving as model parameters. Preliminary results from two example applications establish the premise and promise of a general SLiP framework. In one, SLiP successfully detected loci whose inclusion in a phylogenomic dataset overtakes a consistent and contrasting signal from hundreds of other loci when inferring phylogenetic relationships. In the other example, SLiP revealed loci and biological functional categories that harbor convergent sequence evolutionary patterns associated with the emergence of the same trait in distinct evolutionary lineages. In all of these analyses, SLiP required only a small fraction of the computational time and memory demanded by traditional methods, and it enabled better evolutionary contrasts with fewer assumptions. Consequently, the successful development of SLiP will improve the feasibility, rigor, and reproducibility of large-scale data analysis. It will also democratize big data analytics via shortened analysis time and a relatively small memory footprint, and encourage the development of a new class of methods for phylogenomic analysis. This framework will be accessed from a free library of SLiP functions, which will be directly useable via command line and available in a graphical interface through integration with the MEGA software. Narrative The long-term goal of my research program is to develop methods and tools for comparative analysis of molecular sequences. In this project, we will develop a new class of phylogenomic methods based on sparse machine learning and benchmark their absolute and relative performance. New techniques and their software implementation will greatly facilitate data analyses that are vital for evolutionary and functional genomics.",Methods for Evolutionary Genomics Analysis,10086181,R35GM139540,"['Address', 'Benchmarking', 'Big Data Methods', 'Biological', 'Categories', 'Complement', 'Computer software', 'Data Analyses', 'Data Set', 'Development', 'Gene Proteins', 'Genes', 'Genomic Segment', 'Genomics', 'Goals', 'Libraries', 'Life', 'Machine Learning', 'Memory', 'Methods', 'Modeling', 'Molecular Analysis', 'Molecular Evolution', 'Nucleotides', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Reproducibility', 'Research', 'Signal Transduction', 'Techniques', 'Time', 'Trees', 'base', 'comparative', 'functional genomics', 'genomic locus', 'graphical user interface', 'improved', 'innovation', 'interest', 'large scale data', 'programs', 'tool', 'trait']",NIGMS,TEMPLE UNIV OF THE COMMONWEALTH,R35,2021,396250
"Experimentally guided modeling and simulation for cholera dynamics Project Summary/Abstract Coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), remains a global pandemic at present. Quantitative research is urgently needed to clarify the impacts of the current vaccination campaign on the pandemic evolution and economic growth, and to guide future policy development. The overall objective of this proposal is to establish a new computational modeling framework for an investigation of the COVID-19 vaccination campaign in the US, and to incorporate real data to assess the impacts of COVID-19 vaccination on public health and the economy. To achieve this objective, the team will pursue three specific aims: (1) Modeling the transmission and spread of COVID-19 under the impact of vaccination; (2) Modeling the economic impact of COVID-19 vaccination; (3) Conducting a case study for the Chattanooga region in the state of Tennessee. The proposed research is significant because it will incorporate detailed characteristics and potential limitations of the current vaccination campaign (such as the vaccine efficacy, phased allocation schemes, public resistance to vaccination, and vaccine breakthrough due to new variants of SARS- CoV-2) into a sophisticated modeling framework, which will enable us to make more accurate forecasts on the progression and long-term evolution of the pandemic. As such, the project is expected to advance the current understanding of COVID-19 transmission and to quantify the interaction between epidemic spreading, economic growth, and disease prevention and intervention under the impact of COVID-19 vaccination, all of which are important for the control and management of the pandemic. The approach is innovative in the development of a computational framework that integrates novel mechanistic and machine learning models and that connects the epidemic and economic aspects of COVID-19. The innovation of this project is also reflected by the integration of sophisticated computational modeling, rigorous mathematical analysis, intensive numerical simulation, and detailed data validation. The project represents an interdisciplinary collaboration among an applied and computational mathematician with long-term interest in infectious disease modeling (Wang), an epidemiologist with extensive working experiences at CDC and a current member of the regional COVID-19 task force (Heath), a business and management professor with a background in public heath (Mullen), and a statistician with expertise in machine learning and biomedical data analytics (Ma). The success of this project will not only build a solid knowledge base for the complex transmission dynamics of SARS-CoV-2 and the health and economic impacts of COVID-19 vaccination, but also provide important guidelines for the government agencies and public health administrations in pandemic management and policy development. Project Narrative The proposed project is relevant to public health because a deep understanding of the COVID-19 vaccination campaign and its health and economic impacts will help to inform the pandemic management and improve the current practice in disease prevention and intervention. The mathematical and machine learning models developed in this project will improve such understanding and make new knowledge discovery. This research effort aligns with part of NIH's mission to reduce public health burdens of infectious diseases.",Experimentally guided modeling and simulation for cholera dynamics,10376956,R15GM131315,"['2019-nCoV', 'Address', 'Advisory Committees', 'Attention', 'Businesses', 'COVID-19', 'COVID-19 vaccination', 'Case Study', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Cholera', 'Clinical Research', 'Collaborations', 'Communicable Diseases', 'Complement', 'Complex', 'Computer Models', 'Computer Simulation', 'Country', 'County', 'Coupled', 'Data', 'Data Analytics', 'Data Set', 'Development', 'Differential Equation', 'Economic Factors', 'Economic Models', 'Economics', 'Epidemic', 'Epidemiologist', 'Epidemiology', 'Evolution', 'Foundations', 'Future', 'Goals', 'Government Agencies', 'Growth', 'Guidelines', 'Health', 'Investigation', 'Joints', 'Knowledge Discovery', 'Machine Learning', 'Mathematics', 'Mission', 'Modeling', 'Persons', 'Phase', 'Policy Developments', 'Preventive Intervention', 'Public Health', 'Public Health Administration', 'Research', 'Resistance', 'Route', 'SARS-CoV-2 transmission', 'SARS-CoV-2 variant', 'Scheme', 'Schools', 'Science', 'Solid', 'Techniques', 'Tennessee', 'Theoretical Studies', 'Unemployment', 'United States National Institutes of Health', 'Vaccination', 'Vaccines', 'Validation', 'computer framework', 'disorder prevention', 'dynamic system', 'economic impact', 'economic indicator', 'experience', 'experimental study', 'health economics', 'improved', 'infectious disease model', 'innovation', 'interdisciplinary collaboration', 'interest', 'knowledge base', 'mathematical analysis', 'mathematical learning', 'mathematical model', 'member', 'models and simulation', 'novel', 'pandemic disease', 'professor', 'programs', 'simulation', 'success', 'transmission process', 'vaccine efficacy']",NIGMS,UNIVERSITY OF TENNESSEE CHATTANOOGA,R15,2021,122580
"Detection and characterization of critical under-immunized hotspots - Summer Undergraduate Support Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots - Summer Undergraduate Support,10393815,R01GM109718,"['Communicable Diseases', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Economic Burden', 'Economics', 'Epidemic', 'Geography', 'Growth', 'Health', 'Health Personnel', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Methodology', 'Methods', 'Modeling', 'Policy Maker', 'Population', 'Privatization', 'Public Health', 'Resource Allocation', 'Resources', 'Risk', 'Science', 'Source', 'System', 'Techniques', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Work', 'base', 'data mining', 'health care delivery', 'improved', 'novel', 'provider networks', 'statistics', 'undergraduate student']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2021,11253
