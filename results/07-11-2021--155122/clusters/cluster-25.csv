text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Real-time deep learning to improve speech intelligibility in noise Project Summary/Abstract  One in eight Americans has hearing loss, and this constitutes a major health and economic burden (Blackwell et al., 2014). The primary complaint of hearing-impaired (HI) listeners is difficulty understanding speech when background noise is present (see Dillon, 2012). While hearing aids (HAs) have improved in recent years, they still provide little benefit in noisy environments. For decades, a means of improving the ability to understand speech in background noise appeared unattainable, despite substantial amounts of research by both universities and HA companies. This changed when deep learning provided the first demonstration of a single-microphone algorithm that improves intelligibly in noise for HI listeners (Healy et al., 2013, 2014, 2015). Although this algorithm provides massive intelligibility improvements (even allowing listeners to improve intelligibility from floor to ceiling levels), it is currently not implemented to operate in real time and is therefore not suitable for implementation into HAs and cochlear implants (CIs). What is needed, therefore, is a highly effective noise-reduction algorithm that is capable of operating in real time. This project aims to address this critical need.  The long-term goal of the currently proposed project is to alleviate HI listeners’ predominant hearing handicap, which is difficulty understanding speech in background noise. The first aim introduces a new algorithm, based on a novel foundational scheme, that is designed to provide substantial benefit for any HI listener in real time. This algorithm will be well suited for implementation into HAs, CIs, and other face-to-face communication applications. The effectiveness of this new algorithm will be quantified using both HI and normal-hearing (NH) listeners. The second aim expands upon this new algorithm by modifying it to accept a small amount of future time-frame information, which could improve its noise-reduction performance but will introduce a brief processing delay. The rationale is that different devices have different allowable latencies. Face-to-face communication devices (HAs, CIs, etc.) have strict low-latency requirements, but other important communication systems (e.g., telephones) have different requirements. It is possible that the addition of future time-frame information within these requirements (up to 150 ms) will result in even better speech intelligibility. But the magnitude of any potential benefit is unknown. This critical information will be established currently. Using both HI and NH listeners, we will measure intelligibility for noisy sentences that have been processed using various amounts of future time information.  This comprehensive fellowship training plan will provide individualized, mentored research training from world-class faculty in a highly supportive and productive environment. The proposed work will endow the applicant with the skills needed to transition to the next stage of his research career, transform our treatment of hearing loss, and substantially impact quality of life for millions of Americans. Project Narrative An estimated 37.5 million Americans have hearing loss, which commonly leads to difficulty understanding speech in background noise. The proposed study will test a new noise-reduction system and improve our treatment of hearing loss.",Real-time deep learning to improve speech intelligibility in noise,10155960,F32DC019314,"['Address', 'Algorithms', 'American', 'Area', 'Auditory', 'Cellular Phone', 'Characteristics', 'Cochlear Implants', 'Communication', 'Complex', 'Data', 'Devices', 'Diagnosis', 'Economic Burden', 'Effectiveness', 'Environment', 'Equilibrium', 'Etiology', 'Faculty', 'Fellowship', 'Floor', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hearing', 'Hearing Aids', 'Human', 'Implant', 'Measures', 'Mentors', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Phase', 'Prevention', 'Process', 'Quality of life', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Scheme', 'Seminal', 'Signal Transduction', 'Speech', 'Speech Intelligibility', 'Strategic Planning', 'System', 'Telephone', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Videoconferencing', 'Work', 'artificial neural network', 'base', 'career', 'communication device', 'deep learning', 'deep neural network', 'design', 'experimental study', 'health economics', 'hearing impairment', 'hearing loss treatment', 'improved', 'microphone', 'network architecture', 'neural network', 'normal hearing', 'novel', 'novel strategies', 'operation', 'skills', 'speech in noise', 'wearable device']",NIDCD,OHIO STATE UNIVERSITY,F32,2020,76840,0.16707436561872907
"Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech Project Summary/Abstract  This proposal aims to employ the recent advancement of coupling fiberoptic endoscopes with high-speed videoendoscopy (HSV) systems to obtain HSV recordings during connected speech. The goal is to study vocal mechanisms underlying dysphonia in patients with neurogenic voice disorders. The long-term goal of this line of research is to create clinically applicable quantitative methods for functional measurement of vocal fold vibration in connected speech using innovative laryngeal imaging, an approach that could advance clinical voice assessment and treatment practice. In Aim 1, HSV-based measures of vocal fold kinematics will be developed and the influence of these measures on voice audio-perceptual qualities in the patients will be determined. Image processing techniques will be developed to extract such measures from the HSV data in connected speech. The extracted measures will be given as inputs to the statistical models to determine the source of the differences between the normal controls and the patients for different speech phonetic contexts and words. This aim provides an unbiased HSV-based method to predict voice quality. Developing such HSV-based methodology for functional laryngeal examination in connected speech can enhance clinical voice assessment. In addition, better understanding the influence of phonetic context would lead to optimizing the protocols for functional voice assessment through laryngeal imaging in connected speech. In Aim 2, machine learning approaches will be employed to discover hidden physics and unknown laryngeal mechanisms of voice production in the dysphonic patients. The findings of this project will help make necessary adjustments in biomechanical or physiological characteristics of vocal folds to enhance voice quality in patients with neurogenic voice disorders. Therefore, the outcome of this research will aid clinicians in properly selecting, and developing new treatment strategies (therapeutic, medicinal, or surgical), which are based on the gained knowledge of laryngeal mechanisms of dysphonia. The proposed research is in harmony with multiple priority areas of the NIDCD, described in the 2017-2021 Strategic Plan. Both aims support Priority 3 (improve methods of diagnosis, treatment, and prevention) through developing objective HSV-based measures and predicting the voice quality. Comparing laryngeal mechanisms in normal and disordered voices addresses Priority 1 (deepen our understanding of the normal function of the systems of human communication). Both aims propose to study laryngeal mechanisms in patients with neurogenic and functional voice disorders, which addresses Priority 2 (increase our knowledge about conditions that alter or diminish communication and health). Project Narrative  The goal of this proposal is to determine laryngeal mechanisms underlying dysphonia in connected speech, which will lead to development of clinically applicable quantitative methods for functional laryngeal examination in connected speech using laryngeal imaging. This can potentially result in enhancement of clinical voice assessment and development of new clinical voice management strategies to better help people with voice disorders.",Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech,9901502,K01DC017751,"['Acoustics', 'Address', 'Age', 'Area', 'Auditory', 'Behavior', 'Biomechanics', 'Categories', 'Characteristics', 'Clinical', 'Communication', 'Coupling', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dysphonia', 'Endoscopes', 'Evaluation', 'Functional disorder', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Knowledge', 'Larynx', 'Lead', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paralysed', 'Patients', 'Physics', 'Physiological', 'Prevention', 'Production', 'Protocols documentation', 'Research', 'Series', 'Severities', 'Source', 'Spastic Dysphonias', 'Speech', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Tremor', 'Visual', 'Voice', 'Voice Disorders', 'Voice Disturbances', 'Voice Quality', 'base', 'clinical application', 'clinical development', 'clinical practice', 'clinically relevant', 'cohort', 'flexibility', 'image processing', 'imaging approach', 'improved', 'innovation', 'kinematics', 'sex', 'temporal measurement', 'time use', 'tool', 'treatment strategy', 'vibration', 'vocal cord', 'vocalization']",NIDCD,MICHIGAN STATE UNIVERSITY,K01,2020,137795,0.2640792766808693
"Using Speech Acoustics to Reveal Motor Disruptions in Psychosis Project summary The goal of this project is to investigate the feasibility of using speech acoustics as a clinical biomarker in individuals at clinical high risk (CHR) for developing psychosis. There is evidence that disruptions to cortico-cerebellar circuits in individuals experiencing attenuated psychosis symptoms impact motor control of the face and limbs. This proposal would be the first study to examine whether these motor disruptions in high-risk populations also affect the complex motor control required for speech. In Aim 1 an instrumental approach will be used to investigate the acoustic correlates of psychosis risk. Specifically, speech data will be collected to investigate fine-grained acoustic properties of vowels and consonants in simple repetition tasks as well as during more naturalistic conversational speech. The speech of CHR young adults will be compared to age-matched healthy controls to discover if there are group differences in the speech acoustics that allow us to classify speech samples into healthy and clinical groups. To enable fast, reliable analysis, machine learning-based algorithms will be used to measure the acoustics speech properties of interest. In Aim 2, the speech properties measured in Aim 1 will be compared to other behavioral measures, in order to discover if they correlate with several measures of cerebellar dysfunction (posture control, procedural learning, and motor timing) that are known to occur in CHR individuals. These measures will provide convergent validity for these novel speech measures. Cognitive capabilities which are related and unrelated to speech and motor control will also be assessed, to provide specificity and divergent validity to these measures. In Aim 3, the links between speech features and changes in symptom severity will be assessed at two time points, connecting changes in speech motor control to longitudinal changes in the progression of the symptoms over 12 months. These investigations may reveal speech as a novel and easily-collected biomarker enabling early detection of psychosis risk. Project narrative The goal of this proposal is to determine whether speech patterns can signal vulnerability to psychotic disorders such as schizophrenia. Speech samples will be collected from high-risk and matched healthy control participants in order to: determine if there are abnormalities in the acoustics of vowels and consonants; evaluate if these properties map on to dysfunction of the cerebellum (a brain region impacted in the development of psychosis that also plays a role in speech motor control); and relate these properties to symptom severity and illness progression. This study will lay the groundwork for the use of speech as an inexpensive, non-invasive, and mechanistically-relevant metric that will ultimately support earlier identification and facilitate timely treatment.",Using Speech Acoustics to Reveal Motor Disruptions in Psychosis,9898478,R21MH119677,"['Acoustics', 'Address', 'Affect', 'Age', 'Algorithms', 'Articulators', 'Attenuated', 'Behavioral', 'Biological Markers', 'Brain region', 'Cerebellar Diseases', 'Cerebellum', 'Clinical', 'Cognitive', 'Complex', 'Computers', 'Control Groups', 'Cueing for speech', 'Data', 'Development', 'Diagnosis', 'Dyskinetic syndrome', 'Early Diagnosis', 'Early Intervention', 'Early identification', 'Ensure', 'Equilibrium', 'Etiology', 'Face', 'Fingers', 'Functional disorder', 'Goals', 'Grain', 'Individual', 'Intervention', 'Interview', 'Investigation', 'Larynx', 'Learning', 'Limb structure', 'Linguistics', 'Link', 'Lip structure', 'Literature', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Motor', 'Movement', 'Musculoskeletal Equilibrium', 'National Institute of Mental Health', 'Neurologic', 'Participant', 'Pathogenicity', 'Pattern', 'Play', 'Population', 'Positioning Attribute', 'Posture', 'Process', 'Production', 'Property', 'Psychotic Disorders', 'Research', 'Research Personnel', 'Risk', 'Role', 'Roter', 'Sampling', 'Schizophrenia', 'Scientist', 'Sensory', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Tongue', 'Variant', 'Youth', 'automated algorithm', 'base', 'behavior measurement', 'clinical biomarkers', 'clinical predictors', 'experience', 'high risk', 'high risk population', 'improved', 'indexing', 'individualized medicine', 'interest', 'motor behavior', 'motor control', 'motor deficit', 'motor disorder', 'motor learning', 'novel', 'novel marker', 'potential biomarker', 'relating to nervous system', 'sensory integration', 'tool', 'vocal control', 'young adult']",NIMH,NORTHWESTERN UNIVERSITY,R21,2020,185000,0.34367073673259046
"Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders Abstract: Early and accurate diagnosis of neurocognitive disorders (NCDs) is critical for planning, treatment, and research referral, but demands time and expertise often unavailable to primary care providers. Speech and language are often impaired early in the disease course of several NCDs. Previous research has demonstrated the diagnostic potential of computer speech analysis (CSA), with differences between healthy controls and disorders such as mild cognitive impairment (MCI) and Alzheimer's disease. However, there are several additional steps that must be taken to make CSA a diagnostically viable screening tool. This proposal includes a career development plan providing the applicant with training, mentorship, and experience in the following areas in order to bring CSA techniques into clinical practice: 1) computational linguistics and paralinguistics, 2) longitudinal markers of disease, and 3) design of novel technology for dissemination. As part of this training, academic and professional skills, including ethics in research, will also be expanded. Uniquely qualified mentorship and advisory teams have been selected to ensure the success of the proposed training and research. The proposed study is a prospective, longitudinal, observational, cohort investigation of two distinct research groups. The first group is a highly selected and well-characterized research cohort of healthy control, Alzheimer's disease, and MCI subjects (Group A). In Group A, the performance and reproducibility of a machine learning algorithm will be improved to distinguish Alzheimer's disease and MCI from healthy controls using CSA. Multiple regression and voxel-based morphometry will be used to better understand what may drive group differences in CSA measures in Group A as well. Clinical applications of this algorithm will then be assessed in a clinic-based cohort of patients with different NCDs (Group B) in order reduce spectrum bias likely present in prior studies. As sub-aims in both groups, possible further improvement of the algorithmic outcomes with longitudinal CSA measures will also be examined. The overall objective is to develop intuitive, reliable and reproducible CSA-based clinical measures by correlating them with established neuropsychiatric and imaging markers, determining their efficacy in clinical populations, and determining how they change over time. As a result, this research will validate specific speech traits as useful diagnostic markers of neurocognitive disease and explain why those markers differ between patient groups, both of which are major steps towards the design of novel and easily implemented tools in the screening of NCDs such as Alzheimer's disease. PROJECT NARRATIVE Computational speech analysis (CSA) has shown promise as a cost-effective, rapid screening for patients with neurocognitive disorders (NCDs) by objectively and automatically quantifying speech and language use; however, critical steps must be taken before these measures can become clinically useful. I have training and experience in the neurology of speech and language, but require additional training in computational linguistics and paralinguistics, longitudinal markers of disease including neuroimaging and neuropsychological measures, and design of novel technology for dissemination in order to bring CSA into clinical practice. In this project, we propose to investigate the utility of using CSA measures in two distinct patient groups, including a highly characterized group of research participants that includes healthy controls, Alzheimer's disease patients, and mild cognitive impairment patients (Group A), and a group of consented clinic patients with different NCDs (Group B) and to follow these two groups in prospective, longitudinal studies to correlate spontaneous speech measures with standardized linguistic, neuropsychological, and biological measures.",Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders,9975566,K23AG063900,"['Accent', 'Address', 'Adult', 'Advisory Committees', 'Algorithms', 'Alzheimer disease screening', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease pathology', 'Alzheimer&apos', 's disease patient', 'Area', 'Biological', 'Brain', 'Caregivers', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Computational Linguistics', 'Computers', 'Consent', 'Cross-Sectional Studies', 'Dementia', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Diagnostic Sensitivity', 'Disease', 'Disease Marker', 'Early Diagnosis', 'Effectiveness', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'Fostering', 'Frontotemporal Dementia', 'Goals', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Intuition', 'Investigation', 'Knowledge', 'Language', 'Language Tests', 'Lead', 'Lewy Body Dementia', 'Linguistics', 'Liquid substance', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Memory', 'Mentorship', 'Neurocognitive', 'Neurology', 'Neuropsychological Tests', 'Neuropsychology', 'Outcome', 'Participant', 'Patients', 'Performance', 'Population', 'Positioning Attribute', 'Preparation', 'Prevalence', 'Primary Health Care', 'Quality of life', 'Reproducibility', 'Research', 'Screening procedure', 'Speech', 'Standardization', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'accurate diagnosis', 'aging population', 'base', 'care providers', 'career development', 'clinical application', 'clinical practice', 'cohort', 'cost effective', 'design', 'diagnostic biomarker', 'experience', 'healthy aging', 'imaging biomarker', 'improved', 'machine learning algorithm', 'mild cognitive impairment', 'morphometry', 'neurocognitive disorder', 'neuroimaging', 'neuropsychiatry', 'new technology', 'novel', 'novel diagnostics', 'novel therapeutics', 'patient screening', 'primary outcome', 'prospective', 'recruit', 'screening', 'skills', 'success', 'technology development', 'tool', 'trait']",NIA,UNIVERSITY OF COLORADO DENVER,K23,2020,188384,0.12323264903505086
"Speech markers of cognitive impairment in Parkinson's disease ABSTRACT Dr. Kara Smith is a Movement Disorders neurologist at the University of Massachusetts Medical School (UMMS) whose goal is to become an independent investigator focused on early cognitive impairment in Parkinson disease (PD). Her long-term goal is to develop speech markers of cognitive impairment in PD. Cognitive impairment occurs in the majority of PD patients, leading to increased mortality and decreased quality of life. The current diagnostic tools are resource-intensive and have limited sensitivity. Treatments are often offered late in the course of cognitive decline and do not provide optimal benefit. Speech markers could improve detection, monitoring and treatment of cognitive impairment in PD. Speech markers could be monitored frequently and remotely via mobile technology, capturing sensitive, quantitative data about cognitive function in the context of patients’ daily life and in response to therapeutics. Dr. Smith’s role as a clinical movement disorders specialist ideally positions her to lead the application of advanced speech and language research to feasible, patient-oriented tools for real-life clinical practice and clinical trials. Dr. Smith has assembled an expert interdisciplinary mentorship team ideally suited for the goals of this innovative proposal. Dr. Smith and her team have previously shown that a) speech acoustic markers are associated with cognitive function in non-demented PD patients, and b) PD patients with mild cognitive impairment had linguistic deficits including pauses within utterances and grammaticality. Building on these results, Dr. Smith proposes to study speech and language more comprehensively in PD patients with and without mild cognitive impairment and controls to confirm these preliminary results and identify additional biomarkers. The aims of this study will be 1) to develop algorithms using speech acoustic markers to categorize by cognitive status, 2) to identify linguistic markers associated with mild cognitive impairment in PD, and 3) to assess on-line syntactic processing in PD subjects with mild cognitive impairment. The overall goal of the proposal is to identify speech and language markers of early cognitive dysfunction that can be further refined, validated and implemented using mobile technology into a larger scale, longitudinal R01 proposal. Further work will also address the underlying neurobiological mechanisms of these speech markers. Dr. Smith’s rigorous training plan includes a Master’s degree, linguistics and speech motor physiology courses, and experience in signal processing and speech acoustic analysis. Through her training goals, she will advance her knowledge and skills in patient-centered outcomes measures and instrument validation. She will gain experience in research leadership, presentation and dissemination of scientific work, and in grant writing, culminating in an R01 proposal. This K23 award will be critical for Dr. Smith to establish an independent career as a PD clinician-scientist at the unique intersection of speech and language science and cognitive impairment. PUBLIC HEALTH RELEVANCE: Speech markers have the potential to improve diagnosis, monitoring and treatment of cognitive impairment in Parkinson’s disease (PD). Although the majority of PD patients will develop cognitive impairment, the tools available to assess and treat this disabling complication are fraught with limitations. As a detailed and quantitative assessment tool, speech markers may increase sensitivity to early cognitive dysfunction and to changes over time compared with current measures. They may be automated and then implemented through mobile technology to increase patients’ access to cognitive symptom monitoring outside of the clinic setting. Dr. Smith’s proposed career development plan has potential to fill a major gap in PD research by making inexpensive and easy-to-use cognitive assessment tools accessible to patients in rural and international settings, and by fueling clinical trials to discover new therapeutics capable of slowing cognitive decline in PD.",Speech markers of cognitive impairment in Parkinson's disease,9883772,K23DC016656,"['Acoustics', 'Address', 'Algorithms', 'American', 'Area', 'Articulation', 'Assessment tool', 'Award', 'Biological Markers', 'Biomedical Engineering', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Cognitive Therapy', 'Cognitive deficits', 'Complication', 'Comprehension', 'Data', 'Data Analyses', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Foundations', 'Future', 'Goals', 'Grant', 'Health Services Accessibility', 'Impaired cognition', 'Impairment', 'Individual', 'International', 'Knowledge', 'Language', 'Language Disorders', 'Lead', 'Leadership', 'Life', 'Linguistics', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Master&apos', 's Degree', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Nerve Degeneration', 'Neurobehavioral Manifestations', 'Neurobiology', 'Neurologist', 'Neuropsychological Tests', 'Outcome', 'Outcome Measure', 'Outcomes Research', 'Parkinson Disease', 'Parkinson&apos', 's Dementia', 'Participant', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Positioning Attribute', 'Production', 'Proxy', 'Quality of Care', 'Quality of life', 'Research', 'Research Personnel', 'Resources', 'Role', 'Rural', 'Science', 'Scientist', 'Severities', 'Specialist', 'Speech', 'Speech Acoustics', 'Symptoms', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Universities', 'Validation', 'Work', 'Writing', 'career', 'career development', 'clinical movement disorder', 'clinical practice', 'cognitive change', 'cognitive control', 'cognitive function', 'cognitive impairment in Parkinson&apos', 's', 'cognitive performance', 'cognitive testing', 'common symptom', 'experience', 'handheld mobile device', 'improved', 'innovation', 'instrument', 'language processing', 'large scale data', 'lexical retrieval', 'machine learning method', 'medical schools', 'mild cognitive impairment', 'mobile computing', 'mortality', 'motor deficit', 'neurobiological mechanism', 'non-demented', 'novel', 'novel therapeutics', 'patient oriented', 'public health relevance', 'recruit', 'response', 'screening', 'signal processing', 'skills', 'syntax', 'tool']",NIDCD,UNIV OF MASSACHUSETTS MED SCH WORCESTER,K23,2020,189216,0.18848978455240176
"SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline Early detection of the cognitive decline involved in Alzheimer's Disease and Related Dementias (ADRD)  in older adults living alone is essential for developing, planning, and initiating interventions and support  systems to improve patients' everyday function and quality of life. Conventional, clinic-based methods for  early diagnosis are expensive, time-consuming, and impractical for large-scale screening. This project aims to develop a low-cost, passive, and practical home-based assessment method using Voice Assistant  Systems (VAS) for early detection of cognitive decline, including a set of novel data mining techniques for  sparse time-series speech. The project has three specific aims: 1. Using a recurrent neural network  (RNN) and a softmax regression model, we will develop a transfer learning technique to investigate the  link between the speech from in-lab VAS tasks and cognitive decline. The Pitt corpus from the  DementiaBank database will be used to optimize the RNN parameters and thereby overcome the limited  data problem of VAS. The softmax regression model will allow us to align the feature distributions from the  previous speech data and in-lab VAS speech. 2. We will develop a novel ""many-to-difference"" prediction  model with a symmetric RNN structure to predict the cognitive difference at two ends of a time period from  the sparse time-series data. The proposed model is different from previous ones as the learning focus is  shifted from the short-term pattern differences across users to the pattern difference over time for an  individual user. The proposed model accommodates well for the highly dynamic nature of the inputs and  maximally removes individual characteristics from the prediction result. To analyze the sparse time-series  speech, a new data sampling technique will be used to address the imbalanced data problem, and a data  quality metric will be developed for the proposed model. 3. The team will conduct an 18-month in-lab  evaluation and a 28-month in-home evaluation with a focus on whether the VAS tasks and features from  the in-lab evaluation and the repetition features of the in-home VAS data can measure and predict  cognitive decline in the in-home participants over time. The proposed methods will be integrated into an  interactive system to enable efficient communication on cognitive decline among patients, caregivers, and  clinicians. If successful, the outcomes of this project will provide an opportunity to provide supportive  evidence to clinicians for the early detection of cognitive impairment outside of a clinic-based setting. This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice  Assistant Systems (VAS) for early detection of cognitive decline. If successful, the proposed system may  be widely disseminated for the early diagnosis of cognitive impairment to complement existing diagnostic  modalities that could ultimately enable long-term patient and caregiver planning to maintain individual's  independence at home.",SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline,10019452,R01AG067416,"['Address', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Characteristics', 'Clinic', 'Cognition', 'Cognitive', 'Communication', 'Complement', 'Consumption', 'Custom', 'Data', 'Databases', 'Diagnostic', 'Early Diagnosis', 'Elderly', 'Evaluation', 'Home environment', 'Human', 'Impaired cognition', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Measures', 'Methods', 'Modality', 'Modeling', 'Nature', 'Neural Network Simulation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Population', 'Psychological Transfer', 'Quality of life', 'Research', 'Sampling', 'Series', 'Speech', 'Structure', 'Support System', 'System', 'Techniques', 'Time', 'Visit', 'Voice', 'base', 'cognitive testing', 'cost', 'data mining', 'data quality', 'improved', 'novel', 'predictive modeling', 'recurrent neural network', 'screening']",NIA,UNIVERSITY OF MASSACHUSETTS BOSTON,R01,2020,296138,0.104220840969218
"Temporal Pattern Perception Mechanisms for Acoustic Communication Project Summary/Abstract: Processing acoustic communication signals is among the most difficult yet vital abilities of the auditory system. These abilities lie at the heart of language and speech processing, and their success or failure has profound impacts on quality of life across the lifespan. Understanding the neurobiological mechanisms that support these basic abilities holds promise for advancing assistive listening devices, and for improving diagnoses and treatments for learning disabilities and communication disorders, such as auditory processing disorder, dyslexia, and specific language impairment. Non-invasive neuroscience techniques in humans reveal the loci of language-related processing but do not answer how individual neurons and neural circuits implement language-relevant computations. Thus, circuit-level neuro-computational mechanisms that support acoustic communication signal processing remain poorly understood. Multiple lines of research suggest that songbirds can provide an excellent model for investigating shared auditory processing abilities relevant to language. This proposal investigates neural mechanisms of auditory temporal pattern processing abilities shared between songbirds and humans. In Aim 1, we test the cellular-level predictions of a powerful modelling framework, called predictive coding, proposed as a general computational mechanism to support the learned recognition of complex temporally patterned signals at multiple timescales. We combine state-of-the-art machine learning methods with multi-electrode electrophysiology, to test explicit models for natural stimulus representation, prediction, and error coding in single cortical neurons and neural populations. One aspect of auditory perception integral to speech is the discretization of the signal into learned categorically perceived sounds (phonemes). In Aim 2, we use the predictive coding framework to investigate the learned categorical perception of natural auditory categories in populations of cortical neurons. In humans, the transition statistics between adjacent phonemes can aid or alter phoneme categorization, providing cues for language learners and listeners to disambiguate perceptually similar sounds. Aim2 also examines how categorical neural representations are affected by temporal context. In addition to which phonemes occur in a sequence, speech processing also requires knowing where those elements occur. Sensitivities to the statistical regularities of speech sequences are established long before infants learn to speak, and continue to affect both recognition and comprehension throughout adulthood. Songbirds also attend to the statistical regularities in their vocal communication signals. In Aim 3, we focus on how sequence-specific information is encoded by single neurons and neural populations in auditory cortex. The proposed approach permits progress in the near term towards establishing the basic neurobiological substrates of foundational language-relevant abilities and a general framework within which more complex, uniquely human processes, can be proposed and eventually tested. Project Narrative: Normal speech comprehension and communication are rooted in basic auditory cognitive processes. Deficits in these processes accompany severe communication disorders (e.g. auditory processing disorder, dyslexia, specific language impairment, autism), and their abnormal function can compound the negative impacts of hearing impairments. The proposed project investigates the neuronal mechanisms of auditory temporal pattern processing using natural communication signals, with the ultimate goal of understanding the neurobiological basis of language-relevant abilities and improving treatment of communication processing disorders.",Temporal Pattern Perception Mechanisms for Acoustic Communication,9939507,R01DC018055,"['Acoustics', 'Adult', 'Affect', 'Algorithms', 'Animal Model', 'Auditory', 'Auditory Perception', 'Auditory Perceptual Disorders', 'Auditory area', 'Auditory system', 'Behavior', 'Behavioral', 'Biological Models', 'Categories', 'Code', 'Cognition Disorders', 'Cognitive', 'Communication', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Models', 'Cues', 'Data', 'Detection', 'Diagnosis', 'Disease', 'Dyslexia', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Failure', 'Foundations', 'Generations', 'Goals', 'Grouping', 'Hearing Aids', 'Heart', 'Human', 'Individual', 'Infant', 'Language', 'Language Development', 'Learning', 'Learning Disabilities', 'Longevity', 'Machine Learning', 'Modality', 'Modeling', 'Neurobiology', 'Neurons', 'Neurosciences', 'Parietal', 'Pattern', 'Perception', 'Physiological', 'Plant Roots', 'Population', 'Process', 'Quality of life', 'Research', 'Sensory', 'Signal Transduction', 'Songbirds', 'Speech', 'Speech Perception', 'Speech Sound', 'Stimulus', 'Stream', 'Sturnus vulgaris', 'Superior temporal gyrus', 'Techniques', 'Testing', 'Time', 'Work', 'auditory processing', 'autism spectrum disorder', 'bird song', 'cognitive process', 'computer framework', 'experience', 'experimental study', 'hearing impairment', 'improved', 'language comprehension', 'language processing', 'learned behavior', 'machine learning method', 'model development', 'neural circuit', 'neurobiological mechanism', 'neuromechanism', 'pattern perception', 'relating to nervous system', 'response', 'sensory input', 'sensory system', 'signal processing', 'sound', 'spatiotemporal', 'specific language impairment', 'speech processing', 'speech recognition', 'statistics', 'success']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,334688,0.1579475033884586
"CRCNS: Avian Model for Neural Activity Driven Speech Prostheses  Understanding the physical, computational, and theoretical bases of human vocal communication, speech, is crucial to improved comprehension of voice, speech and language diseases and disorders, and improving their diagnosis, treatment and prevention. Meeting this challenge requires knowledge of the neural and sensorimotor mechanisms of vocal motor control. Our project will directly investigate the neural and sensorimotor mechanisms involved in the production of complex, natural, vocal communication signals. Our results will directly enhance brain-computer interface technology for communication and will accelerate the development of prostheses and other assistive technologies for individuals with communications deficits due to injury or disease. We will develop a vocal prosthetic that directly translates neural signals in cortical sensorimotor and vocal-motor control regions into vocal communication signals output in real-time. Building on success using non-human primates for brain computer interfaces for general motor control, the prosthetic will be developed in songbirds, whose acoustically rich, learned vocalizations share many features with human speech. Because the songbird vocal apparatus is functipnally and anatomically similar to the human larynx, and the cortical regions that control it are closely analogous to speech motor-control areas of the human brain, songbirds offer an ideal model for the proposed studies. Beyond the application of our work to human voice and speech, development of the vocal prosthetic will enable novel speech-relevant studies in the songbird model that can reveal fundamental mechanisms of vocal learning and production. In the first stage of the project, we collect a large data set of simultaneously recorded neural activity and vocalizations. In stage two, we will apply machine learning and artificial intelligence techniques to develop algorithms that map neural recordings to vocal output and enable us to estimate intended vocalizations directly from neural data. In stage three, we will develop computing infrastructure to run these algorithms in real-time, predicting intended vocalizations from neural activity as the animal is actively producing these vocalizations. In stage four, we will test the effectiveness of the prosthetic by substituting the bird's own vocalization with the output from our prosthetic system. Success will set the stage for testing of these technologies in humans and translation to multiple assistive devices. In addition to our research goals, the project will engage graduate, undergraduate, and high school students through the development of novel educational modules that introduce students to brain machine interface and multidisciplinary studies that span engineering and the basic sciences. RELEVANCE (See instructions): Developing a vocal prosthesis will directly enhance brain-computer interface technology for communication and accelerate the realization of prostheses and other assistive technologies for individuals with communications deficits due to injury or disease. The basic knowledge of the neural and sensorimotor mechanisms of vocal motor control acquired will impact understanding of multiple voice, speech, and language diseases and disorders. The techniques developed will enabling novel future studies of vocal production and development. n/a",CRCNS: Avian Model for Neural Activity Driven Speech Prostheses ,9981725,R01DC018446,"['Acoustics', 'Algorithms', 'Anatomy', 'Animal Model', 'Animals', 'Area', 'Artificial Intelligence', 'Basic Science', 'Behavior', 'Behavioral', 'Birds', 'Brain', 'Cell Nucleus', 'Characteristics', 'Clinical Research', 'Clinical assessments', 'Communication', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computer Interface', 'Computer software', 'Computers', 'Course Content', 'Data', 'Data Science', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Educational Materials', 'Educational workshop', 'Electrodes', 'Engineering', 'Evaluation', 'Feedback', 'Finches', 'Future', 'Generations', 'Goals', 'High School Outreach', 'High School Student', 'Human', 'Implant', 'Individual', 'Infrastructure', 'Injury', 'Instruction', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Learning Module', 'Limb Prosthesis', 'Limb structure', 'Limes', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neurodegenerative Disorders', 'Neurosciences', 'Outcome Measure', 'Output', 'Patients', 'Performance', 'Play', 'Prevention', 'Principal Investigator', 'Production', 'Prosthesis', 'Quadriplegia', 'Recording of previous events', 'Research', 'Robot', 'Role', 'Running', 'Self-Help Devices', 'Signal Transduction', 'Songbirds', 'Space Models', 'Speech', 'Speech Development', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Translations', 'Upper Extremity', 'Voice', 'Work', 'auditory feedback', 'base', 'bird song', 'brain computer interface', 'brain machine interface', 'data sharing', 'design', 'effectiveness testing', 'functional electrical stimulation', 'functional restoration', 'graduate student', 'hackathon', 'high school', 'human subject', 'improved', 'large datasets', 'machine learning algorithm', 'meetings', 'mind control', 'model development', 'motor control', 'multidisciplinary', 'neural model', 'neural prosthesis', 'neurodevelopment', 'neurophysiology', 'neurotransmission', 'nonhuman primate', 'novel', 'open source', 'operation', 'programs', 'relating to nervous system', 'repository', 'response', 'signal processing', 'success', 'undergraduate student', 'vocal learning', 'vocalization', 'web site']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,344725,0.21118300955093264
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through “part-of-speech” tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (“which”, “what”, “that”). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis – semantics and syntax – that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9859468,R01MH115332,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Mood Disorders', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Techniques', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'functional outcomes', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,533776,0.2722094767620354
"A holistic approach to identifying functional units of tongue motion during speech PROJECT SUMMARY  Oral cancers have the seventh highest incidence, with roughly 51,540 new cases and 10,030 cancer- related deaths expected to occur in 2018. Although a variety of treatment methods are available, the death rate is higher than that for most cancers with five-year rates of about 50 percent. The most frequently used treatment method, glossectomy surgery, involves the surgical removal of tumors and surrounding tissues, and the addition of grafted tissues, often followed by radiotherapy. Although tongue cancer and its treatment have debilitating effects on speech, the impact of varying degrees of resection and reconstruction on the formation of functional units in speech has remained poorly understood. In order to produce intelligible speech, a variety of local muscle groupings of the tongue—i.e., functional units—emerge and recede rapidly and nimbly in a highly coordinated fashion. Therefore, understanding the formation of functional units that are critical for speech production can provide substantial insights into normal, pathological, and adapted motor control strategies in controls and patients with tongue cancer for novel therapeutic, surgical, and rehabilitative strategies. One of the critical challenges in pre-operative surgical and treatment planning, as well as in post- operative evaluation for tongue cancer is the difficulty in developing objective and quantitative measures and in evaluating their functional outcome predictability. To address this, in this proposal, three integrated approaches will be used in in vivo tongue motion during speech to seamlessly identify the functional units and associated quantitative measures: multimodal MRI methods, multimodal deep learning, and biomechanical simulations. This will provide a convergent approach, thereby allowing us to (1) test hypotheses about the spatiotemporal basis of muscle coordination in a consilient way, and (2) develop objective quantitative measures that are required for understanding the complex biomechanical system as well as for predicting the functional outcomes after various reconstruction methods. The first proof of concept study published by the PI and the team identified the functional units of speech tasks using the sparse non-negative matrix factorization framework, in which the magnitude and angle of displacements from tagged MRI were used as our input quantities. With these advances in place, we will further incorporate muscle fiber anatomy from diffusion MRI and motion tracking from tagged MRI into our framework to yield physiologically and anatomically meaningful functional units. In addition, we will create a completely novel and integrated way of directly relating the functional units to tongue muscle anatomy, learning joint representation via a multimodal deep learning technique, and linking them to biomechanical simulations. Furthermore, 3D and 4D atlases will be utilized to identify objective and quantitative measures based on our functional units analysis. Taken together, the successful implementation of our integrated framework will identify functional units that can be used for research on tongue motion, for surgical planning, and for diagnosis, prognosis, and rehabilitation in a range of speech-related disorders. PROJECT NARRATIVE  Tongue cancer and its treatment affect tongue structure and function, yet little is known about how the changes in tongue structure due to varying degrees of resection and reconstruction affect the formation of functional units of tongue motion during speech. We propose to use novel integrated platform tools to identify functional units seamlessly with unprecedented resolution and precision. Upon success of this proposal, our integrated framework has the potential to aid in an increased understanding of speech motor control strategies in healthy controls and the patient group, thereby benefiting patients through improved diagnosis, treatment, and rehabilitative strategies.",A holistic approach to identifying functional units of tongue motion during speech,9937181,R01DC018511,"['3-Dimensional', 'Acoustics', 'Address', 'Affect', 'Aftercare', 'Anatomy', 'Atlases', 'Behavior', 'Biomechanics', 'Cessation of life', 'Clinical', 'Complex', 'Computing Methodologies', 'Data', 'Death Rate', 'Deglutition', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Evaluation', 'Excision', 'Exhibits', 'Fiber', 'Geometry', 'Glossectomy', 'Goals', 'Grouping', 'Impairment', 'Incidence', 'Joints', 'Knowledge', 'Learning', 'Link', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Muscle', 'Muscle Fibers', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patients', 'Physiological', 'Postoperative Period', 'Predictive Value', 'Procedures', 'Production', 'Proxy', 'Publishing', 'Radiation therapy', 'Rehabilitation therapy', 'Research', 'Resolution', 'Speech', 'Speech Intelligibility', 'Standardization', 'Structure', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Tissue Grafts', 'Tissues', 'Tongue', 'Weight', 'Work', 'base', 'biomechanical model', 'clinical practice', 'deep learning', 'functional outcomes', 'holistic approach', 'improved', 'in vivo', 'insight', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'motor control', 'multimodality', 'muscular structure', 'novel', 'novel therapeutics', 'outcome forecast', 'outcome prediction', 'reconstruction', 'rehabilitation strategy', 'signal processing', 'spatiotemporal', 'success', 'tool', 'treatment planning', 'treatment strategy', 'tumor']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,588403,0.16904219937944515
"Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech Project Summary/Abstract  This proposal aims to employ the recent advancement of coupling fiberoptic endoscopes with high-speed videoendoscopy (HSV) systems to obtain HSV recordings during connected speech. The goal is to study vocal mechanisms underlying dysphonia in patients with neurogenic voice disorders. The long-term goal of this line of research is to create clinically applicable quantitative methods for functional measurement of vocal fold vibration in connected speech using innovative laryngeal imaging, an approach that could advance clinical voice assessment and treatment practice. In Aim 1, HSV-based measures of vocal fold kinematics will be developed and the influence of these measures on voice audio-perceptual qualities in the patients will be determined. Image processing techniques will be developed to extract such measures from the HSV data in connected speech. The extracted measures will be given as inputs to the statistical models to determine the source of the differences between the normal controls and the patients for different speech phonetic contexts and words. This aim provides an unbiased HSV-based method to predict voice quality. Developing such HSV-based methodology for functional laryngeal examination in connected speech can enhance clinical voice assessment. In addition, better understanding the influence of phonetic context would lead to optimizing the protocols for functional voice assessment through laryngeal imaging in connected speech. In Aim 2, machine learning approaches will be employed to discover hidden physics and unknown laryngeal mechanisms of voice production in the dysphonic patients. The findings of this project will help make necessary adjustments in biomechanical or physiological characteristics of vocal folds to enhance voice quality in patients with neurogenic voice disorders. Therefore, the outcome of this research will aid clinicians in properly selecting, and developing new treatment strategies (therapeutic, medicinal, or surgical), which are based on the gained knowledge of laryngeal mechanisms of dysphonia. The proposed research is in harmony with multiple priority areas of the NIDCD, described in the 2017-2021 Strategic Plan. Both aims support Priority 3 (improve methods of diagnosis, treatment, and prevention) through developing objective HSV-based measures and predicting the voice quality. Comparing laryngeal mechanisms in normal and disordered voices addresses Priority 1 (deepen our understanding of the normal function of the systems of human communication). Both aims propose to study laryngeal mechanisms in patients with neurogenic and functional voice disorders, which addresses Priority 2 (increase our knowledge about conditions that alter or diminish communication and health). Project Narrative  The goal of this proposal is to determine laryngeal mechanisms underlying dysphonia in connected speech, which will lead to development of clinically applicable quantitative methods for functional laryngeal examination in connected speech using laryngeal imaging. This can potentially result in enhancement of clinical voice assessment and development of new clinical voice management strategies to better help people with voice disorders.",Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech,9720522,K01DC017751,"['Acoustics', 'Address', 'Age', 'Area', 'Auditory', 'Behavior', 'Biomechanics', 'Categories', 'Characteristics', 'Clinical', 'Communication', 'Coupling', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dysphonia', 'Endoscopes', 'Evaluation', 'Functional disorder', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Knowledge', 'Larynx', 'Lead', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paralysed', 'Patients', 'Physics', 'Physiological', 'Prevention', 'Production', 'Protocols documentation', 'Research', 'Series', 'Severities', 'Source', 'Spastic Dysphonias', 'Speech', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Tremor', 'Visual', 'Voice', 'Voice Disorders', 'Voice Disturbances', 'Voice Quality', 'base', 'clinical application', 'clinical development', 'clinical practice', 'clinically relevant', 'cohort', 'flexibility', 'image processing', 'imaging approach', 'improved', 'innovation', 'kinematics', 'sex', 'temporal measurement', 'time use', 'tool', 'treatment strategy', 'vibration', 'vocal cord', 'vocalization']",NIDCD,MICHIGAN STATE UNIVERSITY,K01,2019,137795,0.2640792766808693
"Speech markers of cognitive impairment in Parkinson's disease ABSTRACT Dr. Kara Smith is a Movement Disorders neurologist at the University of Massachusetts Medical School (UMMS) whose goal is to become an independent investigator focused on early cognitive impairment in Parkinson disease (PD). Her long-term goal is to develop speech markers of cognitive impairment in PD. Cognitive impairment occurs in the majority of PD patients, leading to increased mortality and decreased quality of life. The current diagnostic tools are resource-intensive and have limited sensitivity. Treatments are often offered late in the course of cognitive decline and do not provide optimal benefit. Speech markers could improve detection, monitoring and treatment of cognitive impairment in PD. Speech markers could be monitored frequently and remotely via mobile technology, capturing sensitive, quantitative data about cognitive function in the context of patients’ daily life and in response to therapeutics. Dr. Smith’s role as a clinical movement disorders specialist ideally positions her to lead the application of advanced speech and language research to feasible, patient-oriented tools for real-life clinical practice and clinical trials. Dr. Smith has assembled an expert interdisciplinary mentorship team ideally suited for the goals of this innovative proposal. Dr. Smith and her team have previously shown that a) speech acoustic markers are associated with cognitive function in non-demented PD patients, and b) PD patients with mild cognitive impairment had linguistic deficits including pauses within utterances and grammaticality. Building on these results, Dr. Smith proposes to study speech and language more comprehensively in PD patients with and without mild cognitive impairment and controls to confirm these preliminary results and identify additional biomarkers. The aims of this study will be 1) to develop algorithms using speech acoustic markers to categorize by cognitive status, 2) to identify linguistic markers associated with mild cognitive impairment in PD, and 3) to assess on-line syntactic processing in PD subjects with mild cognitive impairment. The overall goal of the proposal is to identify speech and language markers of early cognitive dysfunction that can be further refined, validated and implemented using mobile technology into a larger scale, longitudinal R01 proposal. Further work will also address the underlying neurobiological mechanisms of these speech markers. Dr. Smith’s rigorous training plan includes a Master’s degree, linguistics and speech motor physiology courses, and experience in signal processing and speech acoustic analysis. Through her training goals, she will advance her knowledge and skills in patient-centered outcomes measures and instrument validation. She will gain experience in research leadership, presentation and dissemination of scientific work, and in grant writing, culminating in an R01 proposal. This K23 award will be critical for Dr. Smith to establish an independent career as a PD clinician-scientist at the unique intersection of speech and language science and cognitive impairment. PUBLIC HEALTH RELEVANCE: Speech markers have the potential to improve diagnosis, monitoring and treatment of cognitive impairment in Parkinson’s disease (PD). Although the majority of PD patients will develop cognitive impairment, the tools available to assess and treat this disabling complication are fraught with limitations. As a detailed and quantitative assessment tool, speech markers may increase sensitivity to early cognitive dysfunction and to changes over time compared with current measures. They may be automated and then implemented through mobile technology to increase patients’ access to cognitive symptom monitoring outside of the clinic setting. Dr. Smith’s proposed career development plan has potential to fill a major gap in PD research by making inexpensive and easy-to-use cognitive assessment tools accessible to patients in rural and international settings, and by fueling clinical trials to discover new therapeutics capable of slowing cognitive decline in PD.",Speech markers of cognitive impairment in Parkinson's disease,9666574,K23DC016656,"['Acoustics', 'Address', 'Algorithms', 'American', 'Area', 'Articulation', 'Assessment tool', 'Award', 'Biological Markers', 'Biomedical Engineering', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Cognitive Therapy', 'Cognitive deficits', 'Complication', 'Comprehension', 'Data', 'Data Analyses', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Foundations', 'Future', 'Goals', 'Grant', 'Health Services Accessibility', 'Impaired cognition', 'Impairment', 'Individual', 'International', 'Knowledge', 'Language', 'Language Disorders', 'Lead', 'Leadership', 'Life', 'Linguistics', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Master&apos', 's Degree', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Nerve Degeneration', 'Neurobehavioral Manifestations', 'Neurobiology', 'Neurologist', 'Neuropsychological Tests', 'Outcome', 'Outcome Measure', 'Outcomes Research', 'Parkinson Disease', 'Parkinson&apos', 's Dementia', 'Participant', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Positioning Attribute', 'Production', 'Proxy', 'Quality of Care', 'Quality of life', 'Research', 'Research Personnel', 'Resources', 'Role', 'Rural', 'Science', 'Scientist', 'Severities', 'Specialist', 'Speech', 'Speech Acoustics', 'Symptoms', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Universities', 'Validation', 'Work', 'Writing', 'career', 'career development', 'clinical movement disorder', 'clinical practice', 'cognitive change', 'cognitive control', 'cognitive function', 'cognitive performance', 'cognitive testing', 'common symptom', 'experience', 'handheld mobile device', 'improved', 'innovation', 'instrument', 'language processing', 'learning strategy', 'lexical retrieval', 'medical schools', 'mild cognitive impairment', 'mobile computing', 'mortality', 'motor deficit', 'neurobiological mechanism', 'non-demented', 'novel', 'novel therapeutics', 'patient oriented', 'public health relevance', 'recruit', 'response', 'screening', 'signal processing', 'skills', 'syntax', 'tool']",NIDCD,UNIV OF MASSACHUSETTS MED SCH WORCESTER,K23,2019,188946,0.18848978455240176
"Using Speech Acoustics to Reveal Motor Disruptions in Psychosis Project summary The goal of this project is to investigate the feasibility of using speech acoustics as a clinical biomarker in individuals at clinical high risk (CHR) for developing psychosis. There is evidence that disruptions to cortico-cerebellar circuits in individuals experiencing attenuated psychosis symptoms impact motor control of the face and limbs. This proposal would be the first study to examine whether these motor disruptions in high-risk populations also affect the complex motor control required for speech. In Aim 1 an instrumental approach will be used to investigate the acoustic correlates of psychosis risk. Specifically, speech data will be collected to investigate fine-grained acoustic properties of vowels and consonants in simple repetition tasks as well as during more naturalistic conversational speech. The speech of CHR young adults will be compared to age-matched healthy controls to discover if there are group differences in the speech acoustics that allow us to classify speech samples into healthy and clinical groups. To enable fast, reliable analysis, machine learning-based algorithms will be used to measure the acoustics speech properties of interest. In Aim 2, the speech properties measured in Aim 1 will be compared to other behavioral measures, in order to discover if they correlate with several measures of cerebellar dysfunction (posture control, procedural learning, and motor timing) that are known to occur in CHR individuals. These measures will provide convergent validity for these novel speech measures. Cognitive capabilities which are related and unrelated to speech and motor control will also be assessed, to provide specificity and divergent validity to these measures. In Aim 3, the links between speech features and changes in symptom severity will be assessed at two time points, connecting changes in speech motor control to longitudinal changes in the progression of the symptoms over 12 months. These investigations may reveal speech as a novel and easily-collected biomarker enabling early detection of psychosis risk. Project narrative The goal of this proposal is to determine whether speech patterns can signal vulnerability to psychotic disorders such as schizophrenia. Speech samples will be collected from high-risk and matched healthy control participants in order to: determine if there are abnormalities in the acoustics of vowels and consonants; evaluate if these properties map on to dysfunction of the cerebellum (a brain region impacted in the development of psychosis that also plays a role in speech motor control); and relate these properties to symptom severity and illness progression. This study will lay the groundwork for the use of speech as an inexpensive, non-invasive, and mechanistically-relevant metric that will ultimately support earlier identification and facilitate timely treatment.",Using Speech Acoustics to Reveal Motor Disruptions in Psychosis,9746454,R21MH119677,"['Acoustics', 'Address', 'Affect', 'Age', 'Algorithms', 'Articulators', 'Attenuated', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain region', 'Cerebellar Diseases', 'Cerebellum', 'Clinical', 'Cognitive', 'Complex', 'Computers', 'Control Groups', 'Cueing for speech', 'Data', 'Development', 'Diagnosis', 'Dyskinetic syndrome', 'Early Diagnosis', 'Early Intervention', 'Early identification', 'Ensure', 'Equilibrium', 'Etiology', 'Face', 'Fingers', 'Functional disorder', 'Goals', 'Grain', 'Individual', 'Intervention', 'Interview', 'Investigation', 'Larynx', 'Learning', 'Limb structure', 'Linguistics', 'Link', 'Lip structure', 'Literature', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Motor', 'Movement', 'Musculoskeletal Equilibrium', 'National Institute of Mental Health', 'Neurologic', 'Participant', 'Pathogenicity', 'Pattern', 'Play', 'Population', 'Positioning Attribute', 'Posture', 'Process', 'Production', 'Property', 'Psychotic Disorders', 'Research', 'Research Personnel', 'Risk', 'Role', 'Roter', 'Sampling', 'Schizophrenia', 'Scientist', 'Sensory', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Tongue', 'Variant', 'Youth', 'base', 'behavior measurement', 'clinical biomarkers', 'clinical predictors', 'experience', 'high risk', 'high risk population', 'improved', 'indexing', 'individualized medicine', 'interest', 'motor control', 'motor deficit', 'motor disorder', 'motor learning', 'novel', 'novel marker', 'potential biomarker', 'relating to nervous system', 'sensory integration', 'tool', 'vocal control', 'young adult']",NIMH,NORTHWESTERN UNIVERSITY,R21,2019,239000,0.34367073673259046
"SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline Early detection of the cognitive decline involved in Alzheimer's Disease and Related Dementias (ADRD)  in older adults living alone is essential for developing, planning, and initiating interventions and support  systems to improve patients' everyday function and quality of life. Conventional, clinic-based methods for  early diagnosis are expensive, time-consuming, and impracticalfor large-scale screening. This project aims to develop a low-cost, passive, and practical home-based assessment method using Voice Assistant  Systems (VAS) for early detection of cognitive decline, including a set of novel data mining techniques for  sparse time-series speech. The project has three specific aims: 1. Using a recurrent neural network  (RNN) and a softmax regression model, we will develop a transfer learning technique to investigate the  link between the speech from in-lab VAS tasks and cognitive decline. The Pitt corpus from the  DementiaBank database will be used to optimize the RNN parameters and thereby overcome the limited  data problem of VAS. The softmax regression model will allow us to align the feature distributions from the  previous speech data and in-lab VAS speech. 2. We will develop a novel ""many-to-difference"" prediction  model with a symmetric RNN structure to predict the cognitive difference at two ends of a time period from  the sparse time-series data. The proposed model is different from previous ones as the learning focus is  shifted from the short-term pattern differences across users to the pattern difference over time for an  individual user. The proposed model accommodates well for the highly dynamic nature of the inputs and  maximally removes individual characteristics from the prediction result. To analyze the sparse time-series  speech, a new data sampling technique will be used to address the imbalanced data problem, and a data  quality metric will be developed for the proposed model. 3. The team will conduct an 18-month in-lab  evaluation and a 28-month in-home evaluation with a focus on whether the VAS tasks and features from  the in-lab evaluation and the repetition features of the in-home VAS data can measure and predict  cognitive decline in the in-home participants over time. The proposed methods will be integrated into an  interactive system to enable efficient communication on cognitive decline among patients, caregivers, and  clinicians. If successful, the outcomes of this project will provide an opportun ity to provide supportive  evidence to clinicians for the early detection of cognitive impairment outside of a clinic-based setting. This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice  Assistant Systems (VAS) for early detection of cognitive decline. If successful, the proposed system may  be widely disseminated for the early diagnosis of cognitive impairment to complement existing diagnostic  modalities that could ultimately enable long-term patient and caregiver planning to maintain individual's  independence at home.",SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline,9977514,R01AG067416,"['Address', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Characteristics', 'Clinic', 'Cognitive', 'Communication', 'Complement', 'Consumption', 'Data', 'Databases', 'Diagnostic', 'Early Diagnosis', 'Elderly', 'Evaluation', 'Home environment', 'Impaired cognition', 'Individual', 'Instruction', 'Intervention', 'Learning', 'Link', 'Measures', 'Methods', 'Modality', 'Modeling', 'Nature', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Psychological Transfer', 'Quality of life', 'Research', 'Sampling', 'Series', 'Speech', 'Structure', 'Support System', 'System', 'Techniques', 'Time', 'Voice', 'base', 'cognitive testing', 'cost', 'data mining', 'data quality', 'improved', 'novel', 'predictive modeling', 'recurrent neural network', 'screening']",NIA,UNIVERSITY OF MASSACHUSETTS BOSTON,R01,2019,300000,0.10437802349698445
"Temporal Pattern Perception Mechanisms for Acoustic Communication Project Summary/Abstract: Processing acoustic communication signals is among the most difficult yet vital abilities of the auditory system. These abilities lie at the heart of language and speech processing, and their success or failure has profound impacts on quality of life across the lifespan. Understanding the neurobiological mechanisms that support these basic abilities holds promise for advancing assistive listening devices, and for improving diagnoses and treatments for learning disabilities and communication disorders, such as auditory processing disorder, dyslexia, and specific language impairment. Non-invasive neuroscience techniques in humans reveal the loci of language-related processing but do not answer how individual neurons and neural circuits implement language-relevant computations. Thus, circuit-level neuro-computational mechanisms that support acoustic communication signal processing remain poorly understood. Multiple lines of research suggest that songbirds can provide an excellent model for investigating shared auditory processing abilities relevant to language. This proposal investigates neural mechanisms of auditory temporal pattern processing abilities shared between songbirds and humans. In Aim 1, we test the cellular-level predictions of a powerful modelling framework, called predictive coding, proposed as a general computational mechanism to support the learned recognition of complex temporally patterned signals at multiple timescales. We combine state-of-the-art machine learning methods with multi-electrode electrophysiology, to test explicit models for natural stimulus representation, prediction, and error coding in single cortical neurons and neural populations. One aspect of auditory perception integral to speech is the discretization of the signal into learned categorically perceived sounds (phonemes). In Aim 2, we use the predictive coding framework to investigate the learned categorical perception of natural auditory categories in populations of cortical neurons. In humans, the transition statistics between adjacent phonemes can aid or alter phoneme categorization, providing cues for language learners and listeners to disambiguate perceptually similar sounds. Aim2 also examines how categorical neural representations are affected by temporal context. In addition to which phonemes occur in a sequence, speech processing also requires knowing where those elements occur. Sensitivities to the statistical regularities of speech sequences are established long before infants learn to speak, and continue to affect both recognition and comprehension throughout adulthood. Songbirds also attend to the statistical regularities in their vocal communication signals. In Aim 3, we focus on how sequence-specific information is encoded by single neurons and neural populations in auditory cortex. The proposed approach permits progress in the near term towards establishing the basic neurobiological substrates of foundational language-relevant abilities and a general framework within which more complex, uniquely human processes, can be proposed and eventually tested. Project Narrative: Normal speech comprehension and communication are rooted in basic auditory cognitive processes. Deficits in these processes accompany severe communication disorders (e.g. auditory processing disorder, dyslexia, specific language impairment, autism), and their abnormal function can compound the negative impacts of hearing impairments. The proposed project investigates the neuronal mechanisms of auditory temporal pattern processing using natural communication signals, with the ultimate goal of understanding the neurobiological basis of language-relevant abilities and improving treatment of communication processing disorders.",Temporal Pattern Perception Mechanisms for Acoustic Communication,9803507,R01DC018055,"['Acoustics', 'Adult', 'Affect', 'Algorithms', 'Animal Model', 'Auditory', 'Auditory Perception', 'Auditory Perceptual Disorders', 'Auditory area', 'Auditory system', 'Behavior', 'Behavioral', 'Biological Models', 'Categories', 'Code', 'Cognition Disorders', 'Cognitive', 'Communication', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Cues', 'Data', 'Detection', 'Diagnosis', 'Disease', 'Dyslexia', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Failure', 'Foundations', 'Generations', 'Goals', 'Grouping', 'Hearing Aids', 'Heart', 'Human', 'Individual', 'Infant', 'Language', 'Language Development', 'Learning', 'Learning Disabilities', 'Longevity', 'Machine Learning', 'Modality', 'Modeling', 'Neurobiology', 'Neurons', 'Neurosciences', 'Parietal', 'Pattern', 'Perception', 'Physiological', 'Plant Roots', 'Population', 'Process', 'Quality of life', 'Research', 'Sensory', 'Signal Transduction', 'Songbirds', 'Speech', 'Speech Perception', 'Speech Sound', 'Stimulus', 'Stream', 'Sturnus vulgaris', 'Superior temporal gyrus', 'Techniques', 'Testing', 'Time', 'Work', 'auditory processing', 'autism spectrum disorder', 'bird song', 'cognitive process', 'computer framework', 'experience', 'experimental study', 'hearing impairment', 'improved', 'language comprehension', 'language processing', 'learned behavior', 'learning strategy', 'model development', 'neural circuit', 'neurobiological mechanism', 'neuromechanism', 'pattern perception', 'relating to nervous system', 'response', 'sensory input', 'sensory system', 'signal processing', 'sound', 'spatiotemporal', 'specific language impairment', 'speech processing', 'speech recognition', 'statistics', 'success']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,334599,0.1579475033884586
"CRCNS: Avian Model for Neural Activity Driven Speech Prostheses  Understanding the physical, computational, and theoretical bases of human vocal communication, speech, is crucial to improved comprehension of voice, speech and language diseases and disorders, and improving their diagnosis, treatment and prevention. Meeting this challenge requires knowledge of the neural and sensorimotor mechanisms of vocal motor control. Our project will directly investigate the neural and sensorimotor mechanisms involved in the production of complex, natural, vocal communication signals. Our results will directly enhance brain-computer interface technology for communication and will accelerate the development of prostheses and other assistive technologies for individuals with communications deficits due to injury or disease. We will develop a vocal prosthetic that directly translates neural signals in cortical sensorimotor and vocal-motor control regions into vocal communication signals output in real-time. Building on success using non-human primates for brain computer interfaces for general motor control, the prosthetic will be developed in songbirds, whose acoustically rich, learned vocalizations share many features with human speech. Because the songbird vocal apparatus is functipnally and anatomically similar to the human larynx, and the cortical regions that control it are closely analogous to speech motor-control areas of the human brain, songbirds offer an ideal model for the proposed studies. Beyond the application of our work to human voice and speech, development of the vocal prosthetic will enable novel speech-relevant studies in the songbird model that can reveal fundamental mechanisms of vocal learning and production. In the first stage of the project, we collect a large data set of simultaneously recorded neural activity and vocalizations. In stage two, we will apply machine learning and artificial intelligence techniques to develop algorithms that map neural recordings to vocal output and enable us to estimate intended vocalizations directly from neural data. In stage three, we will develop computing infrastructure to run these algorithms in real-time, predicting intended vocalizations from neural activity as the animal is actively producing these vocalizations. In stage four, we will test the effectiveness of the prosthetic by substituting the bird's own vocalization with the output from our prosthetic system. Success will set the stage for testing of these technologies in humans and translation to multiple assistive devices. In addition to our research goals, the project will engage graduate, undergraduate, and high school students through the development of novel educational modules that introduce students to brain machine interface and multidisciplinary studies that span engineering and the basic sciences. RELEVANCE (See instructions): Developing a vocal prosthesis will directly enhance brain-computer interface technology for communication and accelerate the realization of prostheses and other assistive technologies for individuals with communications deficits due to injury or disease. The basic knowledge of the neural and sensorimotor mechanisms of vocal motor control acquired will impact understanding of multiple voice, speech, and language diseases and disorders. The techniques developed will enabling novel future studies of vocal production and development. n/a",CRCNS: Avian Model for Neural Activity Driven Speech Prostheses ,9916239,R01DC018446,"['Acoustics', 'Algorithms', 'Anatomy', 'Animal Model', 'Animals', 'Area', 'Artificial Intelligence', 'Basic Science', 'Behavior', 'Behavioral', 'Birds', 'Brain', 'Cell Nucleus', 'Characteristics', 'Clinical Research', 'Clinical assessments', 'Communication', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computer Interface', 'Computer software', 'Computers', 'Course Content', 'Data', 'Data Science', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Educational Materials', 'Educational workshop', 'Effectiveness', 'Electrodes', 'Engineering', 'Evaluation', 'Feedback', 'Finches', 'Future', 'Generations', 'Goals', 'High School Outreach', 'High School Student', 'Human', 'Implant', 'Individual', 'Infrastructure', 'Injury', 'Instruction', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Learning Module', 'Limb Prosthesis', 'Limb structure', 'Limes', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neurodegenerative Disorders', 'Neurosciences', 'Outcome Measure', 'Output', 'Patients', 'Performance', 'Play', 'Prevention', 'Principal Investigator', 'Production', 'Prosthesis', 'Quadriplegia', 'Recording of previous events', 'Research', 'Robot', 'Role', 'Running', 'Self-Help Devices', 'Signal Transduction', 'Songbirds', 'Space Models', 'Speech', 'Speech Development', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Translations', 'Upper Extremity', 'Voice', 'Work', 'auditory feedback', 'base', 'bird song', 'brain computer interface', 'brain machine interface', 'data sharing', 'design', 'functional electrical stimulation', 'functional restoration', 'graduate student', 'hackathon', 'high school', 'human subject', 'improved', 'machine learning algorithm', 'meetings', 'mind control', 'model development', 'motor control', 'multidisciplinary', 'neural model', 'neural prosthesis', 'neurodevelopment', 'neurophysiology', 'neurotransmission', 'nonhuman primate', 'novel', 'open source', 'operation', 'programs', 'relating to nervous system', 'repository', 'response', 'signal processing', 'success', 'undergraduate student', 'vocal learning', 'vocalization', 'web site']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,354676,0.21118300955093264
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through “part-of-speech” tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (“which”, “what”, “that”). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis – semantics and syntax – that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9669113,R01MH115332,"['Address', 'Affective', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,468909,0.2722094767620354
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9548753,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,175477,0.3817328984326686
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9645876,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,189567,0.3817328984326686
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through “part-of-speech” tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (“which”, “what”, “that”). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis – semantics and syntax – that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9435649,R01MH115332,"['Address', 'Affective', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patient risk', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,513989,0.2722094767620354
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,9390468,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2018,686265,0.3840087214769346
"Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder. ﻿    DESCRIPTION (provided by applicant): In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decades, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a range of cognitive processes in an effort to identify core deficits of schizophrenia evident before psychosis onset. Subtle thought disorder, manifest in disturbance of language production, is a feature that predates rather than follows, psychosis onset in CHR individuals, and therefore may be an indicator of schizophrenia liability. Subtle thought disorder in schizophrenia and its risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. Here, we propose to instead use a novel automated machine-learning approach to speech analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language. It also evaluates syntax through ""part-of-speech"" tagging. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture subtle thought disorder and discriminate psychosis outcome among CHR individuals. Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we were able to identify a classifier with high accuracy for psychosis onset in a small CHR cohort at Columbia, which included semantic coherence from phrase to phrase, shortened phrase length, and decreased use of determiner pronouns (""which"", ""what"", ""that""). These features were correlated with prodromal symptoms but outperformed them in terms of classification accuracy. They also discriminated schizophrenia from normal speech. While promising, these automated methods of analysis require validation in a second CHR cohort. In this proposal, in collaboration with IBM, we will validate these automated methods using a large archive of speech data from the UCLA CHR cohort. This dataset has several advantages. First, the UCLA CHR cohort has a high prevalence of psychosis transition, important as machine learning is sensitive to group size. Second, it has undergone prior manual linguistic analysis, identifying features of language production that predicted psychosis outcome; hence, automated and manual methods can be directly compared. Third, there are speech data available from healthy controls and recent-onset psychosis patients (for validation). Fourth, several participants have multiple speech assays (such that stability of the classifier can be examined). Beyond validation of methods, we will maximize group size and combine speech data from Columbia and UCLA to characterize a common classifier of psychosis outcome. Automated methods for language analysis may improve prediction of psychosis onset and inform remediation strategies for its prevention. PUBLIC HEALTH RELEVANCE: Subtle thought disorder is an early core feature of schizophrenia evident before psychosis onset: it has traditionally been evaluated using clinical ratings or labor-intensive manual linguistic analyses. This proposal will apply novel computer-based speech analysis methods to existing datasets to identify abnormal semantic and syntactic features of language production that can predict or classify psychosis outcome among youths at clinical high risk for psychosis. Improved characterization of thought disorder can inform targeted preventive interventions for young people at risk for schizophrenia and related psychotic disorders, so as to reduce the morbidity of psychosis.",Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.,9231498,R03MH108933,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Biological Assay', 'Cereals', 'Classification', 'Clinical', 'Collaborations', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Deltastab', 'Development', 'Diagnosis', 'Disease', 'Elderly', 'Emotional', 'Friends', 'High Prevalence', 'Human', 'Impairment', 'Individual', 'Language', 'Lead', 'Length', 'Linguistics', 'Machine Learning', 'Manuals', 'Mental disorders', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Occupations', 'Outcome', 'Output', 'Participant', 'Patients', 'Population', 'Poverty', 'Prevention', 'Preventive Intervention', 'Production', 'Psychiatrist', 'Psychiatry', 'Psychotic Disorders', 'Research', 'Risk', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Scientist', 'Semantics', 'Site', 'Source', 'Speech', 'Structure', 'Symptoms', 'Testing', 'Text', 'Thinking', 'Training', 'Validation', 'Youth', 'analytical method', 'base', 'cognitive process', 'cohort', 'demographics', 'disabling symptom', 'effective intervention', 'hazard', 'high risk', 'improved', 'indexing', 'novel', 'phrases', 'prevent', 'public health relevance', 'remediation', 'standard of care', 'syntax', 'targeted treatment', 'tool']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2017,35453,0.1801356208403333
"Speech Movement Classification for Assessing and Treating ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",Speech Movement Classification for Assessing and Treating ALS,9341526,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2017,93248,0.3840087214769346
"Multi-Measure Speech Perception in Noise (MMSPIN) Chart: More Scores, Fewer Tests Project summary. This Phase I will establish the feasibility of increasing audiological diagnostic information by carrying out word- and phoneme-level analyses of open set responses during speech audiometry and by obtaining subjective hearing measures. Speech audiometry is used in characterizing functional hearing in settings of hearing screening, diagnosis, hearing aid fitting, counseling, aural rehabilitation/training, occupational fitness, and research. A typical procedure used with word and sentence tests in background noise is to ask the client/patient to repeat back what was just said (i.e., give an open set response). Responses are then scored in terms of words or keywords correct/incorrect. This method discards potentially diagnostic information in response errors, because noise can reveal systematic phonetic feature or phoneme confusions, and with background babble, intrusions from the babble. Other response patterns attributable to cognitive or memory declines may manifest in the paucity or verbosity of response words. Specific types of phoneme perception errors are thought to be associated with extent and configuration of hearing loss; and different types of noise maskers (i.e., energetic and informational maskers) present different types of perceptual problems that vary in severity across individuals. In order to utilize response errors, computational methods are needed to establish their relationships to the stimulus. This is because response errors may incorporate incorrect stimulus-to-response phoneme substitutions, as well as insertions or deletions of phonemes or words relative to the stimulus. We have developed sequence alignment methods to mine errors during speech audiometry, which we propose to evaluate using our system (Multi-Measure SPIN Chart: MMSPIN Chart). MMSPIN chart will be further developed and installed in the George Washington University Speech & Hearing Center (Aim 1). Audiologists will use the system during QuickSin sentence and NU-6 word testing with 200 clients (18-85 years of age) who give permission to access their entire clinic records (Aim 2). Their conventional speech audiometry will be augmented by obtaining subjective hearing accuracy judgments and hearing self-efficacy measures. These subjective judgments are designed to expose discrepancies with objective performance and to reveal individual differences in social cognition associated with hearing loss, both of which may account for the large individual differences in performance and intervention outcomes not accounted for by the audiogram. Evaluation of results in Aim 3 will include developing group and individual profile models comprising objective and subjective clinical data. With our clinician partners, we will develop formats for communicating MMSPIN Chart results to clients. In Aim 4, we will present results in a public lecture for audiologists and solicit opinions about how our new results may best impact clinical practice. Our approach can deliver more informative and efficient speech audiometry using existing test materials and can pave the way to more sensitive speech audiometry, including tests that are adaptive to specific levels of speech processing difficulty. Narrative. The typical approach to speech audiometry is to elicit open set responses that are scored in terms of words/keywords correct, discarding information in response errors. This Phase I will establish the feasibility of increasing diagnostic information provided to audiologists by carrying out word- and phoneme-level analyses of open set responses and obtaining subjective hearing measures in conjunction with speech audiometry. The goal is to improve clinical efficiency and effectiveness and to improve patient outcomes.","Multi-Measure Speech Perception in Noise (MMSPIN) Chart: More Scores, Fewer Tests",9408539,R43DC015749,"['Adult', 'Age', 'Age-Years', 'Attitude to Health', 'Audiometry', 'Authorization documentation', 'Back', 'Classification', 'Client', 'Clinic', 'Clinical', 'Clinical Data', 'Computers', 'Computing Methodologies', 'Confusion', 'Counseling', 'Data', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Effectiveness', 'Evaluation', 'Factor Analysis', 'Focus Groups', 'Goals', 'Hearing', 'Hearing Aids', 'Impaired cognition', 'Individual', 'Individual Differences', 'Intervention', 'Judgment', 'Machine Learning', 'Materials Testing', 'Measures', 'Memory Loss', 'Methods', 'Modeling', 'Noise', 'Occupational', 'Outcome', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Perception', 'Performance', 'Phase', 'Procedures', 'Pure-Tone Audiometry', 'Questionnaires', 'Records', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Role', 'Self Efficacy', 'Sequence Alignment', 'Severities', 'Speech', 'Speech Audiometry', 'Speech Perception', 'Stimulus', 'Supervision', 'System', 'Technology', 'Test Result', 'Testing', 'Time', 'Training', 'Universities', 'Vision', 'Voice', 'Washington', 'Work', 'clinical practice', 'comparison group', 'data modeling', 'design', 'fitness', 'hearing impairment', 'hearing screening', 'improved', 'lectures', 'permissiveness', 'phonology', 'response', 'satisfaction', 'social cognition', 'speech processing', 'speech recognition', 'touchscreen']",NIDCD,"SEEHEAR, LLC",R43,2017,147919,0.26050726236096516
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication ﻿    DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders. PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,9319226,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Articulation', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Impairment', 'Individual', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'public health relevance', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2017,153000,0.3465317848702092
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9356341,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,199885,0.3817328984326686
"Neural mechanisms of auditory temporal pattern perception Project Summary/Abstract: Processing acoustic communication signals is among the most difficult, yet vital capabilities that the auditory system must achieve. These abilities lie at the heart of language and speech processing, and their success or failure can have profound impacts on quality of life across the lifespan. Understanding the neurobiological mechanisms that support these basic abilities holds promise for advancing assistive listening devices, as well as improving diagnoses and treatments for learning disabilities and communication disorders such as auditory processing disorder, dyslexia, and specific language impairment. While much has been learned about the loci of language-related processing using non-invasive neuroscience techniques in humans, these techniques cannot answer how individual neurons and neural circuits implement language-relevant computations. As a result, the explicit cellular circuit-level and neuro-computational mechanisms that support acoustic communication signal processing are poorly understood. Multiple lines of research suggest that songbirds can provide an excellent model for investigating shared auditory processing abilities relevant to language, in particular the processing of temporal patterns within communication signals. The experiments outlined in this proposal investigate the neural mechanisms of auditory temporal pattern processing. In humans, the transition statistics between adjacent speech sounds (phonemes) can aid or alter phoneme categorization, providing cues for language learners and listeners to disambiguate perceptually similar sounds. Sensitivity to transition statistics is not exclusive to speech signals however, but reflects general auditory processes shared by many animals. In Aim 1 we investigate the categorical perception of complex auditory objects in populations of cortical neurons in an animal model, and ask how these neural representations are effected by temporal context. In addition to which elements occur in a sequence, speech processing also requires knowing where those elements occur. Sensitivities to the statistical regularities of speech sequences are established long before infants learn to speak, and continue to affect both recognition and comprehension throughout adulthood. Studies in Aim 2 focus on how sequence-specific information is encoded by single neurons and neural populations in auditory cortex. In Aim 3, we propose a basic circuit in which population level representations of auditory objects could be differentially modulated by patterning rules, and test this proposed pattern processing circuit using direct, casual manipulations. The proposed approach permits progress in the near term towards establishing the basic neurobiological substrates of foundational language-relevant abilities and a general framework within which more complex, uniquely human processes, can be proposed and eventually tested. Project Narrative: Normal speech comprehension and communication are rooted in basic auditory cognitive processes. Deficits in these processes accompany severe communication disorders (e.g. auditory processing disorder, dyslexia, specific language impairment, autism), and their abnormal function can compound the negative impacts of hearing impairments. The proposed project investigates the neuronal mechanisms of auditory temporal pattern processing using natural communication signals, with the ultimate goal of understanding the neurobiological basis of language-relevant abilities and improving treatment of communication processing disorders.",Neural mechanisms of auditory temporal pattern perception,9527903,R56DC016408,"['Acoustics', 'Adult', 'Affect', 'Animal Model', 'Animals', 'Auditory', 'Auditory Perception', 'Auditory Perceptual Disorders', 'Auditory area', 'Auditory system', 'Autistic Disorder', 'Behavior', 'Behavioral', 'Biological Assay', 'Biological Models', 'Birds', 'Categories', 'Code', 'Cognitive', 'Communication', 'Communication impairment', 'Complex', 'Comprehension', 'Computational Technique', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Dyslexia', 'Electrophysiology (science)', 'Elements', 'Environment', 'Failure', 'Foundations', 'Goals', 'Heart', 'Human', 'Individual', 'Infant', 'Knowledge', 'Language', 'Language Development', 'Learning', 'Learning Disabilities', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Neurobiology', 'Neurons', 'Neurosciences', 'Nuclear', 'Parietal', 'Pattern', 'Perception', 'Physiological', 'Plant Roots', 'Population', 'Population Dynamics', 'Process', 'Property', 'Quality of life', 'Research', 'Role', 'Services', 'Signal Transduction', 'Songbirds', 'Speech', 'Speech Perception', 'Speech Sound', 'Stimulus', 'Stream', 'Structure', 'Sturnus vulgaris', 'Superior temporal gyrus', 'System', 'Systems Development', 'Techniques', 'Testing', 'Time', 'Training', 'Transition Elements', 'Work', 'auditory processing', 'bird song', 'cognitive process', 'experimental study', 'hearing impairment', 'improved', 'language processing', 'microstimulation', 'model development', 'neural circuit', 'neural patterning', 'neurobiological mechanism', 'neuromechanism', 'pattern perception', 'relating to nervous system', 'response', 'signal processing', 'sound', 'spatiotemporal', 'specific language impairment', 'speech processing', 'speech recognition', 'statistics', 'success']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R56,2017,363607,0.17472457607050246
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,9185964,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2017,581327,0.3840087214769346
"Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder. ﻿    DESCRIPTION (provided by applicant): In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decades, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a range of cognitive processes in an effort to identify core deficits of schizophrenia evident before psychosis onset. Subtle thought disorder, manifest in disturbance of language production, is a feature that predates rather than follows, psychosis onset in CHR individuals, and therefore may be an indicator of schizophrenia liability. Subtle thought disorder in schizophrenia and its risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. Here, we propose to instead use a novel automated machine-learning approach to speech analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language. It also evaluates syntax through ""part-of-speech"" tagging. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture subtle thought disorder and discriminate psychosis outcome among CHR individuals. Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we were able to identify a classifier with high accuracy for psychosis onset in a small CHR cohort at Columbia, which included semantic coherence from phrase to phrase, shortened phrase length, and decreased use of determiner pronouns (""which"", ""what"", ""that""). These features were correlated with prodromal symptoms but outperformed them in terms of classification accuracy. They also discriminated schizophrenia from normal speech. While promising, these automated methods of analysis require validation in a second CHR cohort. In this proposal, in collaboration with IBM, we will validate these automated methods using a large archive of speech data from the UCLA CHR cohort. This dataset has several advantages. First, the UCLA CHR cohort has a high prevalence of psychosis transition, important as machine learning is sensitive to group size. Second, it has undergone prior manual linguistic analysis, identifying features of language production that predicted psychosis outcome; hence, automated and manual methods can be directly compared. Third, there are speech data available from healthy controls and recent-onset psychosis patients (for validation). Fourth, several participants have multiple speech assays (such that stability of the classifier can be examined). Beyond validation of methods, we will maximize group size and combine speech data from Columbia and UCLA to characterize a common classifier of psychosis outcome. Automated methods for language analysis may improve prediction of psychosis onset and inform remediation strategies for its prevention.         PUBLIC HEALTH RELEVANCE: Subtle thought disorder is an early core feature of schizophrenia evident before psychosis onset: it has traditionally been evaluated using clinical ratings or labor-intensive manual linguistic analyses. This proposal will apply novel computer-based speech analysis methods to existing datasets to identify abnormal semantic and syntactic features of language production that can predict or classify psychosis outcome among youths at clinical high risk for psychosis. Improved characterization of thought disorder can inform targeted preventive interventions for young people at risk for schizophrenia and related psychotic disorders, so as to reduce the morbidity of psychosis.            ",Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.,9017082,R03MH108933,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Biological Assay', 'Cereals', 'Classification', 'Clinical', 'Collaborations', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Deltastab', 'Development', 'Diagnosis', 'Disease', 'Elderly', 'Emotional', 'Friends', 'High Prevalence', 'Human', 'Impairment', 'Individual', 'Language', 'Lead', 'Length', 'Linguistics', 'Machine Learning', 'Manuals', 'Mental disorders', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Occupations', 'Outcome', 'Output', 'Participant', 'Patients', 'Population', 'Poverty', 'Prevention', 'Preventive Intervention', 'Production', 'Psychiatrist', 'Psychiatry', 'Psychotic Disorders', 'Research', 'Risk', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Scientist', 'Semantics', 'Site', 'Source', 'Speech', 'Structure', 'Symptoms', 'Testing', 'Text', 'Thinking', 'Training', 'Validation', 'Youth', 'base', 'cognitive process', 'cohort', 'demographics', 'disabling symptom', 'effective intervention', 'hazard', 'high risk', 'improved', 'indexing', 'novel', 'phrases', 'prevent', 'public health relevance', 'remediation', 'standard of care', 'syntax', 'targeted treatment', 'tool']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2016,81000,0.1801356208403333
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication ﻿    DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders. PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,9114061,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Equilibrium', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Individual', 'Joints', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2016,153000,0.3465317848702092
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9205946,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Area', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image Analysis', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Joints', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'cortex mapping', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'research study', 'response', 'sound', 'spatiotemporal', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2016,199295,0.3817328984326686
"Speech Prosody and Articulatory Dynamics in Spoken Language DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context. One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.",Speech Prosody and Articulatory Dynamics in Spoken Language,9036989,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'cognitive load', 'constriction', 'dynamic system', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2016,477237,0.4171245075768464
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8985675,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2016,585316,0.3840087214769346
"Multimodal Speech Translation for Assistive Communication DESCRIPTION (provided by applicant): Dysarthria, a neuromotor speech disorder impacting over 4 million Americans, is often so severe that speech is rendered unintelligible, requiring the use of augmentative and/or alternative communication (AAC) devices. These devices are dated, cumbersome and bulky. Rather than engaging in face-to-face interaction, AAC users spend a disproportionate amount of time navigating through menus of letters/icons to compose a message, which can then be spoken aloud by an integrated text-to-speech synthesis system. Thus AAC interactions are slow, effortful, unnatural, and often hinder rather than support social, educational and vocational opportunities. In fact, many AAC users continue to vocalize with familiar caregivers implying that consistent patterns must underlie dysarthric productions. It is these imprecise yet consistent productions that we propose to capture via multimodal sensors and classify using pattern recognition algorithms for speech translation. While automatic speech recognition is a viable technology for neurologically intact speakers or those with mild impairments, it fails in acoustically harsh speaking contexts and for those with more severe dysarthria. Instead, we focus on multimodal (lingual kinematic and acoustic; LinKA) representations of speech as they provide redundant and complementary channels of input for improved disambiguation. While other approaches have used computer vision, ultrasound imaging and electromyography to simultaneously estimate articulatory and acoustic parameters of speech, they are limited in portability, cost, and application to clinical settings. The current proposal leverages a novel, lightweight, wearable and low-cost array of magnetic sensors near the cheeks that can recognize the magnetic field patterns generated by a small magnetic tracer placed on the tongue to capture lingual kinematics during speech. Coupling tongue movements with the acoustic signal, captured via microphones mounted on the same headset, provides a multidimensional representation of speech that can then be translated into clear understandable speech for a new generation of wearable, speech-driven AAC devices. The proposed work will optimize the efficiency and robustness of lingual-kinematic and acoustic sensing for mobile speech translation (Aim 1), yield a standardized implementation protocol for training and independent use of the LinKA system (Aim 2), and culminate in a 2-week field test of the LinKA translator with 12 potential users with speech impairment (Aim 3). The current proposal is a first and essential step toward a low-cost, wearable, personalized communication enhancement system that can broaden communication opportunities and networks for individuals with speech impairment and thereby increase communication participation, independence and overall quality of life. PUBLIC HEALTH RELEVANCE: Neuromotor speech disorders limit communication opportunities and access to social, educational and employment activities for nearly 4 million Americans. The LinKA (Lingual Kinematic and Acoustic) system is the first low-cost, wireless and wearable technology to simultaneously capture tongue movements and corresponding acoustics of speech. Coupling multimodal speech detection with sophisticated pattern recognition algorithms and an intelligent user interface, we propose to develop the LinKA Translator - an enabling technology that would disambiguate disordered productions and translate them into clear, understandable speech to broaden communication networks for individuals with speech disorder and thereby increase quality of life and independence.",Multimodal Speech Translation for Assistive Communication,8913172,R21EB018764,"['Acoustics', 'Activities of Daily Living', 'Adherence', 'Algorithms', 'American', 'Articular Range of Motion', 'Augmentative and Alternative Communication device', 'Caregivers', 'Cheek structure', 'Clinical', 'Communication', 'Communication Aids for Disabled', 'Communication impairment', 'Computer Interface', 'Computer Vision Systems', 'Coupled', 'Coupling', 'Data', 'Detection', 'Devices', 'Disease', 'Dysarthria', 'Electromyography', 'Employment', 'Ensure', 'Eye', 'Generations', 'Hawks', 'Health', 'Impairment', 'Individual', 'Laboratories', 'Letters', 'Life', 'Magnetism', 'Modification', 'Morphologic artifacts', 'Motor', 'Movement', 'Muscle', 'Outcome Measure', 'Pattern', 'Pattern Recognition', 'Performance', 'Production', 'Protocols documentation', 'Quality of life', 'Recruitment Activity', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Social support', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Tongue', 'Tracer', 'Training', 'Translating', 'Translations', 'Ultrasonography', 'Wheelchairs', 'Wireless Technology', 'Work', 'alternative communication', 'coping', 'cost', 'design', 'deviant', 'improved', 'iterative design', 'kinematics', 'magnetic field', 'novel', 'phrases', 'portability', 'research study', 'sensor', 'social', 'speech recognition', 'success', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,R21,2015,108940,0.372931628791605
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication ﻿    DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders.         PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.                ",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,8957652,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Equilibrium', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Individual', 'Joints', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'public health relevance', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2015,153000,0.3465317848702092
"Longitudinal Voice Patterns in Bipolar Disorder DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable. PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.",Longitudinal Voice Patterns in Bipolar Disorder,8843048,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Depression', 'Bipolar Disorder', 'Bipolar I', 'Bipolar II', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'bipolar mania', 'bipolar patients', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2015,155500,0.2767445020011008
"Speech Prosody and Articulatory Dynamics in Spoken Language DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context. One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.",Speech Prosody and Articulatory Dynamics in Spoken Language,8828663,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'cognitive load', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2015,472112,0.4171245075768464
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8775639,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Communication Aids for Disabled', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2015,584132,0.3840087214769346
"Multimodal Speech Translation for Assistive Communication     DESCRIPTION (provided by applicant): Dysarthria, a neuromotor speech disorder impacting over 4 million Americans, is often so severe that speech is rendered unintelligible, requiring the use of augmentative and/or alternative communication (AAC) devices. These devices are dated, cumbersome and bulky. Rather than engaging in face-to-face interaction, AAC users spend a disproportionate amount of time navigating through menus of letters/icons to compose a message, which can then be spoken aloud by an integrated text-to-speech synthesis system. Thus AAC interactions are slow, effortful, unnatural, and often hinder rather than support social, educational and vocational opportunities. In fact, many AAC users continue to vocalize with familiar caregivers implying that consistent patterns must underlie dysarthric productions. It is these imprecise yet consistent productions that we propose to capture via multimodal sensors and classify using pattern recognition algorithms for speech translation. While automatic speech recognition is a viable technology for neurologically intact speakers or those with mild impairments, it fails in acoustically harsh speaking contexts and for those with more severe dysarthria. Instead, we focus on multimodal (lingual kinematic and acoustic; LinKA) representations of speech as they provide redundant and complementary channels of input for improved disambiguation. While other approaches have used computer vision, ultrasound imaging and electromyography to simultaneously estimate articulatory and acoustic parameters of speech, they are limited in portability, cost, and application to clinical settings. The current proposal leverages a novel, lightweight, wearable and low-cost array of magnetic sensors near the cheeks that can recognize the magnetic field patterns generated by a small magnetic tracer placed on the tongue to capture lingual kinematics during speech. Coupling tongue movements with the acoustic signal, captured via microphones mounted on the same headset, provides a multidimensional representation of speech that can then be translated into clear understandable speech for a new generation of wearable, speech-driven AAC devices. The proposed work will optimize the efficiency and robustness of lingual-kinematic and acoustic sensing for mobile speech translation (Aim 1), yield a standardized implementation protocol for training and independent use of the LinKA system (Aim 2), and culminate in a 2-week field test of the LinKA translator with 12 potential users with speech impairment (Aim 3). The current proposal is a first and essential step toward a low-cost, wearable, personalized communication enhancement system that can broaden communication opportunities and networks for individuals with speech impairment and thereby increase communication participation, independence and overall quality of life.         PUBLIC HEALTH RELEVANCE: Neuromotor speech disorders limit communication opportunities and access to social, educational and employment activities for nearly 4 million Americans. The LinKA (Lingual Kinematic and Acoustic) system is the first low-cost, wireless and wearable technology to simultaneously capture tongue movements and corresponding acoustics of speech. Coupling multimodal speech detection with sophisticated pattern recognition algorithms and an intelligent user interface, we propose to develop the LinKA Translator - an enabling technology that would disambiguate disordered productions and translate them into clear, understandable speech to broaden communication networks for individuals with speech disorder and thereby increase quality of life and independence.            ",Multimodal Speech Translation for Assistive Communication,8737379,R21EB018764,"['Acoustics', 'Activities of Daily Living', 'Adherence', 'Algorithms', 'American', 'Articular Range of Motion', 'Augmentative and Alternative Communication device', 'Caregivers', 'Cheek structure', 'Clinical', 'Communication', 'Communication Aids for Disabled', 'Communication impairment', 'Computer Interface', 'Computer Vision Systems', 'Coupled', 'Coupling', 'Data', 'Detection', 'Devices', 'Disease', 'Dysarthria', 'Electromyography', 'Employment', 'Ensure', 'Eye', 'Generations', 'Hawks', 'Impairment', 'Individual', 'Laboratories', 'Letters', 'Life', 'Magnetism', 'Modification', 'Morphologic artifacts', 'Motor', 'Movement', 'Muscle', 'Outcome Measure', 'Pattern', 'Pattern Recognition', 'Performance', 'Production', 'Protocols documentation', 'Quality of life', 'Recruitment Activity', 'Reliance', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Simulate', 'Social support', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Tongue', 'Tracer', 'Training', 'Translating', 'Translations', 'Ultrasonography', 'Wheelchairs', 'Wireless Technology', 'Work', 'alternative communication', 'coping', 'cost', 'design', 'deviant', 'improved', 'iterative design', 'kinematics', 'magnetic field', 'novel', 'phrases', 'portability', 'public health relevance', 'research study', 'sensor', 'social', 'speech recognition', 'success', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,R21,2014,97475,0.372931628791605
"Longitudinal Voice Patterns in Bipolar Disorder     DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable.         PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.                ",Longitudinal Voice Patterns in Bipolar Disorder,8658149,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Depression', 'Bipolar Disorder', 'Bipolar I', 'Bipolar II', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'bipolar mania', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'public health relevance', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2014,272125,0.2767445020011008
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.          One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8643200,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2014,475459,0.4171245075768464
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS     DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer.         PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.                    ",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8613983,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Communication Aids for Disabled', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2014,642629,0.3840087214769346
"Longitudinal Voice Patterns in Bipolar Disorder     DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable.         PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.                ",Longitudinal Voice Patterns in Bipolar Disorder,8494970,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Disorder', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'public health relevance', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2013,272125,0.2767445020011008
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.          One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8445225,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,450674,0.4171245075768464
"Infant Statistical Learning in Natural Language Acquisition    DESCRIPTION (provided by applicant): Infants are adept at tracking statistical regularities to segment words in continuous speech. Researchers have documented infants' learning abilities using highly simplified artificial languages as speech input. However, natural language is replete with variability. Infants listening to speech encounter different speakers, different word lengths, and numerous other dimensions of complexity. The proposed experiments will use speech that captures key aspects of the natural variation observed in infants' language environments to test whether infants use statistical learning mechanisms in identifying word boundaries. This research will provide a rigorous test of whether statistical learning is in fact linked to early language acquisition. Specific Aim 1 is to examine how infants segment words given variability in utterance types-specifically, the presence of both continuous speech and isolated words. A preliminary study showed that isolated words enhance infants' attention to statistical regularities in fluent, natural speech. Experiment 1 will test the hypothesis that hearing words in isolation helps infants discover other words in continuous speech. Experiment 2 will explore word segmentation given variation in voices, testing whether isolated words enhance statistical learning when speech comes from multiple talkers. Specific Aim 2 is to investigate how variability in language experience influences infants' subsequent detection of statistical regularities in fluent speech. In Experiment 3, infants will first receive brief laboratory exposure to novel isolated words. This pre-familiarization is expected to constrain infants' abilities to establish word boundaries in continuous speech. Experiment 4 will test whether cross-linguistic differences in the proportion of multisyllabic words in infant-directed speech shape infants' word segmentation abilities. Spanish-learning infants, who hear far more multisyllabic words than English-learning infants over the first year, are expected to show greater facility in segmenting trisyllabic words from continuous speech than English-learning infants. The results will assess the degree to which previous learning leads to expectations that facilitate processing future speech, or whether infants' computational abilities operate continuously at each moment in time. The outcomes of these studies will inform future research exploring statistical learning abilities in young children with emergent language impairments.      PUBLIC HEALTH RELEVANCE: The goal of this research is to understand how typically developing infants discover structure in the complex speech they hear from caregivers. Future studies will use these tasks to assess language learning in children exhibiting difficulties with language (e.g., children at risk for specific language impairment, children with emergent autism spectrum disorder, or deaf children with cochlear implants). The eventual goals are to investigate whether the mechanisms responsible for learning are compromised in these clinical populations, and to develop more appropriate tools for early diagnosis and intervention.           PROJECT NARRATIVE The goal of this research is to understand how typically developing infants discover structure in the complex speech they hear from caregivers. Future studies will use these tasks to assess language learning in children exhibiting difficulties with language (e.g., children at risk for specific language impairment, children with emergent autism spectrum disorder, or deaf children with cochlear implants). The eventual goals are to investigate whether the mechanisms responsible for learning are compromised in these clinical populations, and to develop more appropriate tools for early diagnosis and intervention.",Infant Statistical Learning in Natural Language Acquisition,8262167,F32HD069094,"['Address', 'Adult', 'Affect', 'Attention', 'Caregivers', 'Characteristics', 'Child', 'Clinical', 'Cochlear Implants', 'Complex', 'Cues', 'Detection', 'Dimensions', 'Early Diagnosis', 'Early treatment', 'Environment', 'Exhibits', 'Exposure to', 'Female', 'Future', 'Goals', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Impairment', 'Infant', 'Knowledge', 'Laboratories', 'Language', 'Language Delays', 'Language Development', 'Learning', 'Length', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Outcome Study', 'Pattern', 'Play', 'Population', 'Probability', 'Process', 'Property', 'Research', 'Research Personnel', 'Risk', 'Role', 'Shapes', 'Source', 'Speech', 'Stream', 'Structure', 'Testing', 'Time', 'Variant', 'Vocabulary', 'Voice', 'Yang', 'autism spectrum disorder', 'base', 'expectation', 'experience', 'male', 'natural language', 'novel', 'public health relevance', 'research study', 'simulation', 'specific language impairment', 'tool']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,F32,2012,22400,0.17968597886726023
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.        PUBLIC HEALTH RELEVANCE: One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.              One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8282659,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,549459,0.40978729195566843
"Infant Statistical Learning in Natural Language Acquisition    DESCRIPTION (provided by applicant): Infants are adept at tracking statistical regularities to segment words in continuous speech. Researchers have documented infants' learning abilities using highly simplified artificial languages as speech input. However, natural language is replete with variability. Infants listening to speech encounter different speakers, different word lengths, and numerous other dimensions of complexity. The proposed experiments will use speech that captures key aspects of the natural variation observed in infants' language environments to test whether infants use statistical learning mechanisms in identifying word boundaries. This research will provide a rigorous test of whether statistical learning is in fact linked to early language acquisition. Specific Aim 1 is to examine how infants segment words given variability in utterance types-specifically, the presence of both continuous speech and isolated words. A preliminary study showed that isolated words enhance infants' attention to statistical regularities in fluent, natural speech. Experiment 1 will test the hypothesis that hearing words in isolation helps infants discover other words in continuous speech. Experiment 2 will explore word segmentation given variation in voices, testing whether isolated words enhance statistical learning when speech comes from multiple talkers. Specific Aim 2 is to investigate how variability in language experience influences infants' subsequent detection of statistical regularities in fluent speech. In Experiment 3, infants will first receive brief laboratory exposure to novel isolated words. This pre-familiarization is expected to constrain infants' abilities to establish word boundaries in continuous speech. Experiment 4 will test whether cross-linguistic differences in the proportion of multisyllabic words in infant-directed speech shape infants' word segmentation abilities. Spanish-learning infants, who hear far more multisyllabic words than English-learning infants over the first year, are expected to show greater facility in segmenting trisyllabic words from continuous speech than English-learning infants. The results will assess the degree to which previous learning leads to expectations that facilitate processing future speech, or whether infants' computational abilities operate continuously at each moment in time. The outcomes of these studies will inform future research exploring statistical learning abilities in young children with emergent language impairments.      PUBLIC HEALTH RELEVANCE: The goal of this research is to understand how typically developing infants discover structure in the complex speech they hear from caregivers. Future studies will use these tasks to assess language learning in children exhibiting difficulties with language (e.g., children at risk for specific language impairment, children with emergent autism spectrum disorder, or deaf children with cochlear implants). The eventual goals are to investigate whether the mechanisms responsible for learning are compromised in these clinical populations, and to develop more appropriate tools for early diagnosis and intervention.           The goal of this research is to understand how typically developing infants discover structure in the complex speech they hear from caregivers. Future studies will use these tasks to assess language learning in children exhibiting difficulties with language (e.g., children at risk for specific language impairment, children with emergent autism spectrum disorder, or deaf children with cochlear implants). The eventual goals are to investigate whether the mechanisms responsible for learning are compromised in these clinical populations, and to develop more appropriate tools for early diagnosis and intervention.         ",Infant Statistical Learning in Natural Language Acquisition,8124530,F32HD069094,"['Address', 'Adult', 'Affect', 'Attention', 'Caregivers', 'Characteristics', 'Child', 'Clinical', 'Cochlear Implants', 'Complex', 'Cues', 'Detection', 'Dimensions', 'Early Diagnosis', 'Early treatment', 'Environment', 'Exhibits', 'Exposure to', 'Female', 'Future', 'Goals', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Impairment', 'Infant', 'Knowledge', 'Laboratories', 'Language', 'Language Delays', 'Language Development', 'Learning', 'Length', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Outcome Study', 'Pattern', 'Play', 'Population', 'Probability', 'Process', 'Property', 'Research', 'Research Personnel', 'Research Training', 'Risk', 'Role', 'Shapes', 'Source', 'Speech', 'Stream', 'Structure', 'Testing', 'Time', 'Variant', 'Vocabulary', 'Voice', 'Yang', 'autism spectrum disorder', 'base', 'expectation', 'experience', 'male', 'natural language', 'novel', 'research study', 'simulation', 'specific language impairment', 'tool']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,F32,2011,48398,0.17975005150639847
"Speech Therapy Robot (STR) to assist in the administration of evidence based spee    DESCRIPTION (provided by applicant): This SBIR phase I project will develop a Speech Therapy Robot (STR) to assist in the administration of evidence-based speech and language therapy to provide individualized monitoring of multiple clients simultaneously in a school setting. STR will use biologically plausible artificial intelligence models to prototype a system that is affordable, easy to use, portable and extensible to work with any number of students (clients) with disorder. Most children make some mistakes as they learn to say new words, but a speech sound disorder results when mistakes continue past a certain age. Speech sound disorders include problems with articulation (making sounds) and phonological processes (sound patterns), and it is one of the largest disabilities in the United States. Children with speech disorders are evaluated by a speech-language pathologist (SLP) and treated via speech-language intervention within the child's classroom (classroom-based) or outside of the classroom (pull-out). Multiple studies have demonstrated that classroom-based service is beneficial over pull- out service, but currently it is not widely practiced because of the many challenges facing SLPs: 1) it requires collaboration with classroom teachers and administrators who are not trained in speech-language pathology, 2) it can create a larger client-SLP ratio, 3) a small client-nonclient student in-class ratio, 4) unable to provide adequate intervention, 5) there is a large variation in severity of disorder within clients, 6) longer session hours over pull-out service. The American Speech-Language-Hearing Association (ASHA) recommendations caseloads should not exceed 40, but the median caseload is 50 in elementary and secondary schools, with high of 80 clients. This heavy workload for an SLP limits their capacity to provide effective treatment. Thus a robotic system capable of reducing the workload and assisting SLPs to provide improved individual care is highly desired by those in this field. In this Phase I SBIR, we will develop novel biologically plausible models to address these challenges by developing a robot-assisted therapy system capable of real time monitoring and assessment of client and client-provider interaction during the session, to determine client engagement, performance and to give feedback to providers in real time for improved treatment delivery. The biological models attempt to mimic the expert diagnostic capabilities of a SLP and extend it for use by non-SLPs to work with multiple clients at the same time. The solution will not require specialized training to use, allowing teachers to easily use it in their classrooms. In Phase I, we will demonstrate the feasibility and accuracy of STR. STR is not just a minor improvement over existing technologies but a technology and application that do not exist today. In Phase II, we will extend the capabilities towards a fully biologically plausible system to mimic expert human performance levels to develop a robotic system for speech-language therapy, this will be followed by clinical trials to ensure accuracy, efficacy of STR to facilitate evidence-based therapy. Variations of the system can be used towards phonology, morphology/syntax, pragmatics, language, fluency and/or vocabulary.      PUBLIC HEALTH RELEVANCE: Overall the project provides direct relevance to public health by facilitating new insights through the development of a novel biologically plausible artificial intelligence system capable of real time monitoring and assessment of verbal therapy session content in real time to determine patient engagement, performance and give feedback to providers in real time to improve treatment delivery, in a school setting. The novel biologically plausible device will significantly impact the current known methods of in classroom evaluation, monitoring and treatment of speech disorders. The project can help in substantial improvement in patient client interaction, better treatment, lower burden on speech language pathologists, and will significantly impact the current known methods, technologies, treatments, and address critical barriers to progress in the field.           Overall the project provides direct relevance to public health by facilitating new insights through the development of a novel biologically plausible artificial intelligence system capable of real time monitoring and assessment of verbal therapy session content in real time to determine patient engagement, performance and give feedback to providers in real time to improve treatment delivery, in a school setting. The novel biologically plausible device will significantly impact the current known methods of in classroom evaluation, monitoring and treatment of speech disorders. The project can help in substantial improvement in patient client interaction, better treatment, lower burden on speech language pathologists, and will significantly impact the current known methods, technologies, treatments, and address critical barriers to progress in the field.         ",Speech Therapy Robot (STR) to assist in the administration of evidence based spee,8207025,R43LM011325,"['Accent', 'Address', 'Administrator', 'Affect', 'Age', 'Algorithms', 'American Speech-Language-Hearing Association', 'Antirrhinum', 'Area', 'Artificial Intelligence', 'Auditory system', 'Biological Models', 'Caring', 'Cerebellum', 'Child', 'Client', 'Clinical Trials', 'Cognitive', 'Collaborations', 'Communication impairment', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Early identification', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Feedback', 'Florida', 'Future', 'Glosso-Sterandryl', 'Goals', 'Hour', 'Human', 'Impairment', 'Individual', 'Intervention', 'Joints', 'Language', 'Language Pathology', 'Language Therapy', 'Learning', 'Liquid substance', 'Memory', 'Methods', 'Minor', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'Neurons', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phonetics', 'Process', 'Provider', 'Public Health', 'Recommendation', 'Robot', 'Robotics', 'Running', 'Schools', 'Secondary Schools', 'Services', 'Severities', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Solutions', 'Source', 'Speech', 'Speech Disorders', 'Speech Sound', 'Speech Therapy', 'Speech-Language Pathology', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Time', 'Training', 'United States', 'Universities', 'Variant', 'Vocabulary', 'Voice', 'Work', 'Workload', 'base', 'biological systems', 'cost', 'design', 'disability', 'effective therapy', 'elementary school', 'evidence base', 'improved', 'innovation', 'insight', 'neural model', 'novel', 'phonology', 'prevent', 'professor', 'prototype', 'robot assistance', 'sound', 'speech processing', 'success', 'syntax', 'teacher']",NLM,"AVENTUSOFT, LLC",R43,2011,95949,0.25108734985977305
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,8100589,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Engineering', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'phrases', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2010,211698,0.28904621294210486
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7643857,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Engineering', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'phrases', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2009,213837,0.28904621294210486
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7452462,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Clinical Engineering', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Condition', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Rate', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2008,213837,0.28904621294210486
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7253196,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Clinical Engineering', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Condition', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Rate', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2007,216654,0.28904621294210486
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,7035268,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2006,159900,0.23202288614342376
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,6881272,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2005,164000,0.23202288614342376
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6833535,R01DC000319,"['Parkinson&apos', 's disease', 'adult human (21+)', 'amyotrophic lateral sclerosis', 'artificial intelligence', 'behavioral /social science research tag', 'cerebellar disorders', 'cerebral palsy', 'child (0-11)', 'clinical research', 'disease /disorder classification', 'dysarthria', 'human subject', 'multiple sclerosis', 'nervous system disorder diagnosis', 'neural information processing', 'perception', 'speech', 'speech disorder diagnosis', 'stroke', 'vocabulary', 'voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2005,437328,0.37578243899558283
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,6782453,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2004,164000,0.23202288614342376
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6689543,R01DC000319,"['Parkinson&apos', 's disease', 'adult human (21+)', 'amyotrophic lateral sclerosis', 'artificial intelligence', 'behavioral /social science research tag', 'cerebellar disorders', 'cerebral palsy', 'child (0-11)', 'clinical research', 'disease /disorder classification', 'dysarthria', 'human subject', 'multiple sclerosis', 'nervous system disorder diagnosis', 'neural information processing', 'perception', 'speech', 'speech disorder diagnosis', 'stroke', 'vocabulary', 'voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2004,424599,0.37578243899558283
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6626845,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2003,412234,0.37578243899558283
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6489511,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2002,402685,0.37578243899558283
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6286948,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2001,407733,0.37578243899558283
