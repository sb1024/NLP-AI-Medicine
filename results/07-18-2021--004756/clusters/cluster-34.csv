text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,9739919,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Base Management', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistics', 'subchondral bone', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,588354,-0.01964416397762782
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9713512,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2019,647706,-0.00379696634290627
"AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space Project Summary  An extensible analysis platform will be developed to accurately perform the automated genotyping of PCR/capillary electrophoresis (CE) traces for multiple disease-associated short tandem repeater (STR) assays. This study will evaluate the feasibility of developing generalizable and adaptive molecular analysis models, and will ultimately establish a new paradigm for deep learning analytical tools in the molecular diagnostic space.  Advanced machine learning strategies will be applied to interpret genotypes of inherited disorders caused by genetically unstable STR DNA sequences. STRs have traditionally been difficult to investigate due to their length (on the order of kilobases) and low sequence complexity, which elude detection by traditional and next- generation sequencing technologies. However, advances in PCR/CE technology have enabled the amplification and fragment sizing of STR DNA fragments, advancing clinical research and diagnostic test development for several neurodegenerative disorders, such as fragile X syndrome and amyotrophic lateral sclerosis. Despite these advances, the analysis of PCR/CE data from assays targeting STRs remains a manual, burdensome, and subjective process. There is a clear need to create a system that can scale with the development of new assays, and the proposed approach utilizes modern breakthroughs in artificial intelligence to fulfill that need.  This method will leverage recent advances in representation learning to establish a generalized and adaptive framework for automated PCR/CE annotation that can scale to new assays and improve automatically with the inclusion of new data. The project will benefit from Asuragen’s experience in optimizing repeat-primed chemistries to develop and commercialize multiple high performance assays including the AmplideX PCR/CE FMR1 kit. Importantly, the proposed modeling strategy will borrow-strength across multiple established PCR/CE assays and generalize to future PCR/CE assays for novel STR disease associated biomarkers. This system will be paramount to enabling a continuous learning platform wherein computationally-assisted annotation of PCR/CE assays can be continuously improved and integrated in to clinical research tools and diagnostics. Project Narrative  We are developing AmplideX DeepNet, an artificial intelligence-based analysis system that can accurately perform computationally-assisted analysis of molecular diagnostic assays. The proposed system will build upon recent breakthroughs in artificial intelligence to allow it to easily adapt to new assays and to continue to improve. The system will be applied to assays for several disorders, including fragile X syndrome, amyotrophic lateral sclerosis (ALS), myotonic dystrophy, and Huntington’s disease, and will provide a number of benefits over current analysis methods by reducing turn-around time for assay results and assuring reproducible reporting between operators and labs.","AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space",9678895,R43GM128498,"['Alleles', 'American', 'Amyotrophic Lateral Sclerosis', 'Artificial Intelligence', 'Automated Annotation', 'Biological Assay', 'Biological Markers', 'C9ORF72', 'Capillary Electrophoresis', 'Chemistry', 'Clinical', 'Clinical Research', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'FMR1', 'FMR1 repeat', 'Fragile X Syndrome', 'Future', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hand', 'Hereditary Disease', 'Heritability', 'Huntington Disease', 'Interruption', 'Learning', 'Length', 'Machine Learning', 'Manuals', 'Medical Genetics', 'Methods', 'Modeling', 'Modernization', 'Molecular Analysis', 'Myotonic Dystrophy', 'Neurodegenerative Disorders', 'Nucleotides', 'Pathogenicity', 'Performance', 'Phase', 'Process', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Running', 'Sampling', 'Short Tandem Repeat', 'System', 'Systems Analysis', 'Technology', 'Testing', 'Time', 'Training', 'analysis pipeline', 'analytical tool', 'automated analysis', 'base', 'clinical diagnostics', 'cohort', 'computer framework', 'deep learning', 'design', 'diagnostic assay', 'experience', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'heuristics', 'human-in-the-loop', 'improved', 'instrumentation', 'learning progression', 'learning strategy', 'medical schools', 'molecular diagnostics', 'nervous system disorder', 'next generation sequencing', 'novel', 'research and development', 'success', 'tool']",NIGMS,"ASURAGEN, INC.",R43,2019,269217,-0.001992921559521092
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,9887588,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2019,286435,-0.038927827842897895
"Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine Project Summary  The objective of this proposal is to provide a robust course of training for Gilmer Valdes, PhD, DABR, a candidate with an excellent foundation in clinical and machine learning research, to enable him to become an independent investigator. The proposed research aims to address a tradeoff between interpretability and accuracy of modern machine learning algorithms which limits their use in clinical practice. The candidate’s central hypothesis is that the current tradeoff is not a law of nature but rather a limitation of current interpretable machine learning algorithms. Towards proving this hypothesis, the candidate, leading a multidisciplinary team, have developed unique mathematical frameworks (MediBoost and the Conditional Interpretable Super Learner) to build interpretable and accurate models. The proposed research will I) implement and extensively benchmark these frameworks and II) use the algorithms develop to solve three clinical problems where potentially suboptimal models are currently used to make clinical decisions: 1) predicting mortality in the Intensive Care Unit, 2) predicting risk of Hospital Acquired Venous Thromboembolism, 3) predicting which prostate cancer patients benefit the most from adjuvant radiotherapy. The candidate’s training and research plan, multidisciplinary by nature, takes advantage of the proximity of UC San Francisco, Stanford and UC Berkeley and proposes a training plan that cannot be easily replicated elsewhere. Recognizing the multidisciplinary nature of the work proposed, the author will be mentored and work closely with a stellar committee from three institutions and different scientific areas (Machine Learning, Biostatistics, Statistics, Hospital Medicine, Cancer Research and Quality Assurance in Medicine): Jerome H. Friedman PhD (Stanford Statistics Department), Mark Van der Laan PhD (Berkeley Biostatistics and Statistics Department), Mark Segal (UCSF Epidimiology and Biostatistics Deparments), Andrew Auerbach MD (UCSF Medicine Department), Felix Y. Feng MD (UCSF Radiation Oncology),and Timothy D. Solberg PhD (UCSF Radiation Oncology). This committee will be coordinated by Dr Solberg. The candidate also counts with a strong a multidisciplinary team of collaborators. Successful completion of the proposed research will develop the next generation of accurate and interpretable Machine Learning algorithms and solve three important clinical problems where linear models are currently used in clinical settings. This proposal has wide-ranging implications across the healthcare spectrum. The intermediate-term goal is for the candidate to acquire the knowledge, technical skills and expertise necessary to submit a successful R01 proposal. PROJECT NARRATIVE Current state of the art machine learning algorithms have a marked tradeoff between accuracy and interpretability. In medicine, where errors can have a dire consequence and knowledge representation and validation is as relevant as accuracy, the development of accurate and interpretable algorithms is of paramount importance. My research project will address a critical public health need by developing machine learning algorithms that are both accurate and interpretable, and apply them to solve specific clinical problems.",Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine,9743021,K08EB026500,"['Address', 'Adjuvant Radiotherapy', 'Algorithms', 'Area', 'Benchmarking', 'Biometry', 'Cancer Patient', 'Classification', 'Clinical', 'Collection', 'Communities', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Doctor of Philosophy', 'Foundations', 'Goals', 'Healthcare', 'Hospitals', 'Institution', 'Intensive Care Units', 'Knowledge', 'Label', 'Laws', 'Libraries', 'Limb structure', 'Linear Models', 'Machine Learning', 'Malignant neoplasm of prostate', 'Mathematics', 'Mediating', 'Medical', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Patient Triage', 'Patients', 'Performance', 'Physicians', 'Pneumonia', 'Polynomial Models', 'Public Health', 'Radiation Oncology', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'San Francisco', 'Structure', 'Survival Analysis', 'Technical Expertise', 'Testing', 'Thromboembolism', 'Training', 'Trees', 'Validation', 'Venous', 'Work', 'anticancer research', 'artificial neural network', 'asthmatic patient', 'classification trees', 'clinical decision-making', 'clinical practice', 'design', 'improved', 'information organization', 'machine learning algorithm', 'medical specialties', 'mortality', 'multidisciplinary', 'neural network', 'next generation', 'novel', 'quality assurance', 'random forest', 'regression trees', 'standard care', 'statistics', 'task analysis', 'theories']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2019,191912,-0.03672838184996598
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9733308,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2019,388750,-0.02116943623472907
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,-0.01676380275612213
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9664620,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2019,573396,-0.02281490004533545
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9704042,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoke', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'imaging biomarker', 'improved', 'learning strategy', 'machine learning algorithm', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2019,189000,-0.07061272998642412
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9756832,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Trauma', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'deep learning', 'design', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neurophysiology', 'novel', 'predictive modeling', 'prevent', 'random forest', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,250346,-0.008323577787773952
"2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT SUMMARY/ABSTRACT A 1.5-day symposium, StoneLab will be held December 6-7, 2019 at AUA Headquarters in Linthicum, Maryland, near BWI Airport. The novel StoneLab Symposium will be a meeting where kidney stone researchers can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, medical device design and development. We plan to encourage researchers to “think outside the box” in the search for advances in kidney stone treatment. The target audience for this meeting includes: urologists, nephrologists, basic scientists, early-career investigators, residents, research fellows, and project management members of urology research teams in the area of nephrolithiasis and related technology development. It is essential that the urology community, and especially trainees, early-career physician- scientists, and researchers be provided with accessible opportunities to gain understanding of new knowledge and recent advances in not only their own fields, but in related fields that they may not otherwise be aware of, and leverage this for new research frameworks that can help improve the treatment of kidney stone disease. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons. The intent of these discussions is to spur the development of collaborations and the generation of new ideas and avenues for exploration. Another goal of the meeting is to help identify future funding needs and growth areas for kidney stone research. The R13 support requested in this application will encourage early investigators to participate in the workshop and stimulate their development as researchers and surgeon- scientists. This symposium will have a major impact by bringing together premier kidney stone researchers, along with experts in nephrology, biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development, to explore solutions for the most pressing translational science issues facing kidney stone researchers today. The Principal Investigator (PI), Carolyn J.M. Best, PhD, is AUA Director of Research. The Program Planning Committee consists of a diverse multidisciplinary team of distinguished scientists and urologists, most of whom also serve in leadership roles of several AUA-affiliated subspecialty societies: Khurshid Ghani, MD, MS (Chair); Ben H. Chew, MD, MSc (Co-Chair); Thomas Chi, MD; Benjamin Canales, MD, MPH; Gary Curhan, MD, ScD; Amy Krambeck, MD; Dirk Lange, PhD; Manoj Monga, MD, FACS; Kristina Penniston, PhD,RD; and Aria Olumi, MD (ex-officio). 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE STATEMENT This application seeks support to enable the American Urological Association (AUA) to invite early-career investigators (and late-stage trainees) to participate in a novel interdisciplinary symposium where premier kidney stone researchers, along with experts in nephrology, can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development with the aim of “thinking outside the box” in the search for advances in kidney stone treatment. This meeting will explore solutions for the most pressing translational science issues facing kidney stone researchers today, research in interdisciplinary fields that might be leveraged for new research to improve treatment of kidney stone disease, and help identify future funding needs and growth areas for kidney stone research. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons with the intent to spur the development of collaborations and the generation of new ideas and avenues for exploration.",2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE,9914630,U13DK124023,"['American', 'Area', 'Artificial Intelligence', 'Award', 'Awareness', 'Basic Science', 'Big Data', 'Big Data Methods', 'Biology', 'Biomedical Engineering', 'Chemistry', 'Collaborations', 'Communities', 'Development', 'Development Plans', 'Device or Instrument Development', 'Discipline', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Fostering', 'Funding', 'Future', 'Generations', 'Genomics', 'Goals', 'Grant', 'Growth', 'Kidney Calculi', 'Knowledge', 'Lead', 'Leadership', 'Machine Learning', 'Maryland', 'Medical', 'Medical Device Designs', 'Methodology', 'Miniaturization', 'Nephrolithiasis', 'Nephrology', 'North America', 'Operative Surgical Procedures', 'Optics', 'Physicians', 'Physics', 'Prevalence', 'Principal Investigator', 'Privatization', 'Research', 'Research Personnel', 'Robotics', 'Role', 'Science', 'Scientist', 'Societies', 'Surgeon', 'Technology', 'Translational Research', 'Travel', 'United States', 'Urologist', 'Urology', 'career', 'career development', 'improved', 'innovation', 'interest', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'next generation sequencing', 'novel', 'programs', 'public health relevance', 'symposium', 'technology development', 'urologic']",NIDDK,AMERICAN UROLOGICAL ASSOCIATION,U13,2019,10000,-0.013125330052253383
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9724345,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'random forest', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2019,292500,-0.021632587156074183
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9731367,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'machine learning algorithm', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2019,410000,-0.07806203877178262
"Predicting Parkinson's Disease Progression Rate Using Causal Measures of Functional MRI with Deep Learning Predictive Models Project abstract Parkinson's disease (PD) is the second most common neurodegenerative disease. A critical gap in the treatment of PD patients is that there is no clinically adopted method to predict an individual's progression rate. A predictor would enable the enrichment of disease modifying drug trials with fast progressors likely to show changes in the short duration of a clinical trial and enable a more informed discussion with patients about their prognosis. This proposal develops a composite biomarker of progression rate using the connectivity information provided by resting-state functional Magnetic Resonance Imaging (rs-fMRI) and deep learning. Deep learning (DL) is well suited to form predictive models because it learns both an optimal hierarchy of features and how to combine them for accurate prediction. In rs-fMRI the blood-oxygen level dependent signal can be analyzed to infer connectivity throughout the brain. Traditionally, connectivity has been computed as the correlation between average regional activation time courses. However correlation based connectivity is prone to inferring spurious connections due to its inability to distinguish indirect from direct connectivity and inability to distinguish bidirectional from unidirectional connectivity. A causal connectivity approach can discern these differences and thereby provide a more faithful characterization of the true neurobiological connectivity. The existing literature suggests connectivity, particularly causal connectivity, from rs-fMRI can inform the estimation of PD progression, but the attempt to predict progression rate with causal connectivity in a DL model is unique to this project.  This research develops several distinct approaches for building a progression rate predictor and apply them to three datasets including: the Parkinson's Progression Markers Initiative dataset, the NINDS Parkinson's Disease Biomarkers Program (PDBP) dataset, and the University of Texas Southwestern Medical Center's prospective imaging extension to the NINDS PBDP. In these studies, individual progression rates have been tracked over multiple years using multiple clinical measures. First, causal and correlative measures will be generated regionally and used with a DL model to create a baseline predictor of progression rate. Second, voxel- level causal measures will be generated as the increased granularity is expected to improve prediction accuracy. Third, since purely data-driven DL methods can be sensitive to dataset limitations, such as insufficient subjects and noise, these limitations will be addressed by developing a new structural connectivity regularization approach that constrains causal connectivity by the subject's own diffusion MRI. This regularization method will be general and likely applicable for building predictors for other neurological disorders such as stroke and Alzheimer's disease. This proposal will yield both DL models for predicting progression rate and a novel method to calculate constrained causal connectivity. All predictive models, composite neuroimaging biomarkers of progression rate and software will be publicly disseminated for ready incorporation by the scientific and clinical communities. Project narrative Parkinson's disease is the second most common neurodegenerative disease and this debilitating and incurable disease has no known cure. A cure for PD remains elusive due to the lack of clinically adopted predictors of progression rate, which if constructed would 1) hasten the discovery of disease modifying drugs by enriching clinical trials with fast progressors who likely will show changes over the trial, 2) allow for stratification of patients by progression rate in those trials, and 3) enable an informed discussion with patients about their prognosis. This proposal develops and validates distinct approaches to predict PD progression rate, identifies new biomarkers of progression rate, and yields a generalizable framework for constraining causal measures from fMRI with the subject's own structural connectivity that is readily repurposable for other neurological disorders with connectivity changes such as stroke and Alzheimer's.",Predicting Parkinson's Disease Progression Rate Using Causal Measures of Functional MRI with Deep Learning Predictive Models,9909889,F31NS115348,"['Address', 'Adopted', 'Adoption', 'Alzheimer&apos', 's Disease', 'Biological Markers', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Communities', 'Computer software', 'Data', 'Data Set', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Disease Progression', 'Etiology', 'Fiber', 'Florida', 'Functional Magnetic Resonance Imaging', 'Image', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Medical center', 'Methods', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Nerve Degeneration', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurologic', 'Noise', 'Outcome', 'Parkinson Disease', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Research', 'Rest', 'Signal Transduction', 'Site', 'Stroke', 'Structure', 'Texas', 'Time', 'Training', 'Universities', 'Work', 'base', 'blood oxygen level dependent', 'cognitive testing', 'deep learning', 'improved', 'interest', 'learning community', 'learning strategy', 'nervous system disorder', 'neural network', 'neuroimaging marker', 'novel', 'outcome forecast', 'patient stratification', 'predictive modeling', 'prognostic value', 'programs', 'progression marker', 'prospective', 'tractography']",NINDS,UT SOUTHWESTERN MEDICAL CENTER,F31,2019,34014,-0.06946049150430786
"Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning Project Summary Development of general and efficient methods for functionalization of alcohols is highly warranted due to the ubiquity and prominence of this functional group in natural products. Such methods would allow for late-stage diversification of complex molecules and, consequently, could have a broad impact in natural product synthesis and preparation of relevant pharmaceutical materials. However, owing to the chemical inertness of alcohols, most methods typically require installation of activating groups for functionalization, making them unattractive from an atom- and step-economical perspective. Nonetheless, many advances have been made. In particular, the Barton-McCombie reaction has become an indispensable tool for reductive functionalization of alcohols. Unfortunately, this transformation requires pre- functionalization of the alcohol substrate, employs highly toxic tin reagents, and invokes the use high reaction temperatures or harmful UV light for initiation of radical intermediates. Furthermore, the overall transformation is limited to H-atom incorporation or reductive coupling with alkenes. Lastly, only a few deoxygenation methods exist that are amenable for late-stage and site-selective deoxygenation in complex systems. Moreover, physical organic chemistry tools available to facilitate the selection of a set of conditions or parameters to afford site-selectivity are limited. In this proposal, we will develop a mild and practical photocatalytic deoxygenation of alcohols. Our strategy will focus on solving the inherent limitation of the Barton McCombie reaction by 1) avoiding the use of toxic tin reagents, 2) obviating the need for pre-functionalization of the alcohol substrate, and 3) allowing for modular coupling of formed alkyl radicals via Ni-catalysis. Specific aim 1 explores the development of a novel photoredox-catalyzed deoxygenation of alcohols. In addition, we outline a general protocol for deoxyfunctionalization of alcohols via inception of the alkyl radical intermediate, formed via β-scission, with various radical electrophiles. Moreover, we highlight an innovative method for the direct cross-coupling of alcohols via metallophotoredox catalysis in both racemic and enantioselective fashion. Specific aim 2 addresses the design strategy for implementing physical chemistry techniques such as Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space. Also, we outline applying this strategy to identify a set of optimal conditions to confer site-selective functionalization in complex polyols. Project Narrative Mild and site-controlled deoxygenation of alcohols could significantly accelerate the late-stage synthesis/diversification of important organic molecules; however, current methods often employ toxic tin reagents, harsh reaction conditions, and require prefunctionalization of the alcohols employed. The strategy proposed would allow for a mild photocatalytic deoxygenation, as well as deoxyfunctionalization, of alcohols that solves the aforementioned limitations of prior art. Moreover, the proposed strategy outlines implementation of physical organic chemistry tools Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space.",Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning,9759306,F32GM129910,"['Address', 'Alcohol consumption', 'Alcohols', 'Alkenes', 'Arts', 'Catalysis', 'Chemicals', 'Complex', 'Coupling', 'Development', 'Employment', 'Intercept', 'Machine Learning', 'Methods', 'Natural Products', 'Organic Chemistry', 'Organic Synthesis', 'Performance', 'Pharmacologic Substance', 'Phosphines', 'Physical Chemistry', 'Preparation', 'Protocols documentation', 'Reaction', 'Reagent', 'Site', 'System', 'Techniques', 'Temperature', 'Tin', 'Ultraviolet Rays', 'Visible Radiation', 'alcohol involvement', 'catalyst', 'design', 'functional group', 'innovation', 'novel', 'polyol', 'predictive tools', 'tool']",NIGMS,PRINCETON UNIVERSITY,F32,2019,61226,-0.036473903392272654
"Developing and validating prognostic metabolomic signatures of diabetic kidney disease PROJECT SUMMARY/ABSTRACT Rationale. Diabetes is a leading cause of renal disease, accounting for 40% of the estimated 20 million US adult cases of chronic kidney disease. There is, however, substantial heterogeneity across diabetic patients with regards to development of kidney disease. Hence, there is an urgent need to identify prognostic biomarkers that can provide early and reliable evidence of future kidney disease, so that high-risk patients can receive optimal medical care. Existing clinical, proteomic and genomic markers do not consistently nor accurately predict kidney function decline. Metabolomics, a systematic evaluation of the end-products of cellular function in fluids, has the potential to inform physiological and pathological effects of chronic diseases. Metabolomic analysis combined with advanced quantitative methods could play a key role in building clinically useful prognostic signatures of diabetic kidney disease. Yet, development of computational methods with adequate rigor has lagged behind the technical capacity to perform large scale quantitative metabolomics. In this proposal we aim to address this computational gap in diabetic kidney disease research. Aims. We will implement rigorous computational methods to identify robust prognostic metabolite + clinical + genetic signatures of diabetic kidney disease progression. Specifically, we aim to (i) test the accuracy of previous signatures, and apply state-of-the-art analytic techniques and novel statistical methods to identify new multivariate metabolite sets for predicting kidney disease progression; (ii) quantify patterns of co-regulation of metabolites in diabetic kidney disease, and develop new tools in network biology to discover novel enzymes, proteins, metabolites, and molecular pathways which are implicated in diabetic kidney disease progression; (iii) test if these models can accurately predict kidney disease progression in independent prospective cohorts. Methods. Using clinical, genetic and metabolomic data from large prospective cohorts of > 1200 diverse, well- characterized patients with Type 2 diabetes, we will apply statistical methods for variable selection (e.g., penalized regression), and machine learning methods (e.g., random forest), which are known to perform well in the high-dimensional setting, to identify robust and parsimonious signatures of kidney disease progression. We will quantify inter-metabolite co-regulation patterns and infer biological pathways implicated in diabetic kidney disease. Throughout the modeling process, a rigorous training-validation paradigm will be adopted in order to improve reproducibility of models and reduce chance findings. Impact. A major product of this work will be the development of a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline. Our findings will also provide insight into markers of renal dysfunction, and elucidate possible therapeutic targets for treating diabetic kidney disease, thus potentially informing the design of future clinical trials. PROJECT NARRATIVE Kidney disease, a major and common complication of diabetes, can lead to repeated hospitalizations and premature death. There is an urgent need to develop clinical tools that can provide early evidence that a given diabetic patient is likely to progress to kidney disease in the future. In this proposal, we will identify new urinary biomarkers and use novel statistical modeling methods to create a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline, with the ultimate goal of improving disease management and reducing mortality rates for these patients.",Developing and validating prognostic metabolomic signatures of diabetic kidney disease,9695208,R01DK110541,"['Accounting', 'Address', 'Adopted', 'Adult', 'Albuminuria', 'Algorithms', 'American', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biology', 'Caring', 'Cell physiology', 'Cessation of life', 'Chronic Disease', 'Chronic Kidney Failure', 'Chronic Kidney Insufficiency', 'Clinical', 'Clinical Trials', 'Collaborations', 'Comorbidity', 'Complications of Diabetes Mellitus', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Diagnosis', 'Disease', 'Disease Management', 'Disease Progression', 'Enzymes', 'Evaluation', 'Funding', 'Future', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Heterogeneity', 'Hospitalization', 'Kidney', 'Kidney Diseases', 'Laboratories', 'Lead', 'Link', 'Liquid substance', 'Longitudinal cohort', 'Machine Learning', 'Medical', 'Medical Genetics', 'Methods', 'Modeling', 'Molecular', 'Non-Insulin-Dependent Diabetes Mellitus', 'Pathologic', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Pima Indian', 'Play', 'Process', 'Prognostic Marker', 'Prospective cohort', 'Proteomics', 'Publishing', 'Recommendation', 'Regulation', 'Renal function', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Sampling', 'Sampling Studies', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Testing', 'Training', 'Type 2 diabetic', 'Urine', 'Validation', 'Work', 'bioinformatics tool', 'biological heterogeneity', 'chemical association', 'cohort', 'design', 'diabetic', 'diabetic patient', 'genetic signature', 'genomic biomarker', 'high dimensionality', 'high risk', 'improved', 'innovation', 'insight', 'kidney dysfunction', 'learning strategy', 'metabolome', 'metabolomics', 'model development', 'mortality', 'multidimensional data', 'nephrogenesis', 'network models', 'novel', 'open source', 'personalized medicine', 'predictive modeling', 'predictive signature', 'predictive test', 'premature', 'prognostic', 'prognostic signature', 'prospective', 'protein metabolite', 'random forest', 'targeted treatment', 'therapeutic target', 'tool', 'urinary']",NIDDK,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,335803,-0.00552808174181489
"Toward Precision Medicine for Substance Use Disorders with Co-Occurring PTSD - A Machine Learning Approach PROJECT SUMMARY/ABSTRACT Over 20 years of empirical data document the individual and societal costs of co-occurring substance use disorders (SUD) and posttraumatic stress disorder (PTSD). Although evidence-based treatments are available for SUD+PTSD, the lack of methods or guidelines for personalized treatment recommendations represents a critical barrier to progress in the field. Secondary analyses of existing treatment research datasets have the promise to identify cost-effective and noninvasive variables that represent predictors, moderators, mediators, mechanisms of action, and secondary effects in a way that can be easily replicated in research and expanded to the real world. Importantly, extant datasets provide an opportunity to develop and test innovative analytic approaches that leverage technological advances in computing power to: 1) develop precision medicine algorithms that can prognosticate and prescribe at the patient-level, 2) generate new hypotheses to guide future research on treatment development and dissemination, and 3) innovate existing methods that can be applied to solve related problems in the field. Accordingly, the proposed project will apply machine learning methodologies to the largest existing NIDA-funded multi-site clinical trial for SUD+PTSD in order to: Aim 1: Develop algorithms that can identify the probability of treatment response on both SUD and PTSD outcomes based on routinely collected intake data. Aim 2: Identify key variables associated with a significant increase or decrease in the treatment response probability. Aim 3: Assess the feasibility of providing data-driven, personalized prescriptions that fit patients to the treatment most likely to benefit them based on their individual profile. The proposed machine learning analyses will be developed by the applicant at the Anxiety and Health Behaviors Laboratory of The University of Texas at Austin, and evaluated at the Texas Advanced Computing Center (TACC), which houses the fastest academic supercomputer in the United States of America, and one of the most powerful systems in the world. Importantly, the algorithms (once developed) can be easily disseminated and do not require powerful hardware to execute. Therefore, the combination of advanced machine learning analyses and high-performance computing has the promise to provide methods and findings that can be expanded across extant datasets and future research projects to facilitate scientific discovery in the field of personalized medicine for addiction that: 1) goes beyond comparing mean response across treatments, and 2) that is otherwise not possible with traditional statistical methods. If successful, the project will support a strategy that represents a paradigm shift from one-size-fits-all recommendations derived from treatment comparisons in clinical trials, to a data-driven, precision medicine approach that prognosticates and prescribes at the patient level. PROJECT NARRATIVE The proposed project will apply machine learning methodologies to the largest existing NIDA-funded multi-site clinical trial for substance use disorders (SUD) and posttraumatic stress disorder (PTSD). If the project is successful, it will be the first to: 1) develop algorithms that can identify the probability of treatment response on both SUD and PTSD outcomes based on routinely collected intake data, 2) identify key variables associated with a significant increase or decrease in the probability for treatment response, and 3) assess the feasibility of providing data-driven prescriptions that fit patients to the treatment most likely to benefit them based on their individual profile.",Toward Precision Medicine for Substance Use Disorders with Co-Occurring PTSD - A Machine Learning Approach,9809511,R36DA049122,"['Address', 'Aftercare', 'Algorithms', 'Americas', 'Anxiety', 'Clinical Trials', 'Comorbidity', 'Complex', 'Data', 'Data Collection', 'Data Set', 'Disease', 'Dropout', 'Evidence based treatment', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health behavior', 'Healthcare', 'High Performance Computing', 'Impairment', 'Individual', 'Inpatients', 'Intake', 'Laboratories', 'Machine Learning', 'Measures', 'Mediator of activation protein', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Multi-Institutional Clinical Trial', 'National Institute of Drug Abuse', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Predictive Value', 'Probability', 'Randomized', 'Recommendation', 'Recovery', 'Relapse', 'Research', 'Research Project Grants', 'Running', 'Sample Size', 'Selection for Treatments', 'Signal Transduction', 'Statistical Methods', 'Substance Use Disorder', 'Symptoms', 'System', 'Testing', 'Texas', 'Trauma', 'Treatment outcome', 'United States', 'Universities', 'Work', 'addiction', 'analytical method', 'anxiety treatment', 'austin', 'base', 'cost', 'cost effective', 'high dimensionality', 'improved', 'innovation', 'learning strategy', 'machine learning algorithm', 'outcome forecast', 'patient response', 'personalized medicine', 'precision medicine', 'predicting response', 'profiles in patients', 'response', 'secondary analysis', 'simulation', 'societal costs', 'suicidal risk', 'supercomputer', 'symposium', 'therapy development', 'treatment effect', 'treatment response']",NIDA,"UNIVERSITY OF TEXAS, AUSTIN",R36,2019,49534,-0.010613199483494783
"Using Causal Inference and Machine Learning Methods to Predict Cognitive Behavioral Treatment Response PROJECT SUMMARY/ABSTRACT Many patients with anxiety and fear disorders (AFDs) report minimal benefits when treated with an evidence- based psychotherapy (e.g., cognitive-behavioral therapy [CBT]) or pharmacotherapy (e.g., antidepressants; benzodiazepines). Conversely, some AFD patients are likely to benefit from virtually any treatment (e.g., supportive therapy). Using data collected in randomized controlled trials (RCTs), there has been limited progress determining how to use pre-treatment characteristics to match AFD patients to the treatment that is most likely to provide benefit. As a result, NIMH has forwarded Strategic Objectives focused on identifying treatment moderators and developing tools that predict differential treatment response. The broad goal of this proposed secondary data analysis is to apply causal inference and machine learning methods to prospective observational data to predict differential treatment response among patients with AFDs. The sample (n = 1,528) is from a longstanding NIMH-funded study of AFD patients who received: (a) CBT with concurrent pharmacotherapy, (b) CBT without pharmacotherapy, or (c) treatment as usual (TAU). Targeted maximum likelihood estimation (a causal inference method) and super learning (an ensemble machine learning method) will be used to accomplish the proposed Aims. Aim 1 will estimate the (overall) average effects of the three treatment types. Aim 2 will estimate “optimal treatment rules” to determine if differential treatment response can be meaningfully predicted based on a patient's multidimensional profile of pre-treatment symptoms. Aim 3 will estimate “optimal treatment rules” to determine if differential treatment response can be meaningfully predicted using all available pre-treatment covariates. The proposed study is highly innovative and could significantly impact the growing literature focused on predicting differential treatment effects and personalizing treatment for patients with AFDs. Although treatment effects estimates obtained using observational data and causal inference methods (i.e., adjusted for nonrandom treatment selection) are similar to those estimated in RCTs, this would be the first study to apply such methods to AFD patient data. This study will also be the first to use ensemble machine learning to develop composite moderators for AFDs (i.e., optimal treatment rules). In comparison, prior attempts to develop composite moderators have relied on less flexible model-building procedures prone to overfitting and unable to capture complex predictor-outcome associations (e.g., interactions among predictors; nonlinear associations). In achieving the proposed Aims, the current study would be a catalyst for future research using causal inference and machine learning to study predictors of differential treatment response among AFD patients. Results will be used to justify future research aimed at expanding and validating the models in larger observational samples and pragmatic RCTs (e.g., optimal treatment rules for specific medications/doses; timing of pharmacotherapy relative to CBT; second-wave CBT versus acceptance-based CBT). PROJECT NARRATIVE Some patients with anxiety and fear disorders may benefit only from certain evidence-based treatments, while others may benefit from virtually any treatment (e.g., talk therapy). The current study will use casual inference and machine learning methods to develop predictive models that identify the treatment(s) most likely to benefit a particular patient. The models developed in this study will serve as an important initial step toward efficient and personalized treatment for patients with anxiety and fear disorders.",Using Causal Inference and Machine Learning Methods to Predict Cognitive Behavioral Treatment Response,9724776,R21MH119492,"['Antidepressive Agents', 'Anxiety', 'Assessment tool', 'Benzodiazepines', 'Characteristics', 'Chronic', 'Clinic', 'Cognitive Therapy', 'Communities', 'Comorbidity', 'Complex', 'Counseling', 'Data', 'Data Analyses', 'Diagnosis', 'Dimensions', 'Disease', 'Dose', 'Evidence based treatment', 'Fright', 'Funding', 'Goals', 'Heterogeneity', 'Intervention', 'Learning', 'Life Stress', 'Literature', 'Machine Learning', 'Mental Health', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'National Institute of Mental Health', 'Onset of illness', 'Outpatients', 'Patient observation', 'Patients', 'Personality', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Procedures', 'Psychotherapy', 'Randomized Controlled Trials', 'Recommendation', 'Reporting', 'Sampling', 'Selection for Treatments', 'Severities', 'Supportive care', 'Symptoms', 'Treatment Effectiveness', 'Treatment outcome', 'Validation', 'anxiety treatment', 'base', 'catalyst', 'cost', 'depressive symptoms', 'effective therapy', 'evidence base', 'excessive anxiety', 'experience', 'flexibility', 'functional disability', 'improved', 'improved outcome', 'innovation', 'learning strategy', 'model building', 'optimal treatments', 'outcome prediction', 'personalized medicine', 'predictive modeling', 'predictive tools', 'prospective', 'response', 'sociodemographics', 'stress reactivity', 'symptom treatment', 'treatment as usual', 'treatment disparity', 'treatment effect', 'treatment response', 'virtual']",NIMH,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2019,265477,-0.023225560805781732
"Mechanistic Machine Learning PROJECT SUMMARY / ABSTRACT The goal of this project is to combine empirical data with mechanistic physiologic knowledge to produce personalized, quantitative predictions that can lead to improved treatments. In normal practice, physicians reason by analogy from generic physiologic principles, but the technology exists to exploit even imperfect physiologic models make treatment personalized and quantitatively grounded in physiology, and to improve learning from empirical data. We will apply data assimilation (DA), mechanistic mathematical modeling, machine learning, and control theory, which have revolutionized space travel, weather forecasting, transportation and flight, and manufacturing. Data assimilation and control theory have seen very limited use in medicine, usually applied in data-rich circumstances like continuous glucose monitoring or packemakers. Our previous work demonstrated use of data assimilation with glucose-insulin models to predict glucose in the outpatient type 2 diabetes setting. We will extend data assimilation and control theory using, for example, a constrained ensemble Kalman filter and an offline Markov Chain Monte Carlo algorithm, to better handle sparse, short training sets on rapidly changing patients, and we will apply it in the setting of glucose management in the intensive care unit (ICU). Moreover, we will develop DA for phenotyping applications by exploiting the parameter estimation capabilities of DA. Data assimilation can be used to estimate measureable and unmeasureable physiologic states and parameters, and we will use these estimates to create higher definition phenotypes. While we are focusing on glucose management in the ICU, we will develop methods that are likely to generalize, beginning the effort to develop DA in the context of healthcare more broadly. The work we propose is a necessary step toward being able to use mechanism-driven DA to test, validate and optimize personalized short-term treatment strategies, long-term health forecasts, and mechanistic physiologic understanding. We will carry out the following aims: AIM 1—forecast—extend the DA methodology to allow forecasting, personalization, model evaluation, and model selection in the ICU context, relating treatment input to physiologic outcome; AIM 2—phenotype—extend the DA framework to state and parameter estimation to allow for mechanism-based phenotyping, careful uncertainty quantification, and inference of difficult or impossible-to-measure physiology; AIM 3—control—extend the DA to include a controller that begins with desired clinical outcomes, e.g., glucose range, and estimates the inputs, e.g., insulin or nutrition, required to achieve the outcomes. Narrative The goal of this project is to develop better ways to combine data about individual patients with knowledge about physiology to create personalized forecasts and recommendations about a patient's health. We specifically address the management of glucose in the intensive care unit, an area of high importance that could benefit from improved forecasts and recommendations.",Mechanistic Machine Learning,9767278,R01LM012734,"['Address', 'Admission activity', 'Affect', 'Area', 'Assimilations', 'Carbohydrates', 'Clinical', 'Clinical Treatment', 'Complex', 'Computational Biology', 'Data', 'Data Science', 'Depressed mood', 'Early Intervention', 'Eating', 'Endocrine Physiology', 'Evaluation', 'Fingers', 'Food', 'Future', 'Glomerular Filtration Rate', 'Glucose', 'Goals', 'Health', 'Healthcare', 'Hepatic', 'Hour', 'Insulin', 'Insulin Infusion Systems', 'Insulin-Dependent Diabetes Mellitus', 'Intensive Care Units', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Manuals', 'Markov chain Monte Carlo methodology', 'Measurable', 'Measurement', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Minor Planets', 'Modeling', 'Monitor', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'Outpatients', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physicians', 'Physiological', 'Physiology', 'Power Plants', 'Property', 'Publishing', 'Recommendation', 'Renal function', 'Running', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transportation', 'Treatment outcome', 'Uncertainty', 'Validity of Results', 'Weather', 'Work', 'base', 'clinical phenotype', 'computerized', 'control theory', 'fly', 'glucose metabolism', 'glucose monitor', 'glucose production', 'health record', 'improved', 'individual patient', 'insight', 'insulin secretion', 'interstitial', 'mathematical model', 'nutrition', 'optimal treatments', 'outcome forecast', 'outcome prediction', 'personalized learning', 'personalized medicine', 'physiologic model', 'predictive modeling', 'reduced food intake', 'space travel', 'treatment strategy']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,660597,-0.020424064543238907
"The Environmental and Human Factors that Determine Ixodes scapularis-borne Diseases Incidence Project Summary Vector-borne diseases (VBDs) are the most common types of emerging and re-emerging infectious diseases in the world. VBD epidemics have been increasing over recent decades, with tickborne diseases having doubled in the last decade in the United States. Despite the increase in public health burden, over 80% of vector-control organizations lack preventative capabilities. Understanding the interplay between the environment, vectors, pathogens, and humans that expedite disease spread remains a challenge. The overarching goal of this project is to identify the key environmental and human drivers that have led to the emergence of VBDs. Current models that predict tickborne disease risk have oversimplified the process by focusing only on the vector, i.e. risk of tick exposure. A human’s risk of infection is not only a function of entomological risk but also of factors inherent to the individual including behavior or characteristics that increase susceptibility to disease. This project proposes a novel approach to tickborne disease prediction by developing a comprehensive model that incorporate pathogen population dynamics and human factors to predict disease risk. This study will investigate several pathogens vectored by the black-legged tick (Ixodes scapularis): Borrelia burgdorferi (Lyme disease), Anaplasma phagocytophilum (human granulocytic anaplasmosis), and Babesia microti (babesiosis). The central hypothesis is that the prediction of tickborne disease risk can be improved by using sophisticated statistical methods to identify environmental drivers that impact pathogen population dynamics while incorporating human demographic characteristics. The hypothesis will be addressed in the following aims: (1) Determine the current and historical population dynamic patterns of pathogens vectored by I. scapularis to predict pathogen distribution; (2) Determine the association between human characteristics and tick-borne disease risk in order to develop an improved spatial disease risk model. This model will allow the identification and quantification of factors that are associated with the emergence of tickborne diseases in New York State, which is geographically advantageous because it is representative of much of the natural environment that ticks encounter in the northeastern US including rapid and recent changes in climate and landscapes. The results of this project will be used to develop a public disease warning system that will use contemporary and future climate forecasts to monitor tick populations and predict potential disease outbreaks for areas with vulnerable populations. With climate forecasts predicting an increase in 2-3°C in temperature by 2100, there is uncertainty in how diseases will shift and a warning system will allow preparation accordingly. At the completion of the proposed research project, the applicant will have acquired the following skillsets through intensive, interdisciplinary mentorship: big data analysis, advanced statistics including Bayesian and machine learning methods, spatial analyses, and risk analysis. This will enable the applicant to succeed as an independent investigator to address the challenges posed by emerging infectious diseases. Project Narrative Vector-borne diseases are the most common types of emerging infectious diseases in the world and constitute major threats to public health. My project uses advanced statistical methods to uncover how interactions between environmental factors affecting pathogen population dynamics and human demographic characteristics determine human risk for three tick-borne diseases: Lyme disease, human granulocytic anaplasmosis, and babesiosis. The results of my project will be used to create a risk map to predict the spatial and temporal occurrence of tickborne diseases, which can inform public health strategies in order to mitigate disease burden.",The Environmental and Human Factors that Determine Ixodes scapularis-borne Diseases Incidence,9758579,F31AI133871,"['Accounting', 'Address', 'Adopted', 'Affect', 'Anaplasma phagocytophilum', 'Anaplasmosis', 'Area', 'Award', 'Babesia microti', 'Babesiosis', 'Bayesian Analysis', 'Bayesian learning', 'Behavior', 'Big Data', 'Big Data Methods', 'Bite', 'Black-legged Tick', 'Borrelia burgdorferi', 'Characteristics', 'Climate', 'Communicable Diseases', 'Communities', 'Contracts', 'Dangerousness', 'Data', 'Data Analyses', 'Decision Trees', 'Disease', 'Disease Outbreaks', 'Ecological Change', 'Ecology', 'Emerging Communicable Diseases', 'Entomology', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiological trend', 'Future', 'Geography', 'Goals', 'Health', 'Human', 'Human Characteristics', 'Incidence', 'Individual', 'Infection', 'Linear Regressions', 'Link', 'Lyme Disease', 'Machine Learning', 'Maps', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'New York', 'Pattern', 'Population', 'Population Dynamics', 'Predisposition', 'Preparation', 'Prevalence', 'Process', 'Public Health', 'Research Personnel', 'Research Project Grants', 'Risk', 'Risk Assessment', 'Risk Factors', 'Role', 'Sampling', 'Statistical Methods', 'System', 'Techniques', 'Temperature', 'Tick-Borne Diseases', 'Tick-Borne Infections', 'Ticks', 'Time', 'Training', 'Uncertainty', 'United States', 'Variant', 'Vector-transmitted infectious disease', 'Vulnerable Populations', 'anthropogenesis', 'base', 'burden of illness', 'cost effective', 'decision tree learning', 'demographics', 'disorder risk', 'environmental change', 'geographic population', 'granulocyte', 'human disease', 'human pathogen', 'improved', 'infection risk', 'learning strategy', 'model design', 'novel strategies', 'pathogen', 'pathogen exposure', 'predictive modeling', 'regression trees', 'spatial temporal variation', 'statistics', 'tick-borne pathogen', 'trend', 'vector', 'vector control', 'vector transmission']",NIAID,UNIVERSITY OF PENNSYLVANIA,F31,2019,45016,-0.043264132974270404
"Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1 Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at certain stages of the disease severity spectrum, specifically in the early stage and in advanced disease. These difficulties are due to a variety of causes that change over the course of the disease, including large between-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we build on our long-standing contribution to ocular imaging and propose novel and sensitive means to detect glaucoma and its progression that are optimized to the various stages of disease severity. We will use information gathered from visual fields (functional information) and a leading ocular imaging technology – optical coherence tomography (OCT; structural information) to map the capability of detecting changes across the entire disease severity spectrum to identify optimal parameters for each stage of the disease. Both commonly used parameters provided by the technologies and newly developed parameters with good diagnostic potential will be analyzed. We will use state-of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. We will also utilize a new imaging technology, the visible light OCT, to generate retinal images with outstanding resolution to extract information about the oxygen saturation of the tissue. This will provide in-vivo, real time, and noninvasive insight into tissue functionality. Taken together, this program will advance the use of structural and functional information with a substantial impact on the clinical management of subjects with glaucoma Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies that will substantially improve detection of glaucoma and its progression monitoring in order to prevent blindness.",Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1,9819321,R01EY013178,"['3-Dimensional', 'Blindness', 'Characteristics', 'Clinical', 'Clinical Management', 'Clinical Research', 'Complex', 'Data', 'Detection', 'Development', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Evaluation', 'Eye', 'Floor', 'Future', 'Glaucoma', 'Health', 'Human', 'Image', 'Imaging technology', 'Inner Plexiform Layer', 'Knowledge', 'Laboratories', 'Lead', 'Light', 'Machine Learning', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Oxygen Consumption', 'Oxygen saturation measurement', 'Pathology', 'Research Proposals', 'Resolution', 'Retinal', 'Retinal Diseases', 'Scanning', 'Severities', 'Severity of illness', 'Signal Transduction', 'Source', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Technology', 'Thick', 'Time', 'Tissue Extracts', 'Tissues', 'Translating', 'Visible Radiation', 'Vision', 'Visual Fields', 'Width', 'advanced disease', 'analytical method', 'base', 'clinical practice', 'cohort', 'computerized', 'deep learning', 'density', 'ganglion cell', 'improved', 'in vivo', 'innovation', 'innovative technologies', 'insight', 'instrument', 'invention', 'knowledge base', 'learning strategy', 'longitudinal dataset', 'macula', 'mathematical methods', 'new technology', 'novel', 'novel strategies', 'ocular imaging', 'preservation', 'prevent', 'programs', 'research study', 'retinal imaging', 'retinal nerve fiber layer', 'tissue oxygenation', 'tool']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2019,715489,-0.022542957649557997
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9697824,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Consumption', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neural network', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,564228,-0.01844619117644635
"Data-driven models of symptom heterogeneity to empower transdiagnostic multimodal biomarker discovery in mood disorders PROJECT SUMMARY/ABSTRACT My goal is to pursue an independent career in computational psychiatry by leveraging cutting-edge neuroimaging and data-driven analysis approaches to advance precision medicine in mental health. To build on my strong neuroimaging and computational background, the training component of this award emphasizes coursework and mentorship in the clinical and behavioral aspects of psychopathology. I will also receive mentorship to advance my theoretical and applied understanding of deep learning in this burgeoning field. The overarching research goal in this proposal is to develop computational strategies that account for the heterogeneity of mood disorders to improve the identification of treatment-response biomarkers. Response to pharmaceutical and behavioral antidepressant treatments is low, likely due to the symptomatic and etiological heterogeneity of depression whereby certain treatments may confer differential benefits for patients having particular symptom constellations. In the K99 phase, I will seek to improve prediction of individual antidepressant response using electroconvulsive therapy (ECT), which elicits robust and rapid antidepressant effects, as the treatment model. I will use MRI and clinical data from patients undergoing ECT collected for the large the Global ECT-MRI Research Collaboration (GEMRIC). In Aim 1, I will use exploratory factor analysis to characterize latent symptom dimensions of the GEMRIC cohort before, during, and after ECT. The accuracy of predicting clinical outcomes along the recovered symptom dimensions will be compared to traditional means of evaluating response using the total score of the Hamilton Depression Rating Scale (HDRS). Pursuit of this aim will expand my understanding of clinical psychiatry and lay foundational knowledge for the independent aims. Aim 2 will expand my deep learning and multimodal neuroimaging skillsets as I develop novel deep learning architectures to fuse multimodal imaging features of GEMRIC participants to further improve predictions of treatment response and cognitive impairment following ECT. Rather than simply concatenating multimodal features together, deep network architectures will discover latent feature representations. The R00 phase will be a logical progression of the skill sets I develop in the mentored phase and expand on these lines of research. Aim 3 will draw from a collection of large-scale MRI datasets from patients with more broadly defined mood disorders to identify multimodal imaging markers associated with transdiagnostic symptom domains. Aim 4 uses treatment groups from aim 3, including patients undergoing ketamine, sleep deprivation, cognitive behavioral therapy, and pharmaceuticals, to explore the extent to which biomarkers of therapeutic response, defined along the transdiagnostic symptom dimensions identified in Aim 3, are shared across treatment groups. I anticipate that discrete categorizations of mood disorders artificially obscures discovery of treatment-response biomarkers. Fulfillment of these aims will simultaneously propel me to independence and yield important insight into the treatment of heterogeneous mood disorders. PROJECT NARRATIVE Mood disorders including depression, bipolar, and post-traumatic stress disorder constitute the world's leading cause of disability and their burden is increasing. This proposal seeks to mitigate the burden of these related mood disorders by identifying patterns of brain structure and function indicative of a patient's likelihood of benefiting from various related interventions. This research has the potential to inform more personalized treatment strategies than are currently available and will likely further inform development of precision interventions targeting mood disorders.",Data-driven models of symptom heterogeneity to empower transdiagnostic multimodal biomarker discovery in mood disorders,9891890,K99MH119314,"['Anhedonia', 'Antidepressive Agents', 'Anxiety Disorders', 'Architecture', 'Award', 'Behavioral', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Categories', 'Clinical', 'Clinical Data', 'Cognitive', 'Cognitive Therapy', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease remission', 'Electroconvulsive Therapy', 'Etiology', 'Factor Analysis', 'Foundations', 'Functional Imaging', 'Goals', 'Hamilton Rating Scale for Depression', 'Heterogeneity', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Individual', 'Infusion procedures', 'Intervention', 'Ketamine', 'Knowledge', 'Left', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mentors', 'Mentorship', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Multimodal Imaging', 'National Institute of Mental Health', 'Negative Valence', 'Neurobiology', 'Participant', 'Patients', 'Pattern', 'Pharmacologic Substance', 'Pharmacotherapy', 'Phase', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Psychiatry', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Rest', 'Severity of illness', 'Sleep Deprivation', 'Sleeplessness', 'Structure', 'Symptoms', 'System', 'Therapeutic Intervention', 'Training', 'Transcend', 'Treatment Side Effects', 'antidepressant effect', 'anxious', 'biological systems', 'biomarker discovery', 'biomarker identification', 'career', 'classification algorithm', 'cohort', 'convolutional neural network', 'data archive', 'deep learning', 'disability', 'effective therapy', 'electric field', 'hippocampal morphometry', 'imaging biomarker', 'improved', 'insight', 'learning strategy', 'multimodality', 'network architecture', 'neurobiological mechanism', 'neuroimaging', 'novel', 'personalized intervention', 'personalized medicine', 'precision medicine', 'predict clinical outcome', 'predicting response', 'random forest', 'relating to nervous system', 'response', 'response biomarker', 'side effect', 'skills', 'therapy outcome', 'treatment group', 'treatment response', 'treatment strategy']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,K99,2019,104909,-0.017484312737832967
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9772892,F32HL144101,"['3-Dimensional', 'Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'convolutional neural network', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'supervised learning', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2019,66778,-0.019620122922725833
"Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations Q-Chem is a state-of-the-art commercial computational quantum chemistry program that has aided about 60,000 users in their modeling of molecular processes in a wide range of disciplines, including biology, chemistry, and materials science. In this proposal, we seek to significantly reduce the computational time (now around 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions. Specifically, we propose to use a multiple time step (MTS) simulation method, where a low-level (and less accurate) quantum chemistry method is used to propagate the system (i.e. move all atoms) at each time step (usually 0.5 or 1 fs), and then a high-level (i.e. more accurate and expensive) quantum chemistry method is used to correct the force on the atoms at longer time intervals. In this way, the simulation can be performed at the high-level energy surface in a fraction of time, compared with simulations performed only using the high-level quantum chemical method. In the Phase I proposal, our goal is to allow the high-level force update only once every 40—50 fs by identifying appropriate lower-level theories (Aim 1) and incorporating machine-learning techniques (Aim 2). This will accelerate accurate free energy simulations by 20—25 fold, reducing the overall computer time to around 25,000 CPU hours. Thus, our new MTS simulation method will make it feasible to routinely perform computational studies on enzymatic reaction mechanism. The addition of these new tools will also further strengthen Q-Chem's position as a global leader in the molecular modeling software market, making our program the most efficient and reliable computational quantum chemistry package for simulating large, complex chemical/biological systems. In this project, we seek to significantly reduce the computational time (ca. 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions to ca. 25,000 CPU Hours. Building upon sophisticated quantum mechanics, this can lead to reliable and quick predictions of enzyme activities.",Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations,9778517,R43GM133270,"['Acceleration', 'Accounting', 'Adopted', 'Back', 'Biochemical', 'Biochemical Reaction', 'Biology', 'Biomedical Research', 'Chemicals', 'Chemistry', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Development', 'Discipline', 'Enzymes', 'Foundations', 'Free Energy', 'Goals', 'Hour', 'Hybrids', 'Lead', 'Machine Learning', 'Maps', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Pathway interactions', 'Performance', 'Phase', 'Positioning Attribute', 'Potential Energy', 'Process', 'Protein Conformation', 'Proteins', 'Quantum Mechanics', 'Reaction', 'Recipe', 'Research', 'Research Personnel', 'Sampling', 'Scheme', 'Solvents', 'Surface', 'System', 'Techniques', 'Time', 'Update', 'biological systems', 'computer studies', 'cost', 'density', 'enzyme activity', 'enzyme model', 'improved', 'innovation', 'learning strategy', 'materials science', 'molecular mechanics', 'molecular modeling', 'programs', 'quantum', 'quantum chemistry', 'quantum computing', 'simulation', 'theories', 'time interval', 'tool']",NIGMS,"Q-CHEM, INC.",R43,2019,132011,-0.010652617070340634
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,9886611,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Medical', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Subject Headings', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2019,379614,-0.03932133366598671
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.020373515544318366
"Investigation of Stereotyped High-Frequency Oscillations with Computational Intelligence for the Prediction of Seizure Onset Zone in Epilepsy PROJECT SUMMARY Neurosurgical therapy of refractory epilepsy requires accurate localization of seizure onset zone (SOZ). In clinical practice, intracranial EEG (iEEG) is recorded in the epilepsy monitoring unit (EMU) over many days where multiple seizures are recorded to provide information to localize the SOZ. The prolonged monitoring in the EMU adds to the risk of complications and can include intracranial bleeding and potentially death. Recently, high frequency oscillations (HFO) of iEEG between 80 to 500 Hz are highly valued as a promising clinical biomarker for epilepsy. HFOs are believed to be clinically significant, and thus could be used for SOZ localization. However, HFOs can also be recorded from normal and non-epileptic cerebral structures. When defined only by rate or frequency, pathological HFOs are indistinguishable from physiological ones, which limit their application in epilepsy pre-surgical planning. In this proposal, to the best of our knowledge, we show of a recurrent waveform pattern that distinguishes pathological HFOs from physiological ones. In particular, we observed that the SOZ generates repeatedly a set of stereotyped HFO waveforms whereas the HFOs from nonepileptic regions were irregular in their waveform morphology. Based on these observations, using computational tools built on recent advances in sparse coding and unsupervised machine learning techniques, we propose to detect these stereotyped recurrent HFO waveform patterns directly from the continuous iEEG data of adult and pediatric patients and test their prognostic value by correlating the spatial distribution of detected events to clinical findings such as SOZ, resection zone and seizure freedom. We hypothesize that accurate detection of pathologic HFOs in brief iEEG recordings can identify the SOZ and eliminate the necessity of prolonged EMU monitoring and reduce the associated risks. With these motivations, in this project an interdisciplinary team composed of biomedical engineers, epileptologists and neurosurgeons will work together to develop and test novel computational tools to detect stereotyped HFOs and its subtypes in large iEEG datasets recorded with clinical electrodes. Developed algorithms and iEEG data will be shared with the research community to contribute to the reproducible research and help other research groups to develop novel methods. The results of this study will be essential for achieving our group's long term goal of developing an online neural signal processing system for the rapid and accurate identification of SOZ with brief invasive recording. PROJECT NARRATIVE Prolonged iEEG monitoring for SOZ localization does add to the risk of complications and may include serious issues, such as intracranial bleeding, meningoencephalitis, and eventually death. The intellectual merit of this project is to develop computational intelligence tools based on recent advances in sparse coding and unsupervised machine learning techniques to investigate stereotyped high frequency oscillations (HFOs) in long-term iEEG and test the hypothesis whether the automated detection of HFOs will yield accurate and fast identification of SOZ.",Investigation of Stereotyped High-Frequency Oscillations with Computational Intelligence for the Prediction of Seizure Onset Zone in Epilepsy,9802783,R01NS112497,"['Adult', 'Algorithms', 'Area', 'Biomedical Engineering', 'Brain', 'Cerebrum', 'Cessation of life', 'Characteristics', 'Child', 'Clinical', 'Code', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Electrodes', 'Electroencephalography', 'Environment', 'Epilepsy', 'Event', 'Excision', 'Freedom', 'Frequencies', 'Goals', 'Hemorrhage', 'High Frequency Oscillation', 'Hospitals', 'Hour', 'Investigation', 'Laboratories', 'Language', 'Lesion', 'Meningoencephalitis', 'Methods', 'Modernization', 'Monitor', 'Morphology', 'Motivation', 'Motor', 'Motor Cortex', 'Multicenter Studies', 'Neocortex', 'Neurosurgeon', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Pattern', 'Physiological', 'Recurrence', 'Refractory', 'Reproducibility', 'Research', 'Risk', 'Scheme', 'Seizures', 'Site', 'Spatial Distribution', 'Stereotyping', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Translations', 'Visual', 'Work', 'awake', 'base', 'clinical biomarkers', 'clinical practice', 'clinically significant', 'computational intelligence', 'computerized tools', 'cost', 'neurotransmission', 'novel', 'pediatric patients', 'prognostic value', 'prospective', 'signal processing', 'tool', 'unsupervised learning']",NINDS,UNIVERSITY OF HOUSTON,R01,2019,488027,-0.011772686216260663
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9671422,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Ingestion', 'Kinetics', 'Laboratories', 'Locomotion', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'automated image analysis', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'machine learning algorithm', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2019,475637,-0.021631805451896198
"A Clinical Trial Enrichment Tool Based on Subgroups Defined by Machine Learning Predictive Models ABSTRACT Neurodegenerative disorders, including amyotrophic lateral sclerosis (ALS), Friedreich's ataxia (FA), multiple sclerosis (MS), Duchenne muscular dystrophy (DMD), Alzheimer’s disease (AD), Parkinson’s disease (PD), and Huntington’s disease (HD) are characterized by heterogeneous disease progression. Efforts to identify responder subgroups may uncover subgroups that are more homogeneous in disease-related features than the full study population. As a result, a subgroup may exhibit a statistically significant effect size. However, current methodologies for subgroup analysis are limited by the relatively small number of prognostic and predictive indicators that can be used to describe subgroups. These methods are not well suited to describing subgroups with reduced heterogeneity in disease progression, or in identifying indicators for multifactorial diseases. We have developed and submitted a patent application for a novel subgroup analysis method based on grouping participants with similar predicted disease progression profiles and analyzing nearest neighbor subgroups within a clinical trial. We call this method Detectable Effect Cluster(DEC) analysis. In our phase 1 and phase 2 SBIR grants, we used ALS as a model disease to develop our API product that uses machine learning disease models to improve trial arm randomization and provide covariates for statistical analysis. In the ongoing phase 2 grant we are expanding our disease offerings to include AD. PD and HD. Building on a set of ALS disease progression models that we have previously developed and validated, we seek in this grant application to develop a novel prototype machine-learning based subgroup analysis application that we plan on adding to our product offerings. During this proposed phase 1 grant, we will address research-level questions regarding the nature of the subgroups defined using DEC analysis including how to define confidence intervals of our DEC-clusters, and estimated bounds for using prediction-thresholds as selection criteria for a confirmatory clinical trial. Finally, we will apply DEC analysis to three publicly-available clinical trial data sets in an attempt to identify subgroups with significant treatment effects. Aim 1: We will apply methods used in image analysis for identifying statistically significant subgroups to address the  multiplicity issue inherent in DEC Analysis. Aim 2: We will use statistical methods to model the confidence intervals of a power analysis in which DEC cluster  based selection criteria would be used for a confirmatory trial. Aim 3: We will isolate records from PRO-ACT that include whether a patient was treated with riluzole and two other  publicly available recent ALS datasets to test the application of DEC Analysis. Origent’s current suite of products will answer drug development needs of a full portfolio of neurodegenerative diseases. Ultimately, we see a series of machine learning applications aimed at solving drug development issues for multiple disease areas, including orphan diseases. These models and applications will vastly increase the speed and efficiency of drug development, resulting in faster, cheaper, more efficient drug trials that yield numerous new medications to ease human pain and suffering. NARRATIVE This work will develop Detectable Effect Cluster (DEC) analysis, a novel machine-learning based method of subgroup analysis. DEC analysis shows great promise in identifying patient subgroups with statistically significant drug effects within larger, more heterogeneous, failed therapeutic clinical trials. DEC analysis has the potential to rescue a drug that otherwise would have been discarded as a drug that does not provide therapeutic benefit, when, in fact, the opposite is true.",A Clinical Trial Enrichment Tool Based on Subgroups Defined by Machine Learning Predictive Models,9846759,R43MH122925,"['Address', 'Alzheimer&apos', 's Disease', 'Amyotrophic Lateral Sclerosis', 'Applications Grants', 'Area', 'Autoimmune Diseases', 'Cardiology', 'Clinical', 'Clinical Drug Development', 'Clinical Trials', 'Cluster Analysis', 'Communicable Diseases', 'Confidence Intervals', 'Data', 'Data Set', 'Disease', 'Disease Progression', 'Disease model', 'Drug Industry', 'Duchenne muscular dystrophy', 'Exhibits', 'Friedreich Ataxia', 'Grant', 'Grouping', 'Health', 'Heterogeneity', 'Hot Spot', 'Human', 'Huntington Disease', 'Image Analysis', 'Legal patent', 'Letters', 'Lung diseases', 'Machine Learning', 'Maps', 'Metabolic Diseases', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Nature', 'Neurodegenerative Disorders', 'Neurologic', 'Non-linear Models', 'Pain', 'Parkinson Disease', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Plant Roots', 'Prognostic Marker', 'Randomized', 'Rare Diseases', 'Records', 'Research', 'Riluzole', 'Risk', 'Scientist', 'Secondary to', 'Selection Criteria', 'Series', 'Small Business Innovation Research Grant', 'Speed', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Clinical Trial', 'Training', 'Work', 'arm', 'base', 'cohort', 'drug development', 'drug testing', 'experience', 'improved', 'interest', 'nervous system disorder', 'novel', 'off-patent', 'oncology', 'patient subsets', 'power analysis', 'predictive modeling', 'prognostic', 'prototype', 'research and development', 'response', 'study population', 'systems research', 'tool', 'treatment effect']",NIMH,"ORIGENT DATA SCIENCES, INC.",R43,2019,222907,-0.040857499205232545
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9702053,R21GM128020,"['Address', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'learning algorithm', 'machine learning algorithm', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'preservation', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2019,206250,-0.005996872645686477
"Automatic analysis of 3D skin images for chronic graft-versus-host disease (cGVHD) severity assessment Project Summary More than 20,000 hematopoietic stem cell transplants (including bone marrow transplants) are performed in the U.S. each year to cure a range of diseases ranging from leukemias to sickle cell anemia to autoimmune deficiencies in children. Unfortunately, most long-term non-relapse survivors will die of chronic graft-versus-host disease (cGVHD), which remains a disease of steadily increasing incidence and profound unmet need. A fundamental barrier in cGVHD management and research is a lack of sensitive and objective assessment tools that permit objective and reproducible measures of disease severity and progression. Skin is the most commonly affected organ in cGVHD and automated techniques capable of measuring precisely the surface area of involved skin in photographs may provide the tools necessary for effectively evaluating patient progress. We propose to (1) create the data set necessary to develop machine learning-based methods for the automatic analysis of cGVHD images, and (2) implement and evaluate these methods. Project Narrative  Chronic graft-versus-host disease (cGVHD) is a lethal disease that affects most long-term hematopoietic stem cell transplant (including bone marrow transplants) recipients. Skin images are used to assess disease severity and progression but the technology required to quantitatively and reproducibly analyze these images is lacking. This project aims at developing and evaluating this technology.",Automatic analysis of 3D skin images for chronic graft-versus-host disease (cGVHD) severity assessment,9648538,R21AR074589,"['3-Dimensional', 'Achievement', 'Affect', 'Agreement', 'Allogenic', 'Area', 'Assessment tool', 'Autoimmune Process', 'Body Surface', 'Bone Marrow Transplantation', 'Characteristics', 'Child', 'Circumscribed Lesion', 'Clinic', 'Clinical', 'Clinical Research', 'Cutaneous', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dermatologic', 'Dermatologist', 'Disease', 'Disease Management', 'Disease Progression', 'Documentation', 'Erythema', 'Exanthema', 'Future', 'Goals', 'Hematologic Neoplasms', 'Hematopoietic Stem Cell Transplantation', 'Hematopoietic System', 'Human', 'Image', 'Incidence', 'Industry', 'Institution', 'Label', 'Lead', 'Lesion', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Methods', 'Morbidity - disease rate', 'Organ', 'Outcome', 'Outcome Measure', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Population', 'Protocols documentation', 'Psoriasis', 'Reaction', 'Reproducibility', 'Research', 'Resources', 'Role', 'Scanning', 'Severities', 'Severity of illness', 'Sickle Cell Anemia', 'Site', 'Skin', 'Skin Cancer', 'Standardization', 'Surface', 'Survivors', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Three-dimensional analysis', 'Time', 'Transplant Recipients', 'Visit', 'Vitiligo', 'automated analysis', 'base', 'cancer imaging', 'chronic graft versus host disease', 'data warehouse', 'deep learning', 'deep neural network', 'digital', 'graft vs host disease', 'high risk', 'image processing', 'improved', 'interdisciplinary approach', 'learning strategy', 'leukemia', 'machine vision', 'mortality', 'network architecture', 'neural network architecture', 'novel therapeutics', 'patient subsets', 'prototype', 'repository', 'response', 'skin disorder', 'skin lesion', 'stereoscopic', 'success', 'tool']",NIAMS,VANDERBILT UNIVERSITY,R21,2019,179706,-0.023708141006619988
"Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure Project Summary Glaucoma is the second leading cause of blindness globally, and is characterized by optic nerve damage that leads to the death of retinal ganglion cells with accompanying visual field (VF) loss. The optic nerve head (ONH) is the site of injury to the optic nerve fibers and plays a central role in glaucoma pathogenesis and diagnosis. Traditionally, glaucoma is diagnosed based on fundus inspection of the ONH, which provides information about the surface contour of the ONH. However, the optic nerve damage occurs in the deeper layers. With the development of optical coherence tomography (OCT) techniques for three-dimensional (3D) retinal imaging, parameters derived from the 3D ONH related structure (e.g., Bruch's membrane opening minimum rim width, peripapillary retinal nerve fiber layer thickness, disc tilt etc.) have been studied to better understand glaucoma pathogenesis, and are used to supplement clinical diagnosis. In addition, studies of the ONH biomechanics have also shown that the strain level at the ONH at any given intraocular pressure level depends on the 3D geometry of the ONH related structure. A high strain level is hypothesized to contribute to retinal ganglion cell injury. Previous research has suggested that the 3D ONH related structure is correlated to glaucoma pathogenesis and critically important to glaucoma diagnosis. However, to date, a systematic study using clinical data to determine the impact of the 3D ONH related structure on glaucoma has not been conducted.  We propose to study the relationship between the 3D ONH related structure and glaucoma with a diverse set of combined techniques including image processing, computational mechanics and machine learning. The specific aims of this project are to: (1) Derive features from the 3D ONH related structure and study their implications on VF loss patterns (K99 Phase). (2) Investigate the impact of the strain field patterns at the ONH on glaucoma (K99 Phase). (3) Study the effect of the 3D ONH related features on OCT diagnostic parameters (R00 Phase). (4) Model central vision loss from the 3D ONH related structural features (R00 Phase). Collectively, these studies will provide new insights and perspectives into the structure-function relationships in glaucoma and establish ocular anatomy specific norms of retinal nerve fiber layer profiles, which will advance our current understanding of glaucoma pathogenesis and improve glaucoma diagnosis. Our research is of high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.  Through the proposed research and training plans, the applicant will build a solid knowledge base in ophthalmology and further improve his expertise in mathematical modeling and data science. This project will provide critical training opportunities to further enhance the applicant's capabilities to become an independent computational vision scientist in ophthalmology. Project Narrative The proposed research will study the relationship between the three-dimensional (3D) optic nerve head (ONH) related structure and glaucoma. Our study will advance the current understanding of glaucoma pathogenesis and improve glaucoma diagnosis by gaining new insights into the structure-function relationships in glaucoma and establishing ocular anatomy specific norms of retinal nerve fiber layer profiles. Our research has high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.",Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure,9666293,K99EY028631,"['3-Dimensional', 'Age', 'Anatomy', 'Biomechanics', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Cessation of life', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Cross-Sectional Studies', 'Data', 'Data Science', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Ear', 'Elements', 'Eye', 'Fundus', 'Gaussian model', 'Geometry', 'Glaucoma', 'Hour', 'Image', 'Individual', 'Injury', 'Lead', 'Linear Models', 'Linear Regressions', 'Location', 'Machine Learning', 'Measurement', 'Mechanics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Multivariate Analysis', 'Nerve Fibers', 'Observational Study', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Pathogenesis', 'Patients', 'Pattern', 'Phase', 'Physiologic Intraocular Pressure', 'Play', 'Population Study', 'Process', 'Quality of life', 'Research', 'Research Training', 'Resolution', 'Retinal Ganglion Cells', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Severities', 'Site', 'Solid', 'Source', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Techniques', 'Testing', 'Thick', 'Translating', 'Variant', 'Vision', 'Visual Fields', 'Width', 'base', 'cell injury', 'clinical Diagnosis', 'clinical practice', 'clinically relevant', 'deep neural network', 'demographics', 'fundus imaging', 'image processing', 'improved', 'independent component analysis', 'insight', 'knowledge base', 'learning strategy', 'mathematical model', 'nonlinear regression', 'optic cup', 'retina blood vessel structure', 'retinal imaging', 'retinal nerve fiber layer', 'study population', 'training opportunity', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,K99,2019,146837,-0.021459179362040944
"Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment Summary: The public health burden of major depressive disorder (MDD) is immense and current approaches for selecting antidepressant treatment have had limited success. By some estimates, fewer than one in three MDD patients will respond to their prescribed antidepressant and the quest for a treatment that will work is typically characterized by a lengthy course of trial-and-error. The need to identify patient characteristics (biomarkers) that can be used to objectively select personalized antidepressant treatment is clear. Accordingly, large clinical studies like the NIMH-funded Establishing Moderators and Biosignatures of Antidepressant Response for Clinical Care (EMBARC) study have collected massive amounts of baseline measures including those from various neuroimaging sources in the hope that some can be used to guide antidepressant treatment selection. These data bring with them many statistical challenges that have yet to be effectively addressed. These challenges include (1) dealing with high-dimensionality, (2) handling data missingness, and (3) determining how best to simultaneously model relationships between measures from multiple imaging modalities and the response of interest. The goal of this project is to acquire the essential training and experience to make significant progress in this area by addressing each of these challenges. Aim 1 of this project will employ state-of-the-art ensemble machine learning algorithms and targeted estimation to identify moderators of antidepressant treatment effect using scalar clinical, demographic, and summary neuroimaging data from clinical trials of antidepressant treatments, including EMBARC. Strategies for handling missing data in this context will also be investigated and guidelines on best practices will be proposed. Aim 2 will extend the methods used in Aim 1 and develop user-friendly software to directly incorporate high- dimensional multimodal neuroimaging data into treatment decision rules. Included in this aim will be an investigation into best practices for handling missing high-dimensional imaging data in the context of estimating treatment decision rules. Aim 3 will employ the novel methods developed in Aim 2 and the estimated treatment decision rules will be evaluated and compared with those developed in Aim 1. I have put together a training program that directly supports the completion of these research aims. It includes instruction, mentoring, and hands-on-experience (1) in psychopathology and the neural basis for psychiatric disorders and treatment for those disorders; (2) in the use of neuroimaging data to understand depression and response to antidepressant treatment; (3) in the use of modern algorithms to store, process, manipulate, and analyze big biomedical data like those arising in multimodal neuroimaging studies. This K01 Mentored Research Scientist Development Award will provide the training, time, and resources to be able to make substantial progress in addressing this important problem and will provide the skills and experience that will be crucial in my transition to an independent investigator. Public Health Relevance Statement: This proposal seeks to advance precision medicine through the development of new statistical methods that integrate clinical, demographic, and high-dimensional multimodal neuroimaging data to estimate treatment decision rules. The proposed research and training are laid out in the context of depression but the statistical tools to be developed will be general enough for constructing treatment decision rules for a wide array of diseases using a variety of data types. These statistical tools have the potential to reduce the burden of diseases like depression by providing personalized treatment that has the best chance for success.",Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment,9785626,K01MH113850,"['Address', 'Algorithms', 'Antidepressive Agents', 'Area', 'Automobile Driving', 'Award', 'Biological Markers', 'Brain', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease remission', 'Electroencephalography', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Guidelines', 'Heterogeneity', 'Image', 'Instruction', 'Investigation', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Multimodal Imaging', 'National Institute of Mental Health', 'Neurosciences', 'Outcome', 'Patients', 'Performance', 'Placebos', 'Play', 'Positron-Emission Tomography', 'Process', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Public Health', 'Quality of Care', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Risk', 'Role', 'Selection for Treatments', 'Sertraline', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Training', 'Training Programs', 'Work', 'Writing', 'big biomedical data', 'biosignature', 'burden of illness', 'clinical care', 'clinical imaging', 'data reduction', 'experience', 'flexibility', 'health data', 'high dimensionality', 'imaging modality', 'individual patient', 'individualized medicine', 'innovation', 'insight', 'interest', 'learning strategy', 'machine learning algorithm', 'multimodality', 'neuroimaging', 'neuroimaging marker', 'novel', 'personalized medicine', 'precision medicine', 'public health relevance', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool', 'treatment effect', 'treatment response', 'user friendly software']",NIMH,GEORGE WASHINGTON UNIVERSITY,K01,2019,158696,-0.0018429905443947856
"Detecting Middle Ear Fluid Using Smartphones PROJECT SUMMARY Otitis media is one of the most common childhood diseases in developing countries; many of its complications are preventable if middle ear fluid is detected early. We propose an accessible and accurate smartphone-based screening tool that (i) sends a soft acoustic chirp into the ear canal using the smartphone speaker, (ii) detects reflected sound from the eardrum using the smartphone microphone, and (iii) employs a machine learning model to classify these reflections and predict middle ear fluid status in realtime. Given the ubiquity of smartphones and the inaccuracy of visual otoscopy, the system we propose has the potential to be the default screening tool used in developing countries by healthcare providers and caregivers at home. PROJECT NARRATIVE Otitis media is one of the most common childhood diseases in developing countries affecting over 1.23 billion people in 2013 and can lead to complications such as hearing loss, developmental delay, meningitis, mastoiditis, and death. Many of these complications are preventable if middle ear fluid is detected early. However, the absence of an accurate and accessible method to detect middle ear fluid has led to high misdiagnosis rates. The consequence is associated hearing and speech impairment rates greater than any other pediatric condition and growing microbial resistance as a result of antibiotic over-prescription. Currently, the technique of choice for detecting middle ear fluid by primary care providers is visual otoscopy, which has a diagnostic accuracy as low as 51%. Although more accurate methods like tympanometry and pneumatic otoscopy exist, they require significant expertise and referral to a specialist. Commercial acoustic reflectometers and smartphone-mounted otoscopes require specialized hardware. Thus, there is an urgent, unmet need for an accurate, rapid and easily accessible method for resource-limited healthcare providers and caregivers to detect middle ear fluid. This project aims to demonstrate the feasibility of using the speakers and microphones on existing smartphones to detect middle ear fluid by assessing eardrum mobility. Our proposed system would operate by (i) sending a soft acoustic chirp into the ear canal using the smartphone speaker, (ii) detecting reflected sound from the eardrum using the smartphone microphone, and (iii) employing a machine learning model to classify these reflections and predict middle ear fluid status. No additional attachments would be required beyond a paper funnel, which acts as a speculum to reduce waveform variability and can be constructed with printer paper, scissors, and tape. This technique is the first software-based screening tool for middle ear fluid detection that uses off-the-shelf smartphones which does not require hardware attachments or visual interpretation. Using data from our existing preliminary clinical study we aim to develop signal processing and machine learning algorithms to optimize sensitivity and specificity. We plan to develop a bench testing technique that enables previously unsupported smartphones to to run our test and prospectively validate our optimized algorithm clinically in parallel testing with an acoustic reflectometer. Further we aim to develop a user interface and improved funnel design. These new designs will undergo usability testing in physician and parent populations. Given the ubiquity of smartphones, our app has the potential to be the default screening tool used in developing countries by healthcare providers and caregivers at home.",Detecting Middle Ear Fluid Using Smartphones,9906782,R43DC018434,"['Acoustics', 'Affect', 'Agreement', 'Algorithms', 'Antibiotics', 'Caregivers', 'Cellular Phone', 'Cessation of life', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Computer software', 'Data', 'Detection', 'Developing Countries', 'Development', 'Developmental Delay Disorders', 'Diagnosis', 'Disease', 'Ear', 'Earwax', 'Environment', 'External auditory canal', 'FDA approved', 'Feedback', 'Future', 'Galaxy', 'Health', 'Health Personnel', 'Hearing', 'Home environment', 'Impairment', 'Industry Standard', 'Lead', 'Liquid substance', 'Machine Learning', 'Mastoiditis', 'Measures', 'Meningitis', 'Methods', 'Modeling', 'Obstruction', 'Otitis Media', 'Otoscopes', 'Otoscopy', 'Outcome', 'Output', 'Paper', 'Parents', 'Patients', 'Performance', 'Peripheral', 'Phase', 'Physicians', 'Population', 'Preparation', 'Publishing', 'Research Personnel', 'Resistance', 'Resources', 'Running', 'Screening procedure', 'Sensitivity and Specificity', 'Small Business Innovation Research Grant', 'Specialist', 'Specificity', 'Speculums', 'Speech', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Tympanic membrane', 'Tympanometry', 'Visual', 'base', 'care providers', 'clinical care', 'clinical practice', 'design', 'diagnostic accuracy', 'experience', 'hearing impairment', 'improved', 'machine learning algorithm', 'meetings', 'microbial', 'microphone', 'middle ear', 'prospective', 'screening', 'signal processing', 'sound', 'telehealth', 'tool', 'urgent care', 'usability']",NIDCD,"WAVELY DIAGNOSTICS, INC.",R43,2019,157842,-0.023404635125790316
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9760006,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'computational platform', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'federated computing', 'guided inquiry', 'hands-on learning', 'indexing', 'innovation', 'insight', 'interest', 'learning strategy', 'multimodality', 'patient population', 'patient subsets', 'post-traumatic stress', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'supervised learning', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2019,501996,-0.0008885220460793425
"Quantifying causality for neuroscience Abstract: Causality is central to neuroscience. For example, we might ask about the causal effect of a neuron on another neuron, or its influence on perception, action, or cognition. Moreover, any medical approaches aim at producing a causal effect – effecting improvements for patients. Randomized controlled trials (RCTs) are the gold standard to establish causality, but they are not always practical. For example, while we can electrically or optogenetically activate entire areas, large-scale targeted stimulation of individual neurons is hard. Other ways of establishing causality are problematic: if we observe a correlation it is hard to know its cause. The problem is confounding: there are variables that we do not record that affect the variables we do. This also renders model comparisons problematic – a causally wrong model with few parameters may well fit the observed data better than a causally correct one with many parameters. We thus need data analysis tools that allow authoritatively asking causal questions without the need for random perturbation experiments.  Just like neuroscience now, the field of econometrics once focused on correlations. But since the 1980s, empirical economics has undergone a so-called credibility revolution, requiring the development of rigorous methods to establish causality. Several successful methods have emerged to become the workhorses of empirical economics. The idea underlying these methods is that if one can observe variables that approximate random perturbations, then one can still discover causal relations. This is what economists call a quasi-experiment. We here propose to carry over such quasi-experimental techniques to neuroscience. For example in neuroscience, if there is a random variable that affects only one neuron, then any activity in other neurons correlated with that variable must be causally affected by the neuron. Another famous quasi- experimental method is regression discontinuity design (RDD). This approach effectively uses the noise introduced at the threshold to identify causal relations. Importantly, such techniques have, thanks to decades of research in econometrics, very well understood statistical properties. These approaches promise to considerably enrich the approaches towards causality we have in neuroscience. We have a strong interdisciplinary team, spanning economics, experimental, and computational neuroscience, collaborating on adapting these quasi-experimental techniques to problems in neuroscience through a combination of machine learning and domain-specific engineering. This promises to be a major advance relative to current techniques that generally approach causality in neuroscience through model comparison. Project Narrative: The goal of this project is to develop a set of computational techniques that allow neuroscientists to quantify how neurons causally influence one another. To do so, it utilizes approaches popular in econometrics called quasiexperiments. Such approaches to quantify causality is important as medical perturbations of brains, e.g. treatments of epilepsy or depression are aimed at effecting or causing a change in the brain.",Quantifying causality for neuroscience,9775861,R01EB028162,"['Affect', 'Algorithms', 'Area', 'Brain', 'Code', 'Cognition', 'Communities', 'Computational Technique', 'Confounding Factors (Epidemiology)', 'Data', 'Data Analyses', 'Development', 'Economics', 'Engineering', 'Epilepsy', 'Etiology', 'Glean', 'Goals', 'Gold', 'Individual', 'Injections', 'Intervention', 'Learning', 'Machine Learning', 'Medical', 'Mental Depression', 'Methods', 'Modeling', 'Modernization', 'Neurons', 'Neurosciences', 'Noise', 'Organism', 'Output', 'Patients', 'Perception', 'Performance', 'Physiological', 'Property', 'Quasi-experiment', 'Randomized Controlled Trials', 'Refractory', 'Research', 'Synapses', 'Techniques', 'base', 'computational neuroscience', 'design', 'econometrics', 'experimental study', 'improved', 'optogenetics', 'phrases', 'relating to nervous system', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2019,803000,-0.01695885230092987
"Machine Learning and Network Science for Predicting Kidney Transplant Survival  Chronic kidney disease affects about 10% of adults in the United States and 7-12% of the population worldwide. It may lead to irreversible loss of kidney function, known as end-stage renal disease (ESRD). For patients with ESRD, kidney transplantation is the preferred treatment compared to dialysis in terms of patient survival, quality of life and cost. Despite the advantages of kidney transplants, most patients with ESRD are treated with dialysis primarily because there exist an insufficient number of compatible donors for patients. The human leukocyte antigens (HLAs) of the organ donor and recipient are known to be a significant contributing factor to transplanted organ survival times due to immunogenicity, the immune response of the recipient to the transplanted organ. Mismatches between donor and recipient HLAs are associated with shorter survival times; however, it is extremely rare to identify donors that have a perfect match with recipients, so most transplants involve mismatched HLAs. Our main objective is to accurately predict survival times for kidney transplants by incorporating both data- driven models of HLA compatibility based on outcomes of past transplants and biologically-driven models of HLA immunogenicity. Accurate prediction of survival times can improve patient transplant outcomes by enabling more efficient allocation of donors and recipients, particularly by reducing the number of repeat transplants due to graft failure with a poorly matched donor. We propose to estimate HLA compatibilities using high-dimensional variable selection techniques applied to outcomes of past transplants and through a novel donor-recipient latent space model for the HLA compatibility network. We then propose to incorporate these predicted compatibilities along with biologically-driven models of HLA immunogenicity using amino acid sequences and epitopes into a multi-task classification-based survival prediction algorithm. Our proposed approach for learning integrated data- and biologically-driven models of transplant survival generalizes broadly to organ transplantation (liver, heart, pancreas, lungs) and possibly to bone marrow transplantation. Finding better methods for human leukocyte antigen (HLA) matching between donor and recipient may redefine and improve clinical outcomes for organ and tissue transplants. In fact, new approach may revolutionize the selection of donors with recipients, thereby producing significantly improved long-term organ and tissue transplant survivals.",Machine Learning and Network Science for Predicting Kidney Transplant Survival ,9916110,R01LM013311,"['Adult', 'Affect', 'Amino Acid Sequence', 'Antigens', 'Anus', 'Biological', 'Bone Marrow Transplantation', 'Chronic Kidney Failure', 'Classification', 'Clinical', 'Data', 'Dialysis procedure', 'Donor Selection', 'End stage renal failure', 'Epitopes', 'HLA Antigens', 'Heart', 'Immune response', 'Kidney Transplantation', 'Lead', 'Learning', 'Liver', 'Lung', 'Machine Learning', 'Methods', 'Modeling', 'Organ Donor', 'Organ Survival', 'Organ Transplantation', 'Outcome', 'Pancreas', 'Patients', 'Plants', 'Population', 'Quality of life', 'Renal function', 'Research', 'Science', 'Space Models', 'Techniques', 'Time', 'Tissue Transplantation', 'Transplant Recipients', 'Transplantation', 'Transplanted tissue', 'United States', 'base', 'cost', 'discrete time', 'graft failure', 'hazard', 'high dimensionality', 'human model', 'immunogenicity', 'improved', 'learning network', 'multitask', 'novel', 'novel strategies', 'prediction algorithm', 'survival prediction', 'transplant model']",NLM,UNIVERSITY OF TOLEDO,R01,2019,274577,-0.02256161208212046
"Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria Project Summary/Abstract Fungal and bacterial pathogens are a major threat to human health. Few therapeutics exist to treat fungal infections while bacteria are becoming increasingly resistant to existing therapeutics. Humans have been using natural products to treat infections for thousands of years, long before the causal agents of infection were understood. Natural products have continued to be used as therapeutics in the modern age of medicine. Rates of rediscovery of known natural products have increased in traditional sources of natural products, such as soil bacteria. Recently, symbiotic Actinobacteria from insect agricultural systems have been recognized as a promising source of bioactive compounds, especially antifungal agents. These bacteria often produce natural products that defend an insect’s fungal crop from pathogenic fungus. The work proposed here will use chemical biology approaches such as phenotypic interaction screens, genomics, and a new bioinformatics approach to systematically search for bioactive natural products produced by Actinobacteria symbionts and other organisms in insect agricultural systems. The first part of this proposal focuses on using existing techniques to identify new bioactive natural products. Phenotypic interaction screens can identify bioactive natural products by determining if a symbiotic bacteria produces a natural product that inhibits the growth of a fungal pathogen and vice-versa. We will then use genomic sequencing, bioinformatics, and heterologous expression to identify and characterize biosynthetic gene clusters (BGCs) that are not expressed in the phenotypic interaction screens. The second part of the proposed work involves the use of a new bioinformatics technique to identify interesting bioactive natural products. Existing bioinformatics techniques identify BGCs and predict the most likely chemical structure of the corresponding natural product. However, they do not conclude anything concerning the functional role that the natural product plays. The technique developed here will use machine learning to predict the function that the natural product fulfills in the ecological context of the organism. This algorithm will facilitate the identification of bioactive natural products with therapeutically relevant functions. Project Narrative Fungal infections are an underappreciated threat to human health with high mortality rates and few effective therapeutic agents for treatment. Symbiotic Actinobacteria from insect agricultural systems are a promising source of antifungal agents since they often produce natural products with antifungal activity protecting an insect’s fungal crop from pathogenic fungus. The work proposed here will use phenotypic interaction screens, genome sequencing, and the development of a novel bioinformatics method to systematically mine Actinobacteria for antifungal and antibacterial products – leading to the discovery of new bioactive small molecules along with a deeper understanding of how natural products mediate the interaction between species in insect agricultural systems.",Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria,9716392,F32GM128267,"['Actinobacteria class', 'Age', 'Agriculture', 'Algorithms', 'Anti-Bacterial Agents', 'Antibiotics', 'Antifungal Agents', 'Ants', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bioinformatics', 'Biological Assay', 'Biology', 'Breathing', 'Chemical Structure', 'Chemicals', 'Collaborations', 'Computational Biology', 'Computing Methodologies', 'Data Set', 'Development', 'Ecosystem', 'Gene Cluster', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Infection', 'Insecta', 'Learning', 'Life', 'Literature', 'Machine Learning', 'Mediating', 'Medicine', 'Methods', 'Mining', 'Modernization', 'Molecular Structure', 'Mycoses', 'Natural Products', 'Organism', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Public Health', 'Resistance', 'Role', 'Soil', 'Source', 'Structure', 'System', 'Techniques', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'Validation', 'Work', 'base', 'bioactive natural products', 'drug discovery', 'fungus', 'genetic information', 'genome sequencing', 'human disease', 'machine learning algorithm', 'mortality', 'novel', 'pathogen', 'pathogenic bacteria', 'pathogenic fungus', 'post-doctoral training', 'prediction algorithm', 'small molecule', 'symbiont']",NIGMS,HARVARD MEDICAL SCHOOL,F32,2019,61226,-0.018911710638758843
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates ﻿    DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9766229,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'clinical implementation', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment optimization', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2019,517281,-0.07157800972935863
"Improving Critical Congenital Heart Disease Screening and Detection of ""Secondary"" Targets PROJECT SUMMARY/ABSTRACT  We propose to develop an automated critical congenital heart disease (CCHD) screening algorithm using machine learning techniques to combine non-invasive measurements of perfusion and oxygenation. Oxygen saturation (SpO2)-based screening is the current standard for CCHD screening, however it fails to detect up to 50% of asymptomatic newborns with CCHD or nearly 900 newborns in the United States annually. The majority of newborns missed by SpO2 screening have defects with aortic obstruction, such as coarctation of the aorta (CoA), that do not result in deoxygenated blood entering circulation. Non-invasive measurements of perfusion such as perfusion index (PIx) and pulse oximetry waveform analysis is expected to improve the detection of newborns with defects such as CoA, which is currently the most commonly missed CCHD by SpO2 screening. Both PIx and pulse oximetry waveforms can be measured non-invasively and with the same equipment used for SpO2 screening.  Members of our team recently showed that the addition of PIx, a non-invasive measurement of pulsatile blood flow, has the potential to improve CCHD detection otherwise missed by SpO2 screening. However, variability of PIx over brief time periods (seconds) and human error in its interpretation limit its clinical capabilities. Additionally, human error in interpretation of the current SpO2 screening algorithm leads to missed diagnoses and inappropriate testing in healthy newborns. Therefore, an automated SpO2-PIx screening algorithm is needed to both simplify the screening process, and improve detection of defects that are missed with SpO2 screening. In order to achieve that, we will identify the optimal PIx waveforms to create a metric that discriminates between newborns with and without CCHD. We will perform pulse oximetry waveform analysis to identify other non-invasive components with discriminatory capacity for newborns with CCHD. Additionally, we will apply supervised machine learning techniques to automate the algorithm interpretation.  The proposed research is significant because an automated SpO2-PIx screening algorithm could save the lives of hundreds of newborns with CCHD that are not diagnosed by SpO2 screening. Additionally, this is innovative as it will be the first automatic interpretation of PIx measurement among newborns with CCHD and merging of automated PIx and SpO2, which will allow for easy implementation at later steps. Through collaboration with four pediatric cardiac centers, we will establish the infrastructure and necessary multidisciplinary relationships to conduct future multicenter studies to evaluate this novel combined SpO2-PIx algorithm on a large scale involving thousands of newborns. Improving the detection of CCHD will require a multidisciplinary approach among all the individuals involved in the care and screening of newborns with CCHD. Additionally, collaboration with engineering and computer sciences will be necessary to automate the SpO2-PIx CCHD screening algorithm. PROJECT NARRATIVE A screening approach that improves earlier detection of critical congenital heart defects with systemic obstruction is critically necessary. This application seeks to develop a screening algorithm that will combine the current screening standard, oxygen saturation, with non-invasive measurements of perfusion. This high risk, high reward approach is fundamentally different from other approaches as it will use machine learning techniques, and is expected to improve the detection of critical congenital heart defects with systemic obstruction and automate the interpretation of the screening results.","Improving Critical Congenital Heart Disease Screening and Detection of ""Secondary"" Targets",9805011,R21HD099239,"['Affect', 'Algorithms', 'American Heart Association', 'Aortic coarctation', 'Automatic Data Processing', 'Blood', 'Blood Circulation', 'Blood Pressure', 'Blood flow', 'California', 'Cardiac', 'Caring', 'Cessation of life', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Congenital Abnormality', 'Critical Congenital Heart Defects', 'Critical Illness', 'Data', 'Defect', 'Detection', 'Diagnosis', 'Early Diagnosis', 'Engineering', 'Equipment', 'Evaluation', 'Funding', 'Future', 'Goals', 'Individual', 'Infant', 'Infrastructure', 'Interruption', 'Intervention', 'Lesion', 'Lower Extremity', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Morbidity - disease rate', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'National Institute of Child Health and Human Development', 'Neonatal Screening', 'New York', 'Newborn Infant', 'Obstruction', 'Oxygen', 'Perfusion', 'Physiologic pulse', 'Population', 'Process', 'Pulsatile Flow', 'Pulse Oximetry', 'Research', 'Savings', 'Screening Result', 'Screening procedure', 'Sensitivity and Specificity', 'Specificity', 'Techniques', 'Testing', 'Time', 'Ultrasonography', 'United States', 'Upper Extremity', 'Validation', 'aortic arch', 'base', 'clinical application', 'cohort', 'computer science', 'congenital heart disorder', 'high reward', 'high risk', 'human error', 'improved', 'indexing', 'infant death', 'innovation', 'interdisciplinary approach', 'member', 'mortality', 'multidisciplinary', 'neonatal period', 'novel', 'prenatal', 'prevent', 'screening', 'supervised learning']",NICHD,UNIVERSITY OF CALIFORNIA AT DAVIS,R21,2019,240804,-0.03353294122156179
"Multiplexed high-content assay for toxicity profiling using live iPSC-derived cardiomyocyte lines with lineage-specific barcoding Project Summary/Abstract Human induced pluripotent stem cells (hiPSCs) are poised to transform toxicological evaluation, however new approaches to enable their functional and structural profiling are needed to improve the utility of hiPSC -based models for predictive and mechanistic toxicology screening. This need is addressed by our project’s Specific Aims that encompass (1) development of a novel platform for generation of hiPSC-derived reporter cells; (2) generation of a panel of multicolor hiPSC-derived cardiomyocytes (hiPSC-CMs) with stable lineage specific fluorescent reporters; and (3) implementation and validation of a pilot machine learning-enabled predictive cardiotoxicity screen using these tools. The proposed tools are configured to be extensible to other toxicology- relevant pathways and phenotypes making it uniquely positioned to capitalize on the growing commercial need for high-throughput predictive toxicology assays. The project deliverables benefit public health by improving the ability to rapidly identify liabilities in specific cardiomyocyte lineage types, thus reducing the time and cost to pinpoint cardiotoxicity of pharmaceutical and environmental chemicals. Project Narrative The assay and reagents established in the course of this project directly address the goals of significant initiatives to improve the effectiveness of cardiotoxicity testing, such as the FDA’s CiPA initiative and the work of the Cardiac Safety Research Consortium. The resulting improvements in the pace and precision of drug testing will result in public health benefit through the development of more cost-effective and safer medicines. Beyond toxicological evaluation of therapeutic compounds, our innovative technology will deliver additional benefit to public health by virtue of its utility in investigating the toxicities of environmental chemicals, in line with the focus of government agencies and initiatives such as the EPA and Tox21 in the US and EU-ToxRisk in Europe.",Multiplexed high-content assay for toxicity profiling using live iPSC-derived cardiomyocyte lines with lineage-specific barcoding,9761607,R44TR002572,"['Address', 'Biological Assay', 'CRISPR/Cas technology', 'Cardiac', 'Cardiac Myocytes', 'Cardiotoxicity', 'Cell Line', 'Cell Lineage', 'Cells', 'Cellular Assay', 'Cellular Structures', 'Chemicals', 'Classification', 'Contracts', 'Data', 'Development', 'Drug Modelings', 'Drug toxicity', 'Effectiveness', 'Europe', 'Evaluation', 'Foundations', 'Generations', 'Goals', 'Government Agencies', 'Health Benefit', 'Heart Atrium', 'Human', 'In Vitro', 'Industry Standard', 'Machine Learning', 'Medicine', 'Methods', 'Microscopy', 'Mitochondria', 'Nodal', 'Nuclear', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Population Heterogeneity', 'Positioning Attribute', 'Public Health', 'Reagent', 'Reporter', 'Reporting', 'Research', 'Safety', 'Sarcomeres', 'Site', 'Small Business Innovation Research Grant', 'Specificity', 'Speed', 'Structure', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicology', 'Training', 'Treatment-Related Cancer', 'Validation', 'Ventricular', 'Withdrawal', 'Work', 'base', 'cell immortalization', 'clinical candidate', 'clinical predictors', 'cost', 'cost effective', 'drug discovery', 'drug testing', 'environmental chemical', 'expression vector', 'genome editing', 'high throughput screening', 'immortalized cell', 'improved', 'induced pluripotent stem cell', 'innovation', 'innovative technologies', 'machine learning algorithm', 'model development', 'new technology', 'novel', 'novel strategies', 'phase I trial', 'pre-clinical', 'predictive modeling', 'predictive test', 'predictive tools', 'programs', 'response', 'safety assessment', 'screening', 'site-specific integration', 'success', 'therapeutic development', 'therapeutic evaluation', 'tool']",NCATS,"CAIRN BIOSCIENCES, INC.",R44,2019,821559,-0.024295252604294555
"Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia PROJECT SUMMARY  Abnormalities of white matter are important in schizophrenia. A preponderance of studies have found decreased levels of transcripts for myelin-related proteins in autopsy brains. Some have found decrease in the proteins themselves, and some have not. Hundreds of diffusion tensor imaging (DTI) studies have found reduced fractional anisotropy (FA) in the brains of many people with schizophrenia (SCH). Decreased FA is interpreted as disruption of normal architecture. However, postmortem examination has failed to identify characteristic abnormalities. This suggests that abnormalities are subtle, and perhaps postmortem examinations have not used the right tools to find them. We have therefore been developing, as part of a collaboration supported by our concluding Fogarty project, two new methods to characterize white matter at high resolution. The first is a machine learning protocol to measure axonal diameters and myelin sheath thickness in electron microscope (EM) images of prefrontal white matter, recognizing and avoiding artifacts in EM of autopsy tissue. This will enable us to measure thousands of fibers in EM images produced as part of our concluding Fogarty project, from individuals with SCH, major depressive disorder (MDD), or no psychiatric illness (NPI). The second method, suggested by the DTI findings, is to analyze the arrangement of the axons themselves. We will use 3-dimensional (3D) reconstructions of high-resolution images of the axons themselves, identified by Bielschowsky silver stain or immunohistochemistry for phosphorylated neurofilament protein. To obtain high-resolution images of Bielschowsy stains, we will take advantage of the recent observation by Dr. Mark Sonders, co-investigator on this project, that these and other heavy metal stains luminesce under 2-photon infrared excitation. This yields clear and measurable images of individual axons. We will perform these procedures on sections from existing paraffin blocks that comprise a complete left prefrontal coronal section from 36 triads containing 1 case each of SCH, MDD, or NPI, matched for sex and age. These brains were included in earlier studies that yielded data on protein composition, mRNA for myelin-related proteins, DNA methylation, microglial activation, and semiquantitative myelin histology. In a third, exploratory aim, we will employ graphical models to combine these various types of data with known properties of CNS white matter and myelin to build a model of what is disturbed in schizophrenia. We expect that novel techniques for data fusion will reveal associations based on multidimensional correlations that could not be detected by modeling the single-domain datasets separately. In the process of completing these scientific aims, we will pursue the pedagogic goals of training the first two professional biostatisticians in Macedonia, and an academic pathologist. We will also hold a seminar course for biological researchers to build awareness and understanding of the power of biostatistical and other computational methods to enrich their research. NARRATIVE Our ongoing Fogarty/NIMH research project in Macedonia (R01 MH060877, “Building Schizophrenia Research in Macedonia”), has demonstrated biochemical abnormalities of white matter in schizophrenia that are not present in major depressive disorder. However, we have not seen anatomical abnormalities of white matter, which MRI studies of schizophrenia tell us should exist, and as the biochemistry also suggests. To explore white matter in novel ways, we are developing new methods of microscopy, image analysis and statistical inference, which we now propose to employ on a large scale.",Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia,9953486,R56MH117769,"['3-Dimensional', 'Academy', 'Age', 'Anisotropy', 'Architecture', 'Autopsy', 'Awareness', 'Axon', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Assay', 'Biometry', 'Brain', 'Caliber', 'Cerebrum', 'Characteristics', 'Charge', 'Collaborations', 'Complex', 'Computer-Assisted Diagnosis', 'Computers', 'Computing Methodologies', 'Confocal Microscopy', 'DNA Methylation', 'Data', 'Data Set', 'Deformity', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electron Microscope', 'Electrons', 'Fiber', 'Goals', 'Heavy Metals', 'Histologic', 'Histology', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Individual', 'Informatics', 'International', 'Knowledge', 'Learning', 'Left', 'Macedonia', 'Machine Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measurable', 'Measurement', 'Measures', 'Medical', 'Mental disorders', 'Messenger RNA', 'Methodology', 'Methods', 'Microscopy', 'Modeling', 'Modernization', 'Morphologic artifacts', 'Morphology', 'Multiomic Data', 'Myelin', 'Myelin Sheath', 'National Institute of Mental Health', 'Neurofilament Proteins', 'Paraffin', 'Pathologist', 'Pathology', 'Positioning Attribute', 'Procedures', 'Process', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Schizophrenia', 'Scientist', 'Silver Staining', 'Stains', 'Statistical Data Interpretation', 'Statistical Methods', 'Structural Models', 'Students', 'Techniques', 'Thick', 'Time', 'Tissues', 'Training', 'Transcript', 'Translational Research', 'Triad Acrylic Resin', 'base', 'cognitive function', 'computerized', 'computerized tools', 'data modeling', 'deep neural network', 'diffusion anisotropy', 'high resolution imaging', 'histological image', 'histological studies', 'imaging study', 'innovation', 'interest', 'low and middle-income countries', 'microscopic imaging', 'multidimensional data', 'multimodality', 'network models', 'novel', 'pedagogy', 'reconstruction', 'sex', 'tool', 'two photon microscopy', 'two-photon', 'water diffusion', 'white matter']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R56,2019,10000,-0.03707782484919928
"Systems Biology of Aging: Data-science meets Gero-science PROJECT SUMMARY / ABSTRACT The funds requested in this R13 application are for partial support of “Systems Biology of Aging: Data-science meets Gero-science” annual meetings to be offered each August/September from 2019 through 2022 at The Jackson Laboratory for Genomic Medicine (JAX-GM) in Farmington, Connecticut. This meeting will bring together up to 150 interdisciplinary scientists including molecular biologists, immunologists, computational biologists, and geriatricians, who share a common interest in understanding aging and aging-associated disease at the systems level. Many aging-associated diseases, such as cancer and cardiovascular disease, are influenced by dysfunctions in the immune system. Recent advances in genomic profiling techniques (e.g., single cell transcriptomics) provide an opportunity to uncover aging-related changes in human cells/tissues and to link these changes to health and lifespan. The wealth and complexity of data produced using these technologies is ever increasing, as is the need to develop advanced computational methods to mine and integrate these data. Despite this need, there are currently no formal venues at which scientists, specifically those in the aging field, can be trained in the basics and application of data mining techniques (i.e., machine learning algorithms). Furthermore, current conferences on aging are not aimed at specifically bringing together computational biologists, immunologists and basic and clinical aging researchers. Therefore, the objectives of this meeting are: (1) to recognize and emphasize the highly interdisciplinary nature of the aging field and to promote and accelerate collaborations and cross-pollination of ideas across the three disciplines: aging, immunology, and computational biology; (2) to provide trainees (students and postdoctoral fellows) an opportunity to closely interact with, and gain feedback from, more senior investigators to advance their projects and establish connections to help build their careers; and (3) to provide an opportunity for researchers in the field of aging to learn the basics of machine learning techniques, which they will be able to immediately apply to their own research upon return to their home institutions. We will reach these objectives through carrying out the following Aims. In Aim 1, we will organize an interdisciplinary meeting and hands-on workshop focused on aging and aging-related diseases. The meeting will include a 2-day seminar session featuring talks by leading scientists, followed by a 1-day hands-on workshop on the basics of machine learning. In Aim 2, we will promote interactions to foster collaborative research and career advancement, including through a poster session. In Aim 3, we will recruit diverse attendees. Our proposed speaker list features several female scientists, and we will use our partnership networks to specifically recruit attendees from nationally underrepresented racial and ethnic groups. The ultimate goal of the meeting is to advance the aging research field through expediting collaborations and the understanding of aging-related genomic data via application of advanced data mining approaches. PROJECT NARRATIVE / RELEVANCE TO PUBLIC HEALTH Aging and aging-associated diseases, such as Alzheimer's, cancer and cardiovascular disease, represent a significant and growing health and economic burden, with the elderly population of the US projected to double by 2030. Herein, we propose to organize an interdisciplinary conference with a hands-on computational training component that will bring together scientists from the fields of aging, immunology, and computational biology, which will enable creative collaborations and train early career scientists in the aging research field on the basics of advanced computational techniques to mine aging-related genomic data. This is ultimately expected to lead to a better molecular understanding of the aging process and to novel approaches for the improvement of human healthspan and/or lifespan.",Systems Biology of Aging: Data-science meets Gero-science,9912317,R13AG064968,"['Academia', 'Address', 'Affect', 'Aging', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Biology of Aging', 'Cardiovascular Diseases', 'Career Mobility', 'Cell physiology', 'Cells', 'Cities', 'Clinical', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Technique', 'Computing Methodologies', 'Connecticut', 'Data', 'Data Analyses', 'Data Science', 'Data Scientist', 'Development', 'Discipline', 'Disease', 'Economic Burden', 'Educational workshop', 'Elderly', 'Ethnic group', 'Etiology', 'Feedback', 'Female', 'Fostering', 'Functional disorder', 'Funding', 'Genomic medicine', 'Genomics', 'Geroscience', 'Goals', 'Health', 'Home environment', 'Human', 'Immune', 'Immune system', 'Immunologist', 'Immunology', 'Impaired cognition', 'Industry', 'Institution', 'Lead', 'Learning', 'Link', 'Location', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Organism', 'Outcome', 'Participant', 'Phenotype', 'Play', 'Population', 'Postdoctoral Fellow', 'Process', 'Public Health', 'Pythons', 'Race', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scholarship', 'Science', 'Scientist', 'Series', 'Shock', 'Societies', 'Students', 'Support System', 'System', 'Systems Biology', 'Techniques', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissues', 'Training', 'Underrepresented Minority', 'Universities', 'Work', 'aging population', 'cancer type', 'career', 'clinical biomarkers', 'clinically significant', 'data mining', 'epigenome', 'epigenomics', 'genomic biomarker', 'genomic data', 'genomic profiles', 'graduate student', 'health economics', 'healthspan', 'innovation', 'interdisciplinary approach', 'interest', 'machine learning algorithm', 'meetings', 'next generation', 'novel strategies', 'posters', 'programs', 'recruit', 'response', 'senescence', 'skills', 'symposium', 'technology development', 'transcriptome', 'transcriptomics', 'translational approach']",NIA,JACKSON LABORATORY,R13,2019,38566,-0.03788100293040363
"Deep Learning and Fluid Dynamics Based Phenotyping of Expiratory Central Airway Collapse Project Summary/ Abstract Expiratory central airway collapse (ECAC), defined by >50% collapse of large airways during expiration, resulting from either cartilaginous weakening or redundancy of the posterior membranous wall of the trachea, is an increasingly recognized disorder associated with cigarette smoking and chronic obstructive pulmonary disease (COPD). Airflow obstruction in smokers primarily arises from increased resistance to airflow in the small distal conducting airways <2 mm in diameter. It is plausible that in a subset of smokers with and without COPD, central airway collapse results in additional resistance to airflow, resulting in substantial respiratory morbidity. Ninety-two million adults in the Unites States are active or past smokers, and ECAC is present in approximately 5% of current and former smokers. The presence of ECAC is associated with greater dyspnea, worse respiratory-quality of life and greater frequency of exacerbations after adjustment for underlying lung disease. Whether these patients will benefit from interventional therapies such as stenting or tracheopexy depends on whether the airflow resistance caused by ECAC contributes to symptoms, and this in turn depends on the relative contribution of central and small airways to overall airflow resistance. If the overall airflow resistance is primarily due to distal small airways obstruction in a given patient with ECAC, treating central airway collapse is unlikely to benefit such a patient. Our central hypothesis is that ECAC results in additional airflow obstruction beyond that incurred in the small airways, and that in a subset of patients the central airways are the major site of airflow obstruction and hence are amenable to therapy. The complex interplay of proximal and distal airway resistances and transpulmonary pressures does not lend itself to direct measurements in human subjects across a range of physiological pressure and flow changes. We propose a combination of CT-derived imaging and patient-personalized benchtop model and deep learning to answer these questions with the following specific aims. Aim 1 of this application will be to derive personalized patient- specific information on airway geometry and resistance using airway segmentation from computed tomography (CT) scans. We will calculate airway resistances in central and small airways using standard formulae. The goal of Aim 2 is to create bench-top simulations to understand the complex interplay between the resistance of small and large airways. In Aim 3, we will use deep learning to derive probability scores for clinically substantial ECAC from segmented airway images on computed tomography. The results of our study will enable patient-specific personalized therapies for ECAC. The mechanistic insights gained from this study will help identify patients with clinically significant ECAC and hence most likely to benefit from therapeutic interventions. PROJECT NARRATIVE Expiratory central airway collapse (ECAC), greater than 50% collapse of the large airways during expiration, is present in 5% of chronic smokers, and is associated with substantial respiratory morbidity disproportionate to underlying lung disease. Resistance to airflow in smokers is thought to primarily occur in the small conducting airways. By using benchtop models and deep learning to determine the effect of central airway collapse on overall airway resistance and its contribution to airflow obstruction relative to small airway resistance, the proposed project will identify patients with ECAC who will benefit from intervention, and cause a paradigm shift in the therapy of these patients.",Deep Learning and Fluid Dynamics Based Phenotyping of Expiratory Central Airway Collapse,9721392,R21EB027891,"['3-Dimensional', '3D Print', 'Adult', 'Affect', 'Age', 'Air', 'Airway Resistance', 'Area', 'Body mass index', 'Breathing', 'Caliber', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complex', 'Data', 'Data Set', 'Databases', 'Disease', 'Distal', 'Dyspnea', 'Exhalation', 'Forced expiratory volume function', 'Frequencies', 'Generations', 'Geometry', 'Goals', 'Image', 'Imagery', 'Individual', 'Intervention', 'Length', 'Liquid substance', 'Lung diseases', 'Maps', 'Measurement', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Neural Network Simulation', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physiological', 'Probability', 'Pulmonary Emphysema', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Quality of life', 'Race', 'Resistance', 'Scanning', 'Site', 'Smoker', 'Smoking', 'Spirometry', 'Stents', 'Symptoms', 'Testing', 'Therapeutic Intervention', 'Trachea', 'Training', 'Translations', 'Tube', 'United States', 'X-Ray Computed Tomography', 'airway obstruction', 'base', 'cartilaginous', 'cigarette smoking', 'clinical application', 'clinical predictors', 'clinically significant', 'convolutional neural network', 'deep learning', 'expiration', 'human subject', 'individual patient', 'insight', 'patient subsets', 'personalized medicine', 'pressure', 'respiratory', 'response', 'sex', 'simulation', 'three-dimensional modeling']",NIBIB,UNIVERSITY OF ALABAMA AT BIRMINGHAM,R21,2019,226659,-0.024870057000120818
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.0178412472074193
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9729808,K08HL136928,"['Affect', 'Bioinformatics', 'Biological', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Multiomic Data', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2019,172800,-0.01847737165619277
"Big Data, Big Models, and Big Bias?: A decision making framework for vital rate estimates based on extrapolation Project Summary/Abstract The past decades have seen a flurry of methods that use extrapolation, smoothing, and other forms of information sharing to compensate for limited and incomplete data. In places without comprehensive vital registration or public health monitoring systems, extrapolation and information sharing techniques are particularly appealing, since there typically is simply not enough data available to produce estimates with sufficient temporal or spatial resolution to influence public health decision making. This proposal reframes uncertainty in extrapolated estimates of vital rates in terms of decision-making. A decision-making framework (i) is grounded in familiar language for policymakers and public health officials, (ii)characterizes consequential and inconsequential model decisions based on variability in outcomes, and (iii) in- corporates both extrapolation and sampling uncertainty. A cornerstone of this project is a novel collaboration with researchers and policymakers at the World Bank. Through this collaboration, we will pilot the proposed decision-making tools and conduct experiments with local and national policymakers in realistic settings. Project Narrative Predictions based on machine learning models are increasingly common inputs into decision making processes across scientific domains. In this proposal I develop and evaluate strategies for making public health decisions based on predicted vital rates, particularly in places without full coverage civil registration. Results from the project will improve strategies for allocating resources for disease surveillance and health monitoring in scarce resource settings.","Big Data, Big Models, and Big Bias?: A decision making framework for vital rate estimates based on extrapolation",9780910,DP2MH122405,"['Big Data', 'Collaborations', 'Consequentialism', 'Data', 'Decision Making', 'Disease Surveillance', 'Health', 'Language', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Process', 'Public Health', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'System', 'Techniques', 'Uncertainty', 'World Bank', 'base', 'experimental study', 'improved', 'novel', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,DP2,2019,2332500,-0.025372307523795067
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9608754,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2019,356625,-0.03215507790089247
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9786702,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2019,431816,-0.03069807948960138
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9742524,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'supervised learning', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,659674,-0.01134259697656707
"Centralized assay datasets for modelling support of small drug discovery organizations Project Summary We have recently used computational models to identify the protease inhibitor indinavir (used as an antiviral for HIV) as a positive allosteric modulator at the α7-nicotinic acetylcholine receptor. We now propose to build on this discovery as well as other publications describing compounds that are positive allosteric modulators of the α7- nicotinic acetylcholine receptor. We propose identifying further compounds that possess activity against this or one of 9 other targets implicated in Alzheimer’s disease using a combination of Bayesian machine learning and in vitro assays. Generating such data will enable us to potentially provide more specific compounds as well as building datasets that can be used to build predictive models to identify additional compounds with activity at these Alzheimer’s targets. These combined efforts should in the first instance provide commercially viable treatments which will be used to experimentally validate our computational models that can be shared with the Alzheimer’s disease scientific community. We are proposing to build and validate models based on public databases, select compounds for testing and use the data generated as a starting point for further optimization. Project Narrative Alzheimer’s disease (AD) is one of the most common neurodegenerative disorders that causes dementia and it is characterized by amyloid deposition of a 39-42 AA peptide (Aβ) processed from the amyloid precursor protein (APP) and neurofibrillary tangles (NFT). Many palliative drugs are available for the disease but there is still an urgent need for curative drugs with greater efficacy. We need to understand the key factors involved in disease progression and their suitability as drug targets for discovering new drugs against Alzheimer's disease. We hence propose to study several of these targets for Alzheimer’s disease involving all the key steps in the pathway, including several old and new targets. We will use our ‘Assay Central’ software to compile structure-activity data for building computational models, that can be used to selected compounds to test activity in vitro as a starting point for further optimization.",Centralized assay datasets for modelling support of small drug discovery organizations,9881110,R44GM122196,"['Alzheimer&apos', 's Disease', 'Amyloid beta-Protein', 'Amyloid beta-Protein Precursor', 'Amyloid deposition', 'Antiviral Agents', 'Bayesian learning', 'Biological Assay', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Databases', 'Dementia', 'Disease', 'Disease Progression', 'Drug Targeting', 'HIV', 'Indinavir', 'Modeling', 'Neurodegenerative Disorders', 'Neurofibrillary Tangles', 'Pathway interactions', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protease Inhibitor', 'Publications', 'Structure', 'Testing', 'alpha-bungarotoxin receptor', 'base', 'drug discovery', 'in vitro Assay', 'in vitro activity', 'novel therapeutics', 'palliative', 'positive allosteric modulator', 'predictive modeling']",NIGMS,"COLLABORATIONS PHARMACEUTICALS, INC.",R44,2019,188759,-0.09306132838592883
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9739188,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2019,506426,-0.00953104851564133
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,9775212,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Axon', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional']",NIMH,UNIVERSITY OF WASHINGTON,R01,2019,628430,-0.019117037755523182
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9789914,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2019,1500000,-0.023947337918730344
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9663961,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,127308,-0.0043274276226662476
"Evidence to improve heat warning effectiveness in reducing morbidity and mortality Project Summary/Abstract  While exposure to high ambient temperature (i.e., heat) has long been recognized as a threat to public health, the burden of illness and death attributable to heat in the US remains high. In an effort to reduce heat- related mortality and morbidity, the US National Weather Service (NWS) issues heat alerts in advance of forecasted extreme heat events to communicate these risks to the public and government officials. However, it is largely unknown: (1) what are the optimal metrics of heat stress to inform when to issue heat alerts, (2) how effective are heat alerts in protecting the public’s health and (3) what factors make heat alerts comparatively more or less effective in some places or in some people versus others. In the absence of such information, we will fail to maximize the public health benefits of heat alerts.  The goals of this proposal are to identify the optimal health-based and location-specific metrics for issuing heat alerts, to estimate the causal benefits of heat alerts, and to identify characteristics of individuals or communities associated with the greatest reductions in morbidity or mortality following heat alerts. Specifically, using national claims data on deaths and hospital admissions among the large, geographically diverse population of >60 million US Medicare beneficiaries age ≥65 years enrolled between 2001 and 2015, and on emergency department visits among >130 million participants of all ages from one of the nation’s largest health insurers, we propose to: (Aim 1) Use novel machine learning methods to identify the heat metric(s) (e.g., heat index, ambient temperature, spatial synoptic classification, wet bulb globe temperature, absolute humidity) that best predict excess heat-related deaths, emergency hospitalizations, and emergency departments visits in each location, (Aim 2) estimate the causal effects of NWS heat alerts on rates of mortality, hospitalizations, and emergency department visits across the country and within groups stratified by health outcome, sex, and age group, and (Aim 3) assess how the benefits of heat alerts vary across characteristics of communities.  Key innovations of this proposal include a very large sample size, geographic diversity encompassing the entire US, the assessment across multiple health endpoints and age groups, and the use of sophisticated methods in statistical learning and causal inference. Collectively, the findings from this proposal will provide meteorologists, public health and emergency management officials, and local policy-makers with critical information to better protect public health during extreme heat events and guide more targeted future research on strategies to mitigate the adverse health effects of heat. Project Narrative Extreme heat is recognized as an important threat to public health, and the burden of illness and death attributable to heat in the US remains high. In an effort to reduce heat-related mortality and morbidity, the US National Weather Service (NWS) issues heat alerts in advance of forecast extreme heat events to communicate these risks to the public and local government officials, although it remains largely unknown whether such alerts are effective in protecting people. We propose to provide forecasters and public health officials across the country with location-specific, health-based evidence as to the effectiveness of heat alerts and insights into how to improve them to better protect the public’s health.",Evidence to improve heat warning effectiveness in reducing morbidity and mortality,9638880,R01ES029950,"['Age', 'Air Conditioning', 'Cessation of life', 'Characteristics', 'Classification', 'Communities', 'Country', 'Data', 'Effectiveness', 'Emergency Situation', 'Emergency department visit', 'Enrollment', 'Ethnic Origin', 'Event', 'Exposure to', 'Geography', 'Goals', 'Government Agencies', 'Government Officials', 'Health', 'Health Benefit', 'Heat Stress Disorders', 'Hospitalization', 'Humidity', 'Individual', 'Insurance Carriers', 'Knowledge', 'Local Government', 'Location', 'Machine Learning', 'Medicare', 'Methods', 'Morbidity - disease rate', 'Outcome', 'Participant', 'Policy Maker', 'Population Heterogeneity', 'Predisposition', 'Prevalence', 'Public Health', 'Race', 'Research Personnel', 'Risk', 'Sample Size', 'Seasons', 'Services', 'Socioeconomic Status', 'Structure', 'Temperature', 'United States', 'Weather', 'age group', 'attributable mortality', 'base', 'beneficiary', 'burden of illness', 'comparative', 'evidence base', 'extreme heat', 'human old age (65+)', 'improved', 'indexing', 'innovation', 'insight', 'learning strategy', 'mortality', 'mortality risk', 'novel', 'prevent', 'public health emergency', 'response', 'sex']",NIEHS,BROWN UNIVERSITY,R01,2019,662969,-0.025258999776884877
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9788381,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Consumption', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2019,225147,-0.04320035066637992
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9625118,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2019,162123,-0.05801156850670986
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9658873,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,317858,-0.006311311755648708
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9694279,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2019,341691,-0.06905573186628432
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9807074,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Simulation', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'random forest', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2019,482291,-0.023602032190622083
"Public Insurance Design and Health at Older Ages PROJECT ABSTRACT Health insurance has been at the forefront of US public policy debate throughout the last decade. For elderly Americans, who benefit from nearly universal coverage under Medicare, decisions about the scope of their insurance coverage have been a central policy concern since Medicare’s inception in 1965. Medicare’s coverage is extensive, but by statute the program only covers medical services that are “reasonable and necessary,” which can be a controversial definition. Some decisions about what Medicare should cover have engendered intense debate, underscoring the importance of the program’s coverage decisions for millions of patients and doctors. The debate around the scope of Medicare coverage is likely to intensify in the coming decades as options for medical treatment, and testing, expand rapidly, the population ages, and public insurance systems grapple with how to address inequality in access to medical innovations. Yet, despite the importance of decisions about the scope of insurance coverage in and outside of Medicare, we know little about how the scope of coverage (as opposed to patient cost-sharing), affects treatment choices, clinical practice, and health outcomes for the elderly. The research outlined in this proposal aims to start filling this gap. The project investigates how the presence or lack of insurance coverage for specific procedural or pharmacologic therapies affects treatment decisions and health outcomes for the elderly with Alzheimer’s Disease and related Dementias (ADRD). This is a population that may be particularly vulnerable to changes and limits in insurance coverage, as the patients may have limited decision-making capacity and may be disproportionately exposed to treatments that are deemed experimental and lacking effectiveness to clear the “reasonable and necessary” threshold. Aim 1 of the project is to estimate the average effect of coverage decisions across prescription drugs and outpatient procedures on treatment decisions and health outcomes of elderly Medicare enrollees with ADRD. Aim 2 is to predict and characterize the subgroups of ADRD patients that are most likely to be affected by decisions that restrict the scope of insurance coverage using machine learning methods. The proposed empirical method is to use quasi- experimental variation that arises from natural experiments of abrupt changes in insurance coverage within different parts of the Medicare program. The analysis takes advantage of variation in the scope of formularies across Medicare Part D plans, as well as the variation in local coverage decisions for physician services and outpatient procedures under Medicare Part B. These sources of variation coupled with methods for quasi- experimental estimation of treatment effects and machine learning methods for heterogeneity analyses, allow estimating the response of treatment decisions and health of the elderly with ADRD across different drugs, procedures, and subgroups of ADRD patients. PROJECT NARRATIVE For elderly Americans, who benefit from nearly universal coverage under Medicare, decisions about the scope of their insurance coverage have been a central policy concern since Medicare’s inception in 1965 - Medicare’s coverage is extensive, but nevertheless incomplete as by statute the program only covers medical services that are “reasonable and necessary,” which can be hard to define. The debate around the scope of Medicare coverage is likely to intensify in the coming decades as options for medical treatment, and testing, expand rapidly, the population ages, and public insurance systems grapple with how to address inequality in access to medical innovations. In this project I investigate how changes in coverage for prescription drugs, physician office visits, and outpatient procedures affect the treatment received by elderly patients with Alzheimer’s Disease and related dementias (ADRD), who often face coverage limitations due to experimental nature of many ADRD treatments, and yet may be most vulnerable to unintended consequences of any coverage restrictions.",Public Insurance Design and Health at Older Ages,9742306,K01AG059843,"['Address', 'Admission activity', 'Affect', 'Age', 'Alzheimer&apos', 's disease related dementia', 'American', 'Amyloid', 'Anxiety', 'Barbiturates', 'Benzodiazepines', 'Caring', 'Characteristics', 'Clinical Medicine', 'Comorbidity', 'Cost Sharing', 'Coupled', 'Data', 'Diagnostic', 'Diagnostic Procedure', 'Drug Prescriptions', 'Drug Targeting', 'Economics', 'Effectiveness', 'Elderly', 'Ethnic Origin', 'Exposure to', 'Face', 'Family Physicians', 'Formularies', 'Fracture', 'Frequencies', 'Future', 'Geography', 'Geriatrics', 'Gerontology', 'Goals', 'Health', 'Health Benefit', 'Health Insurance', 'Heart', 'Heterogeneity', 'Hospitalization', 'Individual', 'Inequality', 'Insurance', 'Insurance Coverage', 'Investigation', 'Knowledge', 'Learning Skill', 'Length of Stay', 'Light', 'Machine Learning', 'Measures', 'Medical', 'Medicare', 'Medicare Part B', 'Mental Depression', 'Mentors', 'Methods', 'Movement', 'Natural experiment', 'Nature', 'Office Visits', 'Outcome', 'Outcome Measure', 'Outpatients', 'Pain', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Physicians', 'Policies', 'Policy Maker', 'Population', 'Positron-Emission Tomography', 'Procedures', 'Public Policy', 'Race', 'Recording of previous events', 'Research', 'Services', 'Skilled Nursing Facilities', 'Source', 'Subgroup', 'System', 'Testing', 'Therapeutic procedure', 'Time', 'Training', 'Universal Coverage', 'Variant', 'aging population', 'alternative treatment', 'bariatric surgery', 'base', 'clinical practice', 'decision-making capacity', 'design', 'experience', 'high dimensionality', 'innovation', 'insurance claims', 'learning strategy', 'mortality', 'older patient', 'patient population', 'patient subsets', 'precision medicine', 'predictive modeling', 'programs', 'public health insurance', 'response', 'socioeconomics', 'theories', 'tool', 'treatment choice', 'treatment effect', 'treatment response']",NIA,STANFORD UNIVERSITY,K01,2019,126700,-0.018887955647897144
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,-0.02183440442085335
"Integrative Learning to Combine Evidence for Personalized Treatment Strategies. Project Summary:  Mental disorders represent immense healthcare burdens. Intense efforts and resources have been devoted to developing pharmacological and behavioral treatments for mental disorders, but no universally effective treatments are available. Considerable heterogeneity exists in treatment response among individuals with mental disorders, in part because an individual patient's psychosocial characteristics and/or biomarkers are not accounted for when selecting among available treatment options. Barriers to implement personalized treatments in clinical psychiatry include a lack of evidence-based, clinically interpretable, individualized treatment rules (ITRs), a lack of power to detect treatment modifiers from a single study, and a lack of reproducibility for treatment rules estimated from single studies. We propose analytic solutions to tackle these barriers. Specifically, we will provide integrative machine learning methods to build powerful, yet interpretable, individualized treatment strategies that can be easily applied in clinical practice. We will integrate evidence of ITRs identified in multiple randomized controlled trials (RCTs) to increase robustness and reproducibility. In Aim 1, we will provide piece-wise linear decision trees that are  transparent, interpretable, and that have guaranteed performance. Our decision trees will simultaneously identify the optimal treatment for a given patient (qualitative interaction) and subgroups of patients with large benefit  (quantitative interaction). In Aim 2, we propose a novel integrative analysis to synthesize evidence across trials and provide an integrative ITR that improves efficiency and reproducibility. Our method does not require all studies to collect common sets of variables and thus allows evidence to be combined from ITRs identified in recent RCTs that collected emerging biomarkers (e.g., neuroimaging measures) with earlier RCTs that focused on clinical and  behavioral markers. In response to the National Institute of Mental Health Strategic Plan on Research Domain Criteria (RDoC) to center mental health research around “dimensional psychological constructs” shared across disorders, the methods will be applied to a wide range of RCTs that recruited patients with major depressive disorder and other co-morbid mental disorders. This strategy allows examination of ITRs for constructs shared across disorders and will increase generalizability. We will apply our methods to various RCTs, including data available from the  National Database for Clinical Trials Related to Mental Illness. This research will bridge approaches for personalized medicine and integrative analysis in an effort to better understand the complex interplay between biomarkers and clinical manifestations in the context of selecting the best treatments for patients with mental disorders. Project Narrative:  Treatment responses for mental disorders are inadequate and considerable heterogeneity is observed, in part because an individual patient's clinical, psychosocial, and/or biological markers are not accounted for when select- ing treatments among available options. This research proposes novel analytic methods to discover new powerful, yet interpretable personalized treatment strategies and integrate evidence of strategies identified in multiple prior studies to increase robustness and reproducibility.",Integrative Learning to Combine Evidence for Personalized Treatment Strategies.,9774303,R21MH117458,"['Age', 'Anxiety Disorders', 'Behavior Therapy', 'Behavioral', 'Biological Markers', 'Bulimia', 'Characteristics', 'Clinical', 'Clinical Trials Database', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Decision Trees', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Equilibrium', 'Goals', 'Grief reaction', 'Healthcare', 'Heterogeneity', 'Individual', 'Intervention', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Outcome', 'Participant', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmacological Treatment', 'Psychiatry', 'Randomized Controlled Trials', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Resources', 'Sampling', 'Strategic Planning', 'Subgroup', 'Surveys', 'Translating', 'Trees', 'analytical method', 'base', 'burden of illness', 'clinical practice', 'demographics', 'effective therapy', 'evidence base', 'improved', 'individual patient', 'individualized medicine', 'learning strategy', 'mental health center', 'neuroimaging', 'neurophysiology', 'neuropsychiatry', 'novel', 'optimal treatments', 'patient subsets', 'personalized approach', 'personalized medicine', 'psychologic', 'psychosocial', 'response', 'sex', 'success', 'symptomatology', 'treatment effect', 'treatment response', 'treatment strategy']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2019,243000,-0.007065481510890759
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,9846955,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2019,621318,-0.013884110688931132
"TEMPORAL DIETARY PATTERNS: DEVELOPMENT AND EVALUATION AGAINST ADIPOSITY AND METABOLIC BIOMARKERS ABSTRACT There is a growing interest in dietary patterns that capture the overall quality of diet as well as its constituent foods and nutrients. Commonly used dietary patterns are a priori diet score/index based on a set of dietary recommendations for a healthy diet (e.g., Mediterranean diet, Healthy Eating Index) or data-driven dietary patterns (e.g., prudent diet, western diet). Numerous studies have shown that those dietary patterns were related to the risk of chronic diseases such as heart disease, diabetes, and cancer. However, none of these dietary patterns incorporates eating behavior such as when we eat (i.e., eating time) and how often we eat (i.e. eating frequency) during a day. Since the amount of foods and nutrients consumed at one eating occasion influences the food consumption at the subsequent eating occasion and overall intake of the day, eating time and frequency are integral parts of dietary patterns. Furthermore, several lines of evidence consistently suggest that eating time and frequency as well as a meal composition play roles in body weight regulation and metabolic health and also regulate circadian rhythms, all of which may lead to metabolic dysfunctions and ultimately chronic diseases. Given a clear need to expand the dietary patterns framework and close a gap in dietary patterns methodological work, we propose to 1) develop a “temporal” dietary patterns based on temporal distribution of eating time and frequency during a day; and 2) evaluate if the identified temporal dietary patterns are associated with i) overall diet quality and nutrient intakes, ii) adiposity (e.g., BMI, waist circumference), and iii) metabolic biomarkers (e.g., insulin, HOMA-IR, LDL-cholesterol, c-reactive protein). To overcome a limitation that a conventional statistical method cannot capture multidimensional aspects of temporal dietary patterns (e.g., 24-dimensional feature vectors, multivariate dietary intake time-series data), we will use a novel approach combining nutrition and systems science—machine learning method. The Interactive Diet and Activity Tracking in AARP (IDATA) study that repeatedly collected diet, anthropometry, and blood samples from 1,021 men and women, 50-74 years old will be used. During one year, the IDATA study collected 24-hour recalls with clock time for each eating occasion, every other month (total six 24-hour recalls); measured anthropometry three times (baseline and at month 6 and 12); and collected blood twice, 6-month apart. Successful completion of our proposed study will identify temporal dietary patterns that are related to diet quality and metabolic health and validate the utility of temporal dietary patterns as a new tool for future research on diet-health relations and prevention of chronic diseases. NARRATIVE Eating behaviors and its impact on health are complex and multidimensional. The proposed study provides an excellent opportunity to develop new dietary patterns that capture eating behaviors such as when we eat and how often we eat during a day. The findings of the study about healthy eating patterns will also improve dietary recommendations by adding messages on when and how often to eat during a day.",TEMPORAL DIETARY PATTERNS: DEVELOPMENT AND EVALUATION AGAINST ADIPOSITY AND METABOLIC BIOMARKERS,9659103,R01CA226937,"['Advisory Committees', 'Affect', 'Algorithms', 'Animals', 'Anthropometry', 'Biological Markers', 'Blood', 'Blood specimen', 'Body Weight', 'C-reactive protein', 'Calories', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Circadian Rhythms', 'Complex', 'Consumption', 'Data', 'Development', 'Diabetes Mellitus', 'Diet', 'Diet Habits', 'Dietary Practices', 'Dietary intake', 'Dimensions', 'Eating', 'Eating Behavior', 'Energy Intake', 'Evaluation', 'Fasting', 'Fatty acid glycerol esters', 'Food', 'Frequencies', 'Health', 'Healthy Eating', 'Heart Diseases', 'Hour', 'Human', 'Individual', 'Insulin', 'Intake', 'LDL Cholesterol Lipoproteins', 'Lead', 'Machine Learning', 'Macronutrients Nutrition', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediterranean Diet', 'Metabolic', 'Metabolic dysfunction', 'Metabolic syndrome', 'Methodology', 'Modeling', 'Nutrient', 'Obesity', 'Outcome', 'Pattern', 'Persons', 'Physical activity', 'Play', 'Population', 'Positioning Attribute', 'Prevention', 'Recommendation', 'Regulation', 'Risk', 'Role', 'Science', 'Series', 'Statistical Methods', 'System', 'Techniques', 'Time', 'Waist-Hip Ratio', 'Weight maintenance regimen', 'Woman', 'Work', 'base', 'cardiovascular disorder risk', 'dietary guidelines', 'doubly-labeled water', 'epidemiology study', 'food consumption', 'good diet', 'improved', 'indexing', 'interest', 'learning strategy', 'men', 'novel', 'novel strategies', 'nutrient metabolism', 'nutrition', 'obesity risk', 'prudent diet', 'tool', 'vector', 'waist circumference', 'western diet']",NCI,WASHINGTON UNIVERSITY,R01,2019,347716,-0.011474661815043734
"Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Summary:  Spinal cord injury (SCI) patients experience limited functional recovery, owing in part to the paucity of axon regrowth from injured CNS neurons. Effective treatments are lacking, likely because of multiple factors, intrinsic and extrinsic, that inhibit axon growth. Thus we require agents that target more than one source of regeneration failure.  Kinases are ubiquitous signal transducers that regulate most cellular processes, including axon growth. To begin to identify compounds that positively regulate axon growth, we screened 1600 small-molecule kinase inhibitors (KIs) in an in vitro CNS neurite outgrowth assay and identified “hit” KIs that reproducibly and strongly promote outgrowth. Due to homology of catalytic domains, KIs typically inhibit multiple kinases. This makes it difficult to identify the kinase(s) that mediate a KI's effects on cells. We used information theory and machine learning to analyze the inhibition profiles of KIs in relation to their effects on neurite outgrowth. This enabled us to identify, and later validate via siRNA knockdown in primary neurons, multiple kinase targets (i.e. kinases that should be inhibited to promote neurite outgrowth). These included previously known targets that regulate intrinsic and extrinsic inhibitor factors, in addition to several novel candidates. Conversely, we identified kinases whose activity is critical for neurite outgrowth, and whose inhibition must be avoided (anti-targets). We discovered several KIs that inhibit multiple targets and no anti-targets. These KIs strongly promoted neurite outgrowth in vitro.  We tested the KI, RO48, that had the largest effect in vitro in two in vivo models. Our preliminary experiments indicate that RO48 is remarkably effective in vivo. It promoted robust axonal growth of the corticospinal tract (CST) in three separate models of CST injury (pyramidotomy, funiculotomy, dorsal hemisection), and in the dorsal hemisection model, improved forelimb function. We propose to build on these remarkable results to test the working hypothesis that the simultaneous inhibition of RO48's five target kinases (ROCK, PKC, PRKG1, PRKX, and RPS6K) promotes sprouting and regeneration of CST axons. This will be accomplished using viral vectors to knock down expression of the different target kinases individually and in combination. We will do knockdown in CST neurons in the cortex. We will assess CST axon growth at the injury site using light microscopy. We will also perform experiments to determine if RO48-induced CST axon growth promotes axon sprouting, regeneration, or both, and whether RO48 improves behavioral outcomes such as grasping and walking after a contusion injury.  These experiments will 1) validate novel kinases as in vivo targets for future development of SCI therapeutics 2) determine whether these kinases regulate CST axon sprouting, regeneration, or both, and 3) confirm whether the substantial stimulation of axon growth induced by treatment with RO48 improves motor outcomes in a clinically relevant contusion model.  Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Narrative: The proposed experiments aim to understand how small-molecule drug-like compounds increase the ability of nerve cells to grow long processes and re-form connections. Validating the molecular targets of these compounds for in vivo nerve growth will enable future drug discovery projects focused on these targets.",Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury,9688127,R01NS100531,"['Axon', 'Behavioral', 'Biochemical', 'Biological', 'Biological Assay', 'Catalytic Domain', 'Cell physiology', 'Cells', 'Cervical', 'Complement 5a', 'Confocal Microscopy', 'Control Animal', 'Contusions', 'Corticospinal Tracts', 'Data', 'Development', 'Distal', 'Dorsal', 'Dose', 'Failure', 'Forelimb', 'Future', 'Gold', 'Growth', 'In Vitro', 'Individual', 'Information Theory', 'Injury', 'Institution', 'Label', 'Lesion', 'Light', 'Machine Learning', 'Mediating', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Target', 'Morphology', 'Motor', 'Motor Cortex', 'Mus', 'Natural regeneration', 'Nerve', 'Neurites', 'Neurons', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphotransferases', 'Process', 'Rattus', 'Recovery of Function', 'Role', 'Signal Pathway', 'Signal Transduction', 'Site', 'Small Interfering RNA', 'Source', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Spinal cord injury patients', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transducers', 'Viral Vector', 'Walking', 'axon growth', 'axon regeneration', 'axonal sprouting', 'behavior test', 'behavioral outcome', 'central nervous system injury', 'clinically relevant', 'design', 'drug discovery', 'effective therapy', 'experience', 'experimental study', 'grasp', 'gray matter', 'improved', 'in vivo', 'in vivo Model', 'inhibitor/antagonist', 'injured', 'insight', 'kinase inhibitor', 'knock-down', 'light microscopy', 'novel', 'reconstruction', 'regenerative', 'screening', 'small molecule', 'targeted agent', 'therapeutic target']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2019,465464,-0.026815890371606463
"Familial hypercholesterolemia screening in children: population impact of phenotype, genotype, and cascade approaches Project Summary  Familial hypercholesterolemia (FH) is a common genetic disorder, affecting every 200-1000 people, depending on the population and diagnostic criteria. FH leads to lifetime raised low-density lipoprotein (LDL) cholesterol, a high risk for premature atherosclerosis and downstream coronary heart disease. FH is designated as Tier 1 disease by the Center for Disease Control and Prevention, notably one of only three such diseases, because it is common, is associated with a high risk of premature illness, and is treatable with lifestyle or medications. Great uncertainty exists about the optimal approach to FH screening, which is reflected in conflicting recommendations in national screening guidelines.  We propose to synthesize high quality data from national surveys and population-based cohort studies in a health policy computer simulation model comparing the health and economic value of different FH screening strategies. This study will prioritize the optimal approaches to FH screening in the U.S. population, identifying optimal initial screening age and defining the role of genetic testing in screening.  We have assembled a team of experts in pediatric preventive cardiology, decision analysis, cardiovascular disease epidemiology, population genetics, biostatistics, health economic evaluation, and computer simulation modeling in order to evaluate and compare different FH screening strategies in children and adults. We aim to use this expertise and these methods in order to:   Quantify diagnostic yield, clinical effectiveness, and economic value of universal FH phenotype  screening in childhood or adulthood, and the added value of FH genotype screening   Compare universal FH screening to the alternatives of using family history or a Big Data-based  algorithm to direct targeted screening limited to children and adults with possible FH diagnosis   Quantify the health and economic value of cascade screening families of FH cases  We hypothesize that FH screening in childhood will be the highest value screening strategy in the U.S. population, and that genetic testing will improve diagnosis and treatment decisions most in cases of diagnostic uncertainty (e.g., borderline high cholesterol or absent family history). We hypothesize that a machine-learning algorithm will avoid the costs and complexity of universal screening, while yielding a similar case yield, as long as cholesterol testing is sufficiently common in children.  This study will identify the optimal approach to FH screening in the U.S. population and the most influential data based on current knowledge and set the stage for efficiently designed clinical trials of FH screening. This study will be a test case for the concept of a “precision” population health approach to screening for genetically-determined diseases in the general population. Familial hypercholesterolemia (FH) is a common genetic disorder characterized by lifetime elevated cholesterol, which, if uncontrolled, is associated with premature atherosclerotic cardiovascular disease. Conflicting current national guidelines highlight that the optimal approach to FH screening in the U.S. population is controversial: it is unclear if screening should start in childhood or adulthood, or if it should include genetic testing. We propose to synthesize data from national surveys, high quality cohort studies, and clinical trials of cholesterol lowering interventions in a lifetime cardiovascular disease risk computer simulation model to project the life time health impact of different FH screening approaches and identify optimal screening strategies in children and adults.","Familial hypercholesterolemia screening in children: population impact of phenotype, genotype, and cascade approaches",9661259,R01HL141823,"['Adult', 'Adverse effects', 'Advisory Committees', 'Affect', 'Age', 'Algorithms', 'Atherosclerosis', 'Big Data', 'Biometry', 'Cardiology', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Childhood', 'Cholesterol', 'Clinical Trials', 'Clinical Trials Design', 'Clinical effectiveness', 'Cohort Studies', 'Computer Simulation', 'Conflict (Psychology)', 'Coronary heart disease', 'County', 'Data', 'Data Quality', 'Decision Analysis', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Event', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Future', 'General Population', 'Genetic Diseases', 'Genetic Screening', 'Genetic screening method', 'Genotype', 'Guidelines', 'Health', 'Health Benefit', 'Health Policy', 'Hepatocyte', 'Influentials', 'Intervention', 'Knowledge', 'LDL Cholesterol Lipoproteins', 'Laboratories', 'Life', 'Life Style', 'Low Prevalence', 'Low-Density Lipoproteins', 'Machine Learning', 'Medical Care Costs', 'Methods', 'Mutation', 'Parents', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Population Genetics', 'Prevalence', 'Preventive', 'Preventive service', 'Preventive treatment', 'Proxy', 'Puberty', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recording of previous events', 'Role', 'Serum', 'Surveys', 'Testing', 'Time', 'Uncertainty', 'Visit', 'Youth', 'base', 'cardiovascular disorder epidemiology', 'cardiovascular disorder risk', 'clinical practice', 'cost', 'cost effective', 'cost effectiveness', 'diagnostic accuracy', 'economic evaluation', 'economic value', 'health economics', 'high risk', 'improved', 'improved outcome', 'learning strategy', 'lifestyle intervention', 'machine learning algorithm', 'models and simulation', 'pediatric patients', 'population based', 'population health', 'premature', 'premature atherosclerosis', 'prevent', 'screening', 'screening guidelines', 'screening program', 'treatment strategy', 'uptake']",NHLBI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,894544,-0.032174991098022916
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9737676,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Quality', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,804907,-0.0009505744286558409
"Driver Genes for Engineered Rotator Cuff Development Rotator cuff tears affect over 15% of Americans and impair shoulder joint biomechanics and function. Following repair of symptomatic tears, functional deficits frequently persist and re-tears are common, due to the complex anatomy and high functional demands on the rotator cuff tendons. Rotator cuff tendon tissue engineering research is focused on devices to improve immediate mechanical support to the repair and to stimulate early and rapid tendon regeneration rather than scarring and fibrosis, particularly for the supraspinatus tendon (SST), the most commonly torn tendon in the rotator cuff. Aligned electrospun scaffolds that mimic both the highly aligned medial region of the SST, and bi-axially aligned electrospun scaffolds that mimic the multi-axially aligned isotropic anterior region of the SST have been evaluated with promising results when seeded with adipose- derived stem cells (ASCs). However, progress in this area of rotator cuff tendon engineering and in other areas of tendon research is hindered by the lack of definitive markers for SST or for its regional heterogeneity, the lack of understanding to what extent ASCs are tenogenic and can assume the identity of tendon fibroblasts, the lack of specific markers for tendon fibroblast identity and tenogenic differentiation, and by a lack of markers for tendon maturation and response to mechanical loading in engineered tendon. Therefore, is it difficult to assess how successful current tendon tissue engineering approaches really are, or to predict how well tendon tissue engineered approaches will function in translation when autologous or allogeneic ASCs from diverse human populations are used to enhance rotator cuff repair via augmentation or interposition with engineered tendon devices. These studies will evaluate the epigenome (methylome), transcriptome, proteome, lipidome, metabolome and phenome (phenotype) of native human SST and donor-matched tissue engineered tendon produced from SST fibroblasts and ASCs. Bioinformatics approaches will be used to integrate the data to an integrated multiome, which will then be used with machine learning approaches to extract key causal ‘driver’ genes, or tendon specific genes or molecules responsible for: 1) SST heterogeneity between medial and anterior regions. 2) Tendon cell identity and the extent of tenogenesis by ASCs on electrospun scaffolds. 3) The heterogenetic response by ASCs on uni- vs. bi-axially aligned electrospun scaffolds that mimic the native heterogeneity of the SST. 4) The response of engineered tendon to dynamic loading. Identified driver genes or molecules will be validated though over-expression or silencing approaches, thus providing therapeutic targets for manipulation to enhance tenogenesis, and engineered tendon development and maturation. Together these innovative studies will provide a template for improved external validity of benchtop tendon tissue engineering and pre-clinical studies towards successful translation in diverse patient populations. In addition, the bioinformatics and multiomics toolboxes and assays that result from this work will be invaluable to not only the tendon research community, but also to the wider musculoskeletal and regenerative medicine fields. A small number of driver genes in other medical fields are responsible for changes in expression and protein levels in hundreds of other genes, and subsequent tissue metabolism. The process of tissue engineered rotator cuff tendon development and formation as it relates to adult tendon after surgical repair is not well understood, despite rotator cuff tears being an important clinical problem and representing almost one third of orthopedic injuries. Evaluating changes in the ability of genes to be translated to proteins at the DNA, RNA, small metabolite, and protein level of tissue engineered tendon formation under different loading conditions compared to normal adult tendon will identify driver genes and potential new ways to improve currently available options for rotator cuff repair.",Driver Genes for Engineered Rotator Cuff Development,9684263,R01AR073882,"['Address', 'Adipose tissue', 'Adult', 'Affect', 'Allogenic', 'American', 'Anatomy', 'Anterior', 'Area', 'Autologous', 'Biochemical', 'Biocompatible Materials', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Biomechanics', 'Cadaver', 'Cells', 'Cicatrix', 'Clinical', 'Communities', 'Complex', 'Computational Technique', 'Cues', 'DNA', 'Data', 'Data Set', 'Development', 'Devices', 'Engineered Gene', 'Engineering', 'Evaluation', 'Extracellular Matrix', 'Fibroblasts', 'Fibrosis', 'Genes', 'Genetic', 'Harvest', 'Head', 'Heterogeneity', 'Human', 'Impairment', 'In Vitro', 'Injury', 'Investigation', 'Joint structure of shoulder region', 'Knowledge', 'Machine Learning', 'Measures', 'Mechanical Stimulation', 'Mechanics', 'Medial', 'Medical', 'Mesenchymal', 'Mesenchymal Stem Cells', 'Metabolism', 'Methylation', 'Modeling', 'Modification', 'Molecular', 'Musculoskeletal', 'Natural regeneration', 'Operative Surgical Procedures', 'Orthopedics', 'Outcome', 'Phenotype', 'Population', 'Process', 'Property', 'Proteins', 'Proteome', 'RNA', 'Regenerative Medicine', 'Research', 'Resources', 'Rotator Cuff', 'Seeds', 'Stem cells', 'Techniques', 'Tendon structure', 'Testing', 'Tissue Engineering', 'Tissues', 'Translating', 'Translations', 'Work', 'design', 'epigenome', 'improved', 'innovation', 'interest', 'mechanical load', 'mechanical properties', 'metabolome', 'methylome', 'multiple omics', 'overexpression', 'patient population', 'phenome', 'phenomics', 'preclinical study', 'progenitor', 'regenerative', 'repaired', 'response', 'rotator cuff tear', 'scaffold', 'spatial relationship', 'supraspinatus muscle', 'tendon development', 'therapeutic target', 'transcriptome', 'translation to humans']",NIAMS,PURDUE UNIVERSITY,R01,2019,485405,-0.04016178527581582
"Optimal dynamic treatment strategies for controlling alcohol use: novel methods for selecting and incorporating effect modifiers Project Summary: The cyclical and heterogeneous nature of many substance use disorders highlights the need to adapt the type or the dose of treatment to accommodate the specific and changing needs of individuals. This proposal is motivated by the Extending Treatment Effectiveness of Naltrexone (ExTENd) trial, a sequential multiple assignment randomized trial (SMART) designed to find a (personalized) rescue treatment for those who are non-responsive to initial Naltrexone. One of the main challenges in this trial is the presence of the many variables available for consideration when making treatment decisions at each stage of the trial. This feature has made it virtually impossible for investigators to fully explore the possibility of building high quality treatment strategies using the data. Our overarching aim is to address this particular challenge through developing and subsequently applying new statistical methods to the ExTENd trial data. A SMART trial is a multi-stage trial that can inform the design of a dynamic treatment regime (DTR) which formalizes an individualized treatment plan and where current treatment strategy can depend on a patient's past medical and treatment history. An optimal DTR is one that maximizes a specified health outcome of interest. Q-learning can be used with data from both SMARTs and observational studies to estimate an optimal DTR. However, like other model-based approaches, model misspecification can seriously affect the results and lead to the identification of suboptimal DTRs. The potential for misspecification increases with the number of variables that may influence treatment decisions through, e.g., incorrect assumptions on the relationship of variables to the outcome and the inclusion (exclusion) of unimportant (important) variables. These features represent the main analytical challenges for the ExTENd trial. We propose a new approach to Q-learning that leverages machine learning approaches to reduce the chances of misspecifying the relationship between the expected outcome and a given set of variables. We also develop a variable selection technique specifically designed for Q-learning that enables investigators to select the important variables from a long list of possibilities (e.g., genetic and demographic information, medical history over time) when estimating an optimal DTR. In both settings, we will develop new methods for conducting valid inferences (e.g., confidence intervals and p-values), including when there exist patients for whom treatment is neither beneficial nor harmful at a given decision stage (i.e., when an important technical assumption, “uniqueness”, is violated). Finally, we will develop easy-to-use, publicly available software in the R language that implements our methods. This will allow re-analysis of the ExTENd trial data with a goal of constructing a DTR that improves upon the current rescue treatment strategy for those non-responsive to initial Naltrexone. It will also provide an expandable platform that will assist researchers in developing new optimal DTRs for patients suffering from alcoholism and other substance use disorders. Narrative: This project aims to address the need for robust, rigorous and computationally efficient methods for estimating the optimal treatment regime in substance use disorder. We will develop a variable selection technique that enables investigators to select important variables for the decision- making process among a long list of variables (e.g., patient’s genetic information, demographic characteristics, and medical history over time). We will also leverage statistical machine learning approaches to improve the quality of the constructed optimal treatment regimes while providing valid statistical inference.",Optimal dynamic treatment strategies for controlling alcohol use: novel methods for selecting and incorporating effect modifiers,9721702,R21AA027571,"['Address', 'Adoption', 'Affect', 'Alcohol consumption', 'Alcohol dependence', 'Alcoholism', 'Behavior Therapy', 'Characteristics', 'Code', 'Computer software', 'Confidence Intervals', 'Data', 'Decision Making', 'Dose', 'Exclusion', 'Fibrinogen', 'Future', 'Genetic', 'Goals', 'Health', 'Heterogeneity', 'Individual', 'Language', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Medical Care Costs', 'Medical History', 'Methodology', 'Methods', 'Modeling', 'Naltrexone', 'Nature', 'Observational Study', 'Outcome', 'Patients', 'Periodicity', 'Phenotype', 'Process', 'Prognostic Factor', 'Randomized', 'Recording of previous events', 'Research', 'Research Personnel', 'Software Tools', 'Specific qualifier value', 'Statistical Methods', 'Subgroup', 'Substance Use Disorder', 'Techniques', 'Testing', 'Time', 'Treatment Effectiveness', 'Treatment Protocols', 'Work', 'alcohol use disorder', 'base', 'care providers', 'clinical care', 'clinical decision-making', 'design', 'dosage', 'expectation', 'falls', 'genetic information', 'high dimensionality', 'improved', 'individualized medicine', 'interest', 'novel', 'novel strategies', 'open source', 'optimal treatments', 'overtreatment', 'personalized medicine', 'randomized trial', 'repository', 'response', 'treatment planning', 'treatment strategy', 'trial design', 'virtual']",NIAAA,UNIVERSITY OF ROCHESTER,R21,2019,237402,-0.010775909401038637
"Diagnostic and prognostic biomarkers for subtypes of addiction-related circuit dysfunction Project Summary Substance use disorders (SUDs) are increasing in prevalence and are already a leading cause of disability, due in part to the fact that our understanding of the underlying pathophysiology is incomplete. Like most neuropsychiatric syndromes, SUDs are highly heterogeneous, and distinct mechanisms may be operative in some individuals but not in others, even within a single diagnostic category. Furthermore, SUDs frequently co- occur with depression, anxiety, and other psychiatric syndromes, complicating efforts to identify molecular and circuit-level mechanisms, and disentangle them from those involved in mood and anxiety disorders. Diagnostic heterogeneity is thus a fundamental obstacle to developing better treatments, identifying biomarkers for quantifying risk for different forms of addiction, and predicting treatment response and relapse. Recently, we developed and validated an approach to discovering and diagnosing subtypes of depression using fMRI measures of functional connectivity, which in turn predicted subtype-specific clinical symptom profiles and treatment outcomes. Here, in response to PAR-18-062, we propose a secondary data analysis that would extend this approach to SUDs, leveraging multiple deeply characterized and large-scale neuroimaging datasets. Our central hypothesis is that individual differences in mechanisms underlying impairments in response inhibition and salience attribution (iRISA) are mediated by distinct forms of dysfunctional connectivity in addiction-related circuits, which in turn interact and give rise to distinct neurophysiological addiction subtypes. In Aim 1, we will use statistical clustering and machine learning methods to delineate these subtypes and optimize classifiers (fMRI biomarkers) for diagnosing them in individual patients, focusing initially on cocaine addiction. In Aim 2, we will validate these subtype-specific biomarkers by first replicating them in a new dataset and then evaluating their longitudinal stability and predictive utility. In Aim 3, we will test whether subtype-specific circuit mechanisms generalize to mediate iRISA functions in other forms of addiction, and define their interactions with distinct mechanisms mediating anhedonia and anxious arousal in patients with comorbid depression and anxiety. Project Narrative Biomarkers have transformed how physicians care for patients with cancer, cardiovascular disease, and a host of other medical conditions by providing quantitative tools for diagnosing disease processes and individualizing treatment decisions. In contrast, biomarkers for addictions remain relatively elusive. This project will test a new strategy for developing neuroimaging (brain scan) biomarkers for diagnosing novel subtypes of addiction in individual patients and then investigate how dysfunction in specific circuits gives rise to specific addiction- related behaviors and clinical symptoms.",Diagnostic and prognostic biomarkers for subtypes of addiction-related circuit dysfunction,9840077,R01DA047851,"['Abstinence', 'Amygdaloid structure', 'Anhedonia', 'Animal Model', 'Anterior', 'Anxiety', 'Anxiety Disorders', 'Arousal', 'Behavior', 'Biological Markers', 'Brain', 'Brain scan', 'Cardiovascular Diseases', 'Categories', 'Clinical', 'Cocaine', 'Cocaine Dependence', 'Cognition', 'Cognitive', 'Communities', 'Comorbidity', 'Corpus striatum structure', 'Data Analyses', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Dorsal', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Habits', 'Heterogeneity', 'Impairment', 'Individual', 'Individual Differences', 'Lateral', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medial', 'Mediating', 'Medical', 'Mental Depression', 'Molecular', 'Mood Disorders', 'Motivation', 'Negative Reinforcements', 'Neurobiology', 'Nicotine', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacology', 'Physicians', 'Prediction of Response to Therapy', 'Prevalence', 'Process', 'Prognostic Marker', 'Psychiatric Diagnosis', 'Regulation', 'Relapse', 'Research', 'Research Personnel', 'Rest', 'Rewards', 'Risk', 'Scanning', 'Subgroup', 'Substance Use Disorder', 'Symptoms', 'Syndrome', 'System', 'Testing', 'Transcend', 'Treatment outcome', 'Ventral Striatum', 'Withdrawal', 'addiction', 'animal data', 'anxious', 'base', 'biobank', 'biomarker validation', 'cocaine use', 'comorbid depression', 'craving', 'diagnostic biomarker', 'disability', 'disease classification', 'disease diagnosis', 'drug seeking behavior', 'individual patient', 'individualized medicine', 'interest', 'learning strategy', 'negative affect', 'negative emotional state', 'network dysfunction', 'neuroimaging', 'neurophysiology', 'neuropsychiatry', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'patient subsets', 'reinforcer', 'response', 'specific biomarkers', 'substance misuse', 'tool']",NIDA,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2019,621348,-0.015364100134528963
"Noninvasive assessment of the cornea by diffusion OCT Keratoconus is a degenerative disease of the cornea that is a major cause of reduced vision-related quality of life in the United States, often leading to corneal transplantation. Ectasia after refractive surgery is a vision-threatening complication that can occur in apparently low-risk patients despite current screening technology. The biomechanical properties of the cornea play a central role in these diseases, but diagnostics are still rooted in shape measures because doctors lack direct measures of biomechanical change. While several methods have shown early promise for addressing this gap, most require contact with or perturbation of the cornea, cannot spatially resolve biomechanical properties, or involve expensive optical systems.  To address the need for direct biomechanical measurement of the cornea and the limitations of other approaches, we introduce phase-decorrelation OCT (phd-OCT). Phd-OCT makes use optical coherence tomography (OCT) to quantify random nanoscale mobility that is related to the strength and cohesion of the cornea. Preliminary results strongly support the rationale, feasibility and potential advantages of the approach. Our objectives are: (1) to refine our method to optimize detection sensitivity and speed, and (2) to determine if the technique is clinically useful. We will achieve these objectives through the following aims: 1. To develop and validate mobility-sensitive phd-OCT for corneal imaging. Spectral-domain OCT will be used for anterior segment phase-decorrelation imaging and the analysis algorithm will be optimized. The system will be validated using phantoms and torsional rheometry. 2. To investigate the potential influence of physiological factors (intraocular pressure (IOP), hydration, and temperature). Factorial design experiments in porcine and human donor globes will establish the sensitivity of phd-OCT measurements to potential confounders. 3. To develop and validate data acquisition and processing methods to enable clinical testing. GPU processing and machine learning will be configured to minimize processing time. Scan patterns will be optimized to minimize scan time and return clinically useful data. 4. To assess feasibility and preliminary diagnostic performance of clinical mobility-sensitive OCT imaging of the cornea. A first-in-human study will characterize repeatability and test the hypotheses that phd-OCT mobility measurements are increased in the region of a LASIK flap and decreased in CXL.  Expected Outcomes: The proposed studies will establish a new, non-contact method for imaging corneal biomechanical properties with the potential to address many shortcomings of current and emerging methods. Success could lead to earlier detection of of ectasia risk and allow more appropriate timing and customization of corneal treatments. Future integration of data into computational models has the potential to greatly impact the field by driving a shift from empirical planning and risk analysis to patient-specific strategies. The mechanical properties of the cornea, the transparent front of the eye, are important for diseases that affect vision as well as for corrective treatments like crosslinking therapy and LASIK. We have developed a new optical coherence tomography (OCT)-based method to detect and map corneal mechanical properties that has advantages that may make it readily translatable to clinical use. The objective of this proposal is to determine whether this technique is clinically feasible and useful by refining the technology, testing it in animal and human donor corneas, and performing studies in patients with corneal conditions of interest.",Noninvasive assessment of the cornea by diffusion OCT,9735326,R01EY028667,"['Achievement', 'Address', 'Affect', 'Algorithmic Analysis', 'Algorithms', 'Animals', 'Anterior', 'Automobile Driving', 'Biomechanics', 'Clinical', 'Clinical Research', 'Complication', 'Computational Technique', 'Computer Simulation', 'Cornea', 'Custom', 'Data', 'Degenerative Disorder', 'Detection', 'Development', 'Diagnostic', 'Diffusion', 'Disease', 'Doctor of Philosophy', 'Early Diagnosis', 'Ensure', 'Exhibits', 'Eye', 'Family suidae', 'Feedback', 'Future', 'Human', 'Hydration status', 'Image', 'Image Analysis', 'Keratoconus', 'Keratoplasty', 'Laser In Situ Keratomileusis', 'Lead', 'Life', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathological Dilatation', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic Intraocular Pressure', 'Physiological', 'Plant Roots', 'Play', 'Procedures', 'Property', 'Proxy', 'Quality of life', 'Risk', 'Role', 'Scanning', 'Shapes', 'Speed', 'Structure', 'Surgical Flaps', 'Surgical incisions', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Torsion', 'United States', 'Validation', 'Vision', 'Visual impairment', 'base', 'clinical practice', 'cohesion', 'computerized data processing', 'crosslink', 'data acquisition', 'data integration', 'design', 'experimental study', 'first-in-human', 'human study', 'imaging modality', 'interest', 'mechanical properties', 'nanoscale', 'novel', 'outcome prediction', 'research clinical testing', 'screening', 'simulation', 'success', 'tool', 'treatment effect']",NEI,CASE WESTERN RESERVE UNIVERSITY,R01,2019,396887,-0.01601674057797006
"In-Office, Ultrasound-Based Breakage and Removal of Urinary stones OVERALL SUMMARY  The main focus of this Program Project Grant is to discover the foundational and translational knowledge needed to create an office-based handheld ultrasound device to target, detach, break, and expel stones and stone fragments from the urinary space to facilitate natural clearance. This system will obviate costly and inefficient emergency department visits that typically include repetitive exposure to ionizing radiation from diagnostic imaging, and will significantly reduce the often lengthy (days to weeks) wait time patients must endure before procedures for stone removal can be scheduled and performed. As the proposed therapy system is entirely noninvasive, patients will be treated on an outpatient basis. Further, as the system is designed to efficiently and painlessly break stones of any size and expel the fragments from the kidney, the treatment of both symptomatic and asymptomatic stones using this technology will reduce the high retreatment and stone event recurrence rates associated with current surgical interventions for stone removal.  In this effort, we will combine stone breakage by burst wave lithotripsy (BWL), clearance of fragments by ultrasonic propulsion (UP), and stone-specific ultrasound imaging (S-mode) into an integrated system in which exposure strategies are adapted during treatment in response to real-time acoustic feedback to enhance comminution efficiency and patient safety. We will tailor treatment by investigating numerically and in lab tests the primary mechanisms - cavitation and elastic waves - involved in the comminution process over a broad parameter space. We will develop acoustics-based feedback including model- based, machine learning and passive acoustic mapping (PAM) of the bubble field to signal the need to adjust the energy output. We will investigate the morphological and functional response of the kidney in living animals and in ex vivo perfused porcine kidneys, and pursue tissue protective treatment strategies such as power ramping.  These studies will include the first in-human test of BWL in which we will compare the comminution effectiveness and safety of treatment with and without adaptive output control in response to acoustic feedback. In addition, we will conduct a randomized controlled trial of the benefits and risks of fragmenting and expelling symptomatic and asymptomatic stones in the clinic. Toward application of the system for use in humans, we will refine and validate the use of UP and S-mode together to improve stone and fragment detection. With our eye on the future of stone management, we will develop and validate in vivo an extracorporeal acoustic tractor beam to grasp and carry fragments through the complex three- dimensional path of the urinary space and out of the kidney. OVERALL NARRATIVE Urinary stone disease, which affects 1 in 11 Americans, is one of the most painful diseases and also the costliest non-malignant urologic disease, because current management is limited to observation for the stone to pass or performing surgery. The main focus of this Program Project Grant is to discover the foundational and translational knowledge needed to create an office-based, handheld ultrasound device to target, detach, break, and expel stones and stone fragments from the urinary space to facilitate natural clearance. This system will obviate costly and temporizing emergency department visits and radiation exposing imaging while the patient waits in pain for days to weeks for the stone to pass or be surgically removed.","In-Office, Ultrasound-Based Breakage and Removal of Urinary stones",9791705,P01DK043881,"['Acoustics', 'Address', 'Affect', 'American', 'Anatomy', 'Animals', 'Benefits and Risks', 'Biostatistics Core', 'Bowman&apos', 's space', 'Calibration', 'Clinic', 'Clinical', 'Collaborations', 'Complex', 'Data', 'Data Set', 'Detection', 'Devices', 'Diagnostic Imaging', 'Dimensions', 'Disease', 'Effectiveness', 'Emergency department visit', 'Engineering', 'Ensure', 'Event', 'Excision', 'Exposure to', 'Eye', 'Family suidae', 'Feedback', 'Foundations', 'Fracture', 'Future', 'Health', 'Human', 'Image', 'Ionizing radiation', 'Kidney', 'Knowledge', 'Lithotripsy', 'Machine Learning', 'Measures', 'Methodology', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Non-Malignant', 'Operative Surgical Procedures', 'Outpatients', 'Output', 'Pain', 'Painless', 'Patients', 'Procedures', 'Process', 'Productivity', 'Program Research Project Grants', 'Publications', 'Radiation exposure', 'Ramp', 'Randomized Controlled Trials', 'Recurrence', 'Renal Tissue', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Retreatment', 'Safety', 'Sample Size', 'Schedule', 'Signal Transduction', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Model', 'Tissues', 'Ultrasonics', 'Ultrasonography', 'Urinary Calculi', 'Urologic Diseases', 'Wait Time', 'base', 'biomechanical model', 'calcification', 'cost', 'design', 'experimental study', 'first-in-human', 'grasp', 'improved', 'in vivo', 'individualized medicine', 'next generation', 'patient safety', 'programs', 'response', 'safety study', 'simulation', 'success', 'treatment strategy']",NIDDK,UNIVERSITY OF WASHINGTON,P01,2019,1619137,-0.024870587683835408
"Evaluation of multiple medication exposures concurrently using a novel algorithm PROJECT SUMMARY The development of large observational health databases (OHD) has expanded the data available for analysis by pharmacoepidemiology research. The efficiency of these studies may be improved by simultaneously studying the association of multiple medications with a disease of interest. Unfortunately, prior research has demonstrated that it is difficult to distinguish true-positive from false-positive results when studying multiple exposures simultaneously, thus limiting the conclusions drawn from these types of studies and representing a major gap in the field. The objective of this proposal, which is the first step in achieving the applicant's long- term goal of improving the diagnosis and treatment of gastrointestinal diseases using insights derived from OHD, is to evaluate and validate medication class enrichment analysis (MCEA), a novel set-based signal-to- noise enrichment algorithm developed by the applicant to analyze multiple exposures from OHD with high sensitivity and specificity. The central hypothesis of this proposal is that MCEA has equal sensitivity and greater specificity compared to logistic regression, the most widely used analytic method for OHD, for identifying true associations between medications and clinical outcomes. The applicant will complete the following two interrelated specific aims to test the hypothesis: Aim 1 – to calculate the sensitivity and specificity of medication class enrichment analysis (MCEA) and logistic regression (LR) for identifying medication associations with Clostridium difficile infection (CDI) and Aim 2 – to calculate the sensitivity and specificity of MCEA and LR for identifying medication associations with gastrointestinal hemorrhage (GIH). The rationale for these aims is that by reproducing known medication-disease associations without false positives, MCEA can be used to identify novel pharmacologic associations with gastrointestinal diseases in future studies. The expected outcome for the proposed research is that it will demonstrate MCEA as a valid method for pharmacoepidemiology research, opening new research opportunities for the study of multi-exposure OHD. These new research opportunities may lead to more rapid identification of potential pharmacologic causes of emerging diseases and discovery of unanticipated beneficial medication effects, allowing such medications to be repurposed for new indications. To attain the expected outcome, the applicant will complete additional coursework that builds on his Master of Science in Clinical Epidemiology to learn computational biology, machine learning, and econometrics techniques. With the support of this grant and his institution, he will also directly apply these techniques to pharmacoepidemiology applications under the close mentorship of a carefully selected team of faculty with extensive experience in gastroenterology, pharmacoepidemiology, medical informatics, and mentoring prior K-award grant recipients. Through these activities, the applicant will develop the skills necessary to obtain NIH R01-level funding and become a leader in developing novel techniques for application to the epidemiologic study of gastrointestinal diseases. PROJECT NARRATIVE Traditionally, research studying medications associated with diseases are limited to analyzing one medication at a time. This novel proposal will validate medication class enrichment analysis, a recently developed algorithm to study multiple medications simultaneously for association with a disease of interest. Validation of this method will allow researchers to use existing medical databases to more rapidly identify potential medication causes of emerging diseases and identify medications with unanticipated beneficial effects, allowing such medications to be repurposed for new indications.",Evaluation of multiple medication exposures concurrently using a novel algorithm,9645854,K08DK119475,"['Address', 'Algorithms', 'Aminoglycosides', 'Antibiotics', 'Anticoagulants', 'Antiplatelet Drugs', 'Big Data to Knowledge', 'Biological', 'Carbapenems', 'Case-Control Studies', 'Cephalosporins', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Computational Biology', 'Computer software', 'Data', 'Databases', 'Development', 'Development Plans', 'Diagnosis', 'Digestive System Disorders', 'Disease', 'Electronic Health Record', 'Epidemiologic Methods', 'Evaluation', 'Faculty', 'Fluoroquinolones', 'Funding', 'Future', 'Gastroenterology', 'Gastrointestinal Diseases', 'Gastrointestinal Hemorrhage', 'Generations', 'Genomics', 'Goals', 'Grant', 'Health', 'Infection', 'Informatics', 'Inpatients', 'Institution', 'K-Series Research Career Programs', 'Lead', 'Learning', 'Logistic Regressions', 'Machine Learning', 'Master of Science', 'Medical', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Noise', 'Non-Steroidal Anti-Inflammatory Agents', 'Outcome', 'Penicillins', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacology', 'Research', 'Research Design', 'Research Personnel', 'Sensitivity and Specificity', 'Signal Transduction', 'Specificity', 'Techniques', 'Testing', 'Time', 'United Kingdom', 'United States National Institutes of Health', 'Validation', 'Veterans', 'analytical method', 'base', 'beta-Lactams', 'career development', 'clinical epidemiology', 'econometrics', 'epidemiology study', 'experience', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'research study', 'simulation', 'skills', 'usability']",NIDDK,UNIVERSITY OF PENNSYLVANIA,K08,2019,169440,-0.011018525149680873
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9690817,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'multidimensional data', 'multimodal data', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2019,758607,-0.02362866887395924
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10012976,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2019,277141,-0.024931959307730275
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9767141,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2019,886748,-0.024931959307730275
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9678366,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'causal variant', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodal data', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'patient subsets', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2019,602448,-0.005078141250066684
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9744013,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Algorithms', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelium', 'Esophageal', 'Esophageal Adenocarcinoma', 'Esophagitis', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Intelligence', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stem cells', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2019,171720,-0.04199193537594426
"Automated Phenotyping in Epilepsy There are 65 million people worldwide with epilepsy and 150,000 new cases of epilepsy are diagnosed in the US annually. However, treatment options for epilepsy remain inadequate, with many patients suffering from treatment-resistant seizures, cognitive comorbidities and the negative side effects of treatment. A major obstacle to progress towards the development of new therapies is the fact that preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). Recently, the Datta lab showed that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales and arranged according to specific rules (“grammar”). These syllables can be detected without observer bias using a method called motion sequencing (MoSeq) that employs video imaging with a 3D camera combined with artificial intelligence (AI)-assisted video analysis to characterize behavior. Through collaboration between the Soltesz and Datta labs, exciting data were obtained that demonstrated that MoSeq can be adapted for epilepsy research to perform objective, inexpensive and automated phenotyping of mice in a mouse model of chronic temporal lobe epilepsy. Here we propose to test and improve MoSeq further to address long-standing, fundamental challenges in epilepsy research. This includes the development of an objective alternative to the Racine scale, testing of MoSeq as an automated anti-epileptic drug (AED) screening method, and the development of human observer- independent behavioral biomarkers for seizures, epileptogenesis, and cognitive comorbidities. In addition, we plan to dramatically extend the epilepsy-related capabilities of MoSeq to include the automated tracking of finer-scale body parts (e.g., forelimb and facial clonus) that are not possible with the current approach. Finally, we propose to develop the analysis pipeline for MoSeq into a form that is intuitive, inexpensive, user-friendly and thus easily sharable with the research community. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user- independent, inexpensive analysis of both acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will develop breakthrough technologies involving artificial intelligence (AI)-assisted analysis of 3-dimensional video data of mouse behavior to address long-standing, fundamental challenges in preclinical epilepsy research. If successful, this innovative approach is expected to have a significant and sustained impact on epilepsy research by enabling investigators to perform objective, automated, inexpensive, reproducible assessment of epilepsy in experimental animals to aid the testing of anti-seizure drugs and other novel therapies.",Automated Phenotyping in Epilepsy,9862231,R01NS114020,"['3-Dimensional', 'Address', 'Animal Behavior', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological Markers', 'Body part', 'Cannabidiol', 'Carbamazepine', 'Cells', 'Chronic', 'Clonazepam', 'Clonus', 'Cognitive', 'Collaborations', 'Communities', 'Comorbidity', 'Complex', 'Data', 'Development', 'Diagnosis', 'Drug Screening', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Face', 'Forelimb', 'Future', 'Genetic', 'Genetic Models', 'Head', 'Hippocampus (Brain)', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Intuition', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Mus', 'Observer Variation', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Phenytoin', 'Pilocarpine', 'Probability', 'Reproducibility', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Rest', 'Seizures', 'Specificity', 'Stereotyping', 'Structure', 'Tail', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-dimensional analysis', 'Time', 'Treatment Side Effects', 'Valproic Acid', 'analysis pipeline', 'base', 'behavior test', 'drug use screening', 'improved', 'innovation', 'kainate', 'method development', 'mouse model', 'novel therapeutics', 'optogenetics', 'pre-clinical', 'pre-clinical research', 'selective expression', 'user-friendly']",NINDS,STANFORD UNIVERSITY,R01,2019,450365,-0.009401378755816236
"Microscopy-Based Antimicrobial Susceptibility Testing (MAST) Antibiotic resistance is compromising our ability to treat bacterial infections. Clinical microbiology laboratories guide appropriate treatment through antimicrobial susceptibility testing (AST) of patient bacterial isolates. However, increasingly, pathogens are developing resistance to a broad range of antimicrobials, requiring AST of less commonly used or recently introduced agents for which no commercially available or FDA-cleared testing methods exist. Agar and broth dilution are gold standard methods for AST that can be used to test any antimicrobial; however, labor and technical complexity precludes their use in hospital-based clinical laboratories. Therefore, bacterial isolates often must be sent to a reference laboratory with a 4-6 day delay in results. Furthermore, even standard methods require overnight incubation prior to readout. Therefore, there exists a significant AST testing gap in which current methodologies cannot adequately address the need for rapid results in the face of unpredictable susceptibility profiles. Our laboratory has recently verified inkjet printer-based digital dispensing technology as a novel platform to facilely perform reference AST for any antimicrobial at will. In this proposal, we aim to combine this methodology with advanced microscopy to leapfrog traditional AST capabilities through: (1) development of a method for microscopic imaging of bacterial replication on a solid-phase, 384-well microplate AST format, thereby allowing determination of susceptibility for any drug at will in 4 hours and (2) development and application of advanced image analysis for automated susceptibility calls. This new platform is designated MAST for microscopy-based antimicrobial susceptibility testing. The clinical diagnostic performance of the platform will be optimized against an AST reference method for accuracy and precision using a large panel of well-characterized clinical isolates. We anticipate establishing a prototype platform that will address the AST testing gap and thereby help our health system more effectively address the antimicrobial resistance threat. With the emergence of multi-drug resistant bacteria, it is no longer possible to accurately predict which antimicrobials will be effective against life-threatening bacterial illness. Testing bacteria directly for response available therapies may take several days. Therefore, a new technology platform called MAST is proposed to allow us to determine which antibiotics can treat a bacteria infection in a matter of hours and thereby address our current, clinically unacceptable antimicrobial testing gap.",Microscopy-Based Antimicrobial Susceptibility Testing (MAST),9697702,R21AI130434,"['Address', 'Agar', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Antimicrobial susceptibility', 'Bacteria', 'Bacterial Infections', 'Clinical', 'Clinical Microbiology', 'Development', 'Diagnostic tests', 'Goals', 'Gold', 'Growth', 'Health system', 'Hospitals', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Laboratories', 'Life', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Monitor', 'Multiple Bacterial Drug Resistance', 'Nutrient', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Predisposition', 'Printing', 'Regimen', 'Resistance', 'Resistance development', 'Solid', 'Surface', 'Technology', 'Test Result', 'Testing', 'antimicrobial', 'automated image analysis', 'base', 'biomaterial compatibility', 'clinical diagnostics', 'convolutional neural network', 'digital', 'direct application', 'microscopic imaging', 'new technology', 'next generation', 'novel', 'pathogen', 'performance tests', 'prototype', 'response', 'supervised learning']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R21,2019,223354,-0.022676588184776993
"Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia PROJECT SUMMARY Preeclampsia (PE) is a devastating hypertensive disorder of pregnancy that is a leading cause of maternal and fetal mortality and morbidity worldwide. It affects 2-10% of women and accounts for 16% of maternal deaths related to childbirth in developed countries. PE is difficult to study because it is a multifactorial disorder with varying molecular mechanisms associated with the development of the spectrum of clinical symptoms associated with early onset, late onset, and severe PE. A personalized medicine approach applied to dissecting the underlying mechanisms of PE may be beneficial to reduce clinical incidence of this devastating syndrome. The placenta plays a key role in the development of PE, leading to widespread maternal endothelial dysfunction, hypertension, and systemic multi-organ failure in PE. Extracellular vesicles (EVs) containing protein, RNA, and lipid cargo are continuously extruded from the placenta, and are capable of interacting with maternal organs including the kidney. PE is primarily associated with placental and renal dysfunction, and PE is the most common cause of acute kidney injury during pregnancy. However, no studies have investigated the potential of placenta-derived RNA cargo as a link between placental and renal dysfunction in PE. Urinary EVs are derived from multiple tissue types and represent a trove of biomarkers that are increasingly being utilized to diagnose renal disorders. Further, urine samples can be obtained throughout pregnancy non-invasively and could potentially be utilized to identify biomarkers related to placental dysfunction in PE. In the proposed research, during the mentored phase, cutting-edge RNA-Seq technology coupled with computational biological and machine learning approaches will be applied to profile the transcriptome of urinary EVs in women with PE compared to normal pregnancy. Preliminary data indicates that it is possible to isolate and profile the transcriptome of urinary EVs from maternal urine throughout normal gestation, and that placenta-derived and placenta-specific mRNA and miRNA can be detected within the urinary EV population. This presents a novel technique that has potential to identify biomarkers as well as provide information on placental dysfunction in PE in a non-invasive manner. During the independent phase, the candidate will utilize an in vitro approach to investigate the effect of uptake of placenta-derived EVs with miRNA cargo associated with PE on the function of proximal tubule epithelial cells and cortical collecting duct cells. These two renal-specific cell types are involved in tubular reabsorption in the nephron, a process that is compromised leading to increased excretion of protein in the urine in some preeclamptic pregnancies. This proposal is multidisciplinary, utilizing basic biology, clinical research, and high-performance computing applied to investigating placental dysfunction in PE. These experiments are significant because they will generate novel information on the role of placenta- derived EVs in renal dysfunction in PE, as well as point the way towards preventative and therapeutic targets that may be transformative and clinically relevant. PROJECT NARRATIVE The proposed research will examine the transcriptome of urinary extracellular vesicles obtained non-invasively as a source of placenta-derived nucleic acids reflective of the development of preeclampsia (PE), and will con- duct in vitro experiments to determine the role of placental extracellular vesicles in renal dysfunction in PE.",Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia,9974957,K99HD096125,"['Acute Renal Failure with Renal Papillary Necrosis', 'Affect', 'Apoptosis', 'Biological', 'Biological Markers', 'Biology', 'Blood Circulation', 'Brain', 'Cells', 'Characteristics', 'Childbirth', 'Chromosomes', 'Clinical', 'Clinical Research', 'Coupled', 'Data', 'Developed Countries', 'Developing Countries', 'Development', 'Diagnosis', 'Disease', 'Disease Pathway', 'Ductal Epithelial Cell', 'Epithelial Cells', 'Etiology', 'Excretory function', 'Failure', 'Fetal Mortality Statistics', 'Functional disorder', 'Genetic Transcription', 'Gestational Age', 'Goals', 'Health Care Costs', 'High Performance Computing', 'Human', 'Hypertension', 'Hypoxia', 'Immune response', 'Immunohistochemistry', 'In Vitro', 'Incidence', 'Injury', 'Kidney', 'Kidney Diseases', 'Libraries', 'Link', 'Lipids', 'Liver', 'Lung', 'Machine Learning', 'Maternal Mortality', 'Mentors', 'Messenger RNA', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Disease', 'Morbidity - disease rate', 'Mus', 'Nephrons', 'Nucleic Acids', 'Organ', 'Pathogenesis', 'Pathologic', 'Pathology', 'Phase', 'Placenta', 'Play', 'Population', 'Population Heterogeneity', 'Pre-Eclampsia', 'Pregnancy', 'Process', 'Proteins', 'Proteinuria', 'RNA', 'Research', 'Role', 'Sampling', 'Small RNA', 'Source', 'Stress', 'Syndrome', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Transcript', 'Tubular formation', 'Urine', 'Woman', 'associated symptom', 'biobank', 'cell type', 'clinically relevant', 'early onset', 'endothelial dysfunction', 'experimental study', 'extracellular vesicles', 'improved', 'innovation', 'kidney dysfunction', 'maternal serum', 'mortality', 'multidisciplinary', 'neonatal morbidity', 'novel', 'personalized medicine', 'pregnancy disorder', 'premature', 'prevent', 'response', 'stillbirth', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'uptake', 'urinary']",NICHD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2019,104534,-0.026268381858144466
"2/2 Leveraging electronic health records for pharmacogenomics of psychiatric diorders Abstract  Schizophrenia (SCZ) and major depressive disorder (MDD) are highly heritable, debilitating diseases with lifetime prevalences of ~1% and 15%, respectively. Both disorders carry substantial morbidity and mortality and are associated with severe societal and personal costs. Despite the availability of efficacious treatments for both disorders, ~1/3 of individuals will not achieve symptomatic improvement even after multiple rounds of medication. Identifying individuals at greater risk for such treatment nonresponse, or treatment resistance, could facilitate more targeted interventions for these individuals.  A burgeoning literature has identified genomic variation associated with treatment response. In particular, antidepressant response has been suggested to be highly heritable; convergent data from rodent studies likewise suggest that antipsychotic and antidepressant response phenotypes are influenced by genetic variation. However, treatment studies to date have had minimal success in identifying variants associated with psychotropic response, likely as a result of limited sample sizes: prior efforts required sequential treatment trials and prospective assessment to characterize outcomes. Longitudinal electronic health records (EHR) data provide an opportunity to efficiently characterize treatment response in many individuals in real-world settings. Coupled with large and expanding biobanks, these cohorts allow for low- cost, large-scale genomic studies that finally achieve sufficient power to detect realistic effect sizes.  The investigators now propose to apply these approaches to the EHRs of two large regional health systems, each linked to a large biobank, to investigate treatment resistance in SCZ and MDD. They will apply canonical indicators of treatment resistance - clozapine treatment for SCZ, and electroconvulsive therapy (ECT) for MDD - to identify coded and uncoded clinical features associated with high probability of treatment resistance in EHR data. These predictors will themselves provide a useful baseline for identifying high risk individuals. Then, they will apply these to study the entire affected population of each biobank, extending existing genomic data with additional genome-wide association, yielding more than 25,000 antidepressant-treated individuals and 2,200 antipsychotic-treated individuals. Rather than simply conducting a case-control study, they will examine treatment resistance as a quantitative trait, applying a method developed by the investigators and shown to substantially increase power for such traits.  The project combines expertise in clinical informatics, machine learning, and analysis of large scale genomics, as well as domain-specific expertise in psychiatric treatment resistance. Spanning two distinct health systems, the algorithms and methods developed have maximal portability, facilitating next- step investigations. Successful identification of risk variants will facilitate efforts at clinical risk stratification as well as investigation of the biology underlying treatment resistance. Public Health Statement Despite the availability of effective treatments for schizophrenia and major depressive disorder, around 1/3 of individuals will not have symptomatic improvement after multiple treatment trials. If individuals at high risk for treatment resistance could be identified early, it might be possible to find more effective treatments for them. In particular, this study seeks to use electronic health records and biobanks to identify genomic variation associated with treatment resistance.",2/2 Leveraging electronic health records for pharmacogenomics of psychiatric diorders,9661173,R01MH116269,"['Academic Medical Centers', 'Address', 'Affect', 'Antidepressive Agents', 'Antipsychotic Agents', 'Biology', 'Case-Control Studies', 'Clinical', 'Clinical Informatics', 'Clozapine', 'Code', 'Computer Simulation', 'Consumption', 'Coupled', 'DNA', 'Data', 'Development', 'Discrimination', 'Disease', 'Effectiveness', 'Electroconvulsive Therapy', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'General Hospitals', 'Generations', 'Genetic', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Heritability', 'Hospitals', 'Individual', 'Intervention', 'Investigation', 'Label', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Major Depressive Disorder', 'Massachusetts', 'Measures', 'Medical Genetics', 'Mental disorders', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Outcome', 'Patient Triage', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacology', 'Pharmacotherapy', 'Phenotype', 'Population', 'Prevalence', 'Probability', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Public Health', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Risk', 'Risk stratification', 'Rodent', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Sequential Treatment', 'Site', 'Structure', 'Suicide attempt', 'Supervision', 'System', 'Therapeutic', 'Time', 'Treatment Failure', 'Treatment Step', 'Treatment outcome', 'Variant', 'Work', 'adverse outcome', 'algorithmic methodologies', 'base', 'biobank', 'biomedical resource', 'clinical predictors', 'clinical risk', 'cohort', 'cost', 'design', 'effective therapy', 'genetic association', 'genome wide association study', 'genomic data', 'genomic predictors', 'genomic variation', 'high risk', 'improved', 'mortality', 'neuropsychiatric disorder', 'personalized intervention', 'portability', 'predictive modeling', 'prevent', 'prospective', 'recruit', 'response', 'risk variant', 'success', 'symptomatic improvement', 'therapy resistant', 'trait', 'treatment response', 'treatment risk', 'treatment strategy', 'treatment trial']",NIMH,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2019,425000,-0.023242221090145804
"1/2 Leveraging electronic health records for pharmacogenomics of psychiatric disorders Schizophrenia (SCZ) and major depressive disorder (MDD) are highly heritable, debilitating diseases with lifetime prevalences of ~1% and 15%, respectively. Both disorders carry substantial morbidity and mortality and are associated with severe societal and personal costs. Despite the availability of efficacious treatments for both disorders, ~1/3 of individuals will not achieve symptomatic improvement even after multiple rounds of medication. Identifying individuals at greater risk for such treatment nonresponse, or treatment resistance, could facilitate more targeted interventions for these individuals.  A burgeoning literature has identified genomic variation associated with treatment response. IN particular, antidepressant response has been suggested to be highly heritable; convergent data from rodent studies likewise suggest that antipsychotic and antidepressant response phenotypes are influenced by genetic variation. However, treatment studies to date have had minimal success in identifying variants associated with psychotropic response, likely as a result of limited sample sizes: prior efforts required sequential treatment trials and prospective assessment to characterize outcomes. Longitudinal electronic health records (EHR) data provide an opportunity to efficiently characterize treatment response in many individuals in real-world settings. Coupled with large and expanding biobanks, these cohorts allow for low- cost, large-scale genomic studies that finally achieve sufficient power to detect realistic effect sizes.  The investigators now propose to apply these approaches to the EHRs of two large regional health systems, each linked to a large biobank, to investigate treatment resistance in SCZ and MDD. They will apply canonical indicators of treatment resistance - clozapine treatment for SCZ, and electroconvulsive therapy (ECT) for MDD - to identify coded and uncoded clinical features associated with high probability of treatment resistance in EHR data. These predictors will themselves provide a useful baseline for identifying high risk individuals. Then, they will apply these to study the entire affected population of each biobank, extending existing genomic data with additional genome-wide association, yielding more than 26,000 antidepressant-treated individuals and 2,500 antipsychotic-treated individuals. Rather than simply conducting a case-control study, they will examine treatment resistance as a quantitative trait, applying a method developed by the investigators and shown to substantially increase power for such traits.  The project combines expertise in clinical informatics, machine learning, and analysis of large scale genomics, as well as domain-specific expertise in psychiatric treatment resistance. Spanning two distinct health systems, the algorithms and methods developed have maximal portability, facilitating next- step investigations. Successful identification of risk variants will facilitate efforts at clinical risk stratification as well as investigation of the biology underlying treatment resistance. Public Health Statement Despite the availability of effective treatments for schizophrenia and major depressive disorder, around 1/3 of individuals will not have symptomatic improvement after multiple treatment trials. If individuals at high risk for treatment resistance could be identified early, it might be possible to find more effective treatments for them. In particular, this study seeks to use electronic health records and biobanks to identify genomic variation associated with treatment resistance.",1/2 Leveraging electronic health records for pharmacogenomics of psychiatric disorders,9660127,R01MH116270,"['Academic Medical Centers', 'Address', 'Affect', 'Antidepressive Agents', 'Antipsychotic Agents', 'Biology', 'Case-Control Studies', 'Clinical', 'Clinical Informatics', 'Clozapine', 'Code', 'Computer Simulation', 'Consumption', 'Coupled', 'DNA', 'Data', 'Development', 'Discrimination', 'Disease', 'Effectiveness', 'Electroconvulsive Therapy', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'General Hospitals', 'Generations', 'Genetic', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Heritability', 'Hospitals', 'Individual', 'Intervention', 'Investigation', 'Label', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Major Depressive Disorder', 'Massachusetts', 'Measures', 'Medical Genetics', 'Mental disorders', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Outcome', 'Patient Triage', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacology', 'Pharmacotherapy', 'Phenotype', 'Population', 'Prevalence', 'Probability', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Public Health', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Risk', 'Risk stratification', 'Rodent', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Sequential Treatment', 'Site', 'Structure', 'Suicide attempt', 'Supervision', 'System', 'Therapeutic', 'Time', 'Treatment Failure', 'Treatment Step', 'Treatment outcome', 'Variant', 'Work', 'adverse outcome', 'algorithmic methodologies', 'base', 'biobank', 'biomedical resource', 'clinical predictors', 'clinical risk', 'cohort', 'cost', 'design', 'effective therapy', 'genetic association', 'genome wide association study', 'genomic data', 'genomic predictors', 'genomic variation', 'high risk', 'improved', 'mortality', 'neuropsychiatric disorder', 'personalized intervention', 'portability', 'predictive modeling', 'prevent', 'prospective', 'recruit', 'response', 'risk variant', 'success', 'symptomatic improvement', 'therapy resistant', 'trait', 'treatment response', 'treatment risk', 'treatment strategy', 'treatment trial']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R01,2019,421250,-0.023527320921341053
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,9864664,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2019,269849,-0.022648281012801964
"High content in vivo screening for acute kidney injury ameliorating drugs ABSTRACT In this application we will perform high throughput, high content, screening for small molecules that improve kidney regeneration after acute kidney injury (AKI). AKI presents a dire unmet medical need because of its high prevalence with long-term adverse health effects and life-threatening sequelae. Mortality is high and the only effective treatments are renal replacement therapies. The onset of the precipitating event is unpredictable, and in many instances once patients are admitted to the hospital, injury has already occurred. Improving recovery from injury therefore presents an attractive opportunity for intervention, but to date, no therapies are available that are effective if administered post injury. The vertebrate kidney has an innate ability to regenerate and follows a well-defined cellular mechanism that encompasses dedifferentiation of surviving renal tubule cells, proliferation of resulting progenitors, and repopulation of the denuded tubule. This sequence of events, together with their respective molecular markers, is conserved between humans, mouse, and zebrafish. During regeneration, transcription factors normally expressed during organogenesis (e.g.,lhx1a, pax2, and pax8) are reactivated. We previously demonstrated that small molecule-mediated augmentation of endogenous Lhx1a expression can ameliorate recovery in zebrafish and mouse models of AKI. Together these data support the overall hypothesis that augmentation by small molecules of cellular programs that drive kidney repair after injury represents a novel pharmacologic approach for the treatment of AKI and associated sequelae. Using a transgenic zebrafish line that expresses Lhx1a-EGFP we have developed an artificial intelligence-based, high-content assay to quantify lhx1a expression in the living embryo. Using multivariate analysis, the assay met accepted HTS assay performance standards and was validated in three-day variability studies and a small pilot library screen. We will perform a primary HTS of 50,000 compounds from the MLPCN collection. Prioritized hits will be subjected to a fully implemented, rigorous secondary assay paradigm encompassing kidney organ development, metabolic stability, in vivo efficacy, and activity profiling in a pathophysiological relevant AKI model. At the end of these studies we will have identified functionally and mechanistically characterized in vivo chemical probes to investigate the biology of kidney injury and regeneration, some of which are expected to have features that make them suitable for development into preclinical leads. NARRATIVE Acute kidney injury (AKI) presents a dire unmet medical need with unacceptably high mortality rates and a lack of therapeutic modalities. The vertebrate kidney has an innate ability to regenerate that can be enhanced by small molecules. In this proposal we will perform high-content, high throughput screening for kidney regeneration in zebrafish to discover novel chemical probes to investigate mechanisms of augmented kidney regeneration after injury.",High content in vivo screening for acute kidney injury ameliorating drugs,9613245,R01DK112652,"['Acute Renal Failure with Renal Papillary Necrosis', 'Appearance', 'Artificial Intelligence', 'Biological Assay', 'Biological Markers', 'Biology', 'Breeding', 'Carboxylic Acids', 'Cell Proliferation', 'Chemicals', 'Collection', 'Complex', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Dose', 'Embryo', 'Embryonic Development', 'Ensure', 'Epithelial Cells', 'Etiology', 'Evaluation', 'Event', 'FDA approved', 'Fibrosis', 'Generations', 'Genetic', 'Health', 'High Prevalence', 'Hospitals', 'Human', 'Injury', 'Intervention', 'Kidney', 'Libraries', 'Life', 'Liver', 'Mediating', 'Medical', 'Metabolic', 'Microsomes', 'Modality', 'Modeling', 'Molecular Bank', 'Morphologic artifacts', 'Multivariate Analysis', 'Mus', 'Natural regeneration', 'Nature', 'Organogenesis', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacology', 'Prodrugs', 'Protocols documentation', 'Recovery', 'Renal Replacement Therapy', 'Renal function', 'Renal tubule structure', 'Reproducibility', 'Specificity', 'Stem cells', 'System', 'Testing', 'Therapeutic', 'Toxic effect', 'Transgenes', 'Transgenic Organisms', 'Tubular formation', 'United States National Institutes of Health', 'Zebrafish', 'analog', 'base', 'effective therapy', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'kidney repair', 'molecular marker', 'mortality', 'mouse model', 'novel', 'organ growth', 'organ regeneration', 'pre-clinical', 'progenitor', 'programs', 'repository', 'response', 'screening', 'small molecule', 'small molecule libraries', 'transcription factor']",NIDDK,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2019,352125,-0.010305179626826607
"Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers PROJECT SUMMARY/ABSTRACT Candidate: Dr. Adel El Boueiz is a pulmonary and critical care physician-scientist completing a period of T32- funded support at the Channing Division of Network Medicine (CDNM) and Harvard Medical School (HMS). He received a Master's of Medical Science in Biomedical Informatics from HMS in May 2016. He will be promoted to Instructor of Medicine at the CDNM and HMS on July 1, 2017. His principal research interests are the genetic epidemiology of chronic obstructive pulmonary disease (COPD) and the translation of genomic discoveries into clinical practice and public health. His long-term goal is to be an independent investigator with expertise in imaging phenotyping, genomics, and predictive analytics of the regional heterogeneity of the various aspects of COPD (emphysema, airway disease, and pulmonary vascular remodeling). Environment: Dr. El Boueiz will continue to pursue his research and career development in the rich and multidisciplinary environment of the CDNM and the Brigham and Women's Hospital Applied Chest Imaging Lab (ACIL). He will be mentored by Drs. Edwin K. Silverman, Peter J. Castaldi, and Raúl San José Estépar, leaders in the field of COPD quantitative imaging, genetic epidemiology, and predictive analytics with excellent track records of mentoring young investigators towards independent research careers. His career development will also be overseen by an advisory committee with expertise related to key areas of his proposal. Research: COPD is a major cause of morbidity and mortality that is of increasing public health importance. COPD is a heterogeneous disease and this heterogeneity complicates the identification of the predictors of disease progression and consequently, the development of effective therapies. Emphysema distribution is an important COPD-related phenotype that emerged as a strong predictor of the response to lung volume reduction procedures. Despite the availability of advanced texture-based CT quantification methods, global threshold-based quantitative metrics have to date been the cornerstone for the radiological characterization of emphysema distribution with inability to differentiate centrilobular, panlobular, and paraseptal emphysema patterns. In this project, we will apply a texture-based CT quantification method to discover novel imaging biomarkers of the regional heterogeneity of centrilobular, panlobular, and paraseptal emphysema in a large cohort of well-characterized smokers and identify their genetic determinants using whole genome sequencing and integrative genomics analyses. The results will be considered for inclusion along with other rich phenotypic and imaging data in COPD disease progression machine learning predictive models. Relevance: Through improved radiographic phenotyping of emphysema distribution, better understanding of disease pathobiology, and more accurate prediction of disease progression, the proposed work will open new avenues of investigation for the development of personalized and improved COPD therapeutic strategies. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a common disease that affects up to 24 million people in the United States, is associated with considerable and increasing morbidity and mortality, and for which there is no available disease-modifying therapy. COPD is associated with significant variation in radiographic, symptomatic and physiologic presentation and exhibits variability in progression. Currently, there is no satisfactory method for progression prediction. This project will identify novel imaging biomarkers of the regional distribution of centrilobular, panlobular, and paraseptal emphysema with particular emphasis on their associations with clinical relevant COPD-related outcomes, their genetic determinants, and their ability to improve prediction of COPD disease progression, above and beyond that provided by the traditional clinical, radiographic, and genetic features. This is an important area of research as predicting those patients who will remain stable from those who will have rapid disease progression is critical in defining prognosis and selecting patients for specific therapeutic interventions.","Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers",9672582,K08HL141601,"['ACVR1B gene', 'Accounting', 'Advisory Committees', 'Affect', 'Airway Disease', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological Process', 'Chest', 'Chronic Obstructive Airway Disease', 'Clinical', 'Clinical Trials', 'Clinical/Radiologic', 'Cohort Studies', 'Collection', 'Comorbidity', 'Complex', 'Computers', 'Computing Methodologies', 'Critical Care', 'Data', 'Detection', 'Development', 'Diagnostic radiologic examination', 'Disease', 'Disease Progression', 'Dyspnea', 'Environment', 'Evaluation', 'Exhibits', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Polymorphism', 'Genomic approach', 'Genomics', 'Goals', 'Heterogeneity', 'Hospitals', 'Image', 'Investigation', 'Lobar', 'Lobe', 'Lung', 'Lung Volume Reductions', 'Lung diseases', 'Machine Learning', 'Measures', 'Medical', 'Medical Genetics', 'Medical Research', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Outcome', 'Pathologic', 'Patients', 'Pattern', 'Phenotype', 'Physicians', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Procedures', 'Public Health', 'Pulmonary Emphysema', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Quantitative Trait Loci', 'Radiology Specialty', 'Records', 'Research', 'Research Personnel', 'Resources', 'Respiratory physiology', 'Science', 'Scientist', 'Smoker', 'Structure of parenchyma of lung', 'Subgroup', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Therapeutic Intervention', 'Tissues', 'Translations', 'United States', 'Variant', 'Vascular remodeling', 'Visual', 'Walking', 'Woman', 'Work', 'attenuation', 'base', 'biomedical informatics', 'career', 'career development', 'clinical practice', 'clinically relevant', 'clinically significant', 'cohort', 'data mining', 'disease heterogeneity', 'disease phenotype', 'disorder risk', 'disorder subtype', 'effective therapy', 'genetic architecture', 'genetic association', 'genetic epidemiology', 'genetic predictors', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic predictors', 'imaging biomarker', 'imaging genetics', 'improved', 'instructor', 'interest', 'learning strategy', 'medical schools', 'mortality', 'multidisciplinary', 'novel', 'outcome forecast', 'personalized care', 'predicting response', 'predictive modeling', 'prognostic', 'quantitative imaging', 'rare variant', 'research and development', 'respiratory', 'tomography', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2019,170639,-0.05135543274143036
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer’s disease, Parkinson’s disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these challenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymorphisms, whole brain neuroimaging, biofluids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mechanisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Specifically, in Aim 1, we develop efficient methods for multi-level semiparametric transformation models to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efficiency. Our methods do not rely on full pedigree genotyping and provide family-specific substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic systems through ordinary differential equations with random inflections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker profiles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efficient machine learning methods to identify predictive markers, estimate optimal individualized therapies, and identify subgroups who may receive the greatest benefit from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art analytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research community. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9694287,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Non-linear Models', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,340259,-0.02120611141109557
"Automated Object Contouring Methods & Software for Radiotherapy Planning Abstract In 2015, 1,658,370 new cancer cases are estimated to occur in the US, where nearly two-thirds will have radiation therapy (RT). Given that there are over 2,300 RT centers in the US, and current systems for contouring organs at risk (OARs) rely mostly on manual methods, there is a strong commercial opportunity for producing a software system that can contour OARs in medical images at a high degree of automation and for impacting current practice of RT planning. Encouraged by our strong Phase I results in thoracic and head and neck (H&N) body regions compared to current industry systems, we seek the accuracy, efficiency, and clinical acceptance of the contours output by our software product to significantly exceed those of existing systems. Our overall aim for Phase II is to advance the algorithms and prototype software developed in Phase I into a leading commercial software product, and demonstrate its efficacy in multiple medical centers across the country with diverse populations. Phase II specific aims are three-fold: (1) Further advance the automatic anatomy recognition algorithms from Phase I using advanced deep learning techniques. (2) Develop a cloud-based software auto contouring service. (3) Perform clinical evaluation of the new software on H&N and thoracic cases. Aim 1 will be accomplished in three stages: (a) Automating the process of defining the body region on given patient CT studies, which is currently done manually in our system, via a new concept of virtual landmarks using deep learning techniques. (b) Improving object recognition/ localization accuracy from the current 2 voxels for “good” quality data sets to 1 voxel and from 4-5 voxels for “poor” quality data sets to 2-3 voxels by using virtual landmarks to learn object relationships. (c) Improving object delineation by combining object localization methods with deep learning techniques applied to the vicinity of the localized objects to bring boundary distance accuracy within 1 voxel. Aim 2 will be achieved by developing a cloud-based Software-as-a-Service model to implement the software that incorporates the algorithms. To accomplish Aim 3, an evaluation study involving four academic RT centers will be undertaken to assess the efficiency, accuracy, and acceptability of the contours output by the new software. To assess efficiency, contouring time taken by the current clinical process will be compared to the time taken by the new software method plus any manual adjustment needed. Accuracy will be assessed by comparing software output to carefully prepared ground truth contours. Acceptability will be determined by conducting a blinded reader study, where an acceptability score (1-5) is given by radiation oncologists to software produced contours, ground truth contours, and contours produced by the normal clinical process, and comparing these scores. Expected clinical outcomes are significantly improved clinical efficiency/ acceptability of contouring compared to current practice. There is a strong commercial opportunity for producing a software system that can contour organs at risk in medical images at a high degree of automation for impacting current practice of radiation therapy planning. Encouraged by strong competitive results from the Phase I part of this project, in this grant, further technical advances and a cloud-based software service are proposed. A multicenter clinical evaluation of the new product is also planned to assess the clinical efficacy of the system.",Automated Object Contouring Methods & Software for Radiotherapy Planning,9761481,R42CA199735,"['Adoption', 'Algorithms', 'Anatomy', 'Automation', 'Back', 'Blinded', 'Body Regions', 'Chest', 'Clinical', 'Collaborations', 'Computer software', 'Country', 'Data Quality', 'Data Set', 'Development', 'Digital Imaging and Communications in Medicine', 'Evaluation', 'Evaluation Studies', 'Grant', 'Head and neck structure', 'Health Personnel', 'Image', 'Imagery', 'Industry', 'Inferior', 'Investigation', 'Investments', 'Learning', 'Location', 'Malignant Neoplasms', 'Malignant neoplasm of thorax', 'Manuals', 'Medical Imaging', 'Medical center', 'Methods', 'Modeling', 'Morphologic artifacts', 'Names', 'Object Attachment', 'Organ', 'Outcome', 'Output', 'Pathology', 'Patients', 'Phase', 'Population Heterogeneity', 'Process', 'Protocols documentation', 'Radiation Oncologist', 'Radiation therapy', 'Reader', 'Research', 'Risk', 'Scanning', 'Service delivery model', 'Services', 'Slice', 'Specific qualifier value', 'System', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Time', 'Work', 'base', 'clinical efficacy', 'cloud based', 'deep learning', 'image processing', 'improved', 'interest', 'learning network', 'learning strategy', 'neural network', 'novel strategies', 'object recognition', 'prototype', 'research clinical testing', 'software as a service', 'software development', 'software systems', 'treatment center', 'treatment planning', 'virtual']",NCI,"QUANTITATIVE RADIOLOGY SOLUTIONS, LLC",R42,2019,873369,-0.04573214646133128
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,-0.00963881401568887
"Evaluation of molecular mechanisms of treatment response in late-life depression DESCRIPTION: Over the past decades, antidepressants and psychotherapy have been the first-line treatments for LLD. Despite being safe and well-tolerated, a large number of patients do not achieve full and persistent remission after initial treatment. About 50% of patients with LLD do not respond after two antidepressant trials, meeting the consensus definition of treatment resistance (TR-LLD). The persistence of chronic and elevated depressive symptoms in older adults has significant clinical and public health implications. This has been correlated to poor general health, reduced quality of life, and a higher risk of mortality when compared to those with sustained remission after treatment. Despite the relevance to public health of TR-LLD, there is little information about the biological mechanisms and no robust clinical prediction model to evaluate at the outset of antidepressant therapy who will or will not respond to treatment. Leveraging an NIMH funded clinical trial, the Incomplete Response in Late-Life Depression: Getting to Remission” (IRL-GREY), across 3 sites, in this study, we propose to evaluate the biological mechanisms related to treatment response in late-life depression and to develop a machine learning based algorithm for prediction of treatment response in these subjects. We will carry out a comprehensive, multiplexed proteomic analysis on 542 samples from patients who completed phase 1 and phase 2 of the clinical trial. We hypothesise that ageing-related biological pathways (i.e. inflammatory response control, proteostasis control, cell damage response, endothelial function) will be associated with poorer treatment response in LLD. Moreover, we hypothesize that a machine learning derived biomarker panel will have sensitivity and specificity greater than 80% to predict treatment response in LLD. Finally, we will evaluate the biological mechanisms related to different depressive symptoms trajectories after treatment. This work will set the stage for a biologically-driven model of treatment response that will be useful to guide, at the outset of antidepressant treatment, those who will benefit more from a specific treatment. If successful, our work can accelerate therapeutic efforts and innovation targeting depression and reduce suffering for large numbers of elderly and their families. Using advanced molecular approaches, we will determine the biological mechanisms related to treatment response in late-life depression. Our proposed study will also develop a predictive tool combining neurocognitive, neuroimaging, and protein data to identify who with late-life depression is more likely to have no benefit from antidepressant treatment at the outset of therapy. The goal of the study is to clarify the mechanisms of treatment response in late-life depression, and whether we can effectively identify those who will benefit from antidepressant therapy at the outset of treatment.",Evaluation of molecular mechanisms of treatment response in late-life depression,9816774,R01MH118311,"['Address', 'Aftercare', 'Aging', 'Antidepressive Agents', 'Bioinformatics', 'Biological', 'Biological Aging', 'Biological Markers', 'Biological Process', 'Blood specimen', 'Brain', 'Cell Cycle', 'Chronic', 'Clinical', 'Clinical Trials', 'Consensus', 'Data', 'Disease remission', 'Double-Blind Method', 'Elderly', 'Endothelium', 'Evaluation', 'Family', 'Funding', 'Geroscience', 'Growth Factor', 'Health', 'Immune', 'Impaired cognition', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Machine Learning', 'Mental Depression', 'Modeling', 'Molecular', 'National Institute of Mental Health', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Phase', 'Phase II Clinical Trials', 'Placebos', 'Population', 'Prediction of Response to Therapy', 'Prevalence', 'Proteomics', 'Psychotherapy', 'Public Health', 'Quality of life', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Structure', 'Testing', 'Therapeutic', 'Tissues', 'Work', 'angiogenesis', 'aripiprazole', 'base', 'biomarker panel', 'cell injury', 'cohort', 'depressive symptoms', 'design', 'endothelial dysfunction', 'geriatric depression', 'high risk', 'meetings', 'mortality risk', 'new therapeutic target', 'novel', 'novel marker', 'novel strategies', 'open label', 'prediction algorithm', 'predictive modeling', 'proteostasis', 'public health relevance', 'recruit', 'response', 'therapy resistant', 'treatment response', 'treatment-resistant depression', 'venlafaxine']",NIMH,CENTRE FOR ADDICTION AND MENTAL HEALTH,R01,2019,518325,-0.024394922418315895
"APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY Abstract  Acute myeloid leukemia (AML) accounts for half of all pediatric leukemia deaths and is the leading cause of leukemia-related death in adulthood. One reason for worse outcomes is the inability to properly assess for minimal residual disease (MRD) following therapy. Unlike ALL, AML presents with multiple subclonal populations without a singular clonal surface marker, and surface markers can change during therapy. The current gold standard for AML MRD is multi-parameter flow cytometry (MPFC), which is predictive of outcomes to frequencies of 0.001, yet 30% of MPFC-MRD-negative patients still relapse. Alternatively, every AML case harbors leukemia-specific mutations that could be markers of disease, except that next-generation sequencing has high error rate of ~1%. In this proposal, we will implement a novel, validated error-corrected sequencing (ECS) strategy, developed by the Druley lab in collaboration with Illumina, to improve MRD assessment of AML subclonal heterogeneity in 990 pediatric de novo AML cases from the Children's Oncology Group AAML1031 study. We hypothesize that using a highly sensitive sequencing method will improve identification of residual AML, provide important insights on subclonal heterogeneity in pediatric AML, improve understanding of the role of germline variability and gene function on relapses or refractory disease and facilitate personalized medicine. To interrogate this hypothesis, we propose the following aims: 1. Define subclonal heterogeneity at diagnosis and end of Induction 1 (EOI1) in 990 pediatric de novo  AML patients (n=1890). By using the largest prospective study of pediatric AML that has ever been  performed, we will perform ECS on 94 genes that are the most frequently mutated genes in pediatric and  adult AML at diagnosis and EOI1 to identify patterns of mutation associated with relapsed disease, FAB  subtypes or other cytogenetic features. 2. Correlate ECS-MRD with existing EOI1 MPFC-MRD for all participants in the COG AAML1031 study.  A major question is whether the “different from normal” cell population identified as residual disease by  MPFC is actually the same population(s) identified by ECS. We will define residual disease by ECS and  compare results to MPFC status (positive/negative), actual MPFC percentages (<0.001) and the clinical  outcomes (relapse risk, disease-free survival and overall survival) of study participants. 3. Integrate germline variation and all subclonal mutations into mechanistic groups that are frequently  mutated in pediatric AML and correlate with outcomes using unbiased machine learning  algorithms. Preliminary data tells us that every patient will have multiple subclones at diagnosis and EOI1  as well as germline variants in AML-associated genes, which may be important for outcome. In this aim, we  will take these mutations into account as well as MPFC, clinical features and cytogenetics for probabilistic  risk assessment using unsupervised machine learning algorithms for improved outcome prognostication. Narrative We have developed Error-Corrected Sequencing (ECS) that enables the highly accurate detection of leukemia- specific mutations in heterogeneous DNA samples to a limit of 0.0001. We will perform ECS with a panel of 94 frequently mutated genes in adult and pediatric AML in matched diagnostic and end of Induction 1 bone marrow samples from 990 pediatric de novo AML patients enrolled on the Children's Oncology Group AAML1031 protocol, which is the largest pediatric AML trial ever performed in North America. With these data, we can truly understand subclonal heterogeneity in pediatric AML, significantly improve minimal residual disease testing in pediatric AML, and integrate these data into an unbiased multivariate probability platform taking into account individual sequencing, flow cytometry, cytogenetic and clinical features to provide truly personalized cancer care.",APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY,9606448,R01CA211711,"['Acute Myelocytic Leukemia', 'Adult', 'Adult Acute Myeloblastic Leukemia', 'Alleles', 'BAY 54-9085', 'Biological Markers', 'Bone Marrow', 'Bortezomib', 'Categories', 'Cessation of life', 'Childhood', 'Childhood Acute Myeloid Leukemia', 'Childhood Leukemia', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Cytogenetics', 'DNA', 'DNA Sequence Alteration', 'DNA sequencing', 'Data', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Marker', 'Disease-Free Survival', 'Enrollment', 'Epigenetic Process', 'Exons', 'FLT3 gene', 'Family', 'Flow Cytometry', 'Frequencies', 'Future', 'Genes', 'Genetic', 'Goals', 'Gold', 'Hematology', 'Immunophenotyping', 'Individual', 'Investigation', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Mutate', 'Mutation', 'Normal Cell', 'North America', 'Oncogenes', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pediatric Oncology Group', 'Phase', 'Point Mutation', 'Population', 'Probability', 'Prospective Studies', 'Protein Tyrosine Kinase', 'Protein phosphatase', 'Protocols documentation', 'Reagent', 'Recurrent disease', 'Refractory Disease', 'Relapse', 'Residual Neoplasm', 'Residual Tumors', 'Residual state', 'Resolution', 'Risk', 'Risk Assessment', 'Risk stratification', 'Role', 'Sampling', 'Site', 'Spliceosomes', 'Surface', 'Testing', 'Time', 'Treatment Efficacy', 'Tumor Suppressor Proteins', 'United States National Institutes of Health', 'Variant', 'analytical method', 'base', 'chemotherapy', 'cohesion', 'design', 'digital', 'falls', 'functional group', 'gene function', 'improved', 'improved outcome', 'insight', 'leukemia', 'leukemia treatment', 'machine learning algorithm', 'next generation', 'next generation sequencing', 'novel', 'nucleocytoplasmic transport', 'outcome prediction', 'pediatric patients', 'personalized cancer care', 'personalized medicine', 'prognostic', 'prospective', 'randomized trial', 'relapse risk', 'response', 'specific biomarkers', 'subclonal heterogeneity', 'transcription factor', 'unsupervised learning']",NCI,WASHINGTON UNIVERSITY,R01,2019,342009,-0.03699468490481828
"Personalizing Glaucoma Diagnosis by Disease Specific Patterns and Individual Eye Anatomy Project Summary/Abstract Glaucoma is a disease of the optic nerve which is accompanied by visual ﬁeld (VF) loss. While accurate VF loss diagnosis and the detection of its progression over time is of high relevance to clinical practitioners as it indicates the initiation of or change in ocular therapy, there is no consensus on objective measures for this purpose, and VF measurements are known to be often unreliable. The main objective of this project is to develop clinically applicable measures to improve the diagnosis of glaucomatous VF loss and of its progression by two approaches: First, the identiﬁcation of representative loss patterns and their progression, achieved by large-scale, customized bioinformatical procedures applied to data from glaucoma patients from nine clinical centers and second, the inclusion of eye and patient speciﬁc personalized parameters. In total, 480,486 VFs, are available for this project. One major aim is to develop novel diagnostic indices based on computationally identiﬁed evolution patterns of VF loss, particularly (1) an index that denotes the probability of glaucomatous vision loss and (2) an index that assigns probabilities to a VF that follow-up measurements will be in a certain defect class. The indices will be statistically evaluated on separate VF samples and compared to existing approaches. Routinely available patient speciﬁc parameters which are candidates to impact glaucomatous vision loss are patient ethnicity, type of glaucoma, spherical equivalent (SE) of refractive error and the location of the blind spot relative to ﬁxation. The effect of these parameters on the vision loss patterns will be systematically studied. The impact of their inclusion in the novel diagnostic indices and their potential improvement on glaucoma diagnosis will be quantiﬁed on a separate data set. A further aim is the calculation of a spatial map speciﬁc to a measured VF that represents the preferred VF locations of future defects as well as their reliability as an aid to event-based progression diagnosis. A second major objective is the investigation of the relationship of VF loss and individual parameters related to retinal structure, based on retinal nerve ﬁber layer thickness (RNFLT) measurements around the optic disc. The inter-relationship of representative patterns of RNFLT and its decrease over time with trajectories of major retinal arteries, SE, and blind spot location is systematically studied, and the impact on patterns of VF loss is quantitatively analyzed with the goal to improve the interpretation of existing VF loss and to predict future glaucomatous vision loss. Main contributions of the project with relevance to clinical practice are publicly available open-source software implementations of new diagnostic indices and maps, enhanced by individual functional and structural parameters, and a detailed and personalized model for the relationship between retinal structure and glaucomatous vision loss. Project Narrative Glaucoma is an ocular disease accompanied by vision loss which may progress over time up to total blindness, but the assessment of glaucomatous vision loss is noisy, and it is often hard for clinical practitioners to decide whether changes over time reﬂect true changes of functional vision or are the result of normal measurement variations or artifacts. This project contributes directly and immediately to public health by exploring the impact of individual anatomical parameters on the spatial patterns of glaucomatous vision loss in order to improve the diagnosis of vision loss and of its progression. Main objective of the project is the development of new quantitative diagnostic indices, implemented as publicly available software.",Personalizing Glaucoma Diagnosis by Disease Specific Patterns and Individual Eye Anatomy,9802123,R01EY030575,"['Anatomy', 'Atrophic', 'Axon', 'Bioinformatics', 'Blindness', 'Clinical', 'Cluster Analysis', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Defect', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Ethnic Origin', 'Event', 'Evolution', 'Eye', 'Future', 'Glaucoma', 'Goals', 'Hemorrhage', 'Impairment', 'Individual', 'Investigation', 'Length', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Modeling', 'Morphologic artifacts', 'Nerve Fibers', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Probability', 'Procedures', 'Public Health', 'Refractive Errors', 'Retinal', 'Retinal Defect', 'Retinal Ganglion Cells', 'Retinal blind spot', 'Sampling', 'Structure', 'Structure-Activity Relationship', 'System', 'Thick', 'Time', 'Variant', 'Vision', 'Visual Fields', 'Work', 'base', 'central retinal artery', 'clinical application', 'clinical practice', 'disease diagnosis', 'follow-up', 'fovea centralis', 'improved', 'indexing', 'multidisciplinary', 'neglect', 'novel diagnostics', 'open source', 'optic nerve disorder', 'outcome forecast', 'retinal nerve fiber layer', 'sample fixation', 'sex']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,R01,2019,534037,-0.0360885893442993
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9817000,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2019,334891,-0.022634024570818742
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,-0.004116428973229227
"Elucidating Sensorial and Functional Characteristics of Topical Formulations Abstract In addition to the defined therapeutic effect caused by an active drug in a product, there is also a placebo and, potentially, a nocebo effect associated with that product. In topical products, these latter effects may account for 30% to 50% of the overall response for some products. They may also explain why some topical products with apparently identical bioavailability are associated with different patient outcomes. This application seeks to address the question of when do subtle excipient and manufacturing changes in a topical product cause a sensorial perception by subjects such that the “feel” of a product has changed either before and/or after it is applied to human skin. A second question is whether the “feel” of a product both before and after application can be quantified by instrumental rheology, tribology and texture analysis methods and whether these, in turn, can be related to the reported sensorial behaviour. We will manufacture topical formulations that systematically vary in Q1, Q2, and/or Q3 attributes and have large and borderline perceptive differences. We will then characterize these products using a range of rheology, tribology and texture analysis methods along with characterization of rate of drying, particle and globule size. In parallel, these products will be evaluated by perceptive testing focus groups, with controls, for their sensory properties or the ‘feel’ of the products. We will then relate these sensorial findings with the variations in formulation nature, composition and manufacture, and their resulting instrumental test results. Our goals are, firstly, to understand the relationships between product nature, instrumental findings and sensorial analyses and, secondly, to derive criteria for instrument tests that indicate what product composition subjects suggest do not differ, uncertain if they differ and do differ in their sensorial behaviour. It is anticipated that we can define the simplest, robust test that accurately and robustly aligns with sensory perceptions. A range of statistical methods, including (potentially) sophisticated, machine learning and deep learning tools will then be used to model the most appropriate instrumental analysis that can, with reasonable confidence predict perceptive attributes. A key outcome is a potential regulatory guideline advocating that generic products should exhibit similar sensorial behaviour as a reference listed drug product, giving boundaries in rheology, tribology and texture analysis as defined by Q1, Q2 and Q3 differences when sensorial behaviour between topical products is likely to be different. Narrative Generic and reference-listed topical products have the potential to have differing placebo and nocebo effects, i.e. effects beyond those of the active drug. This project aims to understand what subtle changes in Q1,Q2 or Q3 between different product formulations lead to a subject reporting sensorial perceptions suggesting that the “feel” of two product is either different or they can no longer perceive a meaningful difference. Compositions (reference and generic) will be manufactured with variations in Q1, Q2 and Q3, characterised by instrumental measures and the results related to sensorial analysis findings.",Elucidating Sensorial and Functional Characteristics of Topical Formulations,9913611,U01FD006700,[' '],FDA,UNIVERSITY OF QUEENSLAND,U01,2019,250000,-0.03494562974371788
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9676860,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2019,7168390,-0.007503923584936272
"Applying statistical learning tools to personalize cardiovascular treatment Abstract Cardiovascular disease (CVD) treatment is often guided by risk stratification tools (to decide who to treat), and randomized controlled trials (to decide which treatments to select). Prior CVD research reveals two major  obstacles to improving our treatment approach: (i) longitudinal cohort data are unavailable for recalibrating risk stratification tools for local-area estimation (by zip code), or for people with major CVD-promoting  comorbidities (e.g., chronic kidney disease); and (ii) the average treatment effect in randomized trials can be highly erroneous when projected onto individuals that vary from the ‘average’ participant in a trial. CVD risk- stratification and treatment effect estimation can be improved and personalized if we overcome a critical barrier to progress: correctly estimating risk and treatment effect from new, large participant data repositories, which have greater population size and include patients with more co-morbid conditions than common cohort studies, and which permit personalized risk/benefit prediction tool development from individual-level data. Our prior studies show that we can critically advance the field by applying novel statistical learning methods to this data, to address: (i) false-positives from multiple testing; (ii) the reliance on standard regressions that cannot account for non-linear, complex interactions between factors; and (iii) identifying the optimal approach among many alternative statistical learning methods. We propose to apply our work in these areas to (Aim 1) Develop CVD risk stratification tools for patients with inadequate sample sizes in common cohort studies. We will enhance CVD risk stratification to include local-area adjustment (by zip code) and major co-morbid conditions affecting CVD risk (e.g., chronic kidney disease). We will additionally (Aim 2) develop personalized treatment effect  prediction tools to guide decisions for CVD therapies with high potential benefit and risk, for therapies where  individual participant data from trials are available. We have obtained the individual participant data from the large randomized trials that reveal wide variations in CVD risk reduction and serious adverse event risk increase from three drug classes: non-vitamin K antagonist oral anticoagulants, intensive blood pressure treatment, and sodium-glucose co-transporter 2 inhibitors for diabetes. Our preliminary research shows that traditional  regression methods cannot distinguish which patients are most likely to benefit or be harmed by such therapies, but our statistical learning methods can. Finally, we will (Aim 3) develop open-source tools to improve the ability of researchers to choose an optimal statistical learning approach for their dataset and problem. While numerous statistical learning methods have been proposed in the literature, a key problem for biomedical scientists  without access to RCT data is: which method should I use to estimate treatment effects from observational data? Building on our innovative approach to identify the optimal inference method for observational data, we will construct an open-source tool to compare methods, identifying which method most often results in optimal treatment decisions that minimize error and maximize performance on standardized metrics. Public health relevance  Using current cardiovascular disease risk stratification tools, about 31.6 million Americans will receive an incorrect cardiovascular disease risk estimate that will lead to wrong treatment, per current guidelines.  Furthermore, extrapolating the average treatment effect for major preventive cardiovascular disease treatments— rather than calculating a personalized risk/benefit score—contributes to 4.9 million serious adverse events from treatment complications each year. In this project, we seek to address both problems: improving  cardiovascular disease risk stratification, and developing personalized calculators for risk and benefit of major  cardiovascular disease treatments.",Applying statistical learning tools to personalize cardiovascular treatment,9637461,R01HL144555,"['Address', 'Affect', 'American', 'Anticoagulants', 'Area', 'Asians', 'Atrial Fibrillation', 'Benefits and Risks', 'Blood Glucose', 'Blood Pressure', 'Cardiovascular Diseases', 'Cardiovascular system', 'Chronic Kidney Failure', 'Code', 'Cohort Studies', 'Comorbidity', 'Complex', 'Data', 'Data Set', 'Diabetes Mellitus', 'Disease Outcome', 'Fasting', 'Glucose', 'Guidelines', 'Individual', 'Literature', 'Longitudinal cohort', 'Machine Learning', 'Metabolism', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Oral', 'Participant', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population Sizes', 'Preventive', 'Randomized Controlled Trials', 'Reading', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Risk', 'Risk Estimate', 'Risk Reduction', 'Risk stratification', 'Sample Size', 'Serious Adverse Event', 'Sodium', 'Standardization', 'Subgroup', 'Testing', 'Thromboplastin', 'Variant', 'Wages', 'Work', 'adverse event risk', 'biomedical scientist', 'cardiovascular disorder risk', 'cardiovascular disorder therapy', 'clinical care', 'data warehouse', 'editorial', 'fasting glucose', 'health data', 'improved', 'individual patient', 'inhibitor/antagonist', 'innovation', 'learning strategy', 'novel', 'open source', 'optimal treatments', 'personalized medicine', 'public health relevance', 'randomized trial', 'stroke risk', 'symporter', 'tool', 'tool development', 'treatment effect']",NHLBI,STANFORD UNIVERSITY,R01,2019,774381,-0.04241925192019111
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9688116,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2019,275934,-0.006452803120542414
"Statistical Methods for Analyzing Complex, Multi-dimensional Data from Cross-sectional and Longitudinal Mental Health Studies Project Summary  To address the burden of mental illness, National institute of Mental Health encourages development of computational approaches that provide novel ways to understand relationships among complex, large datasets to further the understanding of the underlying pathophysiology of mental diseases. These datasets are multi- dimensional, including clinical assessments, behavioral symptoms, biological measurements such as neu- roimaging and psychophysiological data. The overall objective of this grant is to advance methodology for analyzing such data to more effectively extract relevant information that are predictive of disease, to improve the understanding of individual variability in clinical and neurobiological phenotypes, and to provide the capac- ity to handle both cross-sectional and longitudinal data.  Our proposal will leverage two civilian trauma cohorts recruited through the Grady Trauma Project and the Grady Emergency Department Study, and an external validation cohort from the Hill Center study with a similar distribution of trauma exposure. We propose to develop statistically principled, computationally efﬁ- cient statistical learning methods for addressing key challenges in analyzing these large datasets. Challenges include multi-type outcomes, high dimensional data with sparse signals and high noise levels, spatial and tem- poral dependence of neuroimaging data, and heterogeneous effects across patient population. The scientiﬁc premise of this computational psychiatry research is that analytical methods integrating information from brain, behavior, and symptoms will provide much-needed data driven platforms for improving diagnosis and prediction of PTSD and other mental disorders.  In this application, we propose: (1) to develop partial generalized tensor regression methods and partial tensor quantile regression methods that can simultaneously achieve accurate prediction of clinical outcomes and efﬁcient feature extraction from high dimensional neuroimaging biomarkers; (2) to develop tensor response quantile regression methods and global inference that can achieve comprehensive and robust understanding of the heterogeneity in high-dimensional neuroimaging phenotypes in terms of environmental factors such as trauma exposure; and (3) to develop and extend methods in Aims 1 and 2 for longitudinal multi-dimensional data that will enable prediction of future post-trauma symptom severity trajectories in terms of neuroimaging biomarkers and robustify the evaluation of the impact of psychophysiological factors on neuroimaging phe- notypes. The proposed methods will be applied to the two Grady studies to address scientiﬁc hypotheses relevant to PTSD research. We will use the Hill Center study as an independent validation cohort to evaluate the reproducibility and generalizability of the ﬁndings. User-friendly software will be developed. The proposed methodology is generally applicable to many other mental health studies with complex multi-dimensional data. Narrative We propose to develop statistical methods for analyzing large-scale and multi-dimensional data in mental health studies to more effectively extract relevant information that are predictive of disease and to help understand individual variability in clinical and neurobiological phenotypes. The applications of the proposed methods will generate new knowledge to further the understanding of the mechanism and progression of the PTSD that will lead to improved disease management strategies.","Statistical Methods for Analyzing Complex, Multi-dimensional Data from Cross-sectional and Longitudinal Mental Health Studies",9839143,R01MH118771,"['Accident and Emergency department', 'Address', 'Amygdaloid structure', 'Behavioral Symptoms', 'Biological', 'Biological Markers', 'Child Sexual Abuse', 'Clinical', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Demographic Factors', 'Dependence', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Management', 'Environmental Risk Factor', 'Face', 'Fright', 'Functional disorder', 'Future', 'Grant', 'Heterogeneity', 'Image', 'Impact evaluation', 'Individual', 'Knowledge', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Neurobiology', 'Noise', 'Outcome', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Procedures', 'Psyche structure', 'Psychiatry', 'Psychophysiology', 'Public Health', 'Reproducibility', 'Research', 'Severities', 'Signal Transduction', 'Statistical Methods', 'Stimulus', 'Strategic Planning', 'Structure', 'Symptoms', 'Trauma', 'United States', 'Validation', 'analytical method', 'base', 'brain behavior', 'burden of illness', 'clinical predictors', 'cohort', 'flexibility', 'high dimensionality', 'high risk', 'improved', 'individual variation', 'insight', 'learning strategy', 'multidimensional data', 'neural circuit', 'neurobiological mechanism', 'neuroimaging', 'neuroimaging marker', 'novel', 'patient population', 'predict clinical outcome', 'recruit', 'relating to nervous system', 'response', 'trauma exposure', 'trauma symptom', 'user friendly software', 'vector']",NIMH,EMORY UNIVERSITY,R01,2019,614061,-0.007052805496568898
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9724344,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Streptococcus vaccine', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2019,872121,-0.03446756189115408
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9658524,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,305167,-0.019155018576707244
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9635802,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Structure', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'biomarker validation', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunomodulatory therapies', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'multidimensional data', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'serial imaging', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2019,633289,-0.05163948995241162
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9840015,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2019,767374,-0.02642508460211758
"Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease Abstract: Hepatocellular carcinoma (HCC) is the third most common cause of cancer-related death worldwide and is rising in incidence in the US. 90% of patients in the US with liver cancer have underlying cirrhosis, thus guideline recommendations recommend surveillance in all patients with cirrhosis to facilitate early detection. Unfortunately, only 20-30% of patients are detected with early detection and are thus eligible for potentially curative treatments. There is an unmet need for reliable biomarkers for HCC to facilitate adherence to screening and for early detection. In the proposed work we will develop early detection strategies for HCC based on glycoproteomic profiles. Unique changes in glycosylation in proteins, which involve structural changes in glycan groups, have been shown to be important serum biomarkers for early cancer detection. Importantly, the subtle changes may only involve minor structures but they can be very specific in differentiating cirrhosis versus early versus late stage HCC. In addition, these changes may be specific to the etiology of liver disease. These glycan structural changes will be detected and monitored quantitatively using a mass spectrometry approach which has proven to be an accurate way to characterize even minor changes in structure which may be significant as biomarkers based on our previous mass analysis, tandem mass spectrometry measurements and databases which have been developed for glycan and glycopeptide analysis. This will be demonstrated for both glycan and glycopeptide screening from serum using novel extraction and separation methods coupled to mass spectrometry which can ultimately be used to distinguish early stage HCC from cirrhosis. The proposed work will deliver separations and mass spec methods enabling isomeric separation of glycans and glycopeptides, permitting unequivocal assignment of protein glycosylation related to disease state. We will be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. Novel software will be developed and used to assign these glycan structures. The markers will be discovered for specific etiologies of HCV-related, alcohol-related and NAFLD-related etiologies of HCC. This will be a multisite study to include all components required for a tumor biomarker lab including samples and sample preparation, separations and mass spec analysis, bioinformatics evaluation and statistical analysis. Ultimately, we will develop methods for discovery of glycan/glycopeptide markers from patient serum, the identification of potential markers and the development of new assays to provide a limited confirmation of these markers. Project Narrative: The proposed work will use new separations and mass spec methods to provide isomeric separation of glycans and glycopeptides, resulting in detailed assignment of protein glycosylation related to disease state. We expect to be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. The markers will be discovered for specific etiologies of HCV- related, alcohol-related and NAFLD-related etiologies of HCC. Ultimately, we will develop new assays to provide a limited confirmation of these markers.",Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease,9696355,U01CA225753,"['Adherence', 'Alcohol-Related Hepatocellular Carcinoma', 'Alcohols', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Cancer Etiology', 'Carbon', 'Cessation of life', 'Cirrhosis', 'Complex', 'Computer software', 'Coupled', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Etiology', 'Europe', 'Evaluation', 'Frequencies', 'Glycopeptides', 'Glycoproteins', 'Guidelines', 'Hepatitis B Virus', 'Hepatitis C virus', 'Incidence', 'Isomerism', 'Japan', 'Lectin', 'Liver Cirrhosis', 'Liver diseases', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Mass Spectrum Analysis', 'Measurement', 'Methods', 'Minor', 'Monitor', 'Natural graphite', 'Patients', 'Pattern', 'Peptides', 'Performance', 'Polysaccharides', 'Preparation', 'Primary carcinoma of the liver cells', 'Protein Glycosylation', 'Protein Isoforms', 'Proteins', 'Proteome', 'Recommendation', 'Risk', 'Sampling', 'Screening for cancer', 'Serum', 'Serum Markers', 'Site', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Survival Rate', 'Testing', 'Time', 'Tumor Markers', 'Ultrasonography', 'Work', 'alpha-Fetoproteins', 'base', 'carbohydrate structure', 'curative treatments', 'diagnostic screening', 'early detection biomarkers', 'early onset', 'glycoproteomics', 'glycosylation', 'improved', 'non-alcoholic fatty liver disease', 'nonalcoholic steatohepatitis', 'novel', 'patient screening', 'patient stratification', 'precision medicine', 'screening', 'sialylation', 'tandem mass spectrometry', 'tumor']",NCI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,U01,2019,488235,-0.06054587849703
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9785367,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2019,247413,-0.001050658270197014
"Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders Project Summary/Abstract  In the field of Alzheimer’s and related disorder, there has been very little work focusing on imaging genomics biomarker approaches, despite considerable promise. In part this is due to the fact that most studies have fo- cused on candidate gene approaches or those that do not capitalize on capturing (and amplifying) small effects spread across many sites. Even for genome wide studies, the vast majority of imaging genomic studies still rely on massive univariate analyses. The use of multivariate approaches provides a powerful tool for analyzing the data in the context of genomic and connectomic networks (i.e. weighted combinations of voxels and genetic variables). It is clear that imaging and genomic data are high dimensional and include complex relationships that are poorly understood. Multivariate data fusion models that have been proposed to date typically suffer from two key limitations: 1) they require the data dimensionality to match (i.e. 4D fMRI data has to be reduced to 1D to match with the 1D genomic data, and 2) models typically assume linear relationships despite evidence of non- linearity in brain imaging and genomic data. New methods are needed that can handle data that has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. Secondly, methods that can handle complex relationships, such as groups of networks that are tightly coupled or nonlinear relationships in the data. To ad- dress these challenges, we introduce a new framework called flexible subspace analysis (FSA) that can auto- matically identify subspaces (groupings of unimodal or multimodal components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus leveraging the power of deep learning. We will apply the developed models to a large longitudinal dataset of individuals at various stages of cognitive impair- ment and dementia. Using follow-up outcomes data we will evaluate the predictive accuracy of a joint analysis compared to a unimodal analysis, as well as its ability to characterize various clinical subtypes including those driven by vascular effects including subcortical ischemic vascular dementia versus those that are more neuro- degenerative. We will evaluate the single subject predictive power of these profiles in independent data to max- imize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data promises to advance our understanding of Alzheimer’s and related disorders in addition to providing new tools that can be widely applied to other studies of complex disease. 3 Project Narrative  It is clear that multimodal data fusion provides benefits over unimodal analysis, however existing approaches typically require the data to have matched dimensionality, leading to a loss of information. In addition, most models assume linear relationships, despite strong evidence of nonlinear relationships in the data. We propose to develop new flexible models to capture multi-scale brain imaging and genomics data which we will use to study a large data set of individuals with Alzheimer’s disease and Alzheimer’s disease related disorders. 2",Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders,9826772,RF1AG063153,"['3-Dimensional', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Behavior', 'Benchmarking', 'Biological', 'Blood Vessels', 'Brain', 'Brain imaging', 'Brain region', 'Candidate Disease Gene', 'Categories', 'Classification', 'Communities', 'Complex', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Dementia', 'Diagnostic', 'Dimensions', 'Disease', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Image', 'Impaired cognition', 'Individual', 'Joints', 'Lead', 'Linear Models', 'Link', 'Magnetic Resonance Imaging', 'Meta-Analysis', 'Methods', 'Modality', 'Modeling', 'Motivation', 'Nerve Degeneration', 'Neurobiology', 'Noise', 'Outcome', 'Pathway interactions', 'Pattern', 'Research Personnel', 'Rest', 'Sampling', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Structure', 'Subgroup', 'Time', 'Vascular Dementia', 'Work', 'base', 'blind', 'clinical subtypes', 'connectome', 'data anonymization', 'data warehouse', 'deep learning', 'flexibility', 'follow-up', 'functional genomics', 'genome-wide analysis', 'genomic biomarker', 'genomic data', 'longitudinal dataset', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'patient subsets', 'statistics', 'structural genomics', 'subcortical ischemic vascular disease', 'tool', 'user friendly software', 'white matter damage']",NIA,GEORGIA STATE UNIVERSITY,RF1,2019,3319889,-0.06808698044872974
"Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled Iifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. n/a",Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry,9903672,R01GM135927,"['Accelerometer', 'Activity Cycles', 'Algorithms', 'Arrhythmia', 'Awareness', 'Behavior', 'Behavior monitoring', 'Big Data', 'Body Temperature', 'Cellular Phone', 'Chemotherapy-Oncologic Procedure', 'Collection', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Devices', 'Dust', 'Encapsulated', 'Energy Metabolism', 'Geometry', 'Glean', 'Goals', 'Growth', 'Health', 'Health Status', 'Healthcare', 'Heart', 'Heart Rate', 'Home environment', 'Hour', 'Human', 'Individual', 'Lead', 'Life', 'Location', 'Mathematics', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Movement', 'National Institute of General Medical Sciences', 'Outcome', 'Participant', 'Pattern', 'Periodicity', 'Physical activity', 'Population', 'Research', 'Research Project Grants', 'Rest', 'Sampling', 'Series', 'Signal Transduction', 'Swimming', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'Vision', 'Walking', 'Water', 'Work', 'base', 'circadian', 'design', 'diaries', 'health related quality of life', 'heart rate monitor', 'insight', 'interest', 'mathematical methods', 'metastatic colorectal', 'multimodality', 'personalized intervention', 'post stroke', 'programs', 'sensor', 'statistics', 'tool', 'wearable device']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,316425,-0.011757090233976067
"Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD Overall Summary The overarching goal of the proposed center is to leverage molecular and circuit biomarkers to advance the understanding of mechanisms and personalized treatment of topiramate treatment of Alcohol Use Disorder comorbid with PTSD. We propose an integrative translational focus on alterations in excitatory and inhibitory signaling, focusing on GABA and glutamate and related circuitry, to model the neurobiology of PTSD comorbid with PTSD and the mitigating effects of topiramate. We will characterize excitatory and inhibitory molecular markers in an animal model of AUD comorbid with PTSD, utilizing genomic markers in the brain and plasma markers in rodents. In clinical trial participants we will characterize excitatory and inhibitory neuronal signaling by ascertaining plasma markers, GRIK 1 genotype and neural circuit markers utilizing TMS evoked potentials in EEG, task-based functional MRI and MR spectroscopy. This goal will be achieved through the activities of three research projects supported by two research cores, the administrative core and the Scientific Advisory Board (Figure1). In Project 1 lead by Silvia Fossati Ph.D. and Jorge Manzanares Robles Ph.D. we will study the behavioral and molecular effects of two doses of topiramate vs. vehicle in animal models of AUD alone, PTSD alone and AUD+PTSD. In Project 2 lead by Michael Bogenschutz M.D. and Joshua Lee M.D. we will study the behavioral, genetic and plasma biomarker effects of topiramate vs. placebo in 150 participants with co-occurring AUD and PTSD. In project 3 lead by Amit Etkin M.D., Ph.D. and Charles R. Marmar M.D. we will ascertain multi-modal imaging markers including task based fMRI, TMS evoked potentials in EEG and MRS. Imaging markers will be used to characterize excitatory and inhibitory circuits in Project 2 clinical trial participants with AUD+PTSD to determine predictors and mechanisms of topiramate vs. placebo treatment outcomes. Plasma biomarkers in Project 2 will be related to the same or homologous plasma biomarkers in Project 1. Circuit markers from Project 3 will be related to genomic markers in the same or homologous brain regions in Project 1. The Biofluids Biomarker Core (BBC) lead by Dr. Fossati will support collection of plasma biomarkers (GABA, glutamate, HPA axis, neuropeptides, neuroinflammatory and oxidative stress) in animals in Project 1 and clinical trial participants in Project 2. The Analytics and Biostatistics Core (ABC) lead by Eugene Laska Ph.D. and Carole Segal Ph.D. will support experimental design, formulation of hypothesis, power calculations, and data integrity, management and analysis for Project 1, 2 and 3, implementing advanced statistical models for individualized prediction of response to topiramate in Project 1 and Project 2. Overall Narrative We aim to advance personalized topiramate treatment for comorbid AUD and PTSD by focusing on network excitatory and inhibitory imbalances underlying heightened negative emotions and impairments in executive function and emotion regulation. Excitatory and inhibitory targets will be assessed with brain, plasma and circuit markers, where possible harmonizing the approach for optimizing discovery of mechanisms and prediction of topiramate treatment in animal models and in clinical trial participants. We will achieve this aim with a center driven approach to discovery with three integrative design elements and four cross-project center aims, creating translational bridges among animal model, clinical trial and imaging studies.",Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD,9786648,P01AA027057,"['Affect', 'Aftercare', 'Alcohols', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biological', 'Biological Markers', 'Biostatistics Core', 'Blood', 'Brain', 'Brain region', 'Clinical', 'Clinical Treatment', 'Clinical Trials', 'Cognition', 'Cognitive', 'Collection', 'Comorbidity', 'Complement', 'Data Analytics', 'Disease model', 'Doctor of Medicine', 'Doctor of Philosophy', 'Dose', 'Electroencephalography', 'Elements', 'Emotional', 'Emotions', 'Equilibrium', 'Evoked Potentials', 'Experimental Designs', 'Formulation', 'Fostering', 'Freezing', 'Functional Magnetic Resonance Imaging', 'Gene Expression', 'Genetic', 'Genotype', 'Glutamates', 'Goals', 'Human', 'Impairment', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance Spectroscopy', 'Measures', 'Mediating', 'Mediation', 'Mission', 'Modeling', 'Molecular', 'Multimodal Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurobiology', 'Neurocognitive', 'Neurons', 'Neuropeptides', 'Outcome', 'Oxidative Stress', 'Participant', 'Patients', 'Pharmacology', 'Placebos', 'Plasma', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Randomized', 'Randomized Clinical Trials', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Rodent', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Stimulus', 'Structure', 'Symptoms', 'Testing', 'Transcranial magnetic stimulation', 'Translations', 'Treatment outcome', 'Work', 'alcohol abuse therapy', 'alcohol comorbidity', 'alcohol cue', 'alcohol use disorder', 'base', 'behavioral study', 'clinical imaging', 'clinically relevant', 'cognitive neuroscience', 'data integrity', 'data management', 'design', 'drinking', 'dual diagnosis', 'effective therapy', 'emotion regulation', 'executive function', 'gamma-Aminobutyric Acid', 'genomic biomarker', 'imaging biomarker', 'imaging study', 'molecular marker', 'mortality', 'mouse model', 'multimodality', 'neural circuit', 'neurobiological mechanism', 'neuroinflammation', 'neurophysiology', 'neurotransmission', 'personalized medicine', 'personalized predictions', 'pre-clinical', 'precision medicine', 'predicting response', 'predictive modeling', 'protein expression', 'response', 'source localization', 'topiramate', 'treatment effect', 'treatment response']",NIAAA,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P01,2019,1202772,-0.03493234528078444
"Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD Overall Summary The overarching goal of the proposed center is to leverage molecular and circuit biomarkers to advance the understanding of mechanisms and personalized treatment of topiramate treatment of Alcohol Use Disorder comorbid with PTSD. We propose an integrative translational focus on alterations in excitatory and inhibitory signaling, focusing on GABA and glutamate and related circuitry, to model the neurobiology of PTSD comorbid with PTSD and the mitigating effects of topiramate. We will characterize excitatory and inhibitory molecular markers in an animal model of AUD comorbid with PTSD, utilizing genomic markers in the brain and plasma markers in rodents. In clinical trial participants we will characterize excitatory and inhibitory neuronal signaling by ascertaining plasma markers, GRIK 1 genotype and neural circuit markers utilizing TMS evoked potentials in EEG, task-based functional MRI and MR spectroscopy. This goal will be achieved through the activities of three research projects supported by two research cores, the administrative core and the Scientific Advisory Board (Figure1). In Project 1 lead by Silvia Fossati Ph.D. and Jorge Manzanares Robles Ph.D. we will study the behavioral and molecular effects of two doses of topiramate vs. vehicle in animal models of AUD alone, PTSD alone and AUD+PTSD. In Project 2 lead by Michael Bogenschutz M.D. and Joshua Lee M.D. we will study the behavioral, genetic and plasma biomarker effects of topiramate vs. placebo in 150 participants with co-occurring AUD and PTSD. In project 3 lead by Amit Etkin M.D., Ph.D. and Charles R. Marmar M.D. we will ascertain multi-modal imaging markers including task based fMRI, TMS evoked potentials in EEG and MRS. Imaging markers will be used to characterize excitatory and inhibitory circuits in Project 2 clinical trial participants with AUD+PTSD to determine predictors and mechanisms of topiramate vs. placebo treatment outcomes. Plasma biomarkers in Project 2 will be related to the same or homologous plasma biomarkers in Project 1. Circuit markers from Project 3 will be related to genomic markers in the same or homologous brain regions in Project 1. The Biofluids Biomarker Core (BBC) lead by Dr. Fossati will support collection of plasma biomarkers (GABA, glutamate, HPA axis, neuropeptides, neuroinflammatory and oxidative stress) in animals in Project 1 and clinical trial participants in Project 2. The Analytics and Biostatistics Core (ABC) lead by Eugene Laska Ph.D. and Carole Segal Ph.D. will support experimental design, formulation of hypothesis, power calculations, and data integrity, management and analysis for Project 1, 2 and 3, implementing advanced statistical models for individualized prediction of response to topiramate in Project 1 and Project 2. Overall Narrative We aim to advance personalized topiramate treatment for comorbid AUD and PTSD by focusing on network excitatory and inhibitory imbalances underlying heightened negative emotions and impairments in executive function and emotion regulation. Excitatory and inhibitory targets will be assessed with brain, plasma and circuit markers, where possible harmonizing the approach for optimizing discovery of mechanisms and prediction of topiramate treatment in animal models and in clinical trial participants. We will achieve this aim with a center driven approach to discovery with three integrative design elements and four cross-project center aims, creating translational bridges among animal model, clinical trial and imaging studies.",Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD,9938893,P01AA027057,"['Affect', 'Aftercare', 'Alcohols', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biological', 'Biological Markers', 'Biostatistics Core', 'Blood', 'Brain', 'Brain region', 'Clinical', 'Clinical Treatment', 'Clinical Trials', 'Cognition', 'Cognitive', 'Collection', 'Comorbidity', 'Complement', 'Data Analytics', 'Disease model', 'Doctor of Medicine', 'Doctor of Philosophy', 'Dose', 'Electroencephalography', 'Elements', 'Emotional', 'Emotions', 'Equilibrium', 'Evoked Potentials', 'Experimental Designs', 'Formulation', 'Fostering', 'Freezing', 'Functional Magnetic Resonance Imaging', 'Gene Expression', 'Genetic', 'Genotype', 'Glutamates', 'Goals', 'Human', 'Impairment', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance Spectroscopy', 'Measures', 'Mediating', 'Mediation', 'Mission', 'Modeling', 'Molecular', 'Multimodal Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurobiology', 'Neurocognitive', 'Neurons', 'Neuropeptides', 'Outcome', 'Oxidative Stress', 'Participant', 'Patients', 'Pharmacology', 'Placebos', 'Plasma', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Randomized', 'Randomized Clinical Trials', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Rodent', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Stimulus', 'Structure', 'Symptoms', 'Testing', 'Transcranial magnetic stimulation', 'Translations', 'Treatment outcome', 'Work', 'alcohol abuse therapy', 'alcohol comorbidity', 'alcohol cue', 'alcohol use disorder', 'base', 'behavioral study', 'clinical imaging', 'clinically relevant', 'cognitive neuroscience', 'data integrity', 'data management', 'design', 'drinking', 'dual diagnosis', 'effective therapy', 'emotion regulation', 'executive function', 'gamma-Aminobutyric Acid', 'genomic biomarker', 'imaging biomarker', 'imaging study', 'molecular marker', 'mortality', 'mouse model', 'multimodality', 'neural circuit', 'neurobiological mechanism', 'neuroinflammation', 'neurophysiology', 'neurotransmission', 'personalized medicine', 'personalized predictions', 'pre-clinical', 'precision medicine', 'predicting response', 'predictive modeling', 'protein expression', 'response', 'source localization', 'topiramate', 'treatment effect', 'treatment response']",NIAAA,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P01,2019,169483,-0.03493234528078444
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,9903643,R01GM135926,"['Biological Process', 'Data Set', 'Instruction', 'Learning', 'Time']",NIGMS,CORNELL UNIVERSITY,R01,2019,351443,-0.03157461902003728
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine No abstract available PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,10063300,U01LM012675,[' '],NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2019,375751,-0.014389142769014407
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9753389,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Autosomal dominant nocturnal frontal lobe epilepsy\xa0', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'informatics\xa0tool', 'phenotypic data', 'rare variant', 'sample collection']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2019,60925,-0.036169469289590465
"Dissecting the role of the direct and indirect pathways in moment-to-moment action selection. Brains transform sensory information into decisions and decisions into behaviors, which ultimately determine fitness. Behavior can be broken down into a set of discrete chunks of movement, called actions. The basal ganglia (BG) and in particular the input nucleus of the BG, the striatum, is critical for the proper sequencing and selection of actions. At a cellular level, the striatum is comprised of spiny projection neurons (SPNs) that constitute the direct pathway (dSPNs) and indirect pathway (iSPNs). Under the center-surround model of action selection, dSPNs are thought to facilitate the expression of an action while iSPNs are thought to inhibit the expression of other actions. However, it is not clear how each pathway contributes to action selection due to methodological constraints in acquiring an objectively quantitative description of behavior. Our lab has recently developed a pipeline, known as MoSeq, that acquires high-resolution behavioral data and uses an unsupervised algorithm to model stereotyped pose dynamics (actions or “syllables”). Here I propose to combine this state-of- the-art behavioral acquisition and detection technology with both cellular-resolution imaging and optogenetic perturbation to study the population dynamics underlying action selection in the striatum. I hypothesize that SPNs exhibit syllable-specific tuning, where dSPNs are tightly tuned to facilitate the expression of related syllables, while iSPNs are more broadly tuned to suppress the simultaneous expression of other syllables. I will dissect these two processes by recording and manipulating each SPN class during specific syllable expression. In aim 1, I will perform cellular-resolution recordings of the direct or indirect pathway using genetically encoded calcium indicators and miniaturized microendoscopy in the striatum. I will examine the differential roles of the direct and indirect pathways in the context of behavioral tuning. My preliminary data suggest that dSPNs are more sparsely tuned than iSPNs. In aim 2, I will functionally test the center-surround model via direct and indirect pathway inhibition. I will use the inhibitory anion-conducting rhodopsin, ACR2. Using a system capable of detecting syllable expression in real-time, I will perturb each pathway triggered upon the expression of specific syllables to compare the same selection context across many trials. In summary, the experiments proposed here will contribute to a mechanistic understanding of how the BG performs action selection on a moment-to-moment timescale. This proposal is the first to test the predictions made by the center- surround model and will advance our understanding of how the BG encodes actions. Behavior is constructed from actions that are placed into sequences that enable animals to achieve ethologically relevant goals. The striatum, the input nucleus of the basal ganglia, is critical for the proper selection and expression of actions. My work aims to understand how striatal circuits perform action selection on a moment- to-moment timescale.",Dissecting the role of the direct and indirect pathways in moment-to-moment action selection.,9830975,F31NS113385,"['Address', 'Algorithms', 'Animals', 'Anions', 'Basal Ganglia', 'Behavior', 'Behavioral', 'Brain', 'Cell Nucleus', 'Corpus striatum structure', 'Data', 'Detection', 'Endoscopes', 'Environment', 'Exhibits', 'Goals', 'Human', 'Image', 'Individual', 'Methodology', 'Modeling', 'Motion', 'Movement', 'Neurons', 'Organism', 'Output', 'Pathway interactions', 'Pattern', 'Play', 'Population', 'Population Dynamics', 'Posture', 'Process', 'Property', 'Resolution', 'Rhodopsin', 'Rodent', 'Role', 'Sensory', 'Series', 'Stereotyping', 'Substantia nigra structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'calcium indicator', 'complement pathway', 'experimental study', 'fitness', 'indexing', 'microendoscopy', 'miniaturize', 'optogenetics', 'study population', 'tool', 'unsupervised learning']",NINDS,HARVARD MEDICAL SCHOOL,F31,2019,38342,-0.030257308375492727
"An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases. Project Summary / Abstract Diagnostic errors are increasingly recognized as a cause of pain, suffering and increased healthcare costs. Diagnostic delays are an important class of diagnostic errors. While many diagnostic errors occur in hospital settings, emergency departments visits may be especially important to consider because they treat critically ill patients and because most decisions to admit patients to the hospital are made in emergency departments. Thus, to enable a more complete understanding of diagnostic delays requires consideration of healthcare visits across a range of healthcare settings including clinic visits, emergency department visits and hospitalizations. Delays in diagnosing infectious diseases are important to consider. For contagious infectious diseases, diagnostic delays increase the risk of additional exposures, potentially generating more cases. Second, many infectious diseases can be effectively treated, but even short delays in treatment lead to worse clinical outcomes. However, with the exception of a few infectious diseases (e.g., tuberculosis), diagnostic delays for infectious diseases are understudied. Thus, there is a critical need to investigate the incidence, risk factors and clinical impact for diagnostic delays for infectious diseases. The overarching goal of our research is to investigate diagnostic delays associated with infectious diseases using existing data along with methods from the fields of computer science and statistics. While our research relies upon “big data”, we will also use clinical experts to review and contribute to all of our results. Our subject matter experts incorporate expertise in infectious diseases, emergency medicine, acute care, medical education, diagnostic reasoning, healthcare epidemiology, public health, industry, and professional infectious disease societies. Specifically, we will 1) determine the incidence of diagnostic delays for a wide range of infectious diseases; 2) identify the risk factors associated with diagnostic delays for infectious diseases that are frequently delayed or have serious outcomes; and 3) estimate the impact of diagnostic delays in terms of healthcare costs and mortality. With our data, methods and clinical experts, we will be able to translate our results into future interventions designed to decrease diagnostic delays and improve healthcare outcomes. In addition, while our proposal focuses on infectious diseases, the methods and approaches that we will develop can be adopted to investigate non-infectious diseases and conditions. Project Narrative Diagnostic delays for infectious diseases contribute to worse clinical outcomes, increased healthcare costs and, for some infectious diseases, outbreaks of great public health importance. We will use existing large data sets along with machine-learning techniques and expert clinical guidance to detect patterns of healthcare visits representing diagnostic delays. Our goal is to characterize the incidence, risk factors and clinical impact of diagnostic delays for a wide range of infectious diseases to inform future interventions.","An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases.",9938200,R01HS027375,[' '],AHRQ,UNIVERSITY OF IOWA,R01,2019,497273,-0.018284400235122046
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.034031930278022295
"Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents PROJECT ABSTRACT  Antimicrobial resistance is a critical public health issue. Infections with drug resistant pathogens are estimated to cause an additional eight million hospitalization days annually over the hospitalizations that would be seen for infections with susceptible agents. The use of antibiotics (in both clinical and agricultural settings) is being viewed as precursor for these infections and thus, is a major public health concern—particularly as outbreaks become more frequent and severe. However, scientiﬁc evidence describing the hazards associated with antibiotic use is lacking due to inability to quantify the risk of these practices. One promising avenue to elucidate this risk is to use shotgun metagenomics to identify the AMR genes in samples taken through systematic spatiotemporal surveillance. The goal of this proposed work is to develop algorithms that will provide such a means for analysis. The algorithms need to be scalable to very large datasets and thus, will require the development and use succinct data structures.  In order to achieve this goal, the investigative team will develop the theoretical foundations and applied meth- ods needed to study AMR through the use of shotgun metagenomics. A major focus of the proposed work is developing algorithms that can handle very large datasets. To achieve this scalability, we will create novel means to create, compress, reconstruct and update very large de Bruijn graphs that metagenomics data in a manner needed to study AMR. In addition, we will pioneer the study of AMR through long read data by proposing new algorithmic problems and solutions that use data. For example, identifying the location of speciﬁc genes in a metagenomics sample using long read data has not been proposed or studied. Thus, the algorithmic ideas and techniques developed in this project will not only advance the study of AMR, but contribute to the growing domain of big data analysis and pan-genomics.  Lastly, we plan to apply our methods to samples collected from both agricultural and clinical settings in Florida. Analysis of preliminary and new data will allow us to conclude about (1) the public risk associated with antimicro- bial use in agriculture; (2) the effectiveness of interventions used to reduce resistant bacteria, and lastly, (3) the factors that allow resistant bacteria to grow, thrive and evolve. A–1 PROJECT NARRATIVE  Antibiotic use in agriculture is a major public health concern that is receiving a lot of media attention, par- ticularly as antibiotic-resistant infections in become more frequent and severe. This research will build a novel bioinformatics framework for determining how antimicrobial resistant genes evolve, grow, and persist in a system that has been affected by antibiotic use. This will, in turn, facilitate the development of effective intervention methods that reduce resistant pathogens in clinical and agricultural settings. N–1",Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents,9641899,R01AI141810,"['Affect', 'Agriculture', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Attention', 'Bacteria', 'Base Pairing', 'Big Data', 'Bioinformatics', 'Clinical', 'Collaborations', 'Combating Antibiotic Resistant Bacteria', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Compression', 'Data Set', 'Development', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Florida', 'Food production', 'Foundations', 'Genes', 'Genomics', 'Goals', 'Graph', 'Hospitalization', 'Infection', 'International', 'Investigation', 'Length', 'Location', 'Measures', 'Memory', 'Metagenomics', 'Methods', 'Monitor', 'Noise', 'Organism', 'Pathogenicity', 'Plasmids', 'Prevention', 'Public Health', 'Research', 'Resistance', 'Risk', 'Sampling', 'Shotguns', 'Structure', 'Surveillance Methods', 'System', 'Techniques', 'Time', 'Translating', 'Update', 'Work', 'bacterial resistance', 'base', 'combinatorial', 'drug resistant pathogen', 'effective intervention', 'foodborne outbreak', 'genetic variant', 'hazard', 'improved', 'machine learning algorithm', 'method development', 'microbial', 'microbiome analysis', 'microbiome research', 'novel', 'pathogen', 'petabyte', 'reconstruction', 'research and development', 'resistance gene', 'spatiotemporal', 'standard care']",NIAID,UNIVERSITY OF FLORIDA,R01,2019,450459,-0.017834945590245965
Advancing a novel portable detection method for cannabis intoxication No abstract available n/a,Advancing a novel portable detection method for cannabis intoxication,9751264,R42DA043977,"['Acute', 'Adult', 'Age', 'Alcohols', 'Algorithms', 'Area', 'Base of the Brain', 'Biochemical', 'Biological', 'Biological Markers', 'Blood', 'Blood Circulation', 'Body Fluids', 'Brain', 'Cannabis', 'Collaborations', 'Comorbidity', 'Cross-Over Trials', 'Data', 'Detection', 'Devices', 'Dose', 'Double-Blind Method', 'Drug Kinetics', 'Ensure', 'Equipment', 'Evaluation', 'Formulation', 'Future', 'Goals', 'Gold', 'Hour', 'Human Resources', 'Impairment', 'Individual', 'Intoxication', 'Law Enforcement', 'Law Enforcement Officers', 'Letters', 'Licensing', 'Machine Learning', 'Marijuana', 'Measurement', 'Methods', 'Near-Infrared Spectroscopy', 'Oral', 'Patient Self-Report', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Placebos', 'Population', 'Prefrontal Cortex', 'Property', 'Public Health', 'ROC Curve', 'Randomized', 'Readiness', 'Rest', 'Sensitivity and Specificity', 'Source', 'Specificity', 'System', 'THC exposure', 'Testing', 'Tetrahydrocannabinol', 'United States', 'Urine', 'Vendor', 'alcohol exposure', 'base', 'behavior test', 'commercialization', 'density', 'detector', 'driving under influence', 'drug testing', 'field sobriety tests', 'functional disability', 'hemodynamics', 'interest', 'marijuana legalization', 'marijuana use', 'marijuana user', 'novel', 'novel strategies', 'portability', 'response', 'spectroscopic imaging', 'tool', 'user-friendly', 'vehicular accident']",NIDA,"HIGHLIGHTI, INC",R42,2019,335711,-0.008937423617015779
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,-0.015512950440719141
"A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions Abstract. Melanoma is the third most common form of skin cancer with estimated 87,110 new cases diagnosed in the United States in the year 2017. Current routine diagnostic approaches utilize microscopic evaluation of thinly sectioned patient biopsies, but in certain cases diagnosis can be contentious even among experts. The overall goal of this multi-phase SBIR project is to develop, validate, and commercialize MelanoMap™, Frontier Diagnostics' patented assay for the diagnosis of melanoma using a matrix-assisted laser desorption/ionization imaging mass spectrometry (MALDI IMS) platform—and to have this assay available to pathologists in the U.S. as a laboratory developed test. MALDI IMS is a state-of-the-art technology that generates molecular images of tens to thousands of biomolecules from tissue sections in a single analysis. The assay uses formalin-fixed paraffin embedded (FFPE) biopsies used in routine histopathological diagnosis. The proposed assay has pathologists select regions of skin biopsies for analysis via a remote web interface. The acquired IMS data from those regions unambiguously identifies malignant melanoma or benign nevus.  Phase I of this proposal will demonstrate the feasibility of this technology platform to achieve cost-effective diagnosis of melanoma from patient skin biopsies at sample volumes acceptable for a clinical laboratory. Specific Aim 1 focuses on the development of a scalable and robust analytical protocol in both sample preparation and informatics to accurately diagnose melanoma with MALDI IMS. In specific aim 2, we will test the methodology developed in Specific Aim 1 on a cohort of melanocytic lesions with known clinical outcome and subsequently validate the classification accuracy of the proposed test  In Phase II, the protocols developed in Phase I will be integrated into a diagnostic service workflow. This phase will focus on quality control measures, client facing cloud software, clinical diagnostic reporting, and completing the analysis of a 500-patient sample set for final assay validation. Specific Aim 3 of this proposal (initial aim of Phase II) will establish and implement test tissues into standard workflows that will provide performance metrics for standard operation of a test meeting Clinical Laboratory Improvement Amendments (CLIA) standards. Protocols will be developed to monitor reagents, the reproducibility of sample preparation, and mass spectrometer performance on daily basis. Specific Aim 4 will expand software capabilities to include a secure web interface for clients ordering the test and the laboratory performing the test. The software will meet regulatory compliance, perform statistical analysis, and generate and communicate reports of the MALDI IMS analysis. Specific Aim 5 proposes to expand the sample set used in the initial assay from Specific Aim 2 to include a set of 300 patient samples from our clinical collaborators with 5 or more years follow-up data. The test will be independently validated by an additional 200 patient samples with definitive diagnoses. Project Narrative This multi-phase Fast Track SBIR project will develop and validate a new laboratory developed test to differentiate malignant melanocytic tumors from benign nevi and complete development of an imaging mass spectrometry-based diagnostic service platform for a clinical laboratory. The clinical assay developed under this proposal augments current practice by providing molecular measurements that are used as objective criteria in the diagnosis of melanoma. Successful completion of this Fast Track project will result in a fully documented and validated assay ready for launch as a laboratory developed test.",A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions,9905050,R44CA228897,"['Amendment', 'Antigens', 'Area', 'Benign', 'Biological Assay', 'Biopsy', 'Caliber', 'Cells', 'Classification', 'Client', 'Clinical', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Services', 'Digestion', 'Early Diagnosis', 'Ensure', 'Evaluation', 'Formalin', 'Goals', 'Gold', 'Image', 'Incentives', 'Informatics', 'Laboratories', 'Legal patent', 'Lesion', 'Malignant - descriptor', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Melanocytic Neoplasm', 'Metadata', 'Methodology', 'Microscopic', 'Microtomy', 'Molecular', 'Monitor', 'Nevus', 'Outcome', 'Paraffin Embedding', 'Pathologist', 'Pathology Report', 'Patient Care', 'Patients', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Physical shape', 'Physicians', 'Preparation', 'Procedures', 'Proteins', 'Protocols documentation', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Research', 'Retrieval', 'Sampling', 'Secure', 'Security', 'Sensitivity and Specificity', 'Side', 'Skin', 'Skin Cancer', 'Small Business Innovation Research Grant', 'Software Tools', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Standardization', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Treatment Cost', 'United States', 'Validation', 'accurate diagnosis', 'analytical method', 'base', 'clinical diagnostics', 'cloud software', 'cohort', 'cost', 'cost effective', 'data acquisition', 'diagnosis standard', 'diagnostic assay', 'disease classification', 'follow-up', 'frontier', 'histopathological examination', 'instrumentation', 'interest', 'machine learning algorithm', 'mass spectrometer', 'meetings', 'melanoma', 'molecular diagnostics', 'molecular imaging', 'mortality risk', 'off-patent', 'operation', 'prototype', 'quality assurance', 'skin lesion', 'tissue preparation', 'web interface']",NCI,"FRONTIER DIAGNOSTICS, LLC",R44,2019,998671,-0.014523144070393052
"Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study PROJECT SUMMARY ABSTRACT The overarching goals of this R01 proposal are to improve scientific understanding of potential mechanisms by which ophthalmic diseases lead to the risk of Alzheimer’s disease. The investigators will leverage modern ophthalmic data with state-of-the-art imaging and extensive archived clinical data from a well-characterized cohort of older adults. The investigators propose to examine the effect of structural and functional changes in retina and longitudinal severity of ophthalmic diseases on Alzheimer’s disease and related neuropathology. The proposal builds on the resources of the Adult Changes in Thought (ACT) study, a prospective longitudinal, population-based, dementia-free cohort of over 5,500 people to date established in 1994 which has detected >1,014 research quality diagnoses of Alzheimer’s disease and >1,254 dementia to date. ACT follows consenting participants to autopsy and has performed state-of-the arts autopsy on >781 decedents to date. In this extremely well-characterized cohort, the investigators found that several ophthalmic diseases (diabetic retinopathy, glaucoma, age-related macular degeneration) are significantly associated with the risk of developing Alzheimer’s disease. The investigators will use three advanced ophthalmic imaging modalities at both home and clinical research study visits: fundus photography, optical coherence tomography (OCT), and OCT angiography (OCTA), to obtain quantitative data relevant to these ophthalmic diseases. The study team will establish the distribution (Aim 1a) and 2- and 4-year evolution of ophthalmic imaging characteristics found in older adults in the community and determine associations with change in cognition (Aim 1 b, c). Additionally, magnetic resonance imaging (MRI) and MRI angiography (MRA) will be obtained in a subset of participants to investigate the contribution of small (retinal) and large (cerebral) vascular disease towards cognitive changes (Aim 1d). The study team will continue ACT study’s strong commitment for meaningful data sharing. In collaboration with the Laboratory of Neuro Imaging at University of Southern California, the study team will promulgate these ophthalmic data in addition to neuroimaging data to the research community (Aim 1e). In Aim 2, the investigators will use extensive clinical ophthalmology data captured over many decades and incorporate them in novel longitudinal models of eye disease severity. The investigators will analyze eye disease severity along with extensive neuropathology data from the ACT study, including both standard (Aim 2a) and novel quantitative (Aim 2b) neuropathology data, to further scientific understanding of neuropathological mechanisms underlying associations between eye conditions and Alzheimer’s disease risk. The brain is not amenable to direct observations during life. In contrast, the eye is an anterior extension of the central nervous system and may provide a valuable window to illuminate neurodegenerative processes in the aging brain. Proposed investigations will substantially enhance scientific understanding of the role of modern ophthalmic evaluations in delineating risk of Alzheimer's disease and other forms of neuropathology. PROJECT NARRATIVE Using a large, well-characterized, longitudinal, prospective, cohort study, the study team previously found that diabetic retinopathy, glaucoma, and age-related macular degeneration were significantly associated with Alzheimer’s disease risk. The team proposes to use three cutting edge ophthalmic imaging modalities to obtain quantitative data at both home and clinic research study visits in addition to MRI and MRA in a subset of participants to evaluate their associations with change in cognition over time (Aim 1). The team will leverage extensive ophthalmic clinical and neuropathological data already available for 781 study participants to date as well as new state-of-the-art quantitative measures of beta amyloid (A1-42) and phosphorylated tau to elucidate mechanisms underlying associations between ophthalmic conditions and Alzheimer’s disease (Aim 2).",Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study,9816310,R01AG060942,"['Abbreviations', 'Adult', 'Age related macular degeneration', 'Age-associated memory impairment', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Amyloid beta-Protein', 'Angiography', 'Anterior', 'Archives', 'Autopsy', 'Bayesian Modeling', 'Biological Markers', 'Blood Vessels', 'Brain', 'California', 'Cerebrovascular Disorders', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognition', 'Collaborations', 'Communities', 'Consent', 'Data', 'Data Collection', 'Dementia', 'Diabetic Retinopathy', 'Diagnosis', 'Disease', 'Disease model', 'Drusen', 'Elderly', 'Evaluation', 'Evolution', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Home environment', 'Image', 'Impaired cognition', 'Investigation', 'Laboratories', 'Lead', 'Life', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Microvascular Dysfunction', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Nerve Fibers', 'Neuraxis', 'Occipital lobe', 'Ophthalmology', 'Optical Coherence Tomography', 'Participant', 'Pathology', 'Perfusion', 'Persons', 'Predisposition', 'Process', 'Prospective cohort study', 'Provider', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Retina', 'Retinal', 'Risk', 'Role', 'Scanning', 'Selection Bias', 'Series', 'Severities', 'Severity of illness', 'Structure', 'Technology', 'Testing', 'Time', 'Universities', 'Vascular Diseases', 'Visit', 'aging brain', 'area striata', 'base', 'cognitive change', 'cohort', 'data sharing', 'deep learning', 'diagnosis quality', 'epidemiology study', 'fiber cell', 'follow-up', 'high risk', 'imaging biomarker', 'imaging modality', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuropathology', 'novel', 'paired helical filament', 'population based', 'prospective', 'repository', 'research study', 'resilience', 'spelling', 'tau Proteins', 'tau-1', 'vascular contributions']",NIA,UNIVERSITY OF WASHINGTON,R01,2019,3869686,-0.08578881802790787
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9573854,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2018,388750,-0.02116943623472907
"A Deep Learning Model to Improve Pathologist Interpretation of Donor Kidney Biopsies PROJECT SUMMARY/ABSTRACT More people die every year from kidney disease than breast or prostate cancer. Kidney transplantation is life-saving but is limited by a shortage of organ donors and an unacceptably high donor organ discard rate. The decision to use or discard a donor kidney relies heavily on manual quantitation of key microscopic findings by pathologists. A major limitation of this microscopic examination is human variability and inefficiency in interpreting the findings, resulting in potentially healthy organs being deemed unsuitable for transplantation or potentially damaged organs being transplanted inappropriately. Our team developed the first Deep Learning model capable of automatically quantifying percent global glomerulosclerosis in whole slide images of donor kidney frozen section wedge biopsies. This innovative approach has the potential to transform donor kidney biopsy evaluation by improving pathologist efficiency, accuracy, and precision ultimately resulting in optimized donor organ utilization, diminished health care costs, and improved patient outcomes. The goal of this project is to establish our Deep Learning automated quantitative evaluation as the standard practice of donor kidney evaluation prior to transplantation. This will be achieved by assembling a team of expert kidney pathologists and computer scientists specializing in machine learning. The proposal will evaluate the accuracy and precision of the computerized approach to quantifying percent global glomerulosclerosis and compare these results with current standard of care pathologist evaluation. The feasibility of deploying the Deep Learning model to analyze whole slide images on the cloud will also be examined. The end product of this STTR will be a web-based platform to securely deploy Deep Learning image analysis as a tool to assist pathologists with donor kidney biopsy evaluation. PUBLIC HEALTH RELEVANCE STATEMENT Before a kidney can be transplanted, the tissue must be assessed under a microscope to ensure the organ is healthy enough for transplant. A major limitation of microscopic examination is human variability in interpreting the findings, resulting in healthy organs being deemed unsuitable for transplantation. This funding will support developing computer algorithms to assist pathologists in microscopic examination of donor kidney tissues, resulting in more consistent and objective biopsy interpretations, minimizing discard of potentially usable kidneys and optimizing organ placement for transplant.",A Deep Learning Model to Improve Pathologist Interpretation of Donor Kidney Biopsies,9678574,R41DK120253,"['Address', 'Biopsy', 'Blinded', 'Caring', 'Cessation of life', 'Charge', 'Chronic', 'Chronic Kidney Failure', 'Clinical', 'Computational algorithm', 'Computer Assisted', 'Computer software', 'Computers', 'Cost of Illness', 'Data Set', 'Ensure', 'Evaluation', 'Freezing', 'Frozen Sections', 'Funding', 'Goals', 'Health Care Costs', 'Healthcare Systems', 'Human', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Interobserver Variability', 'Kidney', 'Kidney Diseases', 'Kidney Transplantation', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Manuals', 'Measures', 'Medicare', 'Microscope', 'Microscopic', 'Modeling', 'Online Systems', 'Organ', 'Organ Donor', 'Outcome', 'Pathologic', 'Pathologist', 'Pathology', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Personal Satisfaction', 'Phase', 'Process', 'Quantitative Evaluations', 'Reproducibility', 'Research Personnel', 'Savings', 'Scientist', 'Secure', 'Slide', 'Small Business Technology Transfer Research', 'Speed', 'Testing', 'Time', 'Tissues', 'Translating', 'Transplantation', 'Transplanted tissue', 'Universities', 'Washington', 'Work', 'base', 'clinical practice', 'cloud based', 'commercial application', 'computerized', 'deep learning', 'digital', 'glomerulosclerosis', 'improved', 'innovation', 'learning network', 'malignant breast neoplasm', 'meetings', 'power analysis', 'predictive modeling', 'public health relevance', 'software development', 'standard of care', 'technological innovation', 'tool', 'whole slide imaging']",NIDDK,"NEWVENTUREIQ, LLC",R41,2018,214009,-0.014598502164186183
"Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data NARRATIVE SUMMARY The landscape of data formats is rapidly expanding, with image, text and other complex formats becoming available for health related outcomes. By considering such data within the context of observational causal inference, they can be leveraged to improve clinical decisions, help evaluate treatment efficacy by estimating individualized treatment effects and help develop intelligent therapeutic systems where individualized treatments can be deployed. In R01EB025021, we concentrate on understanding how nearly exact matching can be achieved in the presence of a large number of categorical covariates. The proposed approach (called FLAME - Fast Large Almost Matching Exactly) is able to quickly learn which categorical covariates are important and to produce high quality matches \citep{wang2017flame,dieng2018collapsing}. The main shortfall in the proposed work for R01EB025021 is that it does not naturally extend to more complex data types, it only works for categorical data in which each feature is meaningful. {\bf This proposal will develop new statistical and computational tools for causal analysis of complex data structures.} Our new approach is called {\emph Matching After Learning to Stretch (MALTS)}. For each unit (e.g. patient), we propose learn a latent representation of their covariate information and a distance metric on the latent space such that units that are matched tend to provide accurate estimates of treatment effect. MALTS can use deep learning to encode the latent representations for the units, or it can learn basis transformations in linear space (stretching and rotation matrices) for simpler continuous data types. We will develop the MALTS algorithm, and apply it in a medical context. Our goal is to construct high quality matches for the following types of data: (i) medical images, such as x-rays and CT scans, (ii) medical record data, (iii) time series data (continuous EEG data), (iv) a combination of any of the first three types of data. We aim to leverage the newly developed tools to continue our evaluation of the efficacy of isolation for flu-like ailments as well as to apply them more broadly to publicly available modern datasets such as the MIMIC III database. Reliable and consistent causal analysis of public health interventions requires the use of massive previously unavailable datastreams. For example, evaluation of the efficacy of isolation interventions on flu-like-illness spread must include information on friendships and interactions between individuals, biometric information, imaging, longitudinal health record data as well as standard demographic data. The proposed research provides machine learning and deep learning tools for properly employing this data for the identification and quantification of causal effects of such treatments that can lead to the development of better public health interventions.",Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data,9750434,R01EB025021,"['Algorithms', 'Biometry', 'Categories', 'Clinical', 'Complex', 'Data', 'Data Set', 'Databases', 'Development', 'Electroencephalography', 'Friendships', 'Goals', 'Health', 'Image', 'Individual', 'Intervention', 'Lead', 'Learning', 'Machine Learning', 'Medical', 'Medical Imaging', 'Medical Records', 'Methods', 'Modernization', 'Outcome', 'Patients', 'Research', 'Roentgen Rays', 'Rotation', 'Series', 'Stretching', 'Structure', 'System', 'Text', 'Therapeutic', 'Time', 'Treatment Efficacy', 'Work', 'X-Ray Computed Tomography', 'computerized tools', 'data format', 'deep learning', 'efficacy evaluation', 'flu', 'health record', 'improved', 'individualized medicine', 'novel strategies', 'public health intervention', 'tool', 'treatment effect']",NIBIB,DUKE UNIVERSITY,R01,2018,98714,-0.010812842147555415
"Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma PROJECT SUMMARY This project aims to apply novel machine learning techniques to recently developed optical imaging measurement to improve the accurate prediction and detection of glaucomatous progression. Complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and advanced pattern recognition/machine learning-based analysis techniques can find and use that hidden information. We will use mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1,800 patient and healthy eyes, available as the result of long-term NIH funding. We also will investigate deep learning and novel statistical techniques for this purpose. The required longitudinal measurements from several newly developed optical imaging techniques were not available to our previously funded NEI- supported work. The proposed work potentially can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care by informing clinical decision-making based on mathematically based, externally validated methods. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. PROJECT NARRATIVE The proposed project will improve machine learning techniques for predicting and detecting glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments and will make use of a very large amount of data, obtained using previously awarded NIH funds, to do so. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neuro- degeneration within the visual pathways at structural and functional levels. The development of a clinically useful novel, empirical system for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and on the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma,9517942,R21EY027945,"['Address', 'Algorithms', 'Anatomy', 'Award', 'Caring', 'Classification', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Defect', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Environment', 'Eye', 'Frequencies', 'Funding', 'Future', 'Gaussian model', 'Generations', 'Glaucoma', 'Goals', 'Health Personnel', 'Image', 'Imaging Device', 'Imaging Techniques', 'Instruction', 'Laboratories', 'Lasers', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Perimetry', 'Physiologic Intraocular Pressure', 'Reporting', 'Research', 'Scanning', 'Science', 'Series', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Variant', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'care providers', 'clinical care', 'clinical decision-making', 'cost', 'deep learning', 'expectation', 'glaucoma test', 'high dimensionality', 'improved', 'independent component analysis', 'instrument', 'learning strategy', 'markov model', 'mathematical model', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'recruit', 'retinal nerve fiber layer', 'tool']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2018,193750,-0.0065580581975011485
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,-0.01676380275612213
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9520706,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2018,591130,-0.02281490004533545
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9467327,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Algorithms', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Image Analysis', 'Kinetics', 'Laboratories', 'Locomotion', 'Machine Learning', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2018,510448,-0.021631805451896198
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9480874,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoke', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2018,187699,-0.07061272998642412
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,-0.03307281574412723
"Smartphone phenotype collection for diagnostic screening of mild cognitive impairment Project Summary This project addresses a critical need for early detection of mild cognitive impairment (MCI) and other Alzheimer's-related dementias (ADRD). Advances in smartphone hardware, computer vision, and machine learning have enabled the possibility of producing smartphone-based cognitive testing applications able to collect electronic sensor data and transform it into highly informative phenotypes that can serve as early indicators of future disease progression. In this project, we aim to develop a revolutionary new smartphone- based cognitive testing platform, called CTX, that will enable the rapid development and deployment of smartphone-based tests that can capture raw sensor streams in a synchronized fashion, subsample and compress the combined streams, and transmit them to a cloud server for subsequent analysis and modeling. CTX will provide a high-level application development framework that will significantly reduce the time and technical knowledge required to produce a smartphone-based cognitive testing application by providing an application programming interface (API) that enables developers to simply declare what sensor data should be collected and when. The framework will handle all the details of collecting the sensor data, synchronizing it, and transmitting it to a back-end server. The API will also have a variety of other high-level features to facilitate development of cognitive test apps. To demonstrate the feasibility of our vision for CTX, in Aim 1 of this project we will develop the software framework, back-end server software and a prototype smartphone app to exercise and validate many of the platform's features. For Aim 2, we will develop three different tests for this app to test saccade (eye movement) latency, verbal recall, and wrist mobility, each collecting a different type of sensor data (video, audio, and inertial measurement). These tests were selected because their results have been been shown to be predictive of MCI. We will implement phenotype extraction pipelines that employ advanced signal processing, machine learning, and computer vision algorithms to extract the target phenotypes from the sensor data collected for these tests and demonstrate they operate with sufficient accuracy to replicate published experimental designs. Successful completion of this project will eliminate the need for expensive and cumbersome phenotype collection equipment (e.g., eye tracking stations) and create the possibility of generating data from which MCI onset can be predicted. Data collected in Phase II via these and other such tests will enable us to apply our machine learning expertise to produce models able to predict transition to MCI that are both sensitive and specific, transforming any smartphone into an MCI risk assessment tool available for at-home use by millions of people. Project Narrative This NIH Phase I project will address the critical need for early detection of Alzheimer's Disease (AD) and Alzheimer's-related dementias (ADRD) by developing a revolutionary new smartphone-based cognitive testing platform that will provide individuals with an ongoing status of their cognitive health. Doctors who are given access to the results of these tests will be able to monitor patients more closely and provide more timely diagnoses. By studying test results from many people, researchers may someday be able to identify patterns that can distinguish mild cognitive impairment from normative age-related cognitive decline.",Smartphone phenotype collection for diagnostic screening of mild cognitive impairment,9679400,R43AG062072,"['Achievement', 'Address', 'Adult', 'Age', 'Age-associated memory impairment', 'Algorithms', 'Alzheimer disease detection', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Apple', 'Assessment tool', 'Back', 'Big Data', 'Cellular Phone', 'Cognitive', 'Collection', 'Computer Vision Systems', 'Computer software', 'Cyclophosphamide', 'Data', 'Data Set', 'Dementia', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic tests', 'Disease Progression', 'Early Diagnosis', 'Elderly', 'Emotional', 'Equipment', 'Exercise', 'Exhibits', 'Experimental Designs', 'Eye', 'Eye Movements', 'Face', 'Facial Expression', 'Forearm', 'Frequencies', 'Future', 'Genetic Risk', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Image', 'Individual', 'Knowledge', 'Lead', 'Machine Learning', 'Measurement', 'Memory impairment', 'Methods', 'Modeling', 'Monitor', 'Patient Monitoring', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Publishing', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Risk Assessment', 'Rotation', 'Saccades', 'Scanning', 'Secure', 'Sensitivity and Specificity', 'Small Business Innovation Research Grant', 'Software Framework', 'Software Tools', 'Stream', 'Tablets', 'Telephone', 'Test Result', 'Testing', 'Time', 'United States National Institutes of Health', 'Vision', 'Visuospatial', 'Work', 'Wrist', 'Yang', 'age related', 'age related cognitive change', 'application programming interface', 'base', 'cloud platform', 'cognitive development', 'cognitive task', 'cognitive testing', 'cohort', 'cost', 'crowdsourcing', 'data modeling', 'diagnostic screening', 'interest', 'markov model', 'mild cognitive impairment', 'predictive modeling', 'prototype', 'response', 'screening', 'sensor', 'signal processing', 'smartphone Application', 'software development', 'success']",NIA,"PARABON NANOLABS, INC.",R43,2018,394297,-0.05454661445420886
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9564111,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biological Neural Networks', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2018,564228,-0.01844619117644635
"Machine learning for data-driven subtyping of major depression Major depressive disorder is highly prevalent, and represents a major driver of disability as well as health care cost. Progress in improving diagnosis and treatment of this disorder has been hindered by its heterogeneity in clinical presentation and course. Such heterogeneity makes the underlying neurobiology difficult to characterize, and has led to efforts to identify more homogeneous subgroups. These efforts date back to the dawn of the modern psychopharmacologic era - initially focused on atypical and melancholic depression, and more recently on subtypes such as anxious and irritable depression.  Subtyping efforts are complicated by a paucity of large clinical cohorts with similar ascertainment and phenotyping. In particular, the available data often focuses on a very narrow range of depressive symptoms, along with a restricted set of comorbidities, and typically encompasses only the acute phase of treatment. As a result, despite intriguing findings in one or occasionally two cohorts, subtyping has not been widely deployed in clinical practice, nor used to meaningfully improve translational investigation.  The utility of electronic health records and registries to create in silico cohort studies has been demonstrated in numerous settings, including psychiatry. Beyond sample size and efficiency of ascertainment, these data types often have advantages in the range of non-depressive phenotypes captured and availability of longitudinal data.  The present study therefore proposes to create a very large cohort of individuals with MDD, defined by a validated algorithm, spanning two health systems, and to apply novel machine learning methods to identify MDD subtypes. These subtypes will be validated by comparison with standard phenotypic definitions, annotation by trained raters using a standard 'intruder' paradigm, and correlation with medication prescribing Then, as proof of concept the biological basis of these subtypes will be characterized by examining heritability and polygenic risk using a large genetic biobank. Beyond determining convergent validity, this last step will provide proof-of-concept for broader application of data-driven subtypes for translational investigation in biobanks and registries.  The study builds on existing collaborations between a team experienced in mood disorder phenotypic and genomic study as well as application of electronic health records, and a team active in developing and applying emerging methods in machine learning. It will lay the groundwork for further validation and application of data-driven disease subtyping across medicine. Public health significance The wide variation in symptoms of major depressive disorder complicates efforts to understand the underlying causes of this illness. Applying machine learning methods to electronic health records should enable the identification of more specific disease subgroups. These subgroups will facilitate efforts to understand the causes of depression, and to begin to develop more targeted treatments.",Machine learning for data-driven subtyping of major depression,9717620,R56MH115187,"['Acute', 'Algorithms', 'Anhedonia', 'Anxiety', 'Atypical depressive disorder', 'Back', 'Biological', 'Bipolar Disorder', 'Cessation of life', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Cohort Studies', 'Collaborations', 'Comorbidity', 'Computer Simulation', 'Data', 'Data Set', 'Desire for food', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Endocrine System Diseases', 'European', 'Face', 'Genetic', 'Genomics', 'Health Care Costs', 'Health system', 'Heart Diseases', 'Heritability', 'Heterogeneity', 'Hospitals', 'Individual', 'Investigation', 'Laboratories', 'Link', 'Longitudinal cohort', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Medical', 'Medicine', 'Melancholic Depression', 'Mental Depression', 'Meta-Analysis', 'Methods', 'Modernization', 'Mood Disorders', 'Moods', 'Neurobiology', 'Outcome', 'Outpatients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Prevalence', 'Procedures', 'Psychiatry', 'Public Health', 'Quality of life', 'Reactive depression', 'Registries', 'Risk', 'Sample Size', 'Schizophrenia', 'Series', 'Severities', 'Sleep', 'Subgroup', 'Suicide', 'Symptoms', 'System', 'Therapeutic', 'Training', 'Validation', 'Variant', 'Work', 'anxious', 'base', 'biobank', 'clinical practice', 'cohort', 'cost', 'depressive symptoms', 'disability', 'disorder subtype', 'experience', 'genomic data', 'high dimensionality', 'hospital readmission', 'improved', 'innovation', 'interest', 'learning strategy', 'novel', 'novel therapeutics', 'patient population', 'portability', 'precision medicine', 'psychopharmacologic', 'suicidal risk', 'tool', 'treatment response']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R56,2018,595755,-0.016285722184286994
"Pediatric sepsis prediction: a machine learning solution for patient diversity Abstract  Significance:  In  this  SBIR  project,  we  propose  to  predict  and  detect  pediatric  severe  sepsis  by  developing  a  machine-­learning-­based  clinical  decision  support  system  for  electronic  health  record  (EHR)  pediatric  sepsis  screening. The pediatric population is underserved, with fundamental research and understanding of pediatric  sepsis  syndromes  lagging  behind  that  of  the  adult  population.  The  proposed  work  will  develop  machine  learning sepsis predictions on the highly heterogeneous pediatric population, by combining multi-­task learning  methods with expert clinical knowledge of how pediatric sepsis presentation is dependent on age and on pre-­ existing  conditions.  The  multi-­task  learning  approach  will  use  age  or  comorbidities  to  define  “tasks,”  each  one  associated  with  prediction  on  a  particular  subpopulation,  and  then  link  the  learning  process  together  between  tasks. Research Questions: Which methods of using these task-­defining parameters are most effective? How  can  we  most  effectively  learn  the  degree  of  similarity  between  pediatric  subpopulations  and  leverage  this  to  improve classification performance? Prior Work: InSight was originally developed to predict sepsis and septic  shock from adult EHR data. After retraining on pediatric cases, in preliminary experiments with a retrospective  set  of  pediatric  (2-­17  yr)  inpatient  encounters  (n  =  11,127;;  103  [0.9%]  severely  septic),  at  the  University  of  California  San  Francisco  (UCSF),  InSight  achieved  an  AUROC  0.912  and  0.727  for  the  detection  and  4-­hour  pre-­onset  prediction  of  sepsis.  This  performance  can  be  improved  for  better  pediatric  sepsis  prediction.  Specific  Aims:  To  empirically  evaluate  different  learning  schemes  using  age  with  and  without  multi-­task  methods,  within  the  UCSF  pediatric  severe  sepsis  data  set  (Aim  1).  To  exploit  an  expert-­proposed  network  graph  structure  for  comorbidity-­described  pediatric  subpopulations  that  provides  superior  predictive  performance  over  naïve  methods  and  graphs,  both  for  the  overall  population  and  for  underserved  subpopulations  (Aim  2).  Methods:  We  propose  to  use  multi-­task  methods  that  penalize  deviations  between  classifiers  on  neighboring  tasks  and  that  iteratively  learn  the  strength  of  these  links.  These  methods  will  be  compared with total task independence, or passing age into classifier training as an ordinary input. Criteria for  Success:  Success  will  be  shown  by  4-­hour  pre-­onset  AUROC  gains  of  0.02  (overall  population)  and  0.03  for  the previously weakest of three age subpopulations (2-­5, 6-­12, and 13-­17 yrs;; 4-­hour pre-­onset prediction, p <  0.05, McNemar’s test, 4-­fold cross-­validation). The best structure for mapping similarities between comorbidity  tasks will improve the overall AUROC by 0.03 (p < 0.05) and by 0.07 for ≥ 2 comorbidity subpopulations of ≥ 100 patients (p < 0.05). Outcome: These improvements will enable InSight to deliver strong sepsis predictive  performance across the widely heterogeneous pediatric population.         Narrative  Pediatric  sepsis  can  be  difficult  to  diagnose,  in  part  because the  pediatric  population  is  highly  heterogeneous;;  clinical  decision  support  (CDS)  systems  have  the  potential  to  aid  in  the  diagnosis  and  prediction  of  pediatric  sepsis, if the underlying diversity of the pediatric population is correctly addressed. We will develop a machine-­ learning-­based  sepsis  CDS  system  by  using  multi-­task  learning  techniques  to  define  a  set  of  “tasks,”  each  corresponding  to  predicting  severe  sepsis  in  a  clinically  distinct  subpopulation  of  pediatric  inpatients,  together  with  a  set  of  inter-­task  connections  that  share  information  between  similar  subpopulations.    This  will  enable  improved sepsis prediction across the widely heterogeneous and underserved pediatric patient population.     ",Pediatric sepsis prediction: a machine learning solution for patient diversity,9620967,R43HD096961,"['Address', 'Admission activity', 'Adult', 'Age', 'Area', 'California', 'Cancer Patient', 'Cell Count', 'Cessation of life', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Coma', 'Comorbidity', 'Complex', 'Consensus', 'Dangerousness', 'Data', 'Data Set', 'Detection', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Ensure', 'Fatigue', 'Graph', 'Heterogeneity', 'Hospitalization', 'Hospitals', 'Hour', 'Inpatients', 'Knowledge', 'Learning', 'Leukocytes', 'Link', 'Machine Learning', 'Malignant Childhood Neoplasm', 'Medical center', 'Methods', 'Outcome', 'Oxygen', 'Patients', 'Performance', 'Peripheral', 'Phase', 'Physiology', 'Platelet Count measurement', 'Population', 'Process', 'Receiver Operating Characteristics', 'Research', 'San Francisco', 'Scheme', 'Sepsis', 'Sepsis Syndrome', 'Septic Shock', 'Small Business Innovation Research Grant', 'Specificity', 'Structure', 'Survivors', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Transplantation', 'United States', 'Universities', 'Validation', 'Weight', 'Work', 'age effect', 'age group', 'aggressive therapy', 'base', 'care burden', 'clinically relevant', 'clinically significant', 'cohort', 'diagnostic biomarker', 'experimental study', 'fundamental research', 'improved', 'insight', 'learning strategy', 'mortality', 'multitask', 'neonate', 'novel', 'oncology', 'patient population', 'patient subsets', 'pediatric patients', 'prospective', 'screening', 'septic', 'septic patients', 'success']",NICHD,"DASCENA, INC.",R43,2018,299999,-0.029681794900579685
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9611012,F32HL144101,"['Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biological Neural Networks', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Supervision', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2018,63034,-0.019620122922725833
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9519804,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2018,410000,-0.07806203877178262
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9510096,R21GM128020,"['Address', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Learning', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2018,239527,-0.005996872645686477
"Developing and validating prognostic metabolomic signatures of diabetic kidney disease PROJECT SUMMARY/ABSTRACT Rationale. Diabetes is a leading cause of renal disease, accounting for 40% of the estimated 20 million US adult cases of chronic kidney disease. There is, however, substantial heterogeneity across diabetic patients with regards to development of kidney disease. Hence, there is an urgent need to identify prognostic biomarkers that can provide early and reliable evidence of future kidney disease, so that high-risk patients can receive optimal medical care. Existing clinical, proteomic and genomic markers do not consistently nor accurately predict kidney function decline. Metabolomics, a systematic evaluation of the end-products of cellular function in fluids, has the potential to inform physiological and pathological effects of chronic diseases. Metabolomic analysis combined with advanced quantitative methods could play a key role in building clinically useful prognostic signatures of diabetic kidney disease. Yet, development of computational methods with adequate rigor has lagged behind the technical capacity to perform large scale quantitative metabolomics. In this proposal we aim to address this computational gap in diabetic kidney disease research. Aims. We will implement rigorous computational methods to identify robust prognostic metabolite + clinical + genetic signatures of diabetic kidney disease progression. Specifically, we aim to (i) test the accuracy of previous signatures, and apply state-of-the-art analytic techniques and novel statistical methods to identify new multivariate metabolite sets for predicting kidney disease progression; (ii) quantify patterns of co-regulation of metabolites in diabetic kidney disease, and develop new tools in network biology to discover novel enzymes, proteins, metabolites, and molecular pathways which are implicated in diabetic kidney disease progression; (iii) test if these models can accurately predict kidney disease progression in independent prospective cohorts. Methods. Using clinical, genetic and metabolomic data from large prospective cohorts of > 1200 diverse, well- characterized patients with Type 2 diabetes, we will apply statistical methods for variable selection (e.g., penalized regression), and machine learning methods (e.g., random forest), which are known to perform well in the high-dimensional setting, to identify robust and parsimonious signatures of kidney disease progression. We will quantify inter-metabolite co-regulation patterns and infer biological pathways implicated in diabetic kidney disease. Throughout the modeling process, a rigorous training-validation paradigm will be adopted in order to improve reproducibility of models and reduce chance findings. Impact. A major product of this work will be the development of a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline. Our findings will also provide insight into markers of renal dysfunction, and elucidate possible therapeutic targets for treating diabetic kidney disease, thus potentially informing the design of future clinical trials. PROJECT NARRATIVE Kidney disease, a major and common complication of diabetes, can lead to repeated hospitalizations and premature death. There is an urgent need to develop clinical tools that can provide early evidence that a given diabetic patient is likely to progress to kidney disease in the future. In this proposal, we will identify new urinary biomarkers and use novel statistical modeling methods to create a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline, with the ultimate goal of improving disease management and reducing mortality rates for these patients.",Developing and validating prognostic metabolomic signatures of diabetic kidney disease,9418599,R01DK110541,"['Accounting', 'Address', 'Adopted', 'Adult', 'Albuminuria', 'Algorithms', 'American', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biology', 'Caring', 'Cell physiology', 'Cessation of life', 'Chronic Disease', 'Chronic Kidney Failure', 'Chronic Kidney Insufficiency', 'Clinical', 'Clinical Trials', 'Collaborations', 'Comorbidity', 'Complications of Diabetes Mellitus', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Diagnosis', 'Disease', 'Disease Management', 'Disease Progression', 'Enzymes', 'Evaluation', 'Functional disorder', 'Funding', 'Future', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Heterogeneity', 'Hospitalization', 'Kidney', 'Kidney Diseases', 'Laboratories', 'Lead', 'Link', 'Liquid substance', 'Longitudinal cohort', 'Machine Learning', 'Medical', 'Medical Genetics', 'Methods', 'Modeling', 'Molecular', 'Non-Insulin-Dependent Diabetes Mellitus', 'Pathologic', 'Pathway interactions', 'Patient risk', 'Patients', 'Pattern', 'Physiological', 'Pima Indian', 'Play', 'Process', 'Prognostic Marker', 'Prospective cohort', 'Proteomics', 'Publishing', 'Recommendation', 'Regulation', 'Renal function', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Sampling', 'Sampling Studies', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Testing', 'Training', 'Urine', 'Validation', 'Work', 'biological heterogeneity', 'chemical association', 'cohort', 'design', 'diabetic', 'diabetic patient', 'forest', 'genetic signature', 'genomic biomarker', 'high dimensionality', 'high risk', 'improved', 'innovation', 'insight', 'learning strategy', 'metabolome', 'metabolomics', 'model development', 'mortality', 'nephrogenesis', 'network models', 'novel', 'open source', 'personalized medicine', 'predictive modeling', 'predictive signature', 'predictive test', 'premature', 'prognostic', 'prognostic signature', 'prospective', 'protein metabolite', 'targeted treatment', 'therapeutic target', 'tool', 'urinary']",NIDDK,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2018,336980,-0.00552808174181489
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9577591,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Supervision', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'forest', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'tool', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2018,243750,-0.021632587156074183
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates ﻿    DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9549036,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'clinical implementation', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment optimization', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,517281,-0.07157800972935863
"A computational approach to early sepsis detection Abstract Significance: In this SBIR project, we propose to improve the performance of InSight, a machine-learning- based sepsis screening system, in situations of limited training data from the target clinical site. The proposed work will make possible prospective clinical deployments to sites which are smaller or lack clinical data repositories, by significantly reducing the amount of training data necessary down to a few weeks of clinical observation. Classically, a machine-learning-based system like InSight requires complete retraining for each new clinical setting, in turn requiring a new and large collection of data from each target deployment site. We will circumvent this requirement via transfer learning techniques, which transfer knowledge acquired previously in a source clinical setting to a new, target setting. Research Questions: Which transfer learning methods and paired classification algorithms are most suitable for use with InSight, requiring minimal target-site training data while maintaining strong performance? Are these methods and algorithms robust across the several common sepsis-spectrum definitions? Prior Work: We have developed InSight using the MIMIC-III retrospective data set, on which it attains an area under the receiver operating characteristic curve (AUROC) of 0.88 for sepsis detection, and 0.74 for 4-hour early sepsis prediction. We have also conducted pilot transfer learning  ≥ experiments in a different clinical task, mortality forecasting, in which transfer learning yields a 10-fold reduction in the amount of target-site training data required to achieve AUROC 0.80. Specific Aims: Aim 1 - to implement and assess side-by-side four diverse transfer learning methods for a retrospective clinical sepsis prediction task, where the source data set is MIMIC-III and the simulated clinical target is a data set drawn from UCSF. Aim 2 - to determine which among the best methods from Aim 1 also provide robust performance when applied to two additional sepsis-spectrum gold standards. Methods: We will prepare implementations of transfer learning methods which use instance transfer, residual learning and/or feature augmentation, kernel length scale transfer, and feature transfer. We will test these methods with applicable classifiers on subsets of the UCSF set, using cross-validation and quantifying discrimination performance in terms of AUROC. The best method/classifier pairs will require no more than 30 examples of septic patients from the target set and attain AUROC superiorities of 0.05 in 0- and 4-hour pre-onset sepsis prediction/detection, relative to the best tested alternative screening systems (Aim 1). The top three pairs will then be tested for robustness to gold standard choice, using septic shock (0- and 4-hour) and SIRS-based sepsis (0-hour) gold standards; in these tests, at least one pair must again attain 0.05 margin of superiority in AUROC versus the alternative screening systems (Aim 2). Future Directions: The results of these experiments will enable InSight to be robustly deployed to diverse clinical sites, yielding high performance without the need for extensive target-site data acquisition. Narrative Clinical decision support (CDS) systems present critical information to medical professionals by examining patient data and providing relevant information. Machine learning is a powerful method for creating CDS tools, but accessing its full strength requires re-training with retrospective data from each target clinical site. We will use transfer learning techniques to dramatically reduce the amount of target-site training data required by InSight, our machine-learning-based CDS tool for sepsis prediction, and empirically evaluate several such methods on a patient data set, using three different sepsis-related gold standards.",A computational approach to early sepsis detection,9557664,R43TR002221,"['Address', 'Age', 'Algorithms', 'Area', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Collection', 'Custom', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Discrimination', 'Drops', 'Early Diagnosis', 'Early Intervention', 'Future', 'Gold', 'Healthcare', 'Healthcare Systems', 'Hour', 'Image', 'Immune response', 'Institution', 'Knowledge', 'Learning', 'Length', 'Machine Learning', 'Medical', 'Methods', 'Multicenter Studies', 'Nature', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Psychological Transfer', 'Receiver Operating Characteristics', 'Research', 'Residual state', 'Risk', 'SCAP2 gene', 'Sensitivity and Specificity', 'Sepsis', 'Septic Shock', 'Severities', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Survival Rate', 'System', 'Techniques', 'Testing', 'Training', 'Validation', 'Work', 'base', 'clinical data warehouse', 'clinical decision support', 'clinical research site', 'cost', 'data acquisition', 'experimental study', 'improved', 'insight', 'learning strategy', 'mortality', 'performance site', 'portability', 'prospective', 'screening', 'septic', 'septic patients', 'success', 'support tools']",NCATS,"DASCENA, INC.",R43,2018,310782,-0.006086584398538137
"Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment Summary: The public health burden of major depressive disorder (MDD) is immense and current approaches for selecting antidepressant treatment have had limited success. By some estimates, fewer than one in three MDD patients will respond to their prescribed antidepressant and the quest for a treatment that will work is typically characterized by a lengthy course of trial-and-error. The need to identify patient characteristics (biomarkers) that can be used to objectively select personalized antidepressant treatment is clear. Accordingly, large clinical studies like the NIMH-funded Establishing Moderators and Biosignatures of Antidepressant Response for Clinical Care (EMBARC) study have collected massive amounts of baseline measures including those from various neuroimaging sources in the hope that some can be used to guide antidepressant treatment selection. These data bring with them many statistical challenges that have yet to be effectively addressed. These challenges include (1) dealing with high-dimensionality, (2) handling data missingness, and (3) determining how best to simultaneously model relationships between measures from multiple imaging modalities and the response of interest. The goal of this project is to acquire the essential training and experience to make significant progress in this area by addressing each of these challenges. Aim 1 of this project will employ state-of-the-art ensemble machine learning algorithms and targeted estimation to identify moderators of antidepressant treatment effect using scalar clinical, demographic, and summary neuroimaging data from clinical trials of antidepressant treatments, including EMBARC. Strategies for handling missing data in this context will also be investigated and guidelines on best practices will be proposed. Aim 2 will extend the methods used in Aim 1 and develop user-friendly software to directly incorporate high- dimensional multimodal neuroimaging data into treatment decision rules. Included in this aim will be an investigation into best practices for handling missing high-dimensional imaging data in the context of estimating treatment decision rules. Aim 3 will employ the novel methods developed in Aim 2 and the estimated treatment decision rules will be evaluated and compared with those developed in Aim 1. I have put together a training program that directly supports the completion of these research aims. It includes instruction, mentoring, and hands-on-experience (1) in psychopathology and the neural basis for psychiatric disorders and treatment for those disorders; (2) in the use of neuroimaging data to understand depression and response to antidepressant treatment; (3) in the use of modern algorithms to store, process, manipulate, and analyze big biomedical data like those arising in multimodal neuroimaging studies. This K01 Mentored Research Scientist Development Award will provide the training, time, and resources to be able to make substantial progress in addressing this important problem and will provide the skills and experience that will be crucial in my transition to an independent investigator. ! Public Health Relevance Statement: This proposal seeks to advance precision medicine through the development of new statistical methods that integrate clinical, demographic, and high-dimensional multimodal neuroimaging data to estimate treatment decision rules. The proposed research and training are laid out in the context of depression but the statistical tools to be developed will be general enough for constructing treatment decision rules for a wide array of diseases using a variety of data types. These statistical tools have the potential to reduce the burden of diseases like depression by providing personalized treatment that has the best chance for success.",Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment,9527395,K01MH113850,"['Address', 'Algorithms', 'Antidepressive Agents', 'Area', 'Automobile Driving', 'Award', 'Biological Markers', 'Brain', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease remission', 'Electroencephalography', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Guidelines', 'Heterogeneity', 'Image', 'Instruction', 'Investigation', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Multimodal Imaging', 'National Institute of Mental Health', 'Neurosciences', 'Outcome', 'Patients', 'Performance', 'Placebos', 'Play', 'Positron-Emission Tomography', 'Process', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Public Health', 'Quality of Care', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Risk', 'Role', 'Selection for Treatments', 'Sertraline', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Training', 'Training Programs', 'Work', 'Writing', 'big biomedical data', 'biosignature', 'burden of illness', 'clinical care', 'clinical imaging', 'data reduction', 'experience', 'flexibility', 'health data', 'high dimensionality', 'imaging modality', 'individual patient', 'individualized medicine', 'innovation', 'insight', 'interest', 'learning strategy', 'multimodality', 'neuroimaging', 'neuroimaging marker', 'novel', 'personalized medicine', 'precision medicine', 'public health relevance', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool', 'treatment effect', 'treatment response', 'user friendly software']",NIMH,GEORGE WASHINGTON UNIVERSITY,K01,2018,165149,-0.0018429905443947856
"Quantifying NETosis via Automated High Content Imaging Convolutional Neural Networks PROJECT SUMMARY NETosis was identified as a distinct mode of cell death in neutrophils more than a decade ago. Dysregulation of NETosis has been implicated in the etiology of human pathologies such as preeclampsia, sickle cell disease, systemic lupus erythematosus, multiple sclerosis, rheumatoid arthritis, sepsis, cystic fibrosis, lupus nephritis, and coagulopathies that include cancer-associated thrombosis. The literature consistently cites the lack of a standardized methodology for quantitation of NETosis as an impediment to basic and translational research. Thus, the premise is that there is a compelling, unmet need for a standardized, quantitative and automated method for the measurement of NETosis to accelerate neutrophil and inflammation-based research and facilitate the discovery and development of therapeutic compounds. The scope of this STTR project is to develop a high-throughput image analysis and quantitation method by using high content imaging and the revolutionary technology of convolutional neural networks (CNN) for the identification and quantitation of NETosis in human neutrophils. The target readout is based on the primary morphological difference between NETotic and non-NETotic nuclei--the decondensation of chromatin. This image-based quantitative method will be observer-independent and will enable robust and rapid evaluation of a large number of samples that would exceed any attempts at manual assessment. In Phase I we will complete the following Specific Aims: Aim 1: Optimize and standardize the high- throughput platform for quantitation of NETosis in adherent human neutrophils. This includes standard assay optimization procedures, training the CNN to identify and quantitate NETotic neutrophil, and demonstrating that the CNN reliably distinguishes between necrosis and NETosis, whose phenotypes appear similar to the human eye. Aim 2: Validate the NETosis assay biochemically and clinically. This includes concentration-response assays with NETosis agonists, assessment of NETosis inhibitors, and evaluation of the NETotic status of Sickle Cell Disease patient samples (a disease in which aberrant NETosis has been implicated). The expected outcome of this Phase I effort is to demonstrate proof-of-concept for this automated high- throughput NETosis assay. Further, we expect to provide insight into the utility of the assay for assessment of inhibitors of NETosis as therapeutic agents. Upon completion of our Phase I aims, our Phase II program will focus on further optimizing and validating this NETosis assay and preparing it for commercialization.   PROJECT NARRATIVE Aberrant NETosis has been implicated in the etiology of several inflammatory and autoimmune diseases. The lack of a standardized, quantitative and automated method for the measurement of NETosis is impeding basic and translational research. We have developed a high-throughput assay using convolutional neural networks to quantify NETosis in human neutrophils. This assay will accelerate neutrophil and inflammation-based research and facilitate the discovery and development of compounds with therapeutic potential.",Quantifying NETosis via Automated High Content Imaging Convolutional Neural Networks,9465292,R41AI131840,"['Agonist', 'Apoptosis', 'Autoimmune Diseases', 'Basic Science', 'Benchmarking', 'Biochemical', 'Biochemical Pathway', 'Biological Assay', 'Biological Neural Networks', 'Blood Coagulation Disorders', 'Caymans', 'Cell Death', 'Cell Death Process', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Clinical', 'Cystic Fibrosis', 'DNA', 'Data', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Disease', 'Drug Screening', 'Ensure', 'Enzymes', 'Etiology', 'Evaluation', 'Eye', 'Histones', 'Human', 'Human Pathology', 'Image', 'Image Analysis', 'Impairment', 'Infection', 'Inflammation', 'Inflammatory', 'Innate Immune Response', 'Letters', 'Literature', 'Lupus Nephritis', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Morphology', 'Multiple Sclerosis', 'Nature', 'Necrosis', 'Nuclear', 'Opportunistic Infections', 'Outcome', 'Pathway interactions', 'Patients', 'Peptide Hydrolases', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physical condensation', 'Population', 'Pre-Eclampsia', 'Predisposition', 'Procedures', 'Process', 'Publishing', 'Reproducibility', 'Research', 'Rheumatoid Arthritis', 'Sampling', 'Sepsis', 'Severity of illness', 'Sickle Cell Anemia', 'Side', 'Small Business Technology Transfer Research', 'Specificity', 'Stains', 'Standardization', 'Systemic Lupus Erythematosus', 'Technology', 'Therapeutic', 'Therapeutic Agents', 'Thrombosis', 'Training', 'Translational Research', 'antimicrobial', 'antimicrobial peptide', 'base', 'commercial application', 'commercialization', 'extracellular', 'high throughput screening', 'inhibitor/antagonist', 'innovation', 'insight', 'neutrophil', 'novel', 'patient population', 'predictive test', 'programs', 'response', 'suicidal', 'symptom management', 'therapeutic development', 'tool']",NIAID,"EPICYPHER, INC.",R41,2018,299751,-0.025210887648506968
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9612862,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'federated computing', 'guided inquiry', 'hands-on learning', 'indexing', 'innovation', 'insight', 'interest', 'learning strategy', 'patient population', 'patient subsets', 'post-traumatic stress', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2018,547853,-0.0008885220460793425
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9479280,K08HL136928,"['Affect', 'Bioinformatics', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2018,172800,-0.01847737165619277
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9406318,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2018,356625,-0.03215507790089247
"Device for real-time streaming of preclinical research data into a central cloud-based platform Project Summary BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. Traditionally researchers collect data from many disparate instruments and store data on PCs running single license software. Raw data is stored across several locations, analysis is restricted to the PC used to collect the data, and opportunities for collaboration, remote-participation, and data sharing are limited. BehaviorCloud is leveraging cloud data streaming and storage to overcome these barriers to discovery. In 2017 BehaviorCloud released a first version of the underlying cloud platform as well as BehaviorCloud Camera, an open-source “reference implementation” that demonstrates automated video tracking of animal behavior on the BehaviorCloud platform using a consumer-grade smartphone. This tool and the underlying platform are both in active use across academic and pharmaceutical labs. The aim of this Phase I SBIR application is to develop patent-pending “Bridge” technology that allows data streaming from third-party instrumentation into the central web platform. Researchers will bypass the original software and PCs associated with their instruments to control trials through their BehaviorCloud account and receive data back in real-time. BehaviorCloud will provide a public repository to aggregate all of these data and accelerate discovery by providing computational tools for large-scale meta-analysis and machine learning based predictive analytics. Project Narrative BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. The goal of this Phase I SBIR application is to develop the technology to enable streaming of data from all kinds of behavioral and phenotyping instrumentation into the BehaviorCloud platform. BehaviorCloud will aggregate these data into a repository and accelerate discovery by providing tools for collaboration and meta-analysis.",Device for real-time streaming of preclinical research data into a central cloud-based platform,9621228,R43OD025448,"['Adoption', 'Animal Behavior', 'Animal Experimentation', 'Animal Model', 'Area', 'Back', 'Behavioral', 'Bypass', 'Carbon Dioxide', 'Cellular Phone', 'Collaborations', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Devices', 'Goals', 'Heart Rate', 'Information Systems', 'Internet', 'Intervention', 'Legal patent', 'Licensing', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Meta-Analysis', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Process', 'Research', 'Research Contracts', 'Research Personnel', 'Resources', 'Running', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Standardization', 'Stimulus', 'Stream', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'cloud based', 'cloud platform', 'computerized tools', 'control trial', 'data management', 'data sharing', 'data warehouse', 'design', 'experimental study', 'instrument', 'instrumentation', 'laptop', 'open source', 'phenotypic data', 'pre-clinical', 'pre-clinical research', 'prototype', 'repository', 'tool', 'wasting', 'web interface']",OD,"BEHAVIORCLOUD, LLC",R43,2018,220420,-0.022733959112644597
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,9594042,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Research Infrastructure', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional']",NIMH,UNIVERSITY OF WASHINGTON,R01,2018,661507,-0.019117037755523182
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9593406,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2018,1420000,-0.023947337918730344
"Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria Project Summary/Abstract Fungal and bacterial pathogens are a major threat to human health. Few therapeutics exist to treat fungal infections while bacteria are becoming increasingly resistant to existing therapeutics. Humans have been using natural products to treat infections for thousands of years, long before the causal agents of infection were understood. Natural products have continued to be used as therapeutics in the modern age of medicine. Rates of rediscovery of known natural products have increased in traditional sources of natural products, such as soil bacteria. Recently, symbiotic Actinobacteria from insect agricultural systems have been recognized as a promising source of bioactive compounds, especially antifungal agents. These bacteria often produce natural products that defend an insect’s fungal crop from pathogenic fungus. The work proposed here will use chemical biology approaches such as phenotypic interaction screens, genomics, and a new bioinformatics approach to systematically search for bioactive natural products produced by Actinobacteria symbionts and other organisms in insect agricultural systems. The first part of this proposal focuses on using existing techniques to identify new bioactive natural products. Phenotypic interaction screens can identify bioactive natural products by determining if a symbiotic bacteria produces a natural product that inhibits the growth of a fungal pathogen and vice-versa. We will then use genomic sequencing, bioinformatics, and heterologous expression to identify and characterize biosynthetic gene clusters (BGCs) that are not expressed in the phenotypic interaction screens. The second part of the proposed work involves the use of a new bioinformatics technique to identify interesting bioactive natural products. Existing bioinformatics techniques identify BGCs and predict the most likely chemical structure of the corresponding natural product. However, they do not conclude anything concerning the functional role that the natural product plays. The technique developed here will use machine learning to predict the function that the natural product fulfills in the ecological context of the organism. This algorithm will facilitate the identification of bioactive natural products with therapeutically relevant functions. Project Narrative Fungal infections are an underappreciated threat to human health with high mortality rates and few effective therapeutic agents for treatment. Symbiotic Actinobacteria from insect agricultural systems are a promising source of antifungal agents since they often produce natural products with antifungal activity protecting an insect’s fungal crop from pathogenic fungus. The work proposed here will use phenotypic interaction screens, genome sequencing, and the development of a novel bioinformatics method to systematically mine Actinobacteria for antifungal and antibacterial products – leading to the discovery of new bioactive small molecules along with a deeper understanding of how natural products mediate the interaction between species in insect agricultural systems.",Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria,9540546,F32GM128267,"['Actinobacteria class', 'Age', 'Agriculture', 'Algorithms', 'Anti-Bacterial Agents', 'Antibiotics', 'Antifungal Agents', 'Ants', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bioinformatics', 'Biological Assay', 'Biology', 'Breathing', 'Chemical Structure', 'Chemicals', 'Collaborations', 'Computational Biology', 'Computing Methodologies', 'Data Set', 'Development', 'Ecosystem', 'Gene Cluster', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Infection', 'Insecta', 'Learning', 'Life', 'Literature', 'Machine Learning', 'Mediating', 'Medicine', 'Methods', 'Mining', 'Modernization', 'Molecular Structure', 'Mycoses', 'Natural Products', 'Organism', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Public Health', 'Resistance', 'Role', 'Soil', 'Source', 'System', 'Techniques', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'Validation', 'Work', 'base', 'bioactive natural products', 'drug discovery', 'fungus', 'genetic information', 'genome sequencing', 'human disease', 'mortality', 'novel', 'pathogen', 'post-doctoral training', 'prediction algorithm', 'small molecule', 'symbiont']",NIGMS,HARVARD MEDICAL SCHOOL,F32,2018,58282,-0.018911710638758843
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9589711,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2018,439996,-0.03069807948960138
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9557573,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2018,242266,-0.0161051424154442
"FAST platform for same-shift, complete antibiotic menu antibiotic susceptibility testing The goal of this Phase I SBIR proposal is to demonstrate utility to all major pathogenic bacterial strains of SeLux’s rapid, low-cost, phenotypic antibiotic susceptibility test (AST) system (fast-AST or FAST). Utilizing existing optical detectors and standard dried antibiotic microplates, and avoiding pitfalls of metabolic probes, FAST will potentially transform therapy of infections by significantly accelerating AST, thereby facilitating treatment with the optimal antibiotic. Aim 1 will apply FAST to hundreds of samples of pathogenic bacterial strains, while developing and optimizing a predictive algorithm for clinical utility. Aim 2 will extend the FAST platform to slow-growing strains and species as well. SeLux has demonstrated FAST to exceed FDA 510(k) requirements for minimum inhibitory concentration determinations for 25+ strains of Staphylococcus aureus and Escherichia coli with full antibiotic panels. Completion of the proposed aims will expand FAST to all major clinically-relevant, non-fastidious bacterial pathogens. SeLux’s interdisciplinary team has expertise in nanosensing, microbiology, and algorithm design and is buttressed by distinguished experts in Clinical Microbiology, Infectious Disease, and Machine Learning. The new paradigm in clinical medicine is value-based healthcare, which requires rapid and accurate diagnoses leading to optimal patient treatment. Nowhere is this more important than in treating infections, where doctors are currently forced to overprescribe broad-spectrum antibiotics during an agonizing 48+ hour wait for antibiotic susceptibility test (AST) results. The novel, rapid, low-cost AST platform described in this proposal promises to reduce this delay in treatment by as much as 30 hours. This advance would be transformative for the treatment of infections because current over-use of broad-spectrum antibiotics not only harms individual patients but is a primary contributor to the growing epidemic of antibiotic resistance.","FAST platform for same-shift, complete antibiotic menu antibiotic susceptibility testing",9464993,R43AI136125,"['Agreement', 'Algorithm Design', 'Algorithms', 'Amplifiers', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Bacterial Antibiotic Resistance', 'Biological Assay', 'Blinded', 'Blood', 'Centers for Disease Control and Prevention (U.S.)', 'Chemicals', 'Clinical', 'Clinical Medicine', 'Clinical Microbiology', 'Combating Antibiotic Resistant Bacteria', 'Communicable Diseases', 'Detection', 'Development', 'Devices', 'Diagnostic', 'End Point Assay', 'Engineering', 'Ensure', 'Epidemic', 'Escherichia coli', 'Funding', 'Goals', 'Grant', 'Healthcare', 'Hour', 'Human', 'In Vitro', 'Infection', 'Institutes', 'Invaded', 'Laboratories', 'Length of Stay', 'Machine Learning', 'Metabolic', 'Methods', 'Microbiology', 'Minimum Inhibitory Concentration measurement', 'Optics', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phenotype', 'Plague', 'Positioning Attribute', 'Privatization', 'Quality of Care', 'Reagent', 'Recovery', 'Regulatory Affairs', 'Sampling', 'Sampling Studies', 'Scientist', 'Small Business Innovation Research Grant', 'Speed', 'Staphylococcus aureus', 'System', 'Test Result', 'Testing', 'Vancomycin', 'Work', 'accurate diagnosis', 'base', 'clinically relevant', 'combat', 'commercialization', 'cost', 'cost effectiveness', 'design', 'detector', 'individual patient', 'instrument', 'nanosensors', 'novel', 'optimal treatments', 'pathogen', 'performance tests', 'prediction algorithm', 'prototype', 'rapid diagnosis']",NIAID,"SELUX DIAGNOSTICS, INC.",R43,2018,298851,-0.011256300440650546
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9503816,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'dravet syndrome', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2018,237423,-0.0199079677463246
"Assay Classifier Engine (ACE) for enhancing splice sensor assay performance SUMMARY:  The goal of this proposal is to improve the sensitivity and specificity of the Spinach-based splice sensor platform by developing a novel multiprobe (MP) assay design and a companion machine learning-based classification algorithm called assay classifier engine (ACE). Improvement in sensitivity and specificity of the splice sensor platform enables its application to detect endogenous RNA isoforms with low copy number and distinguish alternative RNA isoforms that share high degree of sequence similarities.  The aim of any assay development effort is to achieve excellent assay specificity and sensitivity. However, this is often a futile endeavor since specificity and sensitivity are two inversely correlated factors. The underlying reason for poor sensitivity or specificity is due to the off-target signals generated by competing molecules present in the sample. In the field of diagnostics, one of the ways these issues are addressed is to perform multiple single probe testing instead of one single probe testing. While individual singe probe assays might have poor specificity and sensitivity, when combined, these assays synergistically improve the sensitivity and specificity of the ultimate diagnostic determination. In the field of research and drug discovery, researchers have employed a multitude of strategies (e.g. signal amplification, reaction cascades, or sample enrichment) to improve sensitivity and MP design or strand displacement strategies to improve specificity. Some of the PCR- based methods have combined both enzyme-based signal amplification and MP strategies to improve assay determination. However, when it comes to detecting targets that are highly similar to their competitors, such as detecting single nucleotide polymorphism, DNA methylation, RNA modification and alternative splicing, there is still an unmet need for more sensitive and specific analytical methods.  In the past few years, Lucerna has developed Spinach-based sensors to detect intractable metabolites and biomolecules. One such sensor is the splice sensor, which is a Spinach-based sensor that can generate fluorescence signal based on the alternative RNA isoform of interest. One of the challenges encountered during splice sensor assay development is the lack of sensitivity toward low copy number RNA isoforms and low specificity when distinguishing two splice isoforms that share a high sequence similarity. To overcome this challenge in this proposal, we will develop a MP assay panel comprised of splice sensor variants that recognize the target RNA and the competitor with varying binding affinities and differing signal responses. We will use data sets generated from the MP assay to train a ML-based ACE algorithm to make target determination in test samples. Further, we will develop a quantitative MP data set and re-train the ACE algorithm to classify the assay signals into various categories based on target concentrations in the test sample. This new ACE algorithm will then be tested against conventional single probe assays to determine specificity and sensitivity improvement of the MP assay platform. PROJECT NARRATIVE: Improved specificity and sensitivity are highly sought-after features in assays where there are high similarity between the target and its competitors or when the target exists naturally in very low abundance. To address this unmet need, we will develop a fluorescence sensor-based multiprobe assay approach and a companion machine learning-based assay classifier engine (ACE). The ACE algorithm will integrate the multiprobe assay data and classify them based on trained machine learning models to make sample determination with enhanced specificity, sensitivity, and dynamic range than possible with conventional single probe assays.",Assay Classifier Engine (ACE) for enhancing splice sensor assay performance,9622514,R43GM130258,"['Address', 'Adopted', 'Affinity', 'Algorithms', 'Alternative Splicing', 'Area Under Curve', 'Binding', 'Binding Sites', 'Biochemical', 'Biological Assay', 'Categories', 'Cells', 'Characteristics', 'Classification', 'Companions', 'Custom', 'DNA Methylation', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Evaluation', 'Exhibits', 'Fluorescence', 'Goals', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Modification', 'Molecular', 'Nerve Degeneration', 'Nucleotides', 'Output', 'Pattern', 'Performance', 'Process', 'Protein Isoforms', 'RNA', 'RNA Splicing', 'Reaction', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Series', 'Side', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Specificity', 'Spinach - dietary', 'Technology', 'Testing', 'Time', 'Titrations', 'Training', 'Variant', 'analytical method', 'aptamer', 'assay development', 'base', 'cost', 'design', 'drug discovery', 'experience', 'improved', 'interest', 'novel', 'outcome forecast', 'predictive modeling', 'response', 'sensor', 'targeted biomarker']",NIGMS,"LUCERNA, INC.",R43,2018,224925,-0.023591357796097213
"Multiplexed high-content assay for toxicity profiling using live iPSC-derived cardiomyocyte lines with lineage-specific barcoding Project Summary/Abstract Human induced pluripotent stem cells (hiPSCs) are poised to transform toxicological evaluation, however new approaches to enable their functional and structural profiling are needed to improve the utility of hiPSC -based models for predictive and mechanistic toxicology screening. This need is addressed by our project’s Specific Aims that encompass (1) development of a novel platform for generation of hiPSC-derived reporter cells; (2) generation of a panel of multicolor hiPSC-derived cardiomyocytes (hiPSC-CMs) with stable lineage specific fluorescent reporters; and (3) implementation and validation of a pilot machine learning-enabled predictive cardiotoxicity screen using these tools. The proposed tools are configured to be extensible to other toxicology- relevant pathways and phenotypes making it uniquely positioned to capitalize on the growing commercial need for high-throughput predictive toxicology assays. The project deliverables benefit public health by improving the ability to rapidly identify liabilities in specific cardiomyocyte lineage types, thus reducing the time and cost to pinpoint cardiotoxicity of pharmaceutical and environmental chemicals. Project Narrative The assay and reagents established in the course of this project directly address the goals of significant initiatives to improve the effectiveness of cardiotoxicity testing, such as the FDA’s CiPA initiative and the work of the Cardiac Safety Research Consortium. The resulting improvements in the pace and precision of drug testing will result in public health benefit through the development of more cost-effective and safer medicines. Beyond toxicological evaluation of therapeutic compounds, our innovative technology will deliver additional benefit to public health by virtue of its utility in investigating the toxicities of environmental chemicals, in line with the focus of government agencies and initiatives such as the EPA and Tox21 in the US and EU-ToxRisk in Europe.",Multiplexed high-content assay for toxicity profiling using live iPSC-derived cardiomyocyte lines with lineage-specific barcoding,9623156,R44TR002572,"['Address', 'Algorithms', 'Biological Assay', 'CRISPR/Cas technology', 'Cardiac', 'Cardiac Myocytes', 'Cardiotoxicity', 'Cell Line', 'Cell Lineage', 'Cells', 'Cellular Assay', 'Cellular Structures', 'Chemicals', 'Classification', 'Contracts', 'Data', 'Development', 'Drug Modelings', 'Drug toxicity', 'Effectiveness', 'Europe', 'Evaluation', 'Foundations', 'Generations', 'Goals', 'Government Agencies', 'Health Benefit', 'Heart Atrium', 'Human', 'In Vitro', 'Industry Standard', 'Machine Learning', 'Medicine', 'Methods', 'Microscopy', 'Mitochondria', 'Nodal', 'Nuclear', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Population Heterogeneity', 'Positioning Attribute', 'Public Health', 'Reagent', 'Reporter', 'Reporting', 'Research', 'Safety', 'Sarcomeres', 'Site', 'Small Business Innovation Research Grant', 'Specificity', 'Speed', 'Structure', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicology', 'Training', 'Treatment-Related Cancer', 'Validation', 'Ventricular', 'Withdrawal', 'Work', 'base', 'cell immortalization', 'clinical candidate', 'clinical predictors', 'cost', 'cost effective', 'drug discovery', 'drug testing', 'environmental chemical', 'expression vector', 'genome editing', 'high throughput screening', 'immortalized cell', 'improved', 'induced pluripotent stem cell', 'innovation', 'innovative technologies', 'model development', 'new technology', 'novel', 'novel strategies', 'phase I trial', 'pre-clinical', 'predictive modeling', 'predictive test', 'predictive tools', 'programs', 'response', 'screening', 'site-specific integration', 'success', 'therapeutic development', 'therapeutic evaluation', 'tool']",NCATS,"CAIRN BIOSCIENCES, INC.",R44,2018,1178386,-0.024295252604294555
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9472335,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,418408,-0.0043274276226662476
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9427988,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2018,162800,-0.05801156850670986
"Diagnostic Neuroimaging Biomarkers of Dysfunctional Valence Systems in Depression and Anxiety Project Summary / Abstract Biomarkers have transformed the diagnosis and treatment of cancer, cardiovascular disease, and a host of other medical conditions. In contrast, psychiatric biomarkers remain largely elusive, due in part to the fact that there is a weak correspondence between psychiatric diagnoses and their neurobiological substrates. This is especially true for depression and anxiety disorders, clinically heterogeneous conditions associated with abnormal reactivity to rewarding and aversive stimuli—and to the cues that predict them—and varied patterns of dysfunction in neural circuits that process positive and negative valence. This project will investigate how arousal processes interact with VS circuits to influence the anticipation and experience of rewarding and aversive stimuli in patients seeking treatment for symptoms of depression and anxiety, independent of conventional diagnostic categories. It is specifically designed to advance the goals of the Research Domain Criteria (RDoC) Project, by focusing on four related RDoC constructs that are directly germane to this question—arousal, potential threat threat, approach motivation, and reward attainment—and testing whether they explain variation in symptoms of anxiety and anhedonia in clinical populations. Importantly, this project will also advance RDoC's goal of developing new ways of classifying mental disorders, which will ultimately require methods for identifying novel diagnostic classes linked to homogeneous pathophysiology. To this end, we will test a strategy for discovering novel diagnostic subtypes defined by clustered patterns of abnormal functional connectivity in valence system circuits. We will use statistical clustering and machine learning methods to develop classifiers for diagnosing these subtypes in individual patients, and we will seek to identify distinct mechanisms by which atypical valence system reactivity may contribute to anxiety, anhedonia, and abnormal approach motivation and avoidance behavior in distinct patient subgroups, indexed in the laboratory via integrated, convergent measurements across multiple units of analysis. Project Narrative Biomarkers have transformed how physicians care for patients with cancer, cardiovascular disease, and a host of other medical conditions by providing quantitative tools for diagnosing disease processes and targeting treatments to the patients most likely to benefit from them. In contrast, biomarkers for psychiatric disorders remain relatively elusive. This project will test a new strategy for developing neuroimaging (brain scan) biomarkers for diagnosing novel subtypes of depression and anxiety in individual patients and then investigate how dysfunction in specific circuits gives rise to specific clinical profiles of anxiety, reward processing, and anxious arousal within these subtypes.",Diagnostic Neuroimaging Biomarkers of Dysfunctional Valence Systems in Depression and Anxiety,9733429,R56MH114976,"['Anhedonia', 'Anterior', 'Anxiety', 'Anxiety Disorders', 'Arousal', 'Aversive Stimulus', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain scan', 'Cardiovascular Diseases', 'Categories', 'Clinical', 'Complex', 'Cues', 'Data Set', 'Diagnosis', 'Diagnostic', 'Emotional', 'Event', 'Functional disorder', 'Goals', 'Heterogeneity', 'Individual', 'Individual Differences', 'Laboratories', 'Link', 'Machine Learning', 'Major Depressive Disorder', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Mental Depression', 'Mental disorders', 'Methods', 'Motivation', 'National Institute of Mental Health', 'Negative Valence', 'Neurobiology', 'Pathological anxiety', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Population', 'Positive Valence', 'Process', 'Psychiatric Diagnosis', 'Psychiatry', 'Psychophysiology', 'Regulation', 'Research Domain Criteria', 'Rest', 'Rewards', 'Sampling', 'Scanning', 'Stimulus', 'Stratification', 'Symptoms', 'Syndrome', 'System', 'Testing', 'Transcend', 'Transcranial magnetic stimulation', 'Variant', 'anxiety symptoms', 'anxious', 'approach avoidance behavior', 'avoidance behavior', 'base', 'biomarker validation', 'cancer therapy', 'connectome', 'depressive symptoms', 'design', 'disease diagnosis', 'experience', 'indexing', 'individual patient', 'learning strategy', 'negative affect', 'neural circuit', 'neuroimaging', 'neuroimaging marker', 'novel', 'novel diagnostics', 'novel marker', 'novel strategies', 'patient subsets', 'prospective', 'response', 'reward anticipation', 'reward processing', 'symptom treatment', 'tool']",NIMH,WEILL MEDICAL COLL OF CORNELL UNIV,R56,2018,724068,-0.013572043241615074
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9530020,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2018,201771,-0.04320035066637992
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,-0.02183440442085335
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9515964,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2018,341899,-0.06905573186628432
"Integrative Learning to Combine Evidence for Personalized Treatment Strategies. Project Summary:  Mental disorders represent immense healthcare burdens. Intense efforts and resources have been devoted to developing pharmacological and behavioral treatments for mental disorders, but no universally effective treatments are available. Considerable heterogeneity exists in treatment response among individuals with mental disorders, in part because an individual patient's psychosocial characteristics and/or biomarkers are not accounted for when selecting among available treatment options. Barriers to implement personalized treatments in clinical psychiatry include a lack of evidence-based, clinically interpretable, individualized treatment rules (ITRs), a lack of power to detect treatment modiﬁers from a single study, and a lack of reproducibility for treatment rules estimated from single studies. We propose analytic solutions to tackle these barriers. Speciﬁcally, we will provide integrative machine learning methods to build powerful, yet interpretable, individualized treatment strategies that can be easily applied in clinical practice. We will integrate evidence of ITRs identiﬁed in multiple randomized controlled trials (RCTs) to increase robustness and reproducibility. In Aim 1, we will provide piece-wise linear decision trees that are trans- parent, interpretable, and that have guaranteed performance. Our decision trees will simultaneously identify the optimal treatment for a given patient (qualitative interaction) and subgroups of patients with large beneﬁt (quan- titative interaction). In Aim 2, we propose a novel integrative analysis to synthesize evidence across trials and provide an integrative ITR that improves efﬁciency and reproducibility. Our method does not require all studies to collect common sets of variables and thus allows evidence to be combined from ITRs identiﬁed in recent RCTs that collected emerging biomarkers (e.g., neuroimaging measures) with earlier RCTs that focused on clinical and be- havioral markers. In response to the National Institute of Mental Health Strategic Plan on Research Domain Criteria (RDoC) to center mental health research around “dimensional psychological constructs” shared across disorders, the methods will be applied to a wide range of RCTs that recruited patients with major depressive disorder and other co-morbid mental disorders. This strategy allows examination of ITRs for constructs shared across disorders and will increase generalizability. We will apply our methods to various RCTs, including data available from the Na- tional Database for Clinical Trials Related to Mental Illness. This research will bridge approaches for personalized medicine and integrative analysis in an effort to better understand the complex interplay between biomarkers and clinical manifestations in the context of selecting the best treatments for patients with mental disorders. Project Narrative:  Treatment responses for mental disorders are inadequate and considerable heterogeneity is observed, in part because an individual patient's clinical, psychosocial, and/or biological markers are not accounted for when select- ing treatments among available options. This research proposes novel analytic methods to discover new powerful, yet interpretable personalized treatment strategies and integrate evidence of strategies identiﬁed in multiple prior studies to increase robustness and reproducibility.",Integrative Learning to Combine Evidence for Personalized Treatment Strategies.,9581848,R21MH117458,"['Age', 'Anxiety Disorders', 'Behavior Therapy', 'Behavioral', 'Biological Markers', 'Bulimia', 'Characteristics', 'Clinical', 'Clinical Trials Database', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Decision Trees', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Equilibrium', 'Goals', 'Healthcare', 'Heterogeneity', 'Individual', 'Intervention', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Outcome', 'Parents', 'Participant', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmacological Treatment', 'Psychiatry', 'Randomized Controlled Trials', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Resources', 'Sampling', 'Strategic Planning', 'Subgroup', 'Surveys', 'Time', 'Translating', 'Trees', 'analytical method', 'base', 'burden of illness', 'clinical practice', 'demographics', 'effective therapy', 'evidence base', 'improved', 'individual patient', 'individualized medicine', 'learning strategy', 'mental health center', 'neuroimaging', 'neurophysiology', 'neuropsychiatry', 'novel', 'optimal treatments', 'patient subsets', 'personalized approach', 'personalized medicine', 'psychologic', 'psychosocial', 'response', 'sex', 'success', 'symptomatology', 'treatment effect', 'treatment response', 'treatment strategy']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2018,202500,-0.007165058938333784
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9567622,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Supervision', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2018,659674,-0.01134259697656707
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9570304,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Supervision', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2018,528639,-0.00953104851564133
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9517061,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'data warehouse', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2018,306284,-0.03532517398271782
"Adolescent Personality and the Incidence of Alzheimer's Disease and Alzheimer's Related Dementias in Later Life. Alzheimer’s Disease and Alzheimer’s related dementias (ADRD) are now regarded as a public health problem of pressing importance. While the race for disease-altering treatments continues, another strand of work has focused on identifying social and psychological precursors of these conditions. Such biopsychosocial risks often can be traced back to phases of life that predate symptom onset by years or decades. Therefore, a robust social and life course epidemiology of ADRD requires study designs that feature a) broad and deep psychosocial characterization of b) a large, population-relevant cohort c) during early phases of life, with d) medically-documented outcome data. Parent project R01AG053155 features a) through c), specifically in 90,000 members of the Project Talent cohort assessed in 1960 and again in 1970-74. The current supplement expands its scope to all members of Project Talent baseline (roughly 340,000) from 1960, and focuses on two scientific aims. The first seeks to estimate the relative risk of ADRD by the early 70s arising from adolescent personality traits, as documented in Medicare data linked to the cohort. One key feature of this aim is to determine if aspects of personality in adolescence are associated with ADRD incidence in later life independently of adolescent IQ, which is a known predictor. The second key aspect of this aim is to use the size and population representativeness of the sample to derive reasonably precise population-relevant effect size estimates of personality relative risks, and compare these effect sizes to benchmark risk estimates of adolescent IQ and socioeconomic status, which are considered to have policy and public health significance. The second aim also leverages the size and scope of the sample to identify personality traits which may moderate the ADRD risk of low adolescent IQ, in more complex and realistic patterns than can be studied in smaller or less representative data sets. This is accomplished via machine learning methods focused on identifying non-linear interactions via intensive cross-validation, another scientific question that takes full advantage of the size and scope of this unique cohort. Alzheimer’s Disease and Alzheimer’s related dementias are on the rise, due to both increasing life spans and the size of the Baby Boom generation as it reaches these later years. While scientists have yet to develop cures for dementias, it may be possible to learn how to prevent or delay their onset by studying life circumstances that give rise to these conditions. The goal of this project is to determine how personality traits in adolescence may compensate for the dementia risk posed by low adolescence IQ scores.",Adolescent Personality and the Incidence of Alzheimer's Disease and Alzheimer's Related Dementias in Later Life.,9688395,R01AG053155,"['Adolescence', 'Adolescent', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Alzheimer&apos', 's disease risk', 'Area', 'Awareness', 'Baby Booms', 'Back', 'Behavioral', 'Benchmarking', 'Complex', 'Data', 'Data Set', 'Deltastab', 'Dementia', 'Diagnosis', 'Disease', 'Elderly', 'Etiology', 'Funding', 'Generations', 'Goals', 'High School Student', 'Incidence', 'Knowledge', 'Learning', 'Life', 'Life Cycle Stages', 'Life course epidemiology', 'Link', 'Literature', 'Longevity', 'Machine Learning', 'Measurement', 'Medical', 'Medicare', 'Methods', 'Modification', 'Neurotic Disorders', 'Outcome', 'Pattern', 'Personality', 'Personality Traits', 'Phase', 'Policies', 'Policy Maker', 'Population', 'Psychosocial Factor', 'Public Health', 'Race', 'Relative Risks', 'Research Design', 'Research Personnel', 'Risk', 'Risk Estimate', 'Risk Factors', 'Role', 'Sampling', 'Scientist', 'Socioeconomic Status', 'Symptoms', 'Talents', 'Techniques', 'Testing', 'Validation', 'Work', 'biopsychosocial', 'cohort', 'epidemiologic data', 'exhaustion', 'flexibility', 'follow-up', 'learning strategy', 'member', 'middle age', 'mortality', 'parent project', 'population based', 'pre-clinical', 'prevent', 'prospective', 'psychologic', 'psychosocial', 'public health priorities', 'social', 'trait']",NIA,UNIVERSITY OF ROCHESTER,R01,2018,287001,-0.06118187217344992
"APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY Abstract  Acute myeloid leukemia (AML) accounts for half of all pediatric leukemia deaths and is the leading cause of leukemia-related death in adulthood. One reason for worse outcomes is the inability to properly assess for minimal residual disease (MRD) following therapy. Unlike ALL, AML presents with multiple subclonal populations without a singular clonal surface marker, and surface markers can change during therapy. The current gold standard for AML MRD is multi-parameter flow cytometry (MPFC), which is predictive of outcomes to frequencies of 0.001, yet 30% of MPFC-MRD-negative patients still relapse. Alternatively, every AML case harbors leukemia-specific mutations that could be markers of disease, except that next-generation sequencing has high error rate of ~1%. In this proposal, we will implement a novel, validated error-corrected sequencing (ECS) strategy, developed by the Druley lab in collaboration with Illumina, to improve MRD assessment of AML subclonal heterogeneity in 990 pediatric de novo AML cases from the Children's Oncology Group AAML1031 study. We hypothesize that using a highly sensitive sequencing method will improve identification of residual AML, provide important insights on subclonal heterogeneity in pediatric AML, improve understanding of the role of germline variability and gene function on relapses or refractory disease and facilitate personalized medicine. To interrogate this hypothesis, we propose the following aims: 1. Define subclonal heterogeneity at diagnosis and end of Induction 1 (EOI1) in 990 pediatric de novo  AML patients (n=1890). By using the largest prospective study of pediatric AML that has ever been  performed, we will perform ECS on 94 genes that are the most frequently mutated genes in pediatric and  adult AML at diagnosis and EOI1 to identify patterns of mutation associated with relapsed disease, FAB  subtypes or other cytogenetic features. 2. Correlate ECS-MRD with existing EOI1 MPFC-MRD for all participants in the COG AAML1031 study.  A major question is whether the “different from normal” cell population identified as residual disease by  MPFC is actually the same population(s) identified by ECS. We will define residual disease by ECS and  compare results to MPFC status (positive/negative), actual MPFC percentages (<0.001) and the clinical  outcomes (relapse risk, disease-free survival and overall survival) of study participants. 3. Integrate germline variation and all subclonal mutations into mechanistic groups that are frequently  mutated in pediatric AML and correlate with outcomes using unbiased machine learning  algorithms. Preliminary data tells us that every patient will have multiple subclones at diagnosis and EOI1  as well as germline variants in AML-associated genes, which may be important for outcome. In this aim, we  will take these mutations into account as well as MPFC, clinical features and cytogenetics for probabilistic  risk assessment using unsupervised machine learning algorithms for improved outcome prognostication. Narrative We have developed Error-Corrected Sequencing (ECS) that enables the highly accurate detection of leukemia- specific mutations in heterogeneous DNA samples to a limit of 0.0001. We will perform ECS with a panel of 94 frequently mutated genes in adult and pediatric AML in matched diagnostic and end of Induction 1 bone marrow samples from 990 pediatric de novo AML patients enrolled on the Children's Oncology Group AAML1031 protocol, which is the largest pediatric AML trial ever performed in North America. With these data, we can truly understand subclonal heterogeneity in pediatric AML, significantly improve minimal residual disease testing in pediatric AML, and integrate these data into an unbiased multivariate probability platform taking into account individual sequencing, flow cytometry, cytogenetic and clinical features to provide truly personalized cancer care.",APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY,9395904,R01CA211711,"['Acute Myelocytic Leukemia', 'Adult', 'Adult Acute Myeloblastic Leukemia', 'Algorithms', 'Alleles', 'BAY 54-9085', 'Biological Markers', 'Bone Marrow', 'Bortezomib', 'Categories', 'Cessation of life', 'Childhood', 'Childhood Acute Myeloid Leukemia', 'Childhood Leukemia', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Cytogenetics', 'DNA', 'DNA Sequence Alteration', 'DNA sequencing', 'Data', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Marker', 'Disease-Free Survival', 'Enrollment', 'Epigenetic Process', 'Exons', 'FLT3 gene', 'Family', 'Flow Cytometry', 'Frequencies', 'Future', 'Gene Targeting', 'Genes', 'Genetic', 'Goals', 'Gold', 'Immunophenotyping', 'Individual', 'Investigation', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Mutate', 'Mutation', 'Normal Cell', 'North America', 'Oncogenes', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pediatric Oncology Group', 'Phase', 'Point Mutation', 'Population', 'Probability', 'Prospective Studies', 'Protein Tyrosine Kinase', 'Protein phosphatase', 'Protocols documentation', 'Reagent', 'Recurrent disease', 'Refractory Disease', 'Relapse', 'Residual Neoplasm', 'Residual Tumors', 'Residual state', 'Resolution', 'Risk', 'Risk Assessment', 'Risk stratification', 'Role', 'Sampling', 'Site', 'Spliceosomes', 'Surface', 'Testing', 'Time', 'Treatment Efficacy', 'Tumor Suppressor Proteins', 'United States National Institutes of Health', 'Variant', 'analytical method', 'base', 'chemotherapy', 'cohesion', 'design', 'digital', 'falls', 'functional group', 'gene function', 'improved', 'improved outcome', 'insight', 'leukemia', 'leukemia treatment', 'next generation', 'next generation sequencing', 'novel', 'nucleocytoplasmic transport', 'outcome prediction', 'pediatric patients', 'personalized cancer care', 'personalized medicine', 'prognostic', 'prospective', 'randomized trial', 'relapse risk', 'response', 'specific biomarkers', 'subclonal heterogeneity', 'transcription factor', 'unsupervised learning']",NCI,WASHINGTON UNIVERSITY,R01,2018,352587,-0.03699468490481828
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9468362,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2018,696602,-0.01817011163854207
"An integrated neural network analysis and video microscopy platform for fully automated particle tracking Project Summary/Abstract  Particle tracking (PT) is a biophysical tool for elucidating molecular interactions, transport phenomena of diverse species, and rheological properties of complex materials. PT experiments involve first obtaining high resolution videos that capture time-resolved increments of particles, followed by extraction of traces of entities of interest from videos in the form of spatial locations over time, a process we refer to as path conversion. Finally, quantitative analysis of the traces will yield diffusivities, viscoelasticity, etc.  Lung diseases, such as cystic fibrosis and COPD, are characterized by a highly viscoelastic mucus layer that is incapable of being cleared by mucociliary clearance. Not surprisingly, the viscoelasticity of mucus often directly reflects disease progression. A variety of mucolytics are being investigated, but due to the variable composition and properties of mucus between patients, effective mucolytics treatment will likely be different between individuals; too little/inappropriate mucolytics will not be effective in restoring mucus clearance, whereas too much may result in bronchorrhea. Although microbeads-based rheology has been performed on a variety of mucus specimens in basic research, the capacity for high throughput characterization of rheological properties of biological specimens in a clinical setting is currently not available. This limitation can be attributed to inefficiencies of path conversion: current PT software requires extensive human supervision/intervention to achieve accurate path conversion, not only resulting in poor reproducibility and throughput but also restricting its use to only expert labs. Our vision is to make PT as objective and easy to use as a simple plate reader that can be readily utilized by clinicians (diagnostics, disease progression, therapy effectiveness), pharma (preclinical/clinical drug screening), and research professionals. Towards this goal, we have created a neural network tracker (NNT) that automatically determines the location of all particles in each frame with zero user-input (i.e. no parameter for users to change), and retains the identity of all particles from frame to frame. The innovation is that NNT can robustly, reproducibly, and accurately track a wide range of 2D/3D videos with virtually no need for human intervention, achieving unparalleled time savings. We have already successfully deployed NNT over the Google cloud, which offers exceptional scalability. Nevertheless, for time-sensitive applications, such as an automated PT rheometer, the transfer of large video data files is likely prohibitive. Therefore, in this Phase I STTR, we seek to enable real-time NNT-based PT analysis on the local machine while video microscopy data is being acquired by the microscope, and allow data from PT analysis to drive the operation of the microscope. In Aim 1, we will integrate our NNT with a single objective fluorescence microscope system called Monoptes. Aim 2 will evaluate the performance of our NNT- Monoptes system. If successful, our technology would form the basis of a fully automated PT system capable of measuring rheological properties of fluids/materials or distribution of particle sizes in a 96-well plate format. Project Narrative Particle tracking is a powerful biophysical tool in life and physical sciences, but unfortunately, its application has been strongly limited by inefficiencies in accurately extracting particle traces from raw movies. Unlike conventional particle tracking methods, we have combined artificial intelligence and machine learning to create a software that can consistently provide superior and truly automated tracking performance compared to current alternatives. In this proposal, we will integrate this latest advance with sophisticated instrumentation to develop a microscope system capable of fully automated particle tracking microscopy in a 96-well plate format. If successful, the instrument will likely be utilized by clinicians (diagnostics, disease progression, therapy effectiveness), pharma (preclinical/clinical drug screening of patients), and research professionals.",An integrated neural network analysis and video microscopy platform for fully automated particle tracking,9620574,R41GM130202,"['Acceleration', 'Adopted', 'Antibodies', 'Artificial Intelligence', 'Basic Science', 'Binding', 'Biological', 'Biological Neural Networks', 'Biological Sciences', 'Chronic Obstructive Airway Disease', 'Clinical', 'Clinical Trials', 'Code', 'Complex', 'Computer software', 'Cystic Fibrosis', 'Data', 'Data Files', 'Decision Making', 'Diagnosis', 'Diagnostic', 'Diffuse', 'Disease Progression', 'Drug Carriers', 'Drug Screening', 'Effectiveness', 'Elasticity', 'Engineering', 'Gaussian model', 'Goals', 'HIV Infections', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Intervention', 'Liquid substance', 'Location', 'Lung diseases', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Microscope', 'Microscopy', 'Microspheres', 'Motion', 'Mucociliary Clearance', 'Mucolytics', 'Mucous body substance', 'Output', 'Particle Size', 'Particulate', 'Pathway Analysis', 'Patients', 'Performance', 'Phase', 'Photobleaching', 'Positioning Attribute', 'Process', 'Property', 'Radial', 'Reader', 'Reproducibility', 'Research', 'Resolution', 'Respiratory physiology', 'Rheology', 'Risk', 'Running', 'Sampling', 'Savings', 'Series', 'Small Business Technology Transfer Research', 'Software Tools', 'Specimen', 'Spottings', 'Supervision', 'System', 'Technology', 'TensorFlow', 'Time', 'Video Microscopy', 'Viscosity', 'Vision', 'Woman', 'base', 'biophysical tools', 'cloud based', 'drug development', 'experimental study', 'fluorescence microscope', 'innovation', 'instrument', 'instrumentation', 'interest', 'movie', 'novel', 'operation', 'particle', 'patient screening', 'physical science', 'pre-clinical', 'submicron', 'virtual', 'viscoelasticity']",NIGMS,"AI TRACKING SOLUTIONS, LLC",R41,2018,224894,-0.011508818762586007
"Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Summary:  Spinal cord injury (SCI) patients experience limited functional recovery, owing in part to the paucity of axon regrowth from injured CNS neurons. Effective treatments are lacking, likely because of multiple factors, intrinsic and extrinsic, that inhibit axon growth. Thus we require agents that target more than one source of regeneration failure.  Kinases are ubiquitous signal transducers that regulate most cellular processes, including axon growth. To begin to identify compounds that positively regulate axon growth, we screened 1600 small-molecule kinase inhibitors (KIs) in an in vitro CNS neurite outgrowth assay and identified “hit” KIs that reproducibly and strongly promote outgrowth. Due to homology of catalytic domains, KIs typically inhibit multiple kinases. This makes it difficult to identify the kinase(s) that mediate a KI's effects on cells. We used information theory and machine learning to analyze the inhibition profiles of KIs in relation to their effects on neurite outgrowth. This enabled us to identify, and later validate via siRNA knockdown in primary neurons, multiple kinase targets (i.e. kinases that should be inhibited to promote neurite outgrowth). These included previously known targets that regulate intrinsic and extrinsic inhibitor factors, in addition to several novel candidates. Conversely, we identified kinases whose activity is critical for neurite outgrowth, and whose inhibition must be avoided (anti-targets). We discovered several KIs that inhibit multiple targets and no anti-targets. These KIs strongly promoted neurite outgrowth in vitro.  We tested the KI, RO48, that had the largest effect in vitro in two in vivo models. Our preliminary experiments indicate that RO48 is remarkably effective in vivo. It promoted robust axonal growth of the corticospinal tract (CST) in three separate models of CST injury (pyramidotomy, funiculotomy, dorsal hemisection), and in the dorsal hemisection model, improved forelimb function. We propose to build on these remarkable results to test the working hypothesis that the simultaneous inhibition of RO48's five target kinases (ROCK, PKC, PRKG1, PRKX, and RPS6K) promotes sprouting and regeneration of CST axons. This will be accomplished using viral vectors to knock down expression of the different target kinases individually and in combination. We will do knockdown in CST neurons in the cortex. We will assess CST axon growth at the injury site using light microscopy. We will also perform experiments to determine if RO48-induced CST axon growth promotes axon sprouting, regeneration, or both, and whether RO48 improves behavioral outcomes such as grasping and walking after a contusion injury.  These experiments will 1) validate novel kinases as in vivo targets for future development of SCI therapeutics 2) determine whether these kinases regulate CST axon sprouting, regeneration, or both, and 3) confirm whether the substantial stimulation of axon growth induced by treatment with RO48 improves motor outcomes in a clinically relevant contusion model.  Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Narrative: The proposed experiments aim to understand how small-molecule drug-like compounds increase the ability of nerve cells to grow long processes and re-form connections. Validating the molecular targets of these compounds for in vivo nerve growth will enable future drug discovery projects focused on these targets.",Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury,9515069,R01NS100531,"['Axon', 'Behavioral', 'Biochemical', 'Biological', 'Biological Assay', 'Catalytic Domain', 'Cell physiology', 'Cells', 'Cervical', 'Complement 5a', 'Confocal Microscopy', 'Control Animal', 'Contusions', 'Corticospinal Tracts', 'Data', 'Development', 'Distal', 'Dorsal', 'Dose', 'Failure', 'Forelimb', 'Future', 'Gold', 'Growth', 'In Vitro', 'Individual', 'Information Theory', 'Injury', 'Institution', 'Label', 'Lesion', 'Light', 'Machine Learning', 'Mediating', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Target', 'Morphology', 'Motor', 'Motor Cortex', 'Mus', 'Natural regeneration', 'Nerve', 'Neurites', 'Neurons', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphotransferases', 'Process', 'Rattus', 'Recovery of Function', 'Role', 'Signal Pathway', 'Signal Transduction', 'Site', 'Small Interfering RNA', 'Source', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Spinal cord injury patients', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transducers', 'Viral Vector', 'Walking', 'axon growth', 'axon regeneration', 'behavior test', 'behavioral outcome', 'central nervous system injury', 'clinically relevant', 'design', 'drug discovery', 'effective therapy', 'experience', 'experimental study', 'grasp', 'gray matter', 'improved', 'in vivo', 'in vivo Model', 'inhibitor/antagonist', 'injured', 'insight', 'kinase inhibitor', 'knock-down', 'light microscopy', 'novel', 'reconstruction', 'regenerative', 'screening', 'small molecule', 'targeted agent', 'therapeutic target']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2018,465464,-0.026815890371606463
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9530685,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'high dimensionality', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2018,771712,-0.02362866887395924
"Noninvasive assessment of the cornea by diffusion OCT Keratoconus is a degenerative disease of the cornea that is a major cause of reduced vision-related quality of life in the United States, often leading to corneal transplantation. Ectasia after refractive surgery is a vision-threatening complication that can occur in apparently low-risk patients despite current screening technology. The biomechanical properties of the cornea play a central role in these diseases, but diagnostics are still rooted in shape measures because doctors lack direct measures of biomechanical change. While several methods have shown early promise for addressing this gap, most require contact with or perturbation of the cornea, cannot spatially resolve biomechanical properties, or involve expensive optical systems.  To address the need for direct biomechanical measurement of the cornea and the limitations of other approaches, we introduce phase-decorrelation OCT (phd-OCT). Phd-OCT makes use optical coherence tomography (OCT) to quantify random nanoscale mobility that is related to the strength and cohesion of the cornea. Preliminary results strongly support the rationale, feasibility and potential advantages of the approach. Our objectives are: (1) to refine our method to optimize detection sensitivity and speed, and (2) to determine if the technique is clinically useful. We will achieve these objectives through the following aims: 1. To develop and validate mobility-sensitive phd-OCT for corneal imaging. Spectral-domain OCT will be used for anterior segment phase-decorrelation imaging and the analysis algorithm will be optimized. The system will be validated using phantoms and torsional rheometry. 2. To investigate the potential influence of physiological factors (intraocular pressure (IOP), hydration, and temperature). Factorial design experiments in porcine and human donor globes will establish the sensitivity of phd-OCT measurements to potential confounders. 3. To develop and validate data acquisition and processing methods to enable clinical testing. GPU processing and machine learning will be configured to minimize processing time. Scan patterns will be optimized to minimize scan time and return clinically useful data. 4. To assess feasibility and preliminary diagnostic performance of clinical mobility-sensitive OCT imaging of the cornea. A first-in-human study will characterize repeatability and test the hypotheses that phd-OCT mobility measurements are increased in the region of a LASIK flap and decreased in CXL.  Expected Outcomes: The proposed studies will establish a new, non-contact method for imaging corneal biomechanical properties with the potential to address many shortcomings of current and emerging methods. Success could lead to earlier detection of of ectasia risk and allow more appropriate timing and customization of corneal treatments. Future integration of data into computational models has the potential to greatly impact the field by driving a shift from empirical planning and risk analysis to patient-specific strategies. The mechanical properties of the cornea, the transparent front of the eye, are important for diseases that affect vision as well as for corrective treatments like crosslinking therapy and LASIK. We have developed a new optical coherence tomography (OCT)-based method to detect and map corneal mechanical properties that has advantages that may make it readily translatable to clinical use. The objective of this proposal is to determine whether this technique is clinically feasible and useful by refining the technology, testing it in animal and human donor corneas, and performing studies in patients with corneal conditions of interest.",Noninvasive assessment of the cornea by diffusion OCT,9427452,R01EY028667,"['Achievement', 'Address', 'Affect', 'Algorithmic Analysis', 'Algorithms', 'Animals', 'Anterior', 'Automobile Driving', 'Biomechanics', 'Clinical', 'Clinical Research', 'Complication', 'Computational Technique', 'Computer Simulation', 'Cornea', 'Custom', 'Data', 'Degenerative Disorder', 'Detection', 'Development', 'Diagnostic', 'Diffusion', 'Disease', 'Doctor of Philosophy', 'Early Diagnosis', 'Ensure', 'Exhibits', 'Eye', 'Family suidae', 'Feedback', 'Future', 'Human', 'Hydration status', 'Image', 'Image Analysis', 'Keratoconus', 'Keratoplasty', 'Laser In Situ Keratomileusis', 'Lead', 'Life', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathological Dilatation', 'Patient risk', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic Intraocular Pressure', 'Physiological', 'Plant Roots', 'Play', 'Procedures', 'Property', 'Proxy', 'Quality of life', 'Risk', 'Role', 'Scanning', 'Shapes', 'Speed', 'Surgical Flaps', 'Surgical incisions', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Torsion', 'United States', 'Validation', 'Vision', 'Visual impairment', 'base', 'clinical practice', 'cohesion', 'computerized data processing', 'crosslink', 'data acquisition', 'data integration', 'design', 'experimental study', 'first-in-human', 'human study', 'imaging modality', 'interest', 'mechanical properties', 'nanoscale', 'novel', 'outcome prediction', 'research clinical testing', 'screening', 'simulation', 'success', 'tool', 'treatment effect']",NEI,CASE WESTERN RESERVE UNIVERSITY,R01,2018,400000,-0.01601674057797006
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9415436,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Expert Systems', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic Diseases', 'Genetic study', 'Goals', 'Gold', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'Supervision', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'experimental study', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'public health relevance', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2018,445349,-0.03083753134581628
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9588814,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2018,906891,-0.024931959307730275
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9473087,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'patient subsets', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2018,601486,-0.005078141250066684
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9495704,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2018,396544,-0.012266676431651328
"Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia PROJECT SUMMARY Preeclampsia (PE) is a devastating hypertensive disorder of pregnancy that is a leading cause of maternal and fetal mortality and morbidity worldwide. It affects 2-10% of women and accounts for 16% of maternal deaths related to childbirth in developed countries. PE is difficult to study because it is a multifactorial disorder with varying molecular mechanisms associated with the development of the spectrum of clinical symptoms associated with early onset, late onset, and severe PE. A personalized medicine approach applied to dissecting the underlying mechanisms of PE may be beneficial to reduce clinical incidence of this devastating syndrome. The placenta plays a key role in the development of PE, leading to widespread maternal endothelial dysfunction, hypertension, and systemic multi-organ failure in PE. Extracellular vesicles (EVs) containing protein, RNA, and lipid cargo are continuously extruded from the placenta, and are capable of interacting with maternal organs including the kidney. PE is primarily associated with placental and renal dysfunction, and PE is the most common cause of acute kidney injury during pregnancy. However, no studies have investigated the potential of placenta-derived RNA cargo as a link between placental and renal dysfunction in PE. Urinary EVs are derived from multiple tissue types and represent a trove of biomarkers that are increasingly being utilized to diagnose renal disorders. Further, urine samples can be obtained throughout pregnancy non-invasively and could potentially be utilized to identify biomarkers related to placental dysfunction in PE. In the proposed research, during the mentored phase, cutting-edge RNA-Seq technology coupled with computational biological and machine learning approaches will be applied to profile the transcriptome of urinary EVs in women with PE compared to normal pregnancy. Preliminary data indicates that it is possible to isolate and profile the transcriptome of urinary EVs from maternal urine throughout normal gestation, and that placenta-derived and placenta-specific mRNA and miRNA can be detected within the urinary EV population. This presents a novel technique that has potential to identify biomarkers as well as provide information on placental dysfunction in PE in a non-invasive manner. During the independent phase, the candidate will utilize an in vitro approach to investigate the effect of uptake of placenta-derived EVs with miRNA cargo associated with PE on the function of proximal tubule epithelial cells and cortical collecting duct cells. These two renal-specific cell types are involved in tubular reabsorption in the nephron, a process that is compromised leading to increased excretion of protein in the urine in some preeclamptic pregnancies. This proposal is multidisciplinary, utilizing basic biology, clinical research, and high-performance computing applied to investigating placental dysfunction in PE. These experiments are significant because they will generate novel information on the role of placenta- derived EVs in renal dysfunction in PE, as well as point the way towards preventative and therapeutic targets that may be transformative and clinically relevant. PROJECT NARRATIVE The proposed research will examine the transcriptome of urinary extracellular vesicles obtained non-invasively as a source of placenta-derived nucleic acids reflective of the development of preeclampsia (PE), and will con- duct in vitro experiments to determine the role of placental extracellular vesicles in renal dysfunction in PE.",Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia,9583334,K99HD096125,"['Acute Renal Failure with Renal Papillary Necrosis', 'Affect', 'Apoptosis', 'Biological', 'Biological Markers', 'Biology', 'Blood Circulation', 'Brain', 'Cells', 'Characteristics', 'Childbirth', 'Chromosomes', 'Clinical', 'Clinical Research', 'Coupled', 'Data', 'Developed Countries', 'Developing Countries', 'Development', 'Diagnosis', 'Disease', 'Disease Pathway', 'Ductal Epithelial Cell', 'Epithelial Cells', 'Etiology', 'Excretory function', 'Failure', 'Fetal Mortality Statistics', 'Functional disorder', 'Genetic Transcription', 'Gestational Age', 'Goals', 'Health Care Costs', 'High Performance Computing', 'Human', 'Hypertension', 'Hypoxia', 'Immune response', 'Immunohistochemistry', 'In Vitro', 'Incidence', 'Injury', 'Kidney', 'Kidney Diseases', 'Libraries', 'Link', 'Lipids', 'Liver', 'Lung', 'Machine Learning', 'Maternal Mortality', 'Mentors', 'Messenger RNA', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Disease', 'Morbidity - disease rate', 'Mus', 'Nephrons', 'Nucleic Acids', 'Organ', 'Pathogenesis', 'Pathologic', 'Pathology', 'Phase', 'Placenta', 'Play', 'Population', 'Population Heterogeneity', 'Pre-Eclampsia', 'Pregnancy', 'Process', 'Proteins', 'Proteinuria', 'RNA', 'Research', 'Role', 'Sampling', 'Small RNA', 'Source', 'Stress', 'Syndrome', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Transcript', 'Tubular formation', 'Urine', 'Woman', 'associated symptom', 'biobank', 'cell type', 'clinically relevant', 'early onset', 'endothelial dysfunction', 'experimental study', 'extracellular vesicles', 'improved', 'innovation', 'maternal serum', 'mortality', 'multidisciplinary', 'neonatal morbidity', 'novel', 'personalized medicine', 'pregnancy disorder', 'premature', 'prevent', 'response', 'stillbirth', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'uptake', 'urinary']",NICHD,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,K99,2018,104534,-0.026268381858144466
"High content in vivo screening for acute kidney injury ameliorating drugs ABSTRACT In this application we will perform high throughput, high content, screening for small molecules that improve kidney regeneration after acute kidney injury (AKI). AKI presents a dire unmet medical need because of its high prevalence with long-term adverse health effects and life-threatening sequelae. Mortality is high and the only effective treatments are renal replacement therapies. The onset of the precipitating event is unpredictable, and in many instances once patients are admitted to the hospital, injury has already occurred. Improving recovery from injury therefore presents an attractive opportunity for intervention, but to date, no therapies are available that are effective if administered post injury. The vertebrate kidney has an innate ability to regenerate and follows a well-defined cellular mechanism that encompasses dedifferentiation of surviving renal tubule cells, proliferation of resulting progenitors, and repopulation of the denuded tubule. This sequence of events, together with their respective molecular markers, is conserved between humans, mouse, and zebrafish. During regeneration, transcription factors normally expressed during organogenesis (e.g.,lhx1a, pax2, and pax8) are reactivated. We previously demonstrated that small molecule-mediated augmentation of endogenous Lhx1a expression can ameliorate recovery in zebrafish and mouse models of AKI. Together these data support the overall hypothesis that augmentation by small molecules of cellular programs that drive kidney repair after injury represents a novel pharmacologic approach for the treatment of AKI and associated sequelae. Using a transgenic zebrafish line that expresses Lhx1a-EGFP we have developed an artificial intelligence-based, high-content assay to quantify lhx1a expression in the living embryo. Using multivariate analysis, the assay met accepted HTS assay performance standards and was validated in three-day variability studies and a small pilot library screen. We will perform a primary HTS of 50,000 compounds from the MLPCN collection. Prioritized hits will be subjected to a fully implemented, rigorous secondary assay paradigm encompassing kidney organ development, metabolic stability, in vivo efficacy, and activity profiling in a pathophysiological relevant AKI model. At the end of these studies we will have identified functionally and mechanistically characterized in vivo chemical probes to investigate the biology of kidney injury and regeneration, some of which are expected to have features that make them suitable for development into preclinical leads. NARRATIVE Acute kidney injury (AKI) presents a dire unmet medical need with unacceptably high mortality rates and a lack of therapeutic modalities. The vertebrate kidney has an innate ability to regenerate that can be enhanced by small molecules. In this proposal we will perform high-content, high throughput screening for kidney regeneration in zebrafish to discover novel chemical probes to investigate mechanisms of augmented kidney regeneration after injury.",High content in vivo screening for acute kidney injury ameliorating drugs,9406857,R01DK112652,"['Acute Renal Failure with Renal Papillary Necrosis', 'Appearance', 'Artificial Intelligence', 'Biological Assay', 'Biological Markers', 'Biology', 'Breeding', 'Carboxylic Acids', 'Cell Proliferation', 'Chemicals', 'Collection', 'Complex', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Dose', 'Embryo', 'Embryonic Development', 'Ensure', 'Epithelial Cells', 'Etiology', 'Evaluation', 'Event', 'FDA approved', 'Fibrosis', 'Generations', 'Genetic', 'Health', 'High Prevalence', 'Hospitals', 'Human', 'Injury', 'Intervention', 'Kidney', 'Libraries', 'Life', 'Liver', 'Mediating', 'Medical', 'Metabolic', 'Microsomes', 'Modality', 'Modeling', 'Molecular Bank', 'Morphologic artifacts', 'Multivariate Analysis', 'Mus', 'Natural regeneration', 'Nature', 'Organogenesis', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacology', 'Prodrugs', 'Protocols documentation', 'Recovery', 'Renal Replacement Therapy', 'Renal function', 'Renal tubule structure', 'Reproducibility', 'Specificity', 'Stem cells', 'System', 'Testing', 'Therapeutic', 'Toxic effect', 'Transgenes', 'Transgenic Organisms', 'Tubular formation', 'United States National Institutes of Health', 'Zebrafish', 'analog', 'base', 'effective therapy', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'kidney repair', 'molecular marker', 'mortality', 'mouse model', 'novel', 'organ growth', 'organ regeneration', 'pre-clinical', 'progenitor', 'programs', 'repository', 'response', 'screening', 'small molecule', 'small molecule libraries', 'transcription factor']",NIDDK,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,351001,-0.010305179626826607
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Project Summary:  Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer's disease, Parkinson's disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these chal- lenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymor- phisms, whole brain neuroimaging, bioﬂuids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mecha- nisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Speciﬁcally, in Aim 1, we develop efﬁcient methods for multi-level semiparametric transformation mod- els to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efﬁciency. Our methods do not rely on full pedigree genotyping and provide family-speciﬁc substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic sys- tems through ordinary differential equations with random inﬂections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker proﬁles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efﬁcient machine learning methods to identify predictive markers, estimate optimal individu- alized therapies, and identify subgroups who may receive the greatest beneﬁt from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art ana- lytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research com- munity. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9502388,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2018,340491,-0.021955618755541462
"Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers PROJECT SUMMARY/ABSTRACT Candidate: Dr. Adel El Boueiz is a pulmonary and critical care physician-scientist completing a period of T32- funded support at the Channing Division of Network Medicine (CDNM) and Harvard Medical School (HMS). He received a Master's of Medical Science in Biomedical Informatics from HMS in May 2016. He will be promoted to Instructor of Medicine at the CDNM and HMS on July 1, 2017. His principal research interests are the genetic epidemiology of chronic obstructive pulmonary disease (COPD) and the translation of genomic discoveries into clinical practice and public health. His long-term goal is to be an independent investigator with expertise in imaging phenotyping, genomics, and predictive analytics of the regional heterogeneity of the various aspects of COPD (emphysema, airway disease, and pulmonary vascular remodeling). Environment: Dr. El Boueiz will continue to pursue his research and career development in the rich and multidisciplinary environment of the CDNM and the Brigham and Women's Hospital Applied Chest Imaging Lab (ACIL). He will be mentored by Drs. Edwin K. Silverman, Peter J. Castaldi, and Raúl San José Estépar, leaders in the field of COPD quantitative imaging, genetic epidemiology, and predictive analytics with excellent track records of mentoring young investigators towards independent research careers. His career development will also be overseen by an advisory committee with expertise related to key areas of his proposal. Research: COPD is a major cause of morbidity and mortality that is of increasing public health importance. COPD is a heterogeneous disease and this heterogeneity complicates the identification of the predictors of disease progression and consequently, the development of effective therapies. Emphysema distribution is an important COPD-related phenotype that emerged as a strong predictor of the response to lung volume reduction procedures. Despite the availability of advanced texture-based CT quantification methods, global threshold-based quantitative metrics have to date been the cornerstone for the radiological characterization of emphysema distribution with inability to differentiate centrilobular, panlobular, and paraseptal emphysema patterns. In this project, we will apply a texture-based CT quantification method to discover novel imaging biomarkers of the regional heterogeneity of centrilobular, panlobular, and paraseptal emphysema in a large cohort of well-characterized smokers and identify their genetic determinants using whole genome sequencing and integrative genomics analyses. The results will be considered for inclusion along with other rich phenotypic and imaging data in COPD disease progression machine learning predictive models. Relevance: Through improved radiographic phenotyping of emphysema distribution, better understanding of disease pathobiology, and more accurate prediction of disease progression, the proposed work will open new avenues of investigation for the development of personalized and improved COPD therapeutic strategies. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a common disease that affects up to 24 million people in the United States, is associated with considerable and increasing morbidity and mortality, and for which there is no available disease-modifying therapy. COPD is associated with significant variation in radiographic, symptomatic and physiologic presentation and exhibits variability in progression. Currently, there is no satisfactory method for progression prediction. This project will identify novel imaging biomarkers of the regional distribution of centrilobular, panlobular, and paraseptal emphysema with particular emphasis on their associations with clinical relevant COPD-related outcomes, their genetic determinants, and their ability to improve prediction of COPD disease progression, above and beyond that provided by the traditional clinical, radiographic, and genetic features. This is an important area of research as predicting those patients who will remain stable from those who will have rapid disease progression is critical in defining prognosis and selecting patients for specific therapeutic interventions.","Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers",9503881,K08HL141601,"['ACVR1B gene', 'Accounting', 'Advisory Committees', 'Affect', 'Airway Disease', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological Process', 'Chest', 'Chronic Obstructive Airway Disease', 'Clinical', 'Clinical Trials', 'Clinical/Radiologic', 'Cohort Studies', 'Collection', 'Comorbidity', 'Complex', 'Computers', 'Computing Methodologies', 'Critical Care', 'Data', 'Detection', 'Development', 'Diagnostic radiologic examination', 'Disease', 'Disease Progression', 'Dyspnea', 'Environment', 'Evaluation', 'Exhibits', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Polymorphism', 'Genomic approach', 'Genomics', 'Goals', 'Heterogeneity', 'Hospitals', 'Image', 'Investigation', 'Lobar', 'Lobe', 'Lung', 'Lung Volume Reductions', 'Lung diseases', 'Machine Learning', 'Measures', 'Medical', 'Medical Genetics', 'Medical Research', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Outcome', 'Pathologic', 'Patients', 'Pattern', 'Phenotype', 'Physicians', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Procedures', 'Public Health', 'Pulmonary Emphysema', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Quantitative Trait Loci', 'Radiology Specialty', 'Records', 'Research', 'Research Personnel', 'Resources', 'Respiratory physiology', 'Science', 'Scientist', 'Smoker', 'Structure of parenchyma of lung', 'Subgroup', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Therapeutic Intervention', 'Tissues', 'Translations', 'United States', 'Variant', 'Vascular remodeling', 'Visual', 'Walking', 'Woman', 'Work', 'attenuation', 'base', 'biomedical informatics', 'career', 'career development', 'clinical practice', 'clinically relevant', 'clinically significant', 'cohort', 'data mining', 'disease heterogeneity', 'disease phenotype', 'disorder risk', 'disorder subtype', 'effective therapy', 'genetic architecture', 'genetic association', 'genetic epidemiology', 'genetic predictors', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic predictors', 'imaging biomarker', 'imaging genetics', 'improved', 'instructor', 'interest', 'learning strategy', 'medical schools', 'mortality', 'multidisciplinary', 'novel', 'outcome forecast', 'personalized care', 'predicting response', 'predictive modeling', 'prognostic', 'quantitative imaging', 'rare variant', 'research and development', 'respiratory', 'tomography', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2018,170640,-0.05135543274143036
"Automated Object Contouring Methods & Software for Radiotherapy Planning Abstract In 2015, 1,658,370 new cancer cases are estimated to occur in the US, where nearly two-thirds will have radiation therapy (RT). Given that there are over 2,300 RT centers in the US, and current systems for contouring organs at risk (OARs) rely mostly on manual methods, there is a strong commercial opportunity for producing a software system that can contour OARs in medical images at a high degree of automation and for impacting current practice of RT planning. Encouraged by our strong Phase I results in thoracic and head and neck (H&N) body regions compared to current industry systems, we seek the accuracy, efficiency, and clinical acceptance of the contours output by our software product to significantly exceed those of existing systems. Our overall aim for Phase II is to advance the algorithms and prototype software developed in Phase I into a leading commercial software product, and demonstrate its efficacy in multiple medical centers across the country with diverse populations. Phase II specific aims are three-fold: (1) Further advance the automatic anatomy recognition algorithms from Phase I using advanced deep learning techniques. (2) Develop a cloud-based software auto contouring service. (3) Perform clinical evaluation of the new software on H&N and thoracic cases. Aim 1 will be accomplished in three stages: (a) Automating the process of defining the body region on given patient CT studies, which is currently done manually in our system, via a new concept of virtual landmarks using deep learning techniques. (b) Improving object recognition/ localization accuracy from the current 2 voxels for “good” quality data sets to 1 voxel and from 4-5 voxels for “poor” quality data sets to 2-3 voxels by using virtual landmarks to learn object relationships. (c) Improving object delineation by combining object localization methods with deep learning techniques applied to the vicinity of the localized objects to bring boundary distance accuracy within 1 voxel. Aim 2 will be achieved by developing a cloud-based Software-as-a-Service model to implement the software that incorporates the algorithms. To accomplish Aim 3, an evaluation study involving four academic RT centers will be undertaken to assess the efficiency, accuracy, and acceptability of the contours output by the new software. To assess efficiency, contouring time taken by the current clinical process will be compared to the time taken by the new software method plus any manual adjustment needed. Accuracy will be assessed by comparing software output to carefully prepared ground truth contours. Acceptability will be determined by conducting a blinded reader study, where an acceptability score (1-5) is given by radiation oncologists to software produced contours, ground truth contours, and contours produced by the normal clinical process, and comparing these scores. Expected clinical outcomes are significantly improved clinical efficiency/ acceptability of contouring compared to current practice. There is a strong commercial opportunity for producing a software system that can contour organs at risk in medical images at a high degree of automation for impacting current practice of radiation therapy planning. Encouraged by strong competitive results from the Phase I part of this project, in this grant, further technical advances and a cloud-based software service are proposed. A multicenter clinical evaluation of the new product is also planned to assess the clinical efficacy of the system.",Automated Object Contouring Methods & Software for Radiotherapy Planning,9622047,R42CA199735,"['Adoption', 'Algorithms', 'Anatomy', 'Automation', 'Back', 'Biological Neural Networks', 'Blinded', 'Body Regions', 'Chest', 'Clinical', 'Collaborations', 'Computer software', 'Country', 'Data Quality', 'Data Set', 'Development', 'Evaluation', 'Evaluation Studies', 'Grant', 'Head and neck structure', 'Health Personnel', 'Image', 'Imagery', 'Industry', 'Inferior', 'Investigation', 'Investments', 'Learning', 'Location', 'Malignant Neoplasms', 'Malignant neoplasm of thorax', 'Manuals', 'Medical Imaging', 'Medical center', 'Methods', 'Modeling', 'Morphologic artifacts', 'Names', 'Object Attachment', 'Organ', 'Outcome', 'Output', 'Pathology', 'Patients', 'Phase', 'Population Heterogeneity', 'Process', 'Protocols documentation', 'Radiation Oncologist', 'Radiation therapy', 'Reader', 'Research', 'Risk', 'Scanning', 'Service delivery model', 'Services', 'Slice', 'Specific qualifier value', 'System', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Time', 'Work', 'base', 'clinical efficacy', 'cloud based', 'deep learning', 'image processing', 'improved', 'interest', 'learning network', 'learning strategy', 'novel strategies', 'object recognition', 'prototype', 'research clinical testing', 'software as a service', 'software development', 'software systems', 'treatment center', 'treatment planning', 'virtual']",NCI,"QUANTITATIVE RADIOLOGY SOLUTIONS, LLC",R42,2018,1057846,-0.04573214646133128
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,-0.00963881401568887
"Microscopy-Based Antimicrobial Susceptibility Testing (MAST) Antibiotic resistance is compromising our ability to treat bacterial infections. Clinical microbiology laboratories guide appropriate treatment through antimicrobial susceptibility testing (AST) of patient bacterial isolates. However, increasingly, pathogens are developing resistance to a broad range of antimicrobials, requiring AST of less commonly used or recently introduced agents for which no commercially available or FDA-cleared testing methods exist. Agar and broth dilution are gold standard methods for AST that can be used to test any antimicrobial; however, labor and technical complexity precludes their use in hospital-based clinical laboratories. Therefore, bacterial isolates often must be sent to a reference laboratory with a 4-6 day delay in results. Furthermore, even standard methods require overnight incubation prior to readout. Therefore, there exists a significant AST testing gap in which current methodologies cannot adequately address the need for rapid results in the face of unpredictable susceptibility profiles. Our laboratory has recently verified inkjet printer-based digital dispensing technology as a novel platform to facilely perform reference AST for any antimicrobial at will. In this proposal, we aim to combine this methodology with advanced microscopy to leapfrog traditional AST capabilities through: (1) development of a method for microscopic imaging of bacterial replication on a solid-phase, 384-well microplate AST format, thereby allowing determination of susceptibility for any drug at will in 4 hours and (2) development and application of advanced image analysis for automated susceptibility calls. This new platform is designated MAST for microscopy-based antimicrobial susceptibility testing. The clinical diagnostic performance of the platform will be optimized against an AST reference method for accuracy and precision using a large panel of well-characterized clinical isolates. We anticipate establishing a prototype platform that will address the AST testing gap and thereby help our health system more effectively address the antimicrobial resistance threat. With the emergence of multi-drug resistant bacteria, it is no longer possible to accurately predict which antimicrobials will be effective against life-threatening bacterial illness. Testing bacteria directly for response available therapies may take several days. Therefore, a new technology platform called MAST is proposed to allow us to determine which antibiotics can treat a bacteria infection in a matter of hours and thereby address our current, clinically unacceptable antimicrobial testing gap.",Microscopy-Based Antimicrobial Susceptibility Testing (MAST),9455026,R21AI130434,"['Address', 'Agar', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Antimicrobial susceptibility', 'Bacteria', 'Bacterial Infections', 'Biological Neural Networks', 'Clinical', 'Clinical Microbiology', 'Development', 'Diagnostic tests', 'Goals', 'Gold', 'Growth', 'Health system', 'Hospitals', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Infection', 'Laboratories', 'Life', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Monitor', 'Multiple Bacterial Drug Resistance', 'Nutrient', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Predisposition', 'Printing', 'Regimen', 'Resistance', 'Resistance development', 'Solid', 'Supervision', 'Surface', 'Technology', 'Test Result', 'Testing', 'antimicrobial', 'base', 'biomaterial compatibility', 'clinical diagnostics', 'digital', 'direct application', 'microscopic imaging', 'new technology', 'next generation', 'novel', 'pathogen', 'performance tests', 'prototype', 'response']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R21,2018,263805,-0.022676588184776993
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,-0.004952957923773621
"A Systems Biology Approach to Investigate the Structure Changes of Biological Network Project Summary/Abstract Networks have been widely used to describe many biological processes. Understanding the structure of biological network, especially regulatory network, will provide a key to discovering the mechanisms underlying important biological processes and pathogenesis of diseases. One of the most challenging tasks in systems biology is how to correctly reconstruct the networks from the high-dimensional data generated by modern genomic technology. Most network inference methods assume the network structure is time-invariant. Some recent studies revealed the structures of some biological networks are non-stationary or time-varying. For example, the neural information flow networks of brains are changing during learning process. Importantly, cancer studies found the native T cells would be converted into senescent T cells due to the structure changes of genetic network during tumorigenesis. The stationary network inference methods can't be used to reconstruct the time-varying network. Non-stationary network inference methods are urgently needed to investigate the time-varying networks at different stages. Some researchers have attempted to develop some time-varying network inference methods. However, the inferred networks using existing methods are only correlation or causality graphs, not regulatory networks which require activation & inhibition information. This project aims to develop novel non-stationary network inference methods to reconstruct time-varying regulatory networks from time series data. Since the networks are highly complex, it is not realistic to manually verify large networks as being used by the traditional methods. We will develop a powerful Model Checker, which is a Turing Award winning technique for hardware system verification, to intelligently verify the inferred time-varying networks. Our long-term goal is to integrate the statistical inference and model checking techniques in a unified platform to automatically reconstruct and verify time-varying networks. This integrative systems biology approach will make the large-network inference and verification automatic, intelligent and efficient. Recent cancer studies show that, restoring senescent T cells represents a promising strategy for cancer treatment. In collaboration with cancer immunologist, we will apply computational-experimental approaches to investigate what structure changes of the genetic network and how they induce T-cell's functional changes and influence its fate decision making from naive T-cells to senescent T-cells. Answering these questions will significantly improve our understanding of the mechanisms underlying the T cell differentiation during tumorigenesis. Public Health Relevance/Narrative This project aims to develop a novel systems biology approach to reconstruct the time-varying biological networks from high-dimensional data in collaboration with the cancer immunologist. The proposed research has relevance to public health, because it seeks to investigate what and how the structure changes of genetic network induce the T-cell's functional changes during tumorigenesis, which will ultimately improve our understanding of the mechanisms underlying the T cell differentiation and cancer.",A Systems Biology Approach to Investigate the Structure Changes of Biological Network,9655801,R15GM129696,"['Algorithms', 'Attention', 'Award', 'Bayesian Modeling', 'Biological', 'Biological Process', 'Brain', 'Cancer Immunology Science', 'Cells', 'Code', 'Collaborations', 'Complex', 'Data', 'Databases', 'Decision Making', 'Development', 'Disease', 'Drosophila genus', 'Etiology', 'Evolution', 'Gene Structure', 'Genetic', 'Genomics', 'Goals', 'Graph', 'Immunologist', 'Knowledge', 'Laboratories', 'Learning', 'Life Cycle Stages', 'Location', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Muscle Development', 'Mutation', 'Pathogenesis', 'Process', 'Public Health', 'Regulator Genes', 'Regulatory T-Lymphocyte', 'Research', 'Research Personnel', 'Series', 'Structure', 'System', 'Systems Biology', 'T cell differentiation', 'T-Lymphocyte', 'Techniques', 'Technology', 'Time', 'Work', 'base', 'cancer therapy', 'computer studies', 'exhaust', 'experimental study', 'high dimensionality', 'improved', 'neoplastic cell', 'next generation sequencing', 'novel', 'public health relevance', 'reconstruction', 'relating to nervous system', 'senescence', 'tool', 'tumor microenvironment', 'tumorigenesis']",NIGMS,SAINT LOUIS UNIVERSITY,R15,2018,454500,-0.03274916416348072
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,-0.0035388565779969163
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9502903,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2018,879004,-0.03446756189115408
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9548457,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2018,273031,-0.006452803120542414
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9445086,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,305167,-0.019155018576707244
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9492705,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'biomarker validation', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunomodulatory therapies', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2018,637414,-0.05163948995241162
"Improving safety and efficacy of platelet transfusion through systems biology Project Summary Platelet transfusion is critical for severely bleeding patients and nearly 6 million units are transfused in the United States and Europe annually. In the United States, platelets are typically stored for 5 days resulting in a waste of 20% of their supply. Short storage duration is a consequence of bacterial contamination and platelet quality considerations. Though many methods have been developed for bacterial testing and pathogen inactivation, fewer have been developed for improving quality of stored platelets. Platelet additive solutions have the possibility to increase storage quality and duration, reduce plasma-related allergic reactions, impact the efficacy of pathogen reduction techniques, and save plasma which can then be used as an additional transfusion product. While the benefits are well known, there has been little progress in developing new platelet additive solutions for increasing quality and safety of platelet transfusion because there is a lack of broad understanding of biochemical and signaling changes during storage. There has been interest to utilize high-throughput metabolite profiling for global understanding of platelet metabolic decline but data analysis of complex datasets has been a daunting challenge. In Phase I of this program, we developed the first, robust computational platform involving statistical analysis and systems biology of metabolic and signaling networks to interpret and analyze PLT metabolomic and proteomic profiles in a complete network context. Using time- course global, quantitative metabolite profiling, we determined that PLTs undergo a non-linear decay process and computationally identified key metabolic enzymes and cellular process that drive this decay. Based on the computational results, we have devised two novel additive solution strategies to mitigate the decay process and improve the length of PLT units. In this Phase II proposal, we will validate the computationally determined additive solutions for efficacy in alleviating the non-linear decay process through 1) metabolomics experiments, and 2) non-metabolic PLT physiology experiments including cell activation and hemostatic effectiveness. A successful additive solution will be progressed to media refinement and preclinical testing. Project Narrative Platelet transfusion units are typically stored for five days in the United States leading to a waste of 20% of units and potential quality concerns. The field is open for innovation as most storage media technologies are derived from work from the early 1990s. This proposal will develop novel computational methods to comprehensively understand the degradation of platelets under storage conditions and experimentally validate new additive solutions for increasing platelet quality and extending shelf life, an area that accounts for $2.5 billion of hospital costs.",Improving safety and efficacy of platelet transfusion through systems biology,9506810,R44HL127843,"['Accounting', 'Agreement', 'Algorithms', 'Allergic Reaction', 'Area', 'Biochemical', 'Biological', 'Blood', 'Blood Component Removal', 'Blood Platelets', 'Cell physiology', 'Cells', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Simulation', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Effectiveness', 'Enzymes', 'Equipment and supply inventories', 'Europe', 'Formulation', 'Glutathione', 'Hemorrhage', 'Hemostatic Agents', 'Hospital Costs', 'In Vitro', 'Intervention', 'Length', 'Lesion', 'Life', 'Literature', 'Machine Learning', 'Mathematics', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Methods', 'Modeling', 'Pathway interactions', 'Patients', 'Phase', 'Physiology', 'Plasma', 'Platelet Transfusion', 'Preclinical Testing', 'Process', 'Production', 'Proteomics', 'Reaction', 'Recovery', 'Resources', 'Risk', 'Safety', 'Signal Pathway', 'Signal Transduction', 'State Hospitals', 'Statistical Data Interpretation', 'Supplementation', 'Surveys', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Transfusion', 'United States', 'Validation', 'Work', 'base', 'care costs', 'cost', 'design', 'experimental study', 'human subject', 'improved', 'insight', 'interest', 'metabolic profile', 'metabolomics', 'model design', 'novel', 'open innovation', 'oxidative damage', 'pathogen', 'predictive modeling', 'preservation', 'programs', 'statistics', 'success', 'time use', 'wasting']",NHLBI,"SINOPIA BIOSCIENCES, INC.",R44,2018,428727,-0.01490936352600404
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9559432,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2018,325325,-0.01536988530083576
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,9410515,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Cohort Studies', 'Cross-Sectional Studies', 'Custom', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'biomarker evaluation', 'burden of illness', 'candidate marker', 'case control', 'clinical practice', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'flexibility', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'optimal treatments', 'outcome prediction', 'patient biomarkers', 'patient population', 'patient response', 'predictive marker', 'programs', 'public health relevance', 'randomized trial', 'screening', 'tool', 'treatment effect', 'treatment response']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2018,323434,-0.06880505697344363
"A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions Abstract. Melanoma is the third most common form of skin cancer with estimated 87,110 new cases diagnosed in the United States in the year 2017. Current routine diagnostic approaches utilize microscopic evaluation of thinly sectioned patient biopsies, but in certain cases diagnosis can be contentious even among experts. The overall goal of this multi-phase SBIR project is to develop, validate, and commercialize MelanoMapTM, Frontier Diagnostics' patented assay for the diagnosis of melanoma using a matrix-assisted laser desorption/ionization imaging mass spectrometry (MALDI IMS) platform—and to have this assay available to pathologists in the U.S. as a laboratory developed test. MALDI IMS is a state-of-the-art technology that generates molecular images of tens to thousands of biomolecules from tissue sections in a single analysis. The assay uses formalin-fixed paraffin embedded (FFPE) biopsies used in routine histopathological diagnosis. The proposed assay has pathologists select regions of skin biopsies for analysis via a remote web interface. The acquired IMS data from those regions unambiguously identifies malignant melanoma or benign nevus.  Phase I of this proposal will demonstrate the feasibility of this technology platform to achieve cost-effective diagnosis of melanoma from patient skin biopsies at sample volumes acceptable for a clinical laboratory. Specific Aim 1 focuses on the development of a scalable and robust analytical protocol in both sample preparation and informatics to accurately diagnose melanoma with MALDI IMS. In specific aim 2, we will test the methodology developed in Specific Aim 1 on a cohort of melanocytic lesions with known clinical outcome and subsequently validate the classification accuracy of the proposed test  In Phase II, the protocols developed in Phase I will be integrated into a diagnostic service workflow. This phase will focus on quality control measures, client facing cloud software, clinical diagnostic reporting, and completing the analysis of a 500-patient sample set for final assay validation. Specific Aim 3 of this proposal (initial aim of Phase II) will establish and implement test tissues into standard workflows that will provide performance metrics for standard operation of a test meeting Clinical Laboratory Improvement Amendments (CLIA) standards. Protocols will be developed to monitor reagents, the reproducibility of sample preparation, and mass spectrometer performance on daily basis. Specific Aim 4 will expand software capabilities to include a secure web interface for clients ordering the test and the laboratory performing the test. The software will meet regulatory compliance, perform statistical analysis, and generate and communicate reports of the MALDI IMS analysis. Specific Aim 5 proposes to expand the sample set used in the initial assay from Specific Aim 2 to include a set of 300 patient samples from our clinical collaborators with 5 or more years follow-up data. The test will be independently validated by an additional 200 patient samples with definitive diagnoses. Project Narrative This multi-phase Fast Track SBIR project will develop and validate a new laboratory developed test to differentiate malignant melanocytic tumors from benign nevi and complete development of an imaging mass spectrometry-based diagnostic service platform for a clinical laboratory. The clinical assay developed under this proposal augments current practice by providing molecular measurements that are used as objective criteria in the diagnosis of melanoma. Successful completion of this Fast Track project will result in a fully documented and validated assay ready for launch as a laboratory developed test.",A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions,9557356,R44CA228897,"['Algorithms', 'Amendment', 'Antigens', 'Area', 'Benign', 'Biological Assay', 'Biopsy', 'Caliber', 'Cells', 'Cessation of life', 'Classification', 'Client', 'Clinical', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Services', 'Digestion', 'Early Diagnosis', 'Ensure', 'Evaluation', 'Formalin', 'Goals', 'Gold', 'Image', 'Incentives', 'Informatics', 'Laboratories', 'Legal patent', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Melanocytic Neoplasm', 'Metadata', 'Methodology', 'Microscopic', 'Microtomy', 'Molecular', 'Monitor', 'Nevus', 'Outcome', 'Paraffin Embedding', 'Pathologist', 'Pathology Report', 'Patient Care', 'Patients', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Physical shape', 'Physicians', 'Preparation', 'Procedures', 'Proteins', 'Protocols documentation', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Research', 'Retrieval', 'Risk', 'Sampling', 'Secure', 'Security', 'Sensitivity and Specificity', 'Side', 'Skin', 'Skin Cancer', 'Small Business Innovation Research Grant', 'Software Tools', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Standardization', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Treatment Cost', 'United States', 'Validation', 'accurate diagnosis', 'analytical method', 'base', 'clinical diagnostics', 'cloud software', 'cohort', 'cost', 'cost effective', 'data acquisition', 'diagnosis standard', 'diagnostic assay', 'disease classification', 'follow-up', 'frontier', 'histopathological examination', 'instrumentation', 'interest', 'mass spectrometer', 'meetings', 'melanoma', 'molecular diagnostics', 'molecular imaging', 'operation', 'prototype', 'quality assurance', 'skin lesion', 'tissue preparation', 'web interface']",NCI,"FRONTIER DIAGNOSTICS, LLC",R44,2018,299244,-0.014523144070393052
"Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease Abstract: Hepatocellular carcinoma (HCC) is the third most common cause of cancer-related death worldwide and is rising in incidence in the US. 90% of patients in the US with liver cancer have underlying cirrhosis, thus guideline recommendations recommend surveillance in all patients with cirrhosis to facilitate early detection. Unfortunately, only 20-30% of patients are detected with early detection and are thus eligible for potentially curative treatments. There is an unmet need for reliable biomarkers for HCC to facilitate adherence to screening and for early detection. In the proposed work we will develop early detection strategies for HCC based on glycoproteomic profiles. Unique changes in glycosylation in proteins, which involve structural changes in glycan groups, have been shown to be important serum biomarkers for early cancer detection. Importantly, the subtle changes may only involve minor structures but they can be very specific in differentiating cirrhosis versus early versus late stage HCC. In addition, these changes may be specific to the etiology of liver disease. These glycan structural changes will be detected and monitored quantitatively using a mass spectrometry approach which has proven to be an accurate way to characterize even minor changes in structure which may be significant as biomarkers based on our previous mass analysis, tandem mass spectrometry measurements and databases which have been developed for glycan and glycopeptide analysis. This will be demonstrated for both glycan and glycopeptide screening from serum using novel extraction and separation methods coupled to mass spectrometry which can ultimately be used to distinguish early stage HCC from cirrhosis. The proposed work will deliver separations and mass spec methods enabling isomeric separation of glycans and glycopeptides, permitting unequivocal assignment of protein glycosylation related to disease state. We will be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. Novel software will be developed and used to assign these glycan structures. The markers will be discovered for specific etiologies of HCV-related, alcohol-related and NAFLD-related etiologies of HCC. This will be a multisite study to include all components required for a tumor biomarker lab including samples and sample preparation, separations and mass spec analysis, bioinformatics evaluation and statistical analysis. Ultimately, we will develop methods for discovery of glycan/glycopeptide markers from patient serum, the identification of potential markers and the development of new assays to provide a limited confirmation of these markers. Project Narrative: The proposed work will use new separations and mass spec methods to provide isomeric separation of glycans and glycopeptides, resulting in detailed assignment of protein glycosylation related to disease state. We expect to be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. The markers will be discovered for specific etiologies of HCV- related, alcohol-related and NAFLD-related etiologies of HCC. Ultimately, we will develop new assays to provide a limited confirmation of these markers.",Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease,9495790,U01CA225753,"['Adherence', 'Alcohol-Related Hepatocellular Carcinoma', 'Alcohols', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Cancer Etiology', 'Carbon', 'Cessation of life', 'Cirrhosis', 'Complex', 'Computer software', 'Coupled', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Etiology', 'Europe', 'Evaluation', 'Frequencies', 'Glycopeptides', 'Glycoproteins', 'Guidelines', 'Hepatitis B Virus', 'Hepatitis C virus', 'Incidence', 'Isomerism', 'Japan', 'Lectin', 'Liver Cirrhosis', 'Liver diseases', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Mass Spectrum Analysis', 'Measurement', 'Methods', 'Minor', 'Monitor', 'Natural graphite', 'Patients', 'Pattern', 'Peptides', 'Performance', 'Polysaccharides', 'Preparation', 'Primary carcinoma of the liver cells', 'Protein Glycosylation', 'Protein Isoforms', 'Proteins', 'Proteome', 'Recommendation', 'Risk', 'Sampling', 'Screening for cancer', 'Serum', 'Serum Markers', 'Site', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Survival Rate', 'Testing', 'Time', 'Tumor Markers', 'Ultrasonography', 'Work', 'alpha-Fetoproteins', 'base', 'carbohydrate structure', 'curative treatments', 'diagnostic screening', 'early detection biomarkers', 'early onset', 'glycoproteomics', 'glycosylation', 'improved', 'non-alcoholic fatty liver disease', 'nonalcoholic steatohepatitis', 'novel', 'patient screening', 'patient stratification', 'precision medicine', 'screening', 'sialylation', 'tandem mass spectrometry', 'tumor']",NCI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,U01,2018,529478,-0.06054587849703
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9661636,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2018,263913,-0.001050658270197014
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9465735,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Simulation', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'forest', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2018,178854,-0.023602032190622083
"Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy DESCRIPTION (provided by applicant): Our understanding of cervical remodeling during pregnancy and labor is incomplete, partly due to the lack of in vivo studies on the biochemical changes that occur in the cervix over the course of pregnancy. Elucidation of the mechanisms for cervical ripening could be used to predict the onset of preterm labor. Until recently, in vivo research methods were too invasive to be used as discovery tools, particularly in women who present with preterm labor. This proposal will use in vivo Raman spectroscopy, an optical technique that is sensitive to collagen content, collagen structure, hydration, lipids, proteins, ad other biomolecules to non-invasively investigate the biochemistry of the cervix throughout pregnancy. Using fiber optic in vivo Raman spectroscopy, we recently found significant differences in Raman spectra in at least four important peaks during the course of pregnancy in mice, including discrete signatures for lipids, collagen, amide bonds, and enriched amino acids (proline, tyrosine). Computational analysis of these spectra yielded predictive algorithms with 94% classification accuracy for stage of pregnancy. Studies performed in 2-hour windows at the end of pregnancy identified spectra predictive for the timing of parturition. This approach provides a detailed real-time biomolecular map of cervical ripening that is currently unavailable by other means. In this proposal, we hypothesize that the different mechanisms of premature cervical ripening have unique Raman spectral signatures that correspond to underlying biochemical and mechanical changes that precede preterm birth, which can be detected in vivo. Two Specific Aims are proposed: 1) Determine spectral changes in the cervix of mice with normal and abnormal pregnancy and parturition; 2) Identify specific mediators of cervical remodeling by comparing Raman spectra to mechanical and biochemical changes in the ex vivo cervix during normal and abnormal parturition. Raman spectroscopy has primarily been used for detection of disease. Collaboration between our reproductive biology and bioengineering groups will capitalize on our expertise in Raman analysis of cervical tissues to study dynamic changes in cervix composition during pregnancy. Key elements in cervical biochemistry will be identified. In vivo Raman spectroscopy will be combined with biomechanical studies and imaging mass spectrometry, a powerful tool for in situ proteomic analysis, to examine mice with premature or delayed cervical remodeling. Together, these highly innovative approaches will generate in-depth profiles of cervical biology that will translate into novel non-invasive methods to detect impending premature birth in women. PUBLIC HEALTH RELEVANCE: This proposal will use Raman Spectroscopy, a non-invasive, optical scattering technique, to investigate the composition of the cervix throughout pregnancy and provide detailed real-time information on cervical ripening. These studies will identify spectral differences in the cervix during normal and abnormal cervical maturation; optical and biochemical markers will be identified to help monitor pregnancy non-invasively, as the fiber optic probe only requires brief contact with the external surface of the cervix to obtain measurements. Elucidating the mechanisms that initiate cervical ripening will provide a critical step for early detection and treatment of preterm birth, which is the leading cause of infant morbidity and mortality.",Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy,9505946,R01HD081121,"['Address', 'Algorithms', 'Alprostadil', 'Amides', 'Amino Acids', 'Applications Grants', 'Biochemical', 'Biochemical Markers', 'Biochemistry', 'Biological', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biomechanics', 'Biomedical Engineering', 'Birth', 'Cervical', 'Cervical Ripening', 'Cervix Uteri', 'Classification', 'Clinical', 'Collaborations', 'Collagen', 'Computational algorithm', 'Computer Analysis', 'Data', 'Detection', 'Development', 'Disease', 'Early Diagnosis', 'Early treatment', 'Elasticity', 'Elements', 'Emerging Technologies', 'Etiology', 'Fetal Development', 'Fiber Optics', 'Foundations', 'Generations', 'Goals', 'Health', 'High-Risk Pregnancy', 'Hormonal', 'Hour', 'Hydration status', 'Image', 'Immunohistochemistry', 'Impairment', 'In Situ', 'In Situ Hybridization', 'Interdisciplinary Study', 'Laboratories', 'Lead', 'Link', 'Lipids', 'Machine Learning', 'Maintenance', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Mechanics', 'Mediator of activation protein', 'Medical', 'Methods', 'Mifepristone', 'Modality', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mus', 'Optics', 'Periodicity', 'Phenotype', 'Physiological', 'Pregnancy', 'Premature Birth', 'Premature Labor', 'Prevention', 'Process', 'Proline', 'Property', 'Proteins', 'Proteomics', 'Raman Spectrum Analysis', 'Reproductive Biology', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Spectrum Analysis', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Translating', 'Translations', 'Tyrosine', 'Woman', 'base', 'clinical application', 'in vivo', 'infant morbidity/mortality', 'innovation', 'insight', 'learning strategy', 'mouse model', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'physical science', 'prediction algorithm', 'predictive modeling', 'pregnant', 'premature', 'public health relevance', 'response', 'tool']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2018,387714,-0.015637589233675298
"A Real-Time Computational System for Detecting ARDS Using Ventilator Waveform Data No abstract available Project Narrative: Acute respiratory distress syndrome (ARDS) is a highly lethal disease contracted by over 100,000 Americans each year. We seek to address whether we can create automated technologies to detect onset and change in severity of ARDS in critically ill patients using widely available ventilator waveform data. With these automated diagnostic testing technologies, doctors can promptly deliver necessary treatments for ARDS before the disease worsens.",A Real-Time Computational System for Detecting ARDS Using Ventilator Waveform Data,9612053,F31HL144028,"['Acute', 'Acute respiratory failure', 'Address', 'Adult Respiratory Distress Syndrome', 'Affect', 'Airway Resistance', 'Algorithms', 'American', 'Automation', 'Berlin', 'Chest', 'Classification', 'Clinical', 'Complex', 'Computational algorithm', 'Contracts', 'Critical Illness', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Model', 'Detection', 'Deterioration', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Discipline', 'Disease', 'Electronic Health Record', 'Hospital Mortality', 'Hour', 'Hypoxemia', 'Intensive Care Units', 'Intervention', 'Learning', 'Life', 'Lung', 'Machine Learning', 'Mechanical ventilation', 'Medical History', 'Modeling', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Physiological', 'Physiology', 'Provider', 'Recording of previous events', 'Reporting', 'Reproducibility', 'Research', 'Respiratory Failure', 'Sampling', 'Savings', 'Scanning', 'Series', 'Severities', 'Specific qualifier value', 'Statistical Methods', 'System', 'Techniques', 'Technology', 'Time', 'United States', 'Ventilator', 'Work', 'adjudicate', 'base', 'clinical decision support', 'clinical predictors', 'clinical translation', 'cohort', 'computer science', 'deep learning', 'high dimensionality', 'high risk', 'improved', 'improved outcome', 'innovation', 'insight', 'markov model', 'model development', 'mortality', 'novel', 'predictive modeling', 'predictive tools', 'prevent', 'prognostic', 'recurrent neural network', 'respiratory', 'success', 'targeted treatment']",NHLBI,UNIVERSITY OF CALIFORNIA AT DAVIS,F31,2018,36701,-0.025557080010615272
"Phamarcogenomics of Statin Therapy (SUPPLEMENT) PARENT ABSTRACT The overall objective of the Center ""Pharmacogenomics of Statin Therapy"" (POST) is to apply genomic, transcriptomic, and metabolomic analyses, together with studies in cellular and animal models, and innovative informatic tools, to identify and validate biomarkers for efficacy of statin drugs in reducing risk of cardiovascular disease (CVD), and for adverse effects of statins, specifically myopathy and type 2 diabetes. This multidisciplinary approach is enabled by a team of investigators with expertise in genomics (human, mouse, and molecular), statistics and informatics, and clinical medicine and pharmacology. The Center is comprised of three Projects, two Research Cores, and an Administrative Core. A major aim of Project 1 is the identification of cellular transcriptomic and metabolomic markers for clinical efficacy and adverse effects of statins. This will be accomplished by analyses in statin-exposed lymphoblast cell lines derived from patients with major adverse coronary events, or onset of myopathy or type 2 diabetes on statin treatment, compared with unaffected statin-treated controls. In addition, using genome wide genotypes from these patients, DNA variants will be identified that are associated with statin-induced changes in the transcripts and metabolites that most strongly discriminate affected patients and controls. Project 2 will use a unique, well- characterized panel of 100 inbred mouse strains to discover genetic variation associated with statin- induced myopathy and dysglycemia. Mechanisms underlying these effects will be investigated, with emphasis on the role of dysregulation of autophagy by statin treatment. Projects 1 and 2 will also use relevant cellular and mouse models, respectively, to perform functional studies to validate effects of genes identified in all POST projects as strong candidates for modulating statin efficacy or adverse effects. In Project 3, information derived from genome-wide genotypes, electronic health records, and pharmacy data in a very large and diverse population-based patient cohort will be leveraged to identify and replicate genetic associations with statin efficacy (lipid lowering and CVD event reduction) and adverse effects (myopathy and type 2 diabetes), as well as to assess the overall heritability of these responses. The Clinical Core, based in Kaiser Permanente of Northern California, will provide the clinical information and biologic materials for both Projects 1 and 3. Investigators in the Informatics Core will optimize data analysis and integration of results across all projects. The Administrative Core will provide scientific leadership and management of the Center, and foster scientific interactions and training opportunities. Overall, the research program of this Center provides an innovative model for a ""systems"" approach to pharmacogenomics that incorporates complementary investigative tools to discover and validate genetically influenced determinants of drug response. Moreover, the findings have the potential for guiding more effective use of statins for reducing CVD risk and minimizing adverse effects, and identifying biomarkers of pathways that modulate the multiple actions of this widely used class of drugs. ADMINISTRATIVE SUPPLEMENT ABSTRACT In response to NOT-AG-18-008, our goal is to extend the validation and application of our data integration methodologies into Alzheimer’s disease research. This administrative supplement is designed to extend the work of the existing subaward to the University of Pennsylvania subcontract for the POST Informatics Core. The PGRN POST Informatics Core serves as the central hub for data sharing and coordination across the three POST projects in the PGRN P50 award. One of our jobs is annotating the extensive information that will be collected and providing analysis expertise to the projects as needed. However, to make great strides in scientific progress and ensure that the collective whole of the Center is greater than the sum of the parts, a key function of the Informatics Core is to serve as ‘The Integrator’ to combine these data and information. We and others have shown that integration of complementary omics-based data can provide emergent insights into biological processes compared to what can be learned through any single approach alone. The methods that we are developing to integrate data for statin pharmacogenomic phenotypes will be equally applicable in the area of Alzheimer’s disease. Additionally, recent emphasis on open data science by Alzheimer’s disease researchers provides ample data for us to interrogate our method. We have developed novel statistical analysis tools such as the Analysis Tool for Heritable and Environmental Network Associations (ATHENA), and data visualization tools, such as PhenoGram, both of which are designed to collect and combine information from diverse data sources. With these tools, we will leverage publicly available Alzheimer’s disease datasets to maximize the knowledge gleaned about disease risk for Alzheimer’s diseases. The methodologies that we have been developing as part of the PGRN POST award for the past 2.5 years are clearly applicable to the study of Alzheimer’s disease risk. An important validation step of the application of our methodologies is to apply them to different types of datasets and in different phenotypic areas. This administrative supplement focused on extended research into having an Alzheimer’s disease focus is a great mechanism to simultaneously allow us to validate our methodologies with different types of data and potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease. Finally, we may have the opportunity to identify cross biological implications due to the known pleiotropic relationships between Alzheimer’s disease and cardiovascular disease. PROJECT NARRATIVE This administrative supplement is focused on extended our data integration research into Alzheimer’s disease. This funding mechanism will simultaneously allow us to validate our methodologies with different types of data and also potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease.",Phamarcogenomics of Statin Therapy (SUPPLEMENT),9719267,P50GM115318,"['Administrative Supplement', 'Adverse effects', 'Affect', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Animal Model', 'Area', 'Autophagocytosis', 'Award', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'California', 'Cardiovascular Diseases', 'Cell Line', 'Cell model', 'Clinical', 'Clinical Markers', 'Clinical Medicine', 'Clinical Pharmacology', 'DNA', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Electronic Health Record', 'Ensure', 'Etiology', 'Event', 'Fostering', 'Funding Mechanisms', 'Genes', 'Genetic Variation', 'Genomics', 'Genotype', 'Glean', 'Goals', 'Heritability', 'Human', 'Inbred Strains Mice', 'Informatics', 'Knowledge', 'Leadership', 'Learning', 'Lipids', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Myopathy', 'Non-Insulin-Dependent Diabetes Mellitus', 'Occupations', 'Parents', 'Pathway interactions', 'Patients', 'Pennsylvania', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacy facility', 'Phenotype', 'Population Heterogeneity', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Risk Factors', 'Role', 'Sampling', 'Source', 'Statistical Data Interpretation', 'Sum', 'System', 'Transcript', 'Universities', 'Validation', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'cardiovascular disorder risk', 'clinical efficacy', 'cohort', 'coronary event', 'data access', 'data hub', 'data integration', 'data sharing', 'data visualization', 'design', 'experimental study', 'genetic association', 'genome wide association study', 'genome-wide', 'genotyped patients', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'knowledge integration', 'lymphoblast', 'metabolomics', 'mouse model', 'multiple omics', 'novel', 'novel marker', 'open data', 'population based', 'predictive marker', 'programs', 'response', 'risk minimization', 'statistics', 'tool', 'training opportunity', 'transcriptomics']",NIGMS,CHILDREN'S HOSPITAL & RES CTR AT OAKLAND,P50,2018,395986,-0.06252548380233108
"Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD Overall Summary The overarching goal of the proposed center is to leverage molecular and circuit biomarkers to advance the understanding of mechanisms and personalized treatment of topiramate treatment of Alcohol Use Disorder comorbid with PTSD. We propose an integrative translational focus on alterations in excitatory and inhibitory signaling, focusing on GABA and glutamate and related circuitry, to model the neurobiology of PTSD comorbid with PTSD and the mitigating effects of topiramate. We will characterize excitatory and inhibitory molecular markers in an animal model of AUD comorbid with PTSD, utilizing genomic markers in the brain and plasma markers in rodents. In clinical trial participants we will characterize excitatory and inhibitory neuronal signaling by ascertaining plasma markers, GRIK 1 genotype and neural circuit markers utilizing TMS evoked potentials in EEG, task-based functional MRI and MR spectroscopy. This goal will be achieved through the activities of three research projects supported by two research cores, the administrative core and the Scientific Advisory Board (Figure1). In Project 1 lead by Silvia Fossati Ph.D. and Jorge Manzanares Robles Ph.D. we will study the behavioral and molecular effects of two doses of topiramate vs. vehicle in animal models of AUD alone, PTSD alone and AUD+PTSD. In Project 2 lead by Michael Bogenschutz M.D. and Joshua Lee M.D. we will study the behavioral, genetic and plasma biomarker effects of topiramate vs. placebo in 150 participants with co-occurring AUD and PTSD. In project 3 lead by Amit Etkin M.D., Ph.D. and Charles R. Marmar M.D. we will ascertain multi-modal imaging markers including task based fMRI, TMS evoked potentials in EEG and MRS. Imaging markers will be used to characterize excitatory and inhibitory circuits in Project 2 clinical trial participants with AUD+PTSD to determine predictors and mechanisms of topiramate vs. placebo treatment outcomes. Plasma biomarkers in Project 2 will be related to the same or homologous plasma biomarkers in Project 1. Circuit markers from Project 3 will be related to genomic markers in the same or homologous brain regions in Project 1. The Biofluids Biomarker Core (BBC) lead by Dr. Fossati will support collection of plasma biomarkers (GABA, glutamate, HPA axis, neuropeptides, neuroinflammatory and oxidative stress) in animals in Project 1 and clinical trial participants in Project 2. The Analytics and Biostatistics Core (ABC) lead by Eugene Laska Ph.D. and Carole Segal Ph.D. will support experimental design, formulation of hypothesis, power calculations, and data integrity, management and analysis for Project 1, 2 and 3, implementing advanced statistical models for individualized prediction of response to topiramate in Project 1 and Project 2. Overall Narrative We aim to advance personalized topiramate treatment for comorbid AUD and PTSD by focusing on network excitatory and inhibitory imbalances underlying heightened negative emotions and impairments in executive function and emotion regulation. Excitatory and inhibitory targets will be assessed with brain, plasma and circuit markers, where possible harmonizing the approach for optimizing discovery of mechanisms and prediction of topiramate treatment in animal models and in clinical trial participants. We will achieve this aim with a center driven approach to discovery with three integrative design elements and four cross-project center aims, creating translational bridges among animal model, clinical trial and imaging studies.",Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD,9608495,P01AA027057,"['Affect', 'Aftercare', 'Alcohols', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biological', 'Biological Markers', 'Biostatistics Core', 'Blood', 'Brain', 'Brain region', 'Clinical', 'Clinical Treatment', 'Clinical Trials', 'Cognition', 'Cognitive', 'Collection', 'Comorbidity', 'Complement', 'Data Analytics', 'Disease model', 'Doctor of Medicine', 'Doctor of Philosophy', 'Dose', 'Electroencephalography', 'Elements', 'Emotional', 'Emotions', 'Equilibrium', 'Evoked Potentials', 'Experimental Designs', 'Formulation', 'Fostering', 'Freezing', 'Functional Magnetic Resonance Imaging', 'Gene Expression', 'Genetic', 'Genotype', 'Glutamates', 'Goals', 'Human', 'Impairment', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance Spectroscopy', 'Measures', 'Mediating', 'Mediation', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurobiology', 'Neurocognitive', 'Neurons', 'Neuropeptides', 'Outcome', 'Oxidative Stress', 'Participant', 'Patients', 'Pharmacology', 'Placebos', 'Plasma', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Randomized', 'Randomized Clinical Trials', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Rodent', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Stimulus', 'Structure', 'Symptoms', 'Testing', 'Transcranial magnetic stimulation', 'Translations', 'Treatment outcome', 'Work', 'alcohol abuse therapy', 'alcohol comorbidity', 'alcohol cue', 'alcohol use disorder', 'base', 'behavioral study', 'clinical imaging', 'clinically relevant', 'cognitive neuroscience', 'data integrity', 'data management', 'design', 'drinking', 'dual diagnosis', 'effective therapy', 'emotion regulation', 'executive function', 'gamma-Aminobutyric Acid', 'genomic biomarker', 'imaging biomarker', 'imaging study', 'molecular marker', 'mortality', 'mouse model', 'neural circuit', 'neurobiological mechanism', 'neuroinflammation', 'neurophysiology', 'neurotransmission', 'personalized medicine', 'personalized predictions', 'pre-clinical', 'precision medicine', 'predicting response', 'predictive modeling', 'protein expression', 'response', 'source localization', 'topiramate', 'treatment effect', 'treatment response']",NIAAA,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P01,2018,1202962,-0.03493234528078444
"Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions Project Summary Systematic reviews (SRs) synthesize and critically assess bodies of evidence to produce a comprehensive unbiased assessment of what is known. As such, SRs are vital for evidence-based decision-making. However, given the pace of new research, the process of developing an SR remains too slow. One particularly time-consuming step in the process is citation screening, which requires manual review of thousands of abstracts to identify only a small number of relevant studies. Screening such large numbers of studies is necessary because systematic reviewers place a high priority on identifying all relevant studies to avoid bias. Innovative citation screening tools, which utilize text-mining and new sophisticated machine learning methods, represent one potential solution. Abstrackr (Brown University) and EPPI-Reviewer (University College London) are off- the-shelf, web-based citation screening tools designed to improve screening efficiency. Both programs utilize machine- learning techniques to semi-automate the screening process by modeling the probability that each citation will meet criteria for inclusion. This allows efficiency gains through screening prioritization and screening truncation. With screening prioritization, citations are organized for screening from highest to lowest likelihood of inclusion. This allows earlier retrieval of full-text articles and facilitates workflow planning. Organizing citations by likelihood of inclusion also allows reviewers the option of truncating the screening process when remaining citations fall below a certain threshold. While promising, existing studies have predominantly been performed by computer scientists testing individual tools or comparing different modeling algorithms (e.g., various classifiers). To date, no studies have performed a direct comparison of citation screening tools. Similarly, although automatically excluding citations that fall below particular thresholds could substantively improve efficiency, adoption has been low due to concerns that relevant studies could be missed. However, how often studies would be missed and how important such omissions would be remains unknown. To address these knowledge gaps, this project will (1) Compare screening efficiency for two citation-screening tools, Abstrackr and EPPI-reviewer, and (2) Characterize the potential impact of using thresholds to exclude low probability studies automatically. To address aim 1, using citations from 3 large and 6 small completed evidence reports, we will compare Abstrackr to EPPI-Reviewer for citation screening. Using screening prioritization, we will assess what proportion of articles must be screened to identify all included studies (e.g., to achieve 100% sensitivity). For Aim 2, we will explore the potential impact of excluding all citations that fall below particular thresholds during the screening process. We will also assess to what extent missing these studies would alter report conclusions. By characterizing potential efficiency gains from new, innovative, and widely accessible tools, this project can facilitate wider adoption by evidence based practice centers seeking to speed systematic review production. Project Narrative Systematic reviews provide a comprehensive, unbiased assessment of a body of literature and are vital for timely, evidence-based decision-making. However, development of systematic reviews remains too slow, with one particularly time-consuming step being citation screening, in which thousands of research abstracts are manually reviewed to identify a small number of relevant studies. By testing potential gains in citation screening efficiency offered by two innovative, widely-accessible machine- learning tools (Abstrackr and EPPI–Reviewer), and determining if automatically excluding studies to improve speed compromises report conclusions, we hope to enable more rapid production of systematic reviews to inform clinicians and policy-makers and promote high quality, evidence-based patient care.",Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions,9435231,R03HS025859,[' '],AHRQ,ECRI INSTITUTE,R03,2018,95324,-0.010432932192639919
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9584612,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Frontal Lobe Epilepsy', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'Informatics', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'phenotypic data', 'rare variant', 'sample collection', 'tool']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2018,75911,-0.036169469289590465
"Competing Revision - Precision Medicine and Oral Anticoagulants Pharmacogenomics and other ‘omics methods are cornerstone technologies that will drive biomarker discovery for the Personalized Medicine revolution. However, these technical advances are not yet uniformly applied, as there is under-representation in personalized medicine (PM) research that cuts across ages, genders, racial minorities, and socioeconomic groups. This is particularly true for American Indian and Alaska Native (AI/AN) populations, which are critical communities to involve in discovery research for several reasons. First, these populations have been neglected to date, partly due to a lack of community engagement and historical mistrust around the scientific enterprise. Second, these oftentimes isolated populations can have differences in the prevalence of known, function-disrupting gene variants of clinical significance and/or may possess novel variants with altered function. Third, the effects of unique dietary and other environmental influences among these AI/AN populations on drug response may be modified by genetic variation. We address these issues in Aims 1-3 of Project 2 of our Program Project grant, which seeks a better understanding of the relationship between novel measurements of short- and long-term hepatic vitamin K status, variation in vitamin K cycle genes, and warfarin anticoagulation outcomes. Warfarin is still the most commonly used drug in its pharmacological class in the US and has enormous clinical utility for AI/AN populations, who have more restricted access to emergency hospital facilities and expensive new anticoagulant medications. In Aim 1 we will develop more sensitive and specific biomarkers of hepatic vitamin K status than the existing plasma ELISA-based PIVKA-II and vitamin K assays, using LC-MS/MS-based approaches to quantitate all eleven (proteo)forms of carboxylated prothrombin (Factor II) in plasma and CYP4F2-dependent vitamin K catabolites in urine. This is biologically significant because hepatic vitamin K levels affect Factor II proteoform synthesis and several undercarboxylated proteoforms have clotting factor activity. In Aim 2, we will use our enhanced assays to (A) characterize short- and long-term hepatic vitamin K status in three AI/AN populations and test its association with CYP4F2*3 and (B) directly test whether the gene variant modifies the effect of vitamin K supplementation on hepatic vitamin K status. In Aim 3, we will test whether prospectively applied pharmacogenetic and biomarkers tests of hepatic vitamin K status are associated with long-term hemostasis control in AI/AN (and all other) populations receiving warfarin-based anticoagulation therapy. These aims are highly clinically significant. Specifically, they will enhance our understanding of the regulation of hepatic vitamin K status, and by inference all human vitamin K-dependent Gla proteins, by CYP4F2. The results may also further the development of a new decision tree for warfarin versus DOAC therapy based on the aforementioned PM test results.s. PROGRAM NARRATIVE The coordinated efforts of these Projects and Cores will advance our knowledge of gene-environment-drug interactions in the treatment of cardiovascular disease with anticoagulation and antiplatelet therapies in American Indian and Alaska Native (AI/AN) populations. This, we believe, can lead to safer and more effective drug therapies for the treatment of cardiovascular disease in AI/AN people and the general population. In addition, the program will enhance opportunities for responsible genomic research in AI/AN communities.",Competing Revision - Precision Medicine and Oral Anticoagulants,9416858,P01GM116691,"['Address', 'Admixture', 'Affect', 'Age', 'Alaska', 'Alaska Native', 'American Indians', 'Anticoagulants', 'Anticoagulation', 'Antiplatelet Drugs', 'Applications Grants', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Blood Coagulation Factor', 'Blood Platelets', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Clinical', 'Clinical Trials', 'Coagulation Process', 'Collaborations', 'Communities', 'Decision Trees', 'Development', 'Diet', 'Dietary Factors', 'Drug Interactions', 'Drug usage', 'Emergency Situation', 'Environment', 'Environmental Exposure', 'Environmental Risk Factor', 'Enzyme-Linked Immunosorbent Assay', 'Event', 'Gender', 'General Population', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Hemostatic function', 'Hepatic', 'Heterogeneity', 'Hospitals', 'Human', 'Individual', 'Institution', 'Investigation', 'Knowledge', 'Lead', 'Left', 'Marketing', 'Measurement', 'Methods', 'Modeling', 'Morals', 'Native-Born', 'Oral', 'Outcome', 'Pacific Northwest', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacogenomics', 'Pharmacology', 'Pharmacotherapy', 'Plasma', 'Platelet Activation', 'Polyunsaturated Fatty Acids', 'Population', 'Population Study', 'Prevalence', 'Procedures', 'Program Research Project Grants', 'Proteins', 'Prothrombin', 'Regulation', 'Research', 'Risk', 'Sample Size', 'Scientist', 'Site', 'Source', 'Supplementation', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Translational Research', 'Tribes', 'Universities', 'Urine', 'Variant', 'Venous Thrombosis', 'Vitamin K', 'Warfarin', 'acarboxy prothrombin', 'base', 'biomarker discovery', 'carboxylate', 'clinically significant', 'community based participatory research', 'community consultation', 'cost', 'drug response prediction', 'genetic variant', 'improved', 'individual response', 'neglect', 'novel', 'novel marker', 'personalized medicine', 'pharmacogenetic testing', 'precision medicine', 'prevent', 'programs', 'prospective', 'racial minority', 'response', 'rural Americans', 'socioeconomics', 'specific biomarkers', 'success']",NIGMS,UNIVERSITY OF WASHINGTON,P01,2018,297702,-0.03849111750604939
"Validation of Survey Questions to Distinguish Type 1 and Type 2 Diabetes Among Adults With Diabetes PROJECT SUMMARY/ABSTRACT Type 1 and Type 2 diabetes are distinct clinical conditions with different etiologies, ages of onset, management strategies, risk factors, and outcomes. Currently, the data sources that the Centers for Disease Control and Prevention (CDC) relies upon to monitor trends in diabetes prevalence and incidence are unable to reliably distinguish between types of diabetes. Most of the large federal surveys used for diabetes surveillance have not included questions on diabetes type, and few studies have reported survey-based algorithms for identifying diabetes type. None have compared survey-based identification algorithms with a gold standard case ascertainment in order to validate survey-based assignment of diabetes type. Methods for distinguishing between diabetes types in electronic health records (EHR) data have been tested for children and adults, but additional validation work is needed. Our approach to improving diabetes surveillance in these two areas is based on an integrated study design whereby survey data for diabetes patients are linked with data from their EHR and a gold standard case ascertainment derived from chart review. We will select a diverse sample of diabetes patients that is designed and powered to assess algorithm validity for subpopulations defined by age, sex, and race/ethnicity. We will use rigorous questionnaire development methods to build on items used in previous surveys, cognitively test the new survey module to optimize wording and question order, field the survey using data collection methods similar to established CDC surveys, and analyze the responses relative to a gold standard classification of diabetes type. In addition, we will develop and validate against the gold standard EHR-based algorithms, including a modeling approach that produces a parsimonious rule-based algorithm for determining diabetes type based on the most important clinical variables, and a machine learning approach that uses the gold standard dataset as a starting point to identify implicit patterns that distinguish T1DM and T2DM. PROJECT NARRATIVE The proposed project will result in a set of validated survey questions for distinguishing Type 1 diabetes (T1DM) and Type 2 diabetes (T2DM). It will also produce validated survey-based and EHR-based algorithms for identifying diabetes type. T1DM and T2DM are distinct clinical conditions with different etiologies, ages of onset, management strategies, risk factors, and outcomes. Currently, the data sources that CDC relies upon to monitor trends in diabetes prevalence and incidence are unable to reliably distinguish between types of diabetes. It is important for surveillance systems to be able to distinguish diabetes type to support type-specific analyses of morbidity, mortality, medical care costs, and health-related quality of life. Improved surveillance data by diabetes type can help guide and monitor federal, state, and local diabetes programs, enrich diabetes research, and support people with diabetes.",Validation of Survey Questions to Distinguish Type 1 and Type 2 Diabetes Among Adults With Diabetes,9542094,U01DP006327,[' '],NCCDPHP,"WESTAT, INC.",U01,2018,499828,-0.017028284805405244
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.034031930278022295
"Quantitative microscopy-based rapid phenotyping and screening ﻿    DESCRIPTION:  Synapses are most fundamental to the function of a nervous system. C. elegans is an excellent genetic model system for finding genes and elucidating pathways because of its sequenced genome and the abundance of molecular biology tools and mutants. Due to the simplicity of its nervous system, many breakthroughs have been made in C. elegans for understanding molecular mechanisms in the patterning of the nervous system and synapse development. The current bottlenecks are in the manual and non-quantitative techniques such as visual screens, limiting both the throughput of the experiments and the phenotypes one can examine. Our long-term objective is to develop technologies and to understand how genes, age, and the environment together define and continue to remodel the nervous system of an organism. In the last funding period, we have made large progress in hardware system design (including microtechnologies and automation technologies) and software for quantitative characterization of phenotypes. The objective of this continuation project is to further engineer superior micro devices for large-scale live imaging and quantitative imaging technologies, and combine with the power of genetic and genomic approaches to study synapse development in this in vivo system; genes and pathways emerging from this study could potentially become targets of therapeutics in neurological disorders.  We have shown in the previous phase of the project that quantitative microscopy-based approaches can indeed enable identification of novel genes and pathways that conventional approaches cannot. In the continuation phase, we will further optimize on-chip rapid and high-content in vivo imaging techniques, and in parallel further develop algorithms and quantitative measures for the analysis of such high-content data; we will screen based on novel synthetic phenotype unobservable by eye; we will also exploit powerful genomic techniques to identify loci and potential multigenic interactions that shape the synapse morphology. These experimental approaches will identify genes that cannot have been identified otherwise because of the difficulties associated with the phenotypical profiling, but addressed using our engineered techniques here. The approach is innovative because the technology developed here dramatically increases the throughput, sensitivity, and accuracy of the experiments, and truly enables the utility of extremely powerful genetic and genomic methods. The proposed research is significant because it fills the urgent need in high-throughput and high-content screens as well as identifying novel genes and pathways. In addition, besides the contribution to the specific neurobiology, the technologies are widely applicable to areas such as developmental cell biology, and to other small organisms such as fly larvae and zebrafish embryos. PUBLIC HEALTH RELEVANCE:   Synapse development is an important and active area of research linking genes and environments to the formation and maintenance of synapses in the nervous system. It has direct implications in many human diseases developmental and psychiatric diseases such as Autism Spectrum Disorder and Schizophrenia.",Quantitative microscopy-based rapid phenotyping and screening,9502988,R01GM088333,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alleles', 'Animals', 'Area', 'Automation', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Chromosome Mapping', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Development', 'Developmental Cell Biology', 'Devices', 'Disease', 'Drosophila genus', 'Embryo', 'Engineering', 'Environment', 'Event', 'Eye', 'Fill-It', 'Fluorescence', 'Funding', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Research', 'Genetic Screening', 'Genome', 'Genomic approach', 'Genomics', 'Goals', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Imaging technology', 'Inbreeding', 'Larva', 'Lead', 'Link', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Microfluidics', 'Microscopy', 'Molecular', 'Molecular Biology', 'Morphology', 'Nematoda', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurons', 'Organism', 'Pathway interactions', 'Pattern', 'Phase', 'Phenotype', 'Phylogeny', 'Positioning Attribute', 'Quantitative Microscopy', 'Quantitative Trait Loci', 'Regulatory Pathway', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Speed', 'Synapses', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Vision', 'Visual', 'Work', 'Zebrafish', 'autism spectrum disorder', 'base', 'design', 'developmental disease', 'experience', 'experimental study', 'fly', 'forward genetics', 'genetic approach', 'high dimensionality', 'high throughput screening', 'human disease', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'mutant', 'nerve injury', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'quantitative imaging', 'screening', 'success', 'synaptogenesis', 'targeted treatment', 'technology development', 'tool', 'trait']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2018,305174,-0.040350359363126954
"Neuroethics of Predictive MRI Testing: Parental Attitudes Towards Pre-Symptomatic Identification of Autism Spectrum Disorder No abstract available PROJECT NARRATIVE Machine learning-based statistical techniques, recently applied to neuroimaging data, have allowed researchers to predict disease and disorder from brain data alone. Investigators working at this new frontier in neuroimaging research now face ethical challenges about whether and how to disclose a predictive clinical diagnosis to pre-symptomatic individuals. By combining bioethical theory with the perspectives of participants, the current research will generate new knowledge to guide ethical judgments about the disclosure of predictive diagnoses in future neuroimaging research.",Neuroethics of Predictive MRI Testing: Parental Attitudes Towards Pre-Symptomatic Identification of Autism Spectrum Disorder,9667076,F32MH118689,"['Address', 'Attitude', 'Autistic Disorder', 'BRAIN initiative', 'Behavioral Model', 'Belief', 'Bioethics', 'Brain', 'Brain Diseases', 'Brain imaging', 'Categories', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complement', 'Data', 'Data Collection', 'Data Set', 'Decision Making', 'Development', 'Diagnosis', 'Diagnostic', 'Disclosure', 'Disease', 'Early Intervention', 'Environmental Risk Factor', 'Ethical Issues', 'Ethics', 'Face', 'Future', 'Genetic', 'Genetic Research', 'Genetic screening method', 'Genomics', 'Goals', 'Guidelines', 'Human', 'Individual', 'Infant', 'Intention', 'Interview', 'Judgment', 'Knowledge', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modality', 'Neurosciences', 'Neurosciences Research', 'Parents', 'Participant', 'Pattern', 'Pediatric Hospitals', 'Population', 'Predictive Factor', 'Qualitative Research', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Review Literature', 'Risk', 'Sampling', 'Site', 'Surveys', 'Symptoms', 'Techniques', 'Test Result', 'Testing', 'Time', 'Training', 'University resources', 'Validation', 'Washington', 'Work', 'autism onset', 'autism spectrum disorder', 'base', 'career', 'clinical Diagnosis', 'clinical predictors', 'frontier', 'high risk infant', 'imaging study', 'interest', 'neuroethics', 'neuroimaging', 'parental influence', 'prediction algorithm', 'predictive test', 'programs', 'recruit', 'systematic review', 'theories']",NIMH,UNIVERSITY OF WASHINGTON,F32,2018,63654,-0.03386721616680768
"Identification of Sub-phenotypes in Sepsis Associated Acute Kidney Injury No abstract available PROJECT NARRATIVE Sepsis has been shown to be a key contributing factor for the development of acute kidney injury (AKI). To date, there are no available targeted therapies for sepsis-associated AKI besides supportive care with antibiotics and fluids. The long-term goal of this project is to find new therapeutics for sepsis-associated AKI by first identifying sub-phenotypes based on clinical and biomarker profiles that may inform the molecular pathways of injury and then determining their association with adverse outcomes.",Identification of Sub-phenotypes in Sepsis Associated Acute Kidney Injury,9611100,F32DK118870,"['Accident and Emergency department', 'Acute', 'Acute Kidney Tubular Necrosis', 'Acute Renal Failure with Renal Papillary Necrosis', 'Adult', 'Adult Respiratory Distress Syndrome', 'Allergic', 'Animal Model', 'Anti-inflammatory', 'Antibiotics', 'Apoptotic', 'Aspirin', 'Asthma', 'Biological', 'Biological Markers', 'Blood Circulation', 'Caring', 'Child', 'Chronic Kidney Failure', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Complex', 'Data', 'Deterioration', 'Developed Countries', 'Developing Countries', 'Development', 'Dialysis procedure', 'Disease', 'Electrolyte Balance', 'Extrinsic asthma', 'Failure', 'Fibrinolysis', 'Fluid Balance', 'Foundations', 'Functional disorder', 'Goals', 'Heterogeneity', 'Histologic', 'Hospitals', 'Human', 'Immune', 'Immunologics', 'Individual', 'Inflammation', 'Inflammatory', 'Injury', 'Intensive Care Units', 'Intervention', 'Kidney', 'Lead', 'Learning', 'Length of Stay', 'Liquid substance', 'Machine Learning', 'Measurement', 'Medicine', 'Metabolic', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Oliguria', 'Organ failure', 'Outcome', 'Pathogenesis', 'Pathologic', 'Pathway interactions', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physicians', 'Plasma', 'Population', 'Positioning Attribute', 'Process', 'Prospective cohort', 'Protocols documentation', 'Renal Replacement Therapy', 'Renal function', 'Resuscitation', 'Risk', 'Risk Factors', 'Secondary to', 'Sepsis', 'Septic Shock', 'Subgroup', 'Supportive care', 'Syndrome', 'Techniques', 'Urine', 'Work', 'adjudicate', 'adverse outcome', 'base', 'career', 'cell injury', 'clinical biomarkers', 'cohort', 'hypoperfusion', 'improved', 'improved outcome', 'lung injury', 'molecular targeted therapies', 'mortality', 'novel', 'novel therapeutics', 'outcome forecast', 'renal ischemia', 'research study', 'response', 'septic patients', 'targeted treatment', 'tubular necrosis']",NIDDK,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2018,81158,-0.01858707846210451
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,-0.01676380275612213
"Artificial neural networks for high performance, fully automated particle tracking analysis even at low signal-to-noise regimes Abstract: Particle tracking (PT) is a powerful biophysical tool for elucidating molecular interactions, transport phenomena and rheological properties in complex biological environments. Unfortunately, PT remains a niche tool in life and physical sciences with a limited user base, in large part due to significant time and technical constraints in extracting accurate time-variant positional data from recorded movies. These constraints are exacerbated in experiments with low signal-to-noise ratios or substantial heterogeneity, as frequently encountered with nanoparticles and pathogens in biological fluids. Currently available software that attempts to automate the movie analysis process rely almost exclusively on assigning static image filters based on specific intensity, pixel size and signal-to-noise ratio thresholds. Unfortunately, when applied to actual experimental data with substantial spatial and temporal heterogeneity, the current software generally produces substantial numbers of false positives (i.e. tracking artifacts) or false negatives (i.e. missing actual traces), and frequently both. Frequent user intervention is thus required to ensure accurate tracking even when using sophisticated tracking software, markedly reducing experimental throughput and resulting in substantial user- to-user variations in analyzed data. The time required for accurate particle tracking analysis makes PT experiments exceedingly expensive compared to other commonly used experimental techniques in life sciences. These same tracking analysis limitations have effectively precluded investigators from undertaking more sophisticated 3D PT, even though the microscopy capability to obtain such movies is readily available and critical scientific insights can be gained from 3D PT. To circumvent the challenges with currently available particle tracking software, we have developed a new approach for particle identification and tracking, based on machine learning and convolutional neural networks (CNN). CNN is a type of feed-forward artificial neural network designed to process information in a layered network of connections that mimics the organization of real neural networks in the mammalian retina and visual cortex. Unlike most CNN imaging models that are trained to make predictions on static images, we have trained our CNN to input adjacent frames so that each prediction includes information from the past and future, thus effectively performing convolutions in both space and time to infer particle locations. Similar principles of image analysis are now being harnessed by developers of autonomous vehicle technologies to distinguish the motions of different objects on the road. We have applied our CNN tracking algorithm to a wide range of 2D movies capturing dynamic motions of nanoparticles, viruses and highly motile bacteria, achieving at least 30-fold time savings with virtually no need for human intervention while maintaining robust tracking performance (i.e. low false positive and low false negative rates). In this STTR proposal, we seek to focus on further optimization and testing of our neural network tracking platform for 2D PT, including the use of cloud computing (Aim 1), and extending our neural network tracker to enable accurate 3D PT (Aim 2). Our vision is to popularize PT as a research tool among researchers by minimizing the time and labor costs associated with PT analysis. Narrative Particle tracking is a powerful biophysical tool in life and physical sciences, but unfortunately its application has been strongly limited by inefficiencies in accurately extracting particle traces from raw movies. Unlike conventional particle tracking methods, we have combined artificial intelligence and machine learning to create a computational neural network that can recognize objects in much the same way as the human eye, and which consistently provided superior and truly automated tracking performance compared to current alternatives. This STTR will establish the feasibility of using our computational neural network for robust 2D and 3D particle tracking analysis.","Artificial neural networks for high performance, fully automated particle tracking analysis even at low signal-to-noise regimes",9347679,R41GM123897,"['Adopted', 'Advanced Development', 'Algorithms', 'Artificial Intelligence', 'Automobile Driving', 'Bacteria', 'Binding', 'Biological', 'Biological Neural Networks', 'Biological Sciences', 'Classification', 'Cloud Computing', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diffuse', 'Ensure', 'Environment', 'Eye', 'Future', 'Gaussian model', 'Generations', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Image Analysis', 'Intervention', 'Knowledge', 'Laboratories', 'Lead', 'Life', 'Link', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Microscopy', 'Modeling', 'Morphologic artifacts', 'Motion', 'Noise', 'Performance', 'Phase', 'Photobleaching', 'Process', 'Property', 'Radial', 'Research', 'Research Personnel', 'Retina', 'Savings', 'Scientist', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Software Tools', 'Spottings', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'Virus', 'Vision', 'Visual Cortex', 'Work', 'base', 'biophysical tools', 'cell motility', 'cloud based', 'cost', 'design', 'experimental study', 'feeding', 'field study', 'graduate student', 'improved', 'insight', 'interest', 'macromolecule', 'movie', 'nanoparticle', 'novel strategies', 'particle', 'pathogen', 'physical science', 'response', 'spatiotemporal', 'submicron', 'terabyte', 'tool', 'virtual']",NIGMS,"AI TRACKING SOLUTIONS, LLC",R41,2017,224997,-0.01797823414192672
"Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma PROJECT SUMMARY This project aims to apply novel machine learning techniques to recently developed optical imaging measurement to improve the accurate prediction and detection of glaucomatous progression. Complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and advanced pattern recognition/machine learning-based analysis techniques can find and use that hidden information. We will use mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1,800 patient and healthy eyes, available as the result of long-term NIH funding. We also will investigate deep learning and novel statistical techniques for this purpose. The required longitudinal measurements from several newly developed optical imaging techniques were not available to our previously funded NEI- supported work. The proposed work potentially can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care by informing clinical decision-making based on mathematically based, externally validated methods. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. PROJECT NARRATIVE The proposed project will improve machine learning techniques for predicting and detecting glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments and will make use of a very large amount of data, obtained using previously awarded NIH funds, to do so. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neuro- degeneration within the visual pathways at structural and functional levels. The development of a clinically useful novel, empirical system for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and on the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma,9298423,R21EY027945,"['Address', 'Algorithms', 'Anatomy', 'Award', 'Caring', 'Classification', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Defect', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Environment', 'Eye', 'Frequencies', 'Funding', 'Future', 'Gaussian model', 'Generations', 'Glaucoma', 'Goals', 'Health Personnel', 'Image', 'Imaging Device', 'Imaging Techniques', 'Instruction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Perimetry', 'Physiologic Intraocular Pressure', 'Provider', 'Recruitment Activity', 'Reporting', 'Research', 'Scanning', 'Science', 'Series', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Variant', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'clinical decision-making', 'cost', 'expectation', 'glaucoma test', 'high dimensionality', 'improved', 'independent component analysis', 'instrument', 'learning strategy', 'markov model', 'mathematical model', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'tool']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2017,232500,-0.0065580581975011485
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper. PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.",Deep learning for representation of codes used for SEER-Medicare claims research,9188540,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Investigational Therapies', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'SEER Program', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2017,219753,-0.04063979110965981
"Machine Learning and Personalized Prognosis for Depression Treatment Abstract Depression treatment is effective for approximately 50-60% of patients who receive treatment, but the probability of a successful response is typically unknown before treatment begins. As a result, depression treatment is routinely delivered in a trial-and-error fashion until a satisfactory response is achieved. Our objective is to provide a personalized prognosis by applying ensemble machine learning techniques to discover novel, non-linear combinations of multiple weak predictors that collectively yield accurate predictions of treatment outcome. This statistical approach considers many prediction variables simultaneously and iteratively constructs a complex prediction model that often dramatically outperforms traditional statistical methods. Aim 1 is to apply stochastic gradient boosted decision trees to predict response to citalopram using archival data from the STAR*D clinical trial. In preliminary analyses, we randomly selected 1223 patients to train the model and another 407 patients to independently test the model (a 75-25 split), with tuning parameters selected by cross-validation to minimize log-loss. The resulting prediction on the independent test sample was superior to the no-information rate (p < 0.001), with an overall predictive accuracy of 66%. Although this level of prediction is significantly better than a no information model, we plan to improve the model's prognostication by 1) adding features that capture the “pharmacological noise” of concurrent (non- study) medication use and 2) updating model predictions based on early signs of response. Aim 2 is to use a similar machine learning approach to examine response to internet-based CBT for depression. Internet-based treatments for depression are growing in popularity, provide efficient access to health care, reduce treatment costs, and have good evidence for treatment efficacy. Importantly, we have a large dataset (N = 1,013) within which to develop treatment-matching algorithms that predict treatment response based on patient attributes. Study Impact: The overarching goal of this project is to use machine learning methods to develop treatment matching algorithms. In the long term, we can envision a system that evaluates a patient on a number of important predictor variables and provides a personalized probability of treatment success. These probabilities would then be used to guide treatment selection or modify current treatment if a poor response is predicted. Developing algorithms that successfully predict whether a particular form of treatment is likely to be successful for a patient with a given set of attributes would be a tremendous step towards efficient and personalized depression treatment.   Public Health Narrative Depression treatment is effective for roughly half of the patients who receive treatment, but it is usually unknown how a specific patient with a particular set of attributes will respond to a given treatment. Our objective is to provide a personalized prognosis by applying ensemble machine learning techniques to discover novel combinations of multiple weak predictors that collectively yield accurate predictions of treatment outcome. Developing algorithms that successfully predict whether a particular form of treatment is likely to be successful for a patient would be a tremendous step towards efficient and personalized depression treatment.  ",Machine Learning and Personalized Prognosis for Depression Treatment,9301041,R21MH110758,"['Advocate', 'Algorithms', 'Archives', 'Citalopram', 'Clinical', 'Clinical Trials', 'Coin', 'Complex', 'Data', 'Data Set', 'Databases', 'Decision Trees', 'Development', 'Goals', 'Health', 'Internet', 'Intervention', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Mental Depression', 'Mental Health', 'Methods', 'Modeling', 'Multi-Institutional Clinical Trial', 'Noise', 'Online Systems', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Probability', 'Public Health', 'Recommendation', 'Sample Size', 'Sampling', 'Selection for Treatments', 'Statistical Methods', 'Symptoms', 'System', 'Techniques', 'Testing', 'Training', 'Treatment Cost', 'Treatment Efficacy', 'Treatment outcome', 'United States', 'United States National Institutes of Health', 'Update', 'Validation', 'base', 'clinical practice', 'clinically relevant', 'data archive', 'effective therapy', 'health care availability', 'improved', 'information model', 'learning strategy', 'novel', 'outcome forecast', 'personalized medicine', 'precision medicine', 'predicting response', 'prediction algorithm', 'predictive modeling', 'predictive of treatment response', 'psychologic', 'response', 'success', 'treatment response']",NIMH,"UNIVERSITY OF TEXAS, AUSTIN",R21,2017,195625,-0.021890258579224095
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9316700,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoking', 'clinical imaging', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2017,187400,-0.07061272998642412
"Mechanistic Machine Learning PROJECT SUMMARY / ABSTRACT The goal of this project is to combine empirical data with mechanistic physiologic knowledge to produce personalized, quantitative predictions that can lead to improved treatments. In normal practice, physicians reason by analogy from generic physiologic principles, but the technology exists to exploit even imperfect physiologic models make treatment personalized and quantitatively grounded in physiology, and to improve learning from empirical data. We will apply data assimilation (DA), mechanistic mathematical modeling, machine learning, and control theory, which have revolutionized space travel, weather forecasting, transportation and flight, and manufacturing. Data assimilation and control theory have seen very limited use in medicine, usually applied in data-rich circumstances like continuous glucose monitoring or packemakers. Our previous work demonstrated use of data assimilation with glucose-insulin models to predict glucose in the outpatient type 2 diabetes setting. We will extend data assimilation and control theory using, for example, a constrained ensemble Kalman filter and an offline Markov Chain Monte Carlo algorithm, to better handle sparse, short training sets on rapidly changing patients, and we will apply it in the setting of glucose management in the intensive care unit (ICU). Moreover, we will develop DA for phenotyping applications by exploiting the parameter estimation capabilities of DA. Data assimilation can be used to estimate measureable and unmeasureable physiologic states and parameters, and we will use these estimates to create higher definition phenotypes. While we are focusing on glucose management in the ICU, we will develop methods that are likely to generalize, beginning the effort to develop DA in the context of healthcare more broadly. The work we propose is a necessary step toward being able to use mechanism-driven DA to test, validate and optimize personalized short-term treatment strategies, long-term health forecasts, and mechanistic physiologic understanding. We will carry out the following aims: AIM 1—forecast—extend the DA methodology to allow forecasting, personalization, model evaluation, and model selection in the ICU context, relating treatment input to physiologic outcome; AIM 2—phenotype—extend the DA framework to state and parameter estimation to allow for mechanism-based phenotyping, careful uncertainty quantification, and inference of difficult or impossible-to-measure physiology; AIM 3—control—extend the DA to include a controller that begins with desired clinical outcomes, e.g., glucose range, and estimates the inputs, e.g., insulin or nutrition, required to achieve the outcomes. Narrative The goal of this project is to develop better ways to combine data about individual patients with knowledge about physiology to create personalized forecasts and recommendations about a patient's health. We specifically address the management of glucose in the intensive care unit, an area of high importance that could benefit from improved forecasts and recommendations.",Mechanistic Machine Learning,9427058,R01LM012734,"['Address', 'Admission activity', 'Affect', 'Area', 'Assimilations', 'Carbohydrates', 'Clinical', 'Clinical Treatment', 'Complex', 'Computational Biology', 'Data', 'Data Science', 'Depressed mood', 'Early Intervention', 'Eating', 'Endocrine Physiology', 'Evaluation', 'Fingers', 'Food', 'Future', 'Generic Drugs', 'Glomerular Filtration Rate', 'Glucose', 'Goals', 'Health', 'Healthcare', 'Hepatic', 'Hour', 'Insulin', 'Insulin Infusion Systems', 'Insulin-Dependent Diabetes Mellitus', 'Intensive Care Units', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Manuals', 'Markov chain Monte Carlo methodology', 'Measurable', 'Measurement', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Minor Planets', 'Modeling', 'Monitor', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'Outpatients', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physicians', 'Physiological', 'Physiology', 'Power Plants', 'Property', 'Publishing', 'Recommendation', 'Renal function', 'Running', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transportation', 'Treatment outcome', 'Uncertainty', 'Validity of Results', 'Weather', 'Work', 'base', 'clinical phenotype', 'computerized', 'control theory', 'fly', 'glucose metabolism', 'glucose monitor', 'glucose production', 'health record', 'improved', 'individual patient', 'insight', 'insulin secretion', 'interstitial', 'mathematical model', 'nutrition', 'outcome forecast', 'outcome prediction', 'personalized learning', 'personalized medicine', 'physiologic model', 'reduced food intake', 'space travel', 'treatment strategy']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2017,698661,-0.020424064543238907
"IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217 BCL will expand the current bibliometric methods by developing a Semantic Bibliometric System using machine learning that will examine research publications, rank the publications by quality, and identify research-productive scientific teams. Used in conjunction with current methods this Semantic Bibliometric System will have the dual use of improving the impact of Government Research and improving semantic search on the web and in ecommerce. n/a","IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217",9576638,71201700054C,"['Award', 'BCL1 Oncogene', 'Bibliometrics', 'Data', 'Effectiveness', 'Evaluation', 'Feasibility Studies', 'Government', 'Health Care Research', 'Internet', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Performance', 'Procedures', 'Publications', 'Research', 'Semantics', 'System', 'Text', 'improved']",NIDA,"BCL TECHNOLOGIES, INC.",N43,2017,225000,-0.009198163168065713
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9390382,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biological Neural Networks', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'base', 'bulk motion', 'capillary', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2017,559269,-0.01844619117644635
"A high-throughput imaging and classification system for fruit flies PROJECT SUMMARY / ABSTRACT In this Phase I SBIR application, FlySorter proposes to development a high throughput imaging and classification system to aid research with fruit flies, a widely-used model organism relevant to both basic science as well as studies in human health. The use of animal model systems is essential for research in almost all aspects of biology: genetics, development, neuroscience, disease, physiology, and beyond. The fruit fly – Drosophila melanogaster – is small and easy to care for, but is complex enough an organism to provide a wealth of information that directly relates to human biology and health. Over 75% of human diseases with a genetic basis (including depression, alcoholism, certain forms of cancer, and many more) are either present or have an analog in Drosophila. Modern genetic tools, such as CRISPR/cas9, allow the creation of transgenic flies that provide the opportunity to study diseases, pathways and systems that don’t exist naturally in Drosophila. With these advances, fruit flies are becoming more frequently subjects for drugs screens. For all the advances in the biological tools and techniques applicable to flies, however, the limiting factor in many experiments is the manual labor involved in a few common tasks: moving flies from vial to vial or other lab equipment; classifying and sorting flies by sex, eye color and other phenotypes; and collecting virgin female flies before they mate so that they can be used in controlled crosses, etc. FlySorter’s patent-pending fly dispensing mechanism can reliably deliver a single organism from a vial containing hundreds of awake flies, and our novel FlyPlate system allows storage of individual flies in custom 96 well plates. FlySorter’s robotic fly handling system, co-developed with the de Bivort Lab at Harvard, is capable of manipulating and transporting those individual flies between vial, 96 well plate, and experimental apparatus. The next piece of the automation puzzle to solve is high throughput imaging and classification. To accomplish this goal, FlySorter will: 1) complete a prototype automated image capture hardware system; 2) adapt state-of-the-art computer vision and machine learning algorithms for use on Drosophila; and 3) build a module that can physically sort the classified flies into different vials. Once integrated into the existing FlySorter product ecosystem, this imaging and classification module will greatly expand the kinds of experiments and screens that can be automated, allowing for the study of larger populations or a wider variety of flies, reducing the impact of human error, and freeing up valuable time for researchers. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are one of the most widely used model organisms in biology, for research in genetics, development, neuroscience, disease, and much more. One of the most common tasks in Drosophila labs is sorting flies by various markers and phenotypes using a microscope and paintbrush. FlySorter aims to build an automated system for sorting flies using high resolution digital cameras and modern computer vision algorithms, which will obviate the need for such tedious manual labor.",A high-throughput imaging and classification system for fruit flies,9408980,R43OD023302,"['Air', 'Alcoholism', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Animal Model', 'Animals', 'Appearance', 'Automation', 'Basic Science', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Neural Networks', 'Biology', 'CRISPR/Cas technology', 'Caring', 'Classification', 'Code', 'Complex', 'Computer Vision Systems', 'Custom', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Pathway', 'Dorsal', 'Drosophila genus', 'Drosophila melanogaster', 'Ecosystem', 'Ensure', 'Eye', 'Eye Color', 'Female', 'Floor', 'Genes', 'Genetic', 'Genetic Screening', 'Genetic study', 'Genotype', 'Goals', 'Grant', 'Head', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Legal patent', 'Lighting', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mechanics', 'Mental Depression', 'Methodology', 'Microscope', 'Modernization', 'Motor', 'Mutation', 'Names', 'Neurosciences', 'Obesity', 'Optics', 'Organism', 'Partner in relationship', 'Phase', 'Phenotype', 'Physiology', 'Population', 'Preclinical Drug Evaluation', 'Pump', 'Research', 'Research Personnel', 'Resolution', 'Robot', 'Robotics', 'Sampling', 'Sclera', 'Shapes', 'Small Business Innovation Research Grant', 'Sorting - Cell Movement', 'Standardization', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transgenic Organisms', 'Universities', 'Vial device', 'Walking', 'Work', 'analog', 'awake', 'base', 'depression model', 'digital', 'digital imaging', 'experimental study', 'fly', 'genetic strain', 'human disease', 'improved', 'interest', 'laboratory equipment', 'male', 'meter', 'novel', 'phenotypic biomarker', 'prevent', 'prototype', 'sex', 'tool', 'virtual']",OD,"FLYSORTER, LLC",R43,2017,225000,-0.024019590540789025
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9307096,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2017,407500,-0.07806203877178262
"Developing and validating prognostic metabolomic signatures of diabetic kidney disease PROJECT SUMMARY/ABSTRACT Rationale. Diabetes is a leading cause of renal disease, accounting for 40% of the estimated 20 million US adult cases of chronic kidney disease. There is, however, substantial heterogeneity across diabetic patients with regards to development of kidney disease. Hence, there is an urgent need to identify prognostic biomarkers that can provide early and reliable evidence of future kidney disease, so that high-risk patients can receive optimal medical care. Existing clinical, proteomic and genomic markers do not consistently nor accurately predict kidney function decline. Metabolomics, a systematic evaluation of the end-products of cellular function in fluids, has the potential to inform physiological and pathological effects of chronic diseases. Metabolomic analysis combined with advanced quantitative methods could play a key role in building clinically useful prognostic signatures of diabetic kidney disease. Yet, development of computational methods with adequate rigor has lagged behind the technical capacity to perform large scale quantitative metabolomics. In this proposal we aim to address this computational gap in diabetic kidney disease research. Aims. We will implement rigorous computational methods to identify robust prognostic metabolite + clinical + genetic signatures of diabetic kidney disease progression. Specifically, we aim to (i) test the accuracy of previous signatures, and apply state-of-the-art analytic techniques and novel statistical methods to identify new multivariate metabolite sets for predicting kidney disease progression; (ii) quantify patterns of co-regulation of metabolites in diabetic kidney disease, and develop new tools in network biology to discover novel enzymes, proteins, metabolites, and molecular pathways which are implicated in diabetic kidney disease progression; (iii) test if these models can accurately predict kidney disease progression in independent prospective cohorts. Methods. Using clinical, genetic and metabolomic data from large prospective cohorts of > 1200 diverse, well- characterized patients with Type 2 diabetes, we will apply statistical methods for variable selection (e.g., penalized regression), and machine learning methods (e.g., random forest), which are known to perform well in the high-dimensional setting, to identify robust and parsimonious signatures of kidney disease progression. We will quantify inter-metabolite co-regulation patterns and infer biological pathways implicated in diabetic kidney disease. Throughout the modeling process, a rigorous training-validation paradigm will be adopted in order to improve reproducibility of models and reduce chance findings. Impact. A major product of this work will be the development of a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline. Our findings will also provide insight into markers of renal dysfunction, and elucidate possible therapeutic targets for treating diabetic kidney disease, thus potentially informing the design of future clinical trials. PROJECT NARRATIVE Kidney disease, a major and common complication of diabetes, can lead to repeated hospitalizations and premature death. There is an urgent need to develop clinical tools that can provide early evidence that a given diabetic patient is likely to progress to kidney disease in the future. In this proposal, we will identify new urinary biomarkers and use novel statistical modeling methods to create a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline, with the ultimate goal of improving disease management and reducing mortality rates for these patients.",Developing and validating prognostic metabolomic signatures of diabetic kidney disease,9306637,R01DK110541,"['Accounting', 'Address', 'Adopted', 'Adult', 'Albuminuria', 'Algorithms', 'American', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biology', 'Caring', 'Cell physiology', 'Cessation of life', 'Chronic Disease', 'Chronic Kidney Failure', 'Chronic Kidney Insufficiency', 'Clinical', 'Clinical Trials', 'Collaborations', 'Comorbidity', 'Complications of Diabetes Mellitus', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Diagnosis', 'Disease', 'Disease Management', 'Disease Progression', 'Enzymes', 'Evaluation', 'Functional disorder', 'Funding', 'Future', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Heterogeneity', 'Hospitalization', 'Kidney', 'Kidney Diseases', 'Laboratories', 'Lead', 'Link', 'Liquid substance', 'Longitudinal cohort', 'Machine Learning', 'Medical', 'Medical Genetics', 'Methods', 'Modeling', 'Molecular', 'Non-Insulin-Dependent Diabetes Mellitus', 'Pathologic', 'Pathway interactions', 'Patient risk', 'Patients', 'Pattern', 'Physiological', 'Pima Indian', 'Play', 'Process', 'Prognostic Marker', 'Prospective cohort', 'Proteomics', 'Publishing', 'Recommendation', 'Regulation', 'Renal function', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Sampling', 'Sampling Studies', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Testing', 'Training', 'Urine', 'Validation', 'Work', 'biological heterogeneity', 'chemical association', 'cohort', 'design', 'diabetic', 'diabetic patient', 'forest', 'genetic signature', 'genomic biomarker', 'high dimensionality', 'high risk', 'improved', 'innovation', 'insight', 'learning strategy', 'metabolome', 'metabolomics', 'model development', 'mortality', 'nephrogenesis', 'network models', 'novel', 'open source', 'personalized medicine', 'predictive modeling', 'predictive signature', 'premature', 'prognostic', 'prognostic signature', 'prospective', 'protein metabolite', 'targeted treatment', 'therapeutic target', 'tool', 'urinary']",NIDDK,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2017,340545,-0.00552808174181489
"Building Multistage Treatment Regimens for Depression after Acute Coronary SyndromeSyndrome Building Multistage Treatment Policy for Depression after Acute Coronary Syndrome Project Summary Depression is not only commonly observed among patients who experienced acute coronary syndrome (ACS), but also has been shown to increase risks for recurrent ACS and mortality. Despite its high prevalence and serious impact, management of post-ACS depression remains poor because of ineciencies in depression screen- ing, limited treatment options of depression after ACS, and lack of e ective procedure if initial treatment fails. To address these issues, clinical researchers have tried to develop personalized stepped care procedures for post-ACS depression patients; this involves o ering patients the choice of receiving psychotherapy and/or antidepressant treatment and adjusting treatment as needed. The treatment decisions are usually based on patient demographics, treatment preference, medical history, progress of disease, and comorbid conditions. With the development of modern technologies, the number of available treatments increases, and more pa- tient information are collected in clinical research. Thus excavating useful information for treatment decisions is becoming more challenging. In this project, we propose to develop a principled way to construct simple interpretable multistage treatment policies from high-dimensional data, that can be used to guide treatment selection throughout the course of the disease. Aim 1 of the project is devoted to the development of vari- able selection methodology for constructing multistage treatment policies using statistical machine learning techniques. The proposed research seeks to incorporate the popular variable selection technique (LASSO) into existing treatment policy search approaches, namely Q-learning and A-Learning, for developing optimal treatment policies and for identifying patient response status to initial treatment { an important factor for tailoring treatment in the subsequent stages. Aim 2 evaluates the proposed methods, applies the methods to post-ACS depression data, and addresses some computational challenges. Statistical research in this area has been focused on the development of evidence-based treatment policies using pre-chosen models and variables; few if any discuss how to select models or variables in a principled way. The proposed work aims to ll this gap in methodology using modern machine learning techniques. Project Narratives The proposed research aims to answer the following question:\How to excavate simple interpretable multistage treatment policies from high-dimensional, longitudinal medical data in a principled way?"" This is a crucial step in the management of chronic disease, such as depression, for which a large number of variables are collected over time, by facilitating the construction of a parsimonious clinical decision system.",Building Multistage Treatment Regimens for Depression after Acute Coronary SyndromeSyndrome,9334312,R21MH108999,"['Acute', 'Address', 'Adopted', 'Antidepressive Agents', 'Area', 'Caring', 'Characteristics', 'Chronic', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Comorbidity', 'Coronary', 'Data', 'Decision Making', 'Depression screen', 'Development', 'Dimensions', 'Disease', 'Disease remission', 'Evaluation Studies', 'Evidence based treatment', 'Goals', 'Guidelines', 'Heterogeneity', 'High Prevalence', 'Intervention', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Medical History', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Physicians', 'Policies', 'Procedures', 'Process', 'Property', 'Psychotherapy', 'Recurrence', 'Relapse', 'Research', 'Research Personnel', 'Risk', 'Selection for Treatments', 'System', 'Techniques', 'Technology', 'Time', 'Treatment Protocols', 'Work', 'acute coronary syndrome', 'base', 'clinical practice', 'cost', 'demographics', 'evidence base', 'experience', 'high dimensionality', 'individualized medicine', 'mortality', 'preference', 'prevent', 'psychosocial', 'response', 'simulation', 'treatment response']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2017,240000,-0.01350736340565048
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates ﻿    DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9334170,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,517281,-0.07157800972935863
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,-0.00625984098157846
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9295208,K08HL136928,"['Affect', 'Bioinformatics', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Hereditary Disease', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2017,172800,-0.01847737165619277
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9260548,R01GM120033,"['Address', 'Algorithms', 'Alpha Cell', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular system', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2017,356625,-0.03215507790089247
"IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215. The Contractor will develop advanced and sophisticated analytical models, tools and metrics to enhance the professional evaluation and decision making in life sciences management and administration.  The intended result is a novel set of metrics that can be used by NGOs/disease foundations, advocacy groups, research funders, policy makers and by academic institutional bodies. n/a",IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215.,9583616,71201700041C,"['Advocacy', 'Award', 'Biological Sciences', 'Complement', 'Contractor', 'Data', 'Databases', 'Decision Making', 'Disease', 'Evaluation', 'Foundations', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Modeling', 'Policy Maker', 'PubMed', 'Quality Indicator', 'Records', 'Reproducibility', 'Research', 'Structure', 'Testing', 'Text', 'Translations', 'base', 'economic impact', 'novel', 'tool']",NIDA,"SCITECH STRATEGIES, INC.",N43,2017,225000,-0.006264529000100345
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9241259,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2017,239723,-0.0161051424154442
"Computational modeling of semantic decline in Alzheimer's disease Project Summary To interact, communicate, and navigate the world successfully, people must retrieve relevant information from their semantic memory (memory for facts and general knowledge). Individuals with Alzheimer's disease have difficulty retrieving such knowledge from early in the course of the disease and progressively gets worse as the disease spreads, a process known as semantic decline. This project examines the mechanisms underlying semantic decline in individuals with Alzheimer's disease by developing and applying novel computational tools. The extent to which semantic memory is impaired in individuals with Alzheimer's disease can be probed using behavioral experiments. Individuals with Alzheimer's as well as those at-risk for the disease display a pattern of behavior on these tasks distinct from healthy individuals. Despite decades of research, explanations of these behavioral impairments focus almost exclusively on cognitive mechanisms that may explain a patient's current behavior at a given time point, but without an account of the transition from normal, pre-symptomatic behavior to fully impaired behavior. Existing models fail to explain the mechanisms by which semantic memory and memory retrieval processes degrade over time due to Alzheimer's, limiting our understanding of the development of the disease, as well as hindering our ability for prognosis, early detection measures, and possible interventions. This project will test computational models of how the disease spreads, making specific quantitative predictions about the decline of semantic memory. Additionally, we will develop a novel machine learning method that can be used to map the structure of an individual's semantic memory, creating opportunities for individualized behavioral interventions to improve semantic memory and improve the quality of life for individuals with Alzheimer's disease. Project Narrative This project examines the mechanisms underlying decline of semantic memory (memory for knowledge) in individuals with Alzheimer's disease by developing and applying novel computational models. This research will develop methods for testing neurocognitive theories of Alzheimer's and other neurodegenerative diseases, inform interventions for mitigating decline, and further our understanding of how the disease spreads.",Computational modeling of semantic decline in Alzheimer's disease,9334048,R21AG053467,"['Affect', 'Alzheimer&apos', 's Disease', 'American', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Categories', 'Clinical', 'Cognitive', 'Computer Simulation', 'Data', 'Data Set', 'Dementia', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Early Diagnosis', 'Early Intervention', 'Emotional', 'Family', 'Free Association', 'Huntington Disease', 'Impairment', 'Independent Living', 'Individual', 'Intervention', 'Knowledge', 'Machine Learning', 'Maps', 'Measures', 'Memory', 'Methods', 'Modeling', 'Neurodegenerative Disorders', 'Neurologic', 'Patients', 'Pattern', 'Performance', 'Pharmacology', 'Phenotype', 'Population', 'Process', 'Quality of life', 'Research', 'Retrieval', 'Semantic memory', 'Semantics', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Time', 'Walking', 'Work', 'behavioral impairment', 'computerized tools', 'contagion', 'disorder risk', 'experimental study', 'improved', 'learning strategy', 'longitudinal dataset', 'memory retrieval', 'neurocognitive test', 'novel', 'outcome forecast', 'pre-clinical', 'theories', 'tool', 'trait', 'transmission process']",NIA,UNIVERSITY OF WISCONSIN-MADISON,R21,2017,193393,-0.07659552268924417
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9369284,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Syndrome', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2017,197528,-0.0199079677463246
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9250167,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,400107,-0.0043274276226662476
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9224405,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2017,163452,-0.05801156850670986
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9287487,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2017,367055,-0.06905573186628432
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9382777,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Subgroup', 'Supervision', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2017,711253,-0.01134259697656707
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9306122,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2017,306527,-0.03532517398271782
"Advanced morphological analysis of cerebral blood flow for acute concussion diagnosis and return-to-play determination Project Summary / Abstract Between 1.6 and 3.8 million people each year suffer a mild TBI in the US alone. Reliable diagnosis and prompt treatments are vital to managing the often-serious short and long-term sequelae resulting from mild TBI. However, a reliable objective and accurate method for mild TBI diagnosis outside of a hospital setting, and in particular for determining RTP readiness, has eluded the clinical community. Current diagnosis and RTP assessments are based on patient symptoms, neurocognitive evaluations, and / or physical performance testing. Use of symptom scales are problematic for several reasons including subjectivity and reliability. Neurocognitive evaluations and physical tests (such as balance tests), although less subjective, require pre- injury baseline testing of subjects due to inherently large subject-to-subject variations in evaluation performances. Due to these reasons, current mild TBI diagnostic methods have limited applications and are not suitable for a significant majority of patients who suffer mild TBI. This project is aimed at developing an objective diagnosis of mild traumatic brain injury (mild TBI) based on physiologic changes in a patient after injury and providing a platform capable of RTP guidance. The method is based on quantification of well-known physiologic changes after a concussion, i.e. the impairment of autonomic function and altered cerebral blood flow (CBF) as measured with transcranial Doppler (TCD). The novelty of the proposed approach is the use of a recently-developed analytical machine learning framework for the analysis of the CBF velocity (CBFV) waveforms. In contrast to previous methods used before, the proposed approach utilizes the entire shape of the complex CBFV waveform, thus obtaining subtle changes in blood flow that are lost in other analysis methods. Additionally, comprehensive verification between our platform and MRI will be performed following injury resulting in the first scientific experiments of this kind. The ultimate goal of this Phase II SBIR is to commercialize an objective and accurate software algorithm for reliable diagnosis and management of sports concussions which does not currently exist. The outcome will be a software suite integrated into existing TCD and will be marketed to emergency departments, neurology clinics, and other healthcare providers involved in mild TBI diagnosis and RTP management. Project Narrative Traumatic brain injury (TBI) is a serious public health problem in the United States contributing to a substantial number of deaths and cases of permanent disability. Mild TBI concussions account for over 80% of all TBIs sustained and a major problem is the high rate of mis-diagnosis due to lack of objective measures and delayed onset of symptoms. This project aims to develop the first objective concussion evaluation method using a novel analysis platform that can obtain subtle, physiologic changes in cerebral hemodynamics. Successful completion of this project will result in a portable diagnostic device suitable for use in many scenarios where concussion diagnosis is inaccurate or unavailable today.",Advanced morphological analysis of cerebral blood flow for acute concussion diagnosis and return-to-play determination,9323604,R44NS092209,"['Accident and Emergency department', 'Acute', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area Under Curve', 'Assessment tool', 'Blood flow', 'Brain Concussion', 'Cerebrovascular Circulation', 'Cessation of life', 'Classification', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Controlled Study', 'Core-Binding Factor', 'Data', 'Data Analytics', 'Data Collection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Evaluation', 'Functional disorder', 'Future', 'Goals', 'Gold', 'Guidelines', 'Health Personnel', 'Hospitals', 'Image', 'Impairment', 'Injury', 'Letters', 'Licensing', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Morphology', 'Neurocognitive', 'Neurologist', 'Neurology', 'Outcome', 'Patients', 'Pediatric Neurology', 'Performance', 'Persons', 'Phase', 'Physical Performance', 'Physicians', 'Physiological', 'Play', 'Public Health', 'Publications', 'Readiness', 'Recovery', 'Research', 'Resolution', 'Risk', 'Severities', 'Shapes', 'Site', 'Small Business Innovation Research Grant', 'Spin Labels', 'Sports', 'Sports Medicine', 'Symptoms', 'Syndrome', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Traumatic Brain Injury', 'Ultrasonography', 'United States', 'Variant', 'balance testing', 'base', 'brain health', 'cerebral hemodynamics', 'clinical Diagnosis', 'diagnostic accuracy', 'disability', 'experimental study', 'hemodynamics', 'high school', 'injured', 'innovation', 'mild traumatic brain injury', 'novel', 'pediatric department', 'performance tests', 'portability', 'prevent', 'programs', 'relating to nervous system', 'success', 'tool']",NINDS,"NEURAL ANALYTICS, INC.",R44,2017,1500000,-0.009207297759885755
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9343385,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2017,652921,-0.01817011163854207
"Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Summary:  Spinal cord injury (SCI) patients experience limited functional recovery, owing in part to the paucity of axon regrowth from injured CNS neurons. Effective treatments are lacking, likely because of multiple factors, intrinsic and extrinsic, that inhibit axon growth. Thus we require agents that target more than one source of regeneration failure.  Kinases are ubiquitous signal transducers that regulate most cellular processes, including axon growth. To begin to identify compounds that positively regulate axon growth, we screened 1600 small-molecule kinase inhibitors (KIs) in an in vitro CNS neurite outgrowth assay and identified “hit” KIs that reproducibly and strongly promote outgrowth. Due to homology of catalytic domains, KIs typically inhibit multiple kinases. This makes it difficult to identify the kinase(s) that mediate a KI's effects on cells. We used information theory and machine learning to analyze the inhibition profiles of KIs in relation to their effects on neurite outgrowth. This enabled us to identify, and later validate via siRNA knockdown in primary neurons, multiple kinase targets (i.e. kinases that should be inhibited to promote neurite outgrowth). These included previously known targets that regulate intrinsic and extrinsic inhibitor factors, in addition to several novel candidates. Conversely, we identified kinases whose activity is critical for neurite outgrowth, and whose inhibition must be avoided (anti-targets). We discovered several KIs that inhibit multiple targets and no anti-targets. These KIs strongly promoted neurite outgrowth in vitro.  We tested the KI, RO48, that had the largest effect in vitro in two in vivo models. Our preliminary experiments indicate that RO48 is remarkably effective in vivo. It promoted robust axonal growth of the corticospinal tract (CST) in three separate models of CST injury (pyramidotomy, funiculotomy, dorsal hemisection), and in the dorsal hemisection model, improved forelimb function. We propose to build on these remarkable results to test the working hypothesis that the simultaneous inhibition of RO48's five target kinases (ROCK, PKC, PRKG1, PRKX, and RPS6K) promotes sprouting and regeneration of CST axons. This will be accomplished using viral vectors to knock down expression of the different target kinases individually and in combination. We will do knockdown in CST neurons in the cortex. We will assess CST axon growth at the injury site using light microscopy. We will also perform experiments to determine if RO48-induced CST axon growth promotes axon sprouting, regeneration, or both, and whether RO48 improves behavioral outcomes such as grasping and walking after a contusion injury.  These experiments will 1) validate novel kinases as in vivo targets for future development of SCI therapeutics 2) determine whether these kinases regulate CST axon sprouting, regeneration, or both, and 3) confirm whether the substantial stimulation of axon growth induced by treatment with RO48 improves motor outcomes in a clinically relevant contusion model.  Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Narrative: The proposed experiments aim to understand how small-molecule drug-like compounds increase the ability of nerve cells to grow long processes and re-form connections. Validating the molecular targets of these compounds for in vivo nerve growth will enable future drug discovery projects focused on these targets.",Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury,9401793,R01NS100531,"['Axon', 'Behavioral', 'Biochemical', 'Biological', 'Biological Assay', 'Catalytic Domain', 'Cell physiology', 'Cells', 'Cervical', 'Complement 5a', 'Confocal Microscopy', 'Control Animal', 'Contusions', 'Corticospinal Tracts', 'Data', 'Development', 'Distal', 'Dorsal', 'Dose', 'Failure', 'Forelimb', 'Future', 'Gold', 'Growth', 'In Vitro', 'Individual', 'Information Theory', 'Injectable', 'Injury', 'Institution', 'Label', 'Lesion', 'Light', 'Machine Learning', 'Mediating', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Target', 'Morphology', 'Motor', 'Motor Cortex', 'Mus', 'Natural regeneration', 'Nerve', 'Neurites', 'Neurons', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphotransferases', 'Process', 'Rattus', 'Recovery of Function', 'Role', 'Signal Pathway', 'Signal Transduction', 'Site', 'Small Interfering RNA', 'Source', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Spinal cord injury patients', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transducers', 'Viral Vector', 'Walking', 'axon growth', 'axon regeneration', 'behavior test', 'behavioral outcome', 'central nervous system injury', 'clinically relevant', 'design', 'drug discovery', 'effective therapy', 'experience', 'experimental study', 'grasp', 'gray matter', 'improved', 'in vivo', 'in vivo Model', 'inhibitor/antagonist', 'injured', 'insight', 'kinase inhibitor', 'knock-down', 'light microscopy', 'novel', 'reconstruction', 'regenerative', 'screening', 'small molecule', 'targeted agent', 'therapeutic target']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2017,474406,-0.026815890371606463
"APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY Abstract  Acute myeloid leukemia (AML) accounts for half of all pediatric leukemia deaths and is the leading cause of leukemia-related death in adulthood. One reason for worse outcomes is the inability to properly assess for minimal residual disease (MRD) following therapy. Unlike ALL, AML presents with multiple subclonal populations without a singular clonal surface marker, and surface markers can change during therapy. The current gold standard for AML MRD is multi-parameter flow cytometry (MPFC), which is predictive of outcomes to frequencies of 0.001, yet 30% of MPFC-MRD-negative patients still relapse. Alternatively, every AML case harbors leukemia-specific mutations that could be markers of disease, except that next-generation sequencing has high error rate of ~1%. In this proposal, we will implement a novel, validated error-corrected sequencing (ECS) strategy, developed by the Druley lab in collaboration with Illumina, to improve MRD assessment of AML subclonal heterogeneity in 990 pediatric de novo AML cases from the Children's Oncology Group AAML1031 study. We hypothesize that using a highly sensitive sequencing method will improve identification of residual AML, provide important insights on subclonal heterogeneity in pediatric AML, improve understanding of the role of germline variability and gene function on relapses or refractory disease and facilitate personalized medicine. To interrogate this hypothesis, we propose the following aims: 1. Define subclonal heterogeneity at diagnosis and end of Induction 1 (EOI1) in 990 pediatric de novo  AML patients (n=1890). By using the largest prospective study of pediatric AML that has ever been  performed, we will perform ECS on 94 genes that are the most frequently mutated genes in pediatric and  adult AML at diagnosis and EOI1 to identify patterns of mutation associated with relapsed disease, FAB  subtypes or other cytogenetic features. 2. Correlate ECS-MRD with existing EOI1 MPFC-MRD for all participants in the COG AAML1031 study.  A major question is whether the “different from normal” cell population identified as residual disease by  MPFC is actually the same population(s) identified by ECS. We will define residual disease by ECS and  compare results to MPFC status (positive/negative), actual MPFC percentages (<0.001) and the clinical  outcomes (relapse risk, disease-free survival and overall survival) of study participants. 3. Integrate germline variation and all subclonal mutations into mechanistic groups that are frequently  mutated in pediatric AML and correlate with outcomes using unbiased machine learning  algorithms. Preliminary data tells us that every patient will have multiple subclones at diagnosis and EOI1  as well as germline variants in AML-associated genes, which may be important for outcome. In this aim, we  will take these mutations into account as well as MPFC, clinical features and cytogenetics for probabilistic  risk assessment using unsupervised machine learning algorithms for improved outcome prognostication. Narrative We have developed Error-Corrected Sequencing (ECS) that enables the highly accurate detection of leukemia- specific mutations in heterogeneous DNA samples to a limit of 0.0001. We will perform ECS with a panel of 94 frequently mutated genes in adult and pediatric AML in matched diagnostic and end of Induction 1 bone marrow samples from 990 pediatric de novo AML patients enrolled on the Children's Oncology Group AAML1031 protocol, which is the largest pediatric AML trial ever performed in North America. With these data, we can truly understand subclonal heterogeneity in pediatric AML, significantly improve minimal residual disease testing in pediatric AML, and integrate these data into an unbiased multivariate probability platform taking into account individual sequencing, flow cytometry, cytogenetic and clinical features to provide truly personalized cancer care.",APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY,9216965,R01CA211711,"['Acute Myelocytic Leukemia', 'Adult', 'Adult Acute Myeloblastic Leukemia', 'Algorithms', 'Alleles', 'BAY 54-9085', 'Biological Markers', 'Bone Marrow', 'Bortezomib', 'Categories', 'Cessation of life', 'Childhood', 'Childhood Acute Myeloid Leukemia', 'Childhood Leukemia', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Cytogenetics', 'DNA', 'DNA Sequence Alteration', 'DNA sequencing', 'Data', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Marker', 'Disease-Free Survival', 'Enrollment', 'Epigenetic Process', 'Exons', 'FLT3 gene', 'Family', 'Flow Cytometry', 'Frequencies', 'Future', 'Gene Targeting', 'Genes', 'Genetic', 'Goals', 'Gold', 'Immunophenotyping', 'Individual', 'Investigation', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Mutate', 'Mutation', 'Normal Cell', 'North America', 'Oncogenes', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pediatric Oncology Group', 'Phase', 'Point Mutation', 'Population', 'Probability', 'Prospective Studies', 'Protein Tyrosine Kinase', 'Protein phosphatase', 'Protocols documentation', 'Reagent', 'Recurrent disease', 'Refractory Disease', 'Relapse', 'Residual Neoplasm', 'Residual Tumors', 'Residual state', 'Resolution', 'Risk', 'Risk Assessment', 'Risk stratification', 'Role', 'Sampling', 'Site', 'Spliceosomes', 'Surface', 'Testing', 'Time', 'Treatment Efficacy', 'Tumor Suppressor Proteins', 'United States National Institutes of Health', 'Variant', 'analytical method', 'base', 'chemotherapy', 'cohesion', 'design', 'digital', 'falls', 'functional group', 'gene function', 'improved', 'improved outcome', 'insertion/deletion mutation', 'insight', 'leukemia', 'leukemia treatment', 'next generation', 'next generation sequencing', 'novel', 'nucleocytoplasmic transport', 'outcome prediction', 'pediatric patients', 'personalized cancer care', 'personalized medicine', 'prognostic', 'prospective', 'randomized trial', 'relapse risk', 'response', 'specific biomarkers', 'subclonal heterogeneity', 'transcription factor']",NCI,WASHINGTON UNIVERSITY,R01,2017,388615,-0.03699468490481828
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9314855,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Recruitment Activity', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'high dimensionality', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2017,784664,-0.02362866887395924
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9266422,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Expert Systems', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'Supervision', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'experimental study', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'public health relevance', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2017,445349,-0.03083753134581628
"Low Cost Mobile Platform for Pulmonary Disease Screening ﻿    DESCRIPTION (provided by applicant): Pulmonary disease (including primarily asthma, Chronic Obstructive Pulmonary Disease (COPD), pneumonia, lung cancer, and tuberculosis) is an increasingly large portion of the global health burden. Developing countries with large low-income populations are disproportionately affected, due to increased risk factors (e.g. biomass cooking stoves) as well as poor access to health care and lack of affordable screening tools for early detection. Chronic Obstructive Pulmonary Disease (COPD) alone is currently the third leading cause of death in the world and second leading cause of death in India after ischemic heart disease. In the younger population, pneumonia is a particular concern, being the leading cause of death for children under 5 years of age. Tuberculosis (TB) has also reached alarming proportions in India (24% of all cases worldwide). Despite this great prevalence of pulmonary disease in India, access to modern diagnostics instruments is not possible; furthermore, approximately 60% of general practice (GP) clinic doctors in India are primarily trained in Ayurvedic medicine with little or no training for diagnosing respiratory disease. As a result, many of the patients with lung disease are underdiagnosed or misdiagnosed (often confused with cardiovascular disease). As a result, there is a great need to provide health workers in India with simple tools that can be used to diagnose or screen for respiratory disease in the primary care setting. Addressing this need, our team has been developing a mobile diagnostic platform consisting of a digital stethoscope, peak flow meter, and mobile phone that can be used to screen for symptoms of lung disease and provide a guide for diagnosis. The present study extends this work and has the following aims: (1) To validate and test a low-cost mobile diagnostic platform for the purpose of identifying symptoms of lung disease and providing diagnostic guidance. (2) To assess the acceptance and usability of the mobile diagnostic platform by the local general practitioner (GP) doctors in rural India. We propose to deploy and test a low-cost mobile diagnostic platform, making use of machine learning algorithms that will detect specific symptoms of lung disease and help guide diagnosis. In the first year of the project, we shall create and train the mobile software algorithm using data collected in the field from patients (N=250) that have been previously diagnosed with specific lung diseases. Then year 2, we shall evaluate and test our mobile platform with Indian patients (N=250) recruited from four GP clinic sites in the Pune, India region. The automated mobile phone diagnosis result shall then be compared with the diagnosis from trained pulmonologists, using a traditional stethoscope as well as standard lung function testing instruments. A preliminary diagnosis and qualitative feedback shall also be collected from non-trained GP doctors using the mobile tools in order to ascertain usability and diagnostic value.         PUBLIC HEALTH RELEVANCE: Pulmonary disease is a very large public health concern in India, as well as in most developing countries. Relatively few general practitioner (GP) doctors are trained to properly diagnose pulmonary disease, and affordable tools for diagnostic support simply do not exist. This proposal seeks to validate and test the use of low-cost mobile phone-based diagnostic tools for pulmonary disease.            ",Low Cost Mobile Platform for Pulmonary Disease Screening,9356362,R21TW010245,"['5 year old', 'Address', 'Adult', 'Affect', 'Age', 'Air Pollution', 'Algorithmic Software', 'Algorithms', 'Asthma', 'Ayurvedic Medicine', 'Bacteria', 'Behavioral Medicine', 'Biomass', 'Bronchitis', 'Car Phone', 'Cardiovascular Diseases', 'Categories', 'Cause of Death', 'Child', 'Chronic', 'Chronic Obstructive Airway Disease', 'Chronic lung disease', 'Clinic', 'Clinical', 'Communicable Diseases', 'Communities', 'Cookstove', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Feedback', 'General Practices', 'General Practitioners', 'Health', 'Health Resources', 'Household Air Pollution', 'India', 'Interstitial Lung Diseases', 'Intervention', 'Life', 'Low Income Population', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant neoplasm of lung', 'Medical', 'Modernization', 'Myocardial Ischemia', 'Patients', 'Pharmaceutical Preparations', 'Pneumonia', 'Population', 'Prevalence', 'Public Health', 'Pulmonary Emphysema', 'Pulmonary function tests', 'Quality of life', 'Recruitment Activity', 'Research', 'Risk Factors', 'Rural', 'Sampling', 'Savings', 'Site', 'Stethoscopes', 'Symptoms', 'Testing', 'Training', 'Tuberculosis', 'Work', 'accurate diagnosis', 'base', 'cigarette smoking', 'cost', 'digital', 'global health', 'health care availability', 'improved', 'inclusion criteria', 'instrument', 'mHealth', 'meter', 'mobile computing', 'primary care setting', 'public health relevance', 'rural area', 'screening', 'tool', 'usability']",FIC,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2017,164421,-0.08973489569226449
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9310591,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Subgroup', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2017,607934,-0.005078141250066684
"Repurposing pyronaridine as a treatment for the Ebola virus Summary In 2014, the outbreak of the Ebola virus (EBOV) in West Africa highlighted the need for broad-spectrum antiviral drugs for this and other emerging viruses. Several groups had previously performed high throughput screens in 2013 and identified FDA approved drugs (amodiaquine, chloroquine, clomiphene and toremifene) with in vitro growth inhibitory activities against EBOV. We used these compounds to create a computational pharmacophore to identify additional compounds to test in vitro. In addition, data from a published large scale high throughput screen performed by SRI International and Texas Biomedical Research Institute was used to create machine learning models and then subsequently used to score clinical compounds for testing. We have published on how these combined methods identified 3 compounds for testing which were ultimately found to be nM in vitro. One of these compounds is an antimalarial approved in Europe called pyronaridine. We propose to characterize the ADME and PK properties of this compound prior to determining its efficacy in a mouse model of the Ebola virus infection. Therefore the Aims of this R21 proposal will fill some of the gaps inherent in the published data on pyronaridine so far: Aim 1. Perform preclinical in vitro characterization of pyronaridine. Aim 2. Formulate pyronaridine and perform PK studies in mouse. Aim 3. In vitro characterization of pyronaridine against multiple EBOV strains and in vivo efficacy in the mouse model of Ebola virus infection. The results of these aims will determine go/no go criteria for pursuing larger animal studies in non-human primates prior to clinical studies. In the light of a recent paper in the New England Journal of Medicine showing a clinical observation that EBOV patients treated with artesunate-amodiaquine had a 31% higher survival rate than those treated with artemether- lumefantrine 2, there will be considerable interest in evaluating antimalarials against Ebola. Our proposal to consider testing the efficacy in the mouse EBOV model using pyronaridine (which is used as artesunate- pyronaridine (Pyramax) and would be readily accessible in the clinic), presents a rapid approach to leverage the aforementioned clinical observations with a more potent compound. Pyronaridine also has additional benefits of tolerability which may be important in this patient population. Narrative Preliminary clinical data showed that Ebola virus (EBOV) patients treated with the antimalarials artesunate- amodiaquine had a higher survival rate than those treated with artemether-lumefantrine, in agreement with the in vitro EC50 for amodiaquine EC50 of 2.6µM. The antimalarial pyronaridine, a structural analog of amodiaquine, was identified by a computational repurposing strategy and further shown to have an EC50 of 420 nM against EBOV in vitro. We now propose to fully characterize this compound using standard preclinical ADME assays prior to mouse pharmacokinetic analysis, determine broad-spectrum applicability against multiple EBOV strains and ultimately in vivo efficacy testing in the mouse Ebola virus model prior to testing in a non-human primate model. Our aim is to show whether Pyronaridine is a viable clinical candidate to treat patients infected with EBOV.",Repurposing pyronaridine as a treatment for the Ebola virus,9357736,R21TR001718,"['Africa', 'Agreement', 'Amodiaquine', 'Animal Model', 'Animals', 'Antimalarials', 'Antiviral Agents', 'Area', 'Babesia', 'Behavioral', 'Binding Proteins', 'Biological Assay', 'Biological Availability', 'Biomedical Research', 'Blood specimen', 'Body Weight decreased', 'Bolus Infusion', 'Cessation of life', 'China', 'Chloroquine', 'Chloroquine resistance', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clomiphene', 'Control Groups', 'Data', 'Data Set', 'Disease', 'Disease Outbreaks', 'Dose', 'Drug Kinetics', 'Ebola virus', 'Ensure', 'Enzymes', 'Erythrocytes', 'Europe', 'European', 'FDA approved', 'Family', 'Female', 'Filoviridae', 'Filovirus', 'Formulation', 'Growth', 'Half-Life', 'Hour', 'In Vitro', 'Inbred BALB C Mice', 'Infection', 'International', 'Intestines', 'Journals', 'Knowledge', 'Lethal Dose 50', 'Libraries', 'Liver', 'Machine Learning', 'Malaria', 'Mannich Bases', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Mus', 'Natural Products', 'New England', 'Oral', 'Paper', 'Patients', 'Permeability', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Plasma', 'Plasma Proteins', 'Plasmodium falciparum', 'Plasmodium vivax', 'Property', 'PubChem', 'Publishing', 'Research Institute', 'Route', 'Solubility', 'Survival Rate', 'Techniques', 'Testing', 'Texas', 'Time', 'Toremifene', 'Toxic effect', 'Trypanosoma cruzi', 'Virus', 'Virus Diseases', 'Virus Inhibitors', 'Vivax Malaria', 'Whole Blood', 'analog', 'artemether', 'artesunate', 'base', 'benflumetol', 'clinical candidate', 'design', 'efficacy testing', 'high throughput screening', 'in vitro testing', 'in vivo', 'inhibitor/antagonist', 'interest', 'intraperitoneal', 'male', 'mouse model', 'neurotoxicity', 'nonhuman primate', 'patient population', 'pharmacophore', 'pre-clinical', 'preclinical study', 'prevent', 'pyronaridine', 'research clinical testing', 'response', 'treatment duration']",NCATS,"COLLABORATIONS PHARMACEUTICALS, INC.",R21,2017,312390,-0.0060230504643804805
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9279143,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronaviridae', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2017,202814,-0.012266676431651328
"High content in vivo screening for acute kidney injury ameliorating drugs ABSTRACT In this application we will perform high throughput, high content, screening for small molecules that improve kidney regeneration after acute kidney injury (AKI). AKI presents a dire unmet medical need because of its high prevalence with long-term adverse health effects and life-threatening sequelae. Mortality is high and the only effective treatments are renal replacement therapies. The onset of the precipitating event is unpredictable, and in many instances once patients are admitted to the hospital, injury has already occurred. Improving recovery from injury therefore presents an attractive opportunity for intervention, but to date, no therapies are available that are effective if administered post injury. The vertebrate kidney has an innate ability to regenerate and follows a well-defined cellular mechanism that encompasses dedifferentiation of surviving renal tubule cells, proliferation of resulting progenitors, and repopulation of the denuded tubule. This sequence of events, together with their respective molecular markers, is conserved between humans, mouse, and zebrafish. During regeneration, transcription factors normally expressed during organogenesis (e.g.,lhx1a, pax2, and pax8) are reactivated. We previously demonstrated that small molecule-mediated augmentation of endogenous Lhx1a expression can ameliorate recovery in zebrafish and mouse models of AKI. Together these data support the overall hypothesis that augmentation by small molecules of cellular programs that drive kidney repair after injury represents a novel pharmacologic approach for the treatment of AKI and associated sequelae. Using a transgenic zebrafish line that expresses Lhx1a-EGFP we have developed an artificial intelligence-based, high-content assay to quantify lhx1a expression in the living embryo. Using multivariate analysis, the assay met accepted HTS assay performance standards and was validated in three-day variability studies and a small pilot library screen. We will perform a primary HTS of 50,000 compounds from the MLPCN collection. Prioritized hits will be subjected to a fully implemented, rigorous secondary assay paradigm encompassing kidney organ development, metabolic stability, in vivo efficacy, and activity profiling in a pathophysiological relevant AKI model. At the end of these studies we will have identified functionally and mechanistically characterized in vivo chemical probes to investigate the biology of kidney injury and regeneration, some of which are expected to have features that make them suitable for development into preclinical leads. NARRATIVE Acute kidney injury (AKI) presents a dire unmet medical need with unacceptably high mortality rates and a lack of therapeutic modalities. The vertebrate kidney has an innate ability to regenerate that can be enhanced by small molecules. In this proposal we will perform high-content, high throughput screening for kidney regeneration in zebrafish to discover novel chemical probes to investigate mechanisms of augmented kidney regeneration after injury.",High content in vivo screening for acute kidney injury ameliorating drugs,9262478,R01DK112652,"['Acute Renal Failure with Renal Papillary Necrosis', 'Appearance', 'Artificial Intelligence', 'Biological Assay', 'Biological Markers', 'Biology', 'Breeding', 'Carboxylic Acids', 'Cell Proliferation', 'Chemicals', 'Collection', 'Complex', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Dose', 'Embryo', 'Embryonic Development', 'Ensure', 'Epithelial Cells', 'Etiology', 'Evaluation', 'Event', 'FDA approved', 'Fibrosis', 'Generations', 'Genetic', 'Health', 'High Prevalence', 'Hospitals', 'Human', 'Injury', 'Intervention', 'Kidney', 'Libraries', 'Life', 'Liver', 'Mediating', 'Medical', 'Metabolic', 'Microsomes', 'Modality', 'Modeling', 'Molecular Bank', 'Morphologic artifacts', 'Multivariate Analysis', 'Mus', 'Natural regeneration', 'Nature', 'Organogenesis', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacology', 'Prodrugs', 'Protocols documentation', 'Recovery', 'Renal Replacement Therapy', 'Renal function', 'Renal tubule structure', 'Reproducibility', 'Specificity', 'Stem cells', 'System', 'Testing', 'Therapeutic', 'Toxic effect', 'Transgenes', 'Transgenic Organisms', 'Tubular formation', 'United States National Institutes of Health', 'Zebrafish', 'analog', 'base', 'effective therapy', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'kidney repair', 'molecular marker', 'mortality', 'mouse model', 'novel', 'organ growth', 'organ regeneration', 'pre-clinical', 'progenitor', 'programs', 'repository', 'response', 'screening', 'small molecule', 'small molecule libraries', 'transcription factor']",NIDDK,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2017,348188,-0.010305179626826607
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Project Summary:  Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer's disease, Parkinson's disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these chal- lenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymor- phisms, whole brain neuroimaging, bioﬂuids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mecha- nisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Speciﬁcally, in Aim 1, we develop efﬁcient methods for multi-level semiparametric transformation mod- els to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efﬁciency. Our methods do not rely on full pedigree genotyping and provide family-speciﬁc substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic sys- tems through ordinary differential equations with random inﬂections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker proﬁles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efﬁcient machine learning methods to identify predictive markers, estimate optimal individu- alized therapies, and identify subgroups who may receive the greatest beneﬁt from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art ana- lytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research com- munity. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9308279,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2017,366940,-0.021955618755541462
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9250803,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'experimental study', 'genomic data', 'hazard', 'high dimensionality', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'predictive modeling', 'prevent', 'public health relevance', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,66026,-0.019286249884763913
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,-0.0035388565779969163
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,9297324,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Multimodal Imaging', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical method', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'indexing', 'kinematics', 'member', 'new technology', 'public health relevance', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2017,404000,-0.01736577666274286
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9357870,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Protein Hybridization', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2017,720717,-0.03446756189115408
"Improving safety and efficacy of platelet transfusion through systems biology Project Summary Platelet transfusion is critical for severely bleeding patients and nearly 6 million units are transfused in the United States and Europe annually. In the United States, platelets are typically stored for 5 days resulting in a waste of 20% of their supply. Short storage duration is a consequence of bacterial contamination and platelet quality considerations. Though many methods have been developed for bacterial testing and pathogen inactivation, fewer have been developed for improving quality of stored platelets. Platelet additive solutions have the possibility to increase storage quality and duration, reduce plasma-related allergic reactions, impact the efficacy of pathogen reduction techniques, and save plasma which can then be used as an additional transfusion product. While the benefits are well known, there has been little progress in developing new platelet additive solutions for increasing quality and safety of platelet transfusion because there is a lack of broad understanding of biochemical and signaling changes during storage. There has been interest to utilize high-throughput metabolite profiling for global understanding of platelet metabolic decline but data analysis of complex datasets has been a daunting challenge. In Phase I of this program, we developed the first, robust computational platform involving statistical analysis and systems biology of metabolic and signaling networks to interpret and analyze PLT metabolomic and proteomic profiles in a complete network context. Using time- course global, quantitative metabolite profiling, we determined that PLTs undergo a non-linear decay process and computationally identified key metabolic enzymes and cellular process that drive this decay. Based on the computational results, we have devised two novel additive solution strategies to mitigate the decay process and improve the length of PLT units. In this Phase II proposal, we will validate the computationally determined additive solutions for efficacy in alleviating the non-linear decay process through 1) metabolomics experiments, and 2) non-metabolic PLT physiology experiments including cell activation and hemostatic effectiveness. A successful additive solution will be progressed to media refinement and preclinical testing. Project Narrative Platelet transfusion units are typically stored for five days in the United States leading to a waste of 20% of units and potential quality concerns. The field is open for innovation as most storage media technologies are derived from work from the early 1990s. This proposal will develop novel computational methods to comprehensively understand the degradation of platelets under storage conditions and experimentally validate new additive solutions for increasing platelet quality and extending shelf life, an area that accounts for $2.5 billion of hospital costs.",Improving safety and efficacy of platelet transfusion through systems biology,9347295,R44HL127843,"['Accounting', 'Agreement', 'Algorithms', 'Allergic Reaction', 'Area', 'Biochemical', 'Biological', 'Biological Preservation', 'Blood', 'Blood Component Removal', 'Blood Platelets', 'Caring', 'Cell physiology', 'Cells', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Simulation', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Effectiveness', 'Enzymes', 'Equipment and supply inventories', 'Europe', 'Formulation', 'Glutathione', 'Hemorrhage', 'Hemostatic Agents', 'Hospital Costs', 'In Vitro', 'Intervention', 'Length', 'Lesion', 'Life', 'Literature', 'Machine Learning', 'Mathematics', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Methods', 'Modeling', 'Pathway interactions', 'Patients', 'Phase', 'Physiology', 'Plasma', 'Platelet Transfusion', 'Preclinical Testing', 'Process', 'Production', 'Proteomics', 'Reaction', 'Recovery', 'Resources', 'Risk', 'Safety', 'Signal Pathway', 'Signal Transduction', 'State Hospitals', 'Statistical Data Interpretation', 'Supplementation', 'Surveys', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Transfusion', 'United States', 'Validation', 'Work', 'base', 'cost', 'design', 'experimental study', 'human subject', 'improved', 'insight', 'interest', 'metabolic profile', 'metabolomics', 'model design', 'novel', 'open innovation', 'oxidative damage', 'pathogen', 'programs', 'statistics', 'success', 'time use', 'wasting']",NHLBI,"SINOPIA BIOSCIENCES, INC.",R44,2017,1099022,-0.01490936352600404
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9274155,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2017,269514,-0.006452803120542414
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,8996183,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Custom', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Screening for cancer', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'biomarker evaluation', 'burden of illness', 'candidate marker', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'flexibility', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'outcome prediction', 'patient biomarkers', 'patient population', 'predictive marker', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect', 'treatment response']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2017,323753,-0.06880505697344363
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,9310382,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Crystallization', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Genetic', 'Genetic Structures', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Imaging Device', 'Internships', 'Investigation', 'K-12 student', 'Knowledge', 'Link', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Mosaicism', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Structure', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'experimental study', 'feeding', 'fluorescence imaging', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'neuroinformatics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'role model', 'simulation', 'spatiotemporal', 'synergism', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2017,323033,-0.01064382066116746
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9349367,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2017,326784,-0.01536988530083576
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9378514,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunoregulation', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2017,659178,-0.05163948995241162
"Aptamer-Based Arrays for Detection of Pathogenic IgA1 O-Glycoforms in IgA Nephropathy ﻿    DESCRIPTION (provided by applicant): The glycosylation patterns on IgA1 antibodies are highly complex and heterogeneous. When there are dysregulations in activities of glycosylation enzymes, the hinge domain of IgA1's, i.e., the peptide domain connecting constant and variable regions, can undergo shifts in glycosylation patterns and become galactose- deficient (Gd). The galactose deficiency, together with other triggers, can lead to an auto-immune response in which patients' own antibodies form complexes with Gd-IgA1's; these complexes precipitate and cause damage in glomeruli, eventually leading to IgA1 nephropathy (IgAN).  It is of a great interest to monitor regularly Gd-IgA1's of patients who are predisposed to develop nephropathy. Further, increased understanding of correlations between dynamic variations in glycosylation patterns and the natural development of the disease in individual patients is expected to lead to improved interventions, including individually optimized therapies that could block the formation of offending complexes. None of the current approaches to assess microheterogeneity in glycosylation patterns is completely satisfactory; while elegant, these are also arduous and indirect, limited to highly specialized laboratories and difficult to reproduce in actual patients' samples, in large part due to lack of precise, well-characterized, molecular-level analytical tools  We propose to address this issue by systematic isolation of oligonucleotide-based molecular receptors or aptamers that will interact with clusters of different O-glycosides displayed in the hinge subregions. Aptameric receptors will be isolated from large oligonucleotide pools through the process of an in vitro selection and amplification coupled to the affinity separation via interactions with IgA1 hinge regions isolated from both Gd- IgAN patients and healthy controls. Individual aptamers will interact with substructures within the hinge domain, that is, with shorter peptides displaying one or more oligosaccharides. A large number of identified aptamers will be screened for their ability to interact with fractions of polyclonal IgA1's, and a variety of those aptamers that show a quantitatively different response to IgA1s from patients and matched controls will be selected for a more detailed characterization and incorporation in ""classification sensor arrays"" (CSAs).  As the result of our work, we will have immediately a set of aptamers that would together form a classification sensor array, an artificial ""nose"" capable of distinguishing samples belonging to patients with Gd- IgAN from healthy controls, as well as quantifying the extent of shifts in glycosylation patterns. Further molecular-level characterizatio of epitopes (subdomains) that these aptamers recognize is expected to enable studies towards identification of structures that are responsible for auto-immune responses in individual patients, and correlation with secondary triggers of diseases, which are the key step in the rational design of targeted inhibitors of the complex formation. PUBLIC HEALTH RELEVANCE: Complex saccharide structures ('glycans') play crucial roles in a wide range of biological functions and diseases such as autoimmunity, cancer, and nephropathy. We will help elucidate the role of complex glycans in the pathogenesis of IgA nephropathy, one of the most common causes of kidney failure worldwide, by systematically and exhaustively generating oligonucleotide-based receptors (aptamers) for glycan clusters on IgA1 antibodies. This will allow new diagnostic protocols based on pattern recognition with classification arrays and, in the long-term, personalized therapies to prevent antigen-autoantibody interaction and nephropathy.",Aptamer-Based Arrays for Detection of Pathogenic IgA1 O-Glycoforms in IgA Nephropathy,9308943,R21DK109690,"['Address', 'Affinity', 'Alkaloids', 'Amino Acids', 'Antibodies', 'Antigens', 'Autoantibodies', 'Autoimmunity', 'Biological Process', 'Biology', 'Biopsy', 'Chemicals', 'Chemistry', 'Classification', 'Clinical', 'Complex', 'Coupled', 'Deposition', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Dopamine', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epidemiology', 'Epitopes', 'Galactose', 'Genetic', 'Glycosides', 'IGA Glomerulonephritis', 'IgA1', 'Immune response', 'Immunoglobulin Constant Region', 'Immunoglobulin Variable Region', 'Immunology', 'In Vitro', 'Individual', 'Intervention', 'Kidney', 'Kidney Diseases', 'Kidney Failure', 'Laboratories', 'Lead', 'Lectin', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Molecular', 'Monitor', 'Nephrology', 'Neurotransmitters', 'Nose', 'Nucleic Acids', 'Oligonucleotides', 'Oligosaccharides', 'Pathogenesis', 'Pathogenicity', 'Patients', 'Pattern', 'Pattern Recognition', 'Peptides', 'Play', 'Polysaccharides', 'Process', 'Proteins', 'Protocols documentation', 'Reagent', 'Receiver Operating Characteristics', 'Regulation', 'Resolution', 'Role', 'Sampling', 'Sensitivity and Specificity', 'Series', 'Serum', 'Specificity', 'Sphingosine', 'Standardization', 'Steroids', 'Structure', 'Tertiary Protein Structure', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Variant', 'Work', 'analytical tool', 'aptamer', 'base', 'combinatorial', 'cost effective', 'design', 'disorder control', 'exhaustion', 'glycosylation', 'improved', 'individual patient', 'inhibitor/antagonist', 'inorganic phosphate', 'interest', 'multidisciplinary', 'noninvasive diagnosis', 'novel', 'novel diagnostics', 'patient stratification', 'personalized medicine', 'prevent', 'public health relevance', 'receptor', 'response', 'screening', 'sensor', 'sugar', 'tool']",NIDDK,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2017,200347,-0.020878004099322903
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9320865,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multimodal Imaging', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'analytical tool', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'high dimensionality', 'imaging Segmentation', 'imaging modality', 'imaging study', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'spatiotemporal', 'study population', 'terabyte', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2017,347156,-0.02024329832363095
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,9265960,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Lateral', 'Lead', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinical diagnostics', 'clinical predictors', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'individual patient', 'insight', 'interest', 'multimodality', 'neuroimaging', 'neuroimaging marker', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'prognostic', 'public health relevance', 'rapid diagnosis', 'response', 'screening', 'spinal tract', 'treatment response', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2017,577180,-0.01672907458400965
"Optimizing electrical impedance myography outcomes through data mining Project summary  Electrical impedance myography (EIM) is a non-invasive technology for the assessment of muscle that is based on the application of a weak, high frequency electrical current to a muscle and the measurement of the resulting surface voltages. The further development and application of EIM remains the main business focus of Skulpt, Inc, a small business concern based in Boston and San Francisco (Specific Aims just say San Francisco). Alterations to the condition of the muscle, including myocyte atrophy, fat and connective tissue deposition, and inflammation all alter the EIM data in predictable and consistent ways. To date, through Skulpt, EIM has been applied as a potential biomarker for assessing disease progression and response to therapy in a wide variety of neuromuscular disorders, including amyotrophic lateral sclerosis, Duchenne muscular dystrophy, and spinal muscular atrophy, as well as other disorders that impact muscle condition, such as disuse atrophy and sarcopenia (age related muscle loss); over 1000 people have been studied with Skulpt’s EIM technology. Whereas the results of these applications are promising, the analytic approaches taken to the data sets have been fairly basic, utilizing only simple single frequency or simplistic multifrequency values. However, with every single muscle measurement, over 240 individual data points are acquired at different frequencies, different depths of muscle penetration, and at different angles to the major muscle fiber direction. Moreover, each of the above studies has been done in isolation, and thus how results differ between diseases is unknown. Given the plethora of data, applying more sophisticated analytic approaches has the potential of yielding improved EIM measures. Moreover, collaborators have already collected an associated wealth of animal EIM data that will help further inform this analysis. Thus, in this proposed Phase 1 SBIR, we plan to apply a variety of data mining techniques to the vast set of data already accumulated at Skulpt, Inc such that improved EIM outcomes can be developed and implemented. In Specific Aim 1, we will study human data across all disease types evaluated to determine which data sets are most effective at discriminating diseased from healthy muscle as well as distinguishing between diseases. In Specific Aim 2, we will focus on finding the metrics that are most sensitive to the degree of muscle pathology in a specific disease. In both of these aims, we will evaluate how these new metrics are mirrored in already obtained animal data. In Specific Aim 3, we will study these metrics in a new set of data (a test set) that was not used to develop the analytical paradigms so as to ensure their robustness. With the conclusion of this work, we will plan to pursue a Phase 2 SBIR that will focus on the development of a software suite to assist in EIM data interpretation based upon these results followed by a prospective observational clinical study to evaluate the efficacy of these newly developed metrics for disease diagnosis and tracking of progression/response to therapy. Project Narrative  Electrical impedance myography (EIM) is a non-invasive technology for the assessment of muscle that remains the main focus of Skulpt, Inc. Considerable EIM data has already been collected in a variety of neuromuscular diseases. In this study, the investigators plan to perform a more detailed analysis of all data collected to date (so-called “data mining”), such that improved EIM outcomes can be developed that will be applied to future studies.",Optimizing electrical impedance myography outcomes through data mining,9466075,R43AR073114,"['Age', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Animals', 'Area', 'Atrophic', 'Back Pain', 'Boston', 'Businesses', 'Categories', 'Characteristics', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Set', 'Deposition', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease remission', 'Disuse Atrophy', 'Duchenne muscular dystrophy', 'Electrodes', 'Electrophysiology (science)', 'Ensure', 'Fatty acid glycerol esters', 'Fiber', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Glycogen storage disease type II', 'Health', 'Inclusion Bodies', 'Individual', 'Inflammation', 'Laboratories', 'Machine Learning', 'Measurement', 'Measures', 'Medical Technology', 'Methods', 'Mining', 'Muscle', 'Muscle Cells', 'Muscle Fibers', 'Muscular Dystrophies', 'Musculoskeletal', 'Myography', 'Myopathy', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Penetration', 'Phase', 'Play', 'Positioning Attribute', 'Radiculopathy', 'Research Personnel', 'Role', 'San Francisco', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Testing', 'Tissues', 'Validation', 'Work', 'animal data', 'base', 'commercialization', 'data mining', 'disease classification', 'disease diagnosis', 'electric impedance', 'human data', 'improved', 'indexing', 'neuromuscular', 'potential biomarker', 'prospective', 'response', 'sarcopenia', 'voltage']",NIAMS,"MYOLEX, INC.",R43,2017,149998,-0.004333702234637074
"Development of integrative models for early liver toxicity assessment ﻿    DESCRIPTION (provided by applicant): Computational toxicology has become a critical area of research due to the burgeoning need to evaluate thousands of pharmaceutical and environmental chemicals with unknown toxicity profiles, the high demand in time and resources by current experimental toxicity testing, and the growing ethical concerns over animal use in toxicity studies. Despite tremendous efforts, little success has been attained thus far in the development of predictive computational models for toxicity, primarily due to the complexity of toxicity mechanisms as well as the lack of high-quality experimental data for model development.  A critical challenge in toxicity testing of chemicals is that toxicity effects are doe-dependent: the true toxic hits may show no toxicity at all at low dose level. Therefore, traditiona high-throughput screening (HTS) that test chemicals only at a single concentration is not suitable for toxicity screening. On the contrary, the recently developed quantitative high-throughput screening (qHTS) platforms can evaluate each chemical across a broad range of concentrations, and is gaining ever-increasing popularity as a tool for in vitro toxicity profiling The concentration-response information generated by qHTS are expected to provide more accurate and comprehensive information of the toxicity effects of chemicals, offering promising data that can be mined to estimate in vivo toxicities of chemicals. However, our previous studies showed that if processed inappropriately, such concentration-response information contribute little to improve the toxicity prediction. This is especially true when multiple types of qHTS data are used together. Therefore, in this study, we will extend our previous approaches to develop novel statistical and computational tools that can curate, preprocess, and normalize the concentration-response information from multiple different qHTS databases.  Traditionally, toxicity models are based on either the chemical data (such as the quantitative structure- activity relationship analysis), or the in vitro toxicity profiling data (such as the in vitro-in vivo extrapolations). Our previous experiences suggested that integrating biological descriptors such as the in vitro cytotoxicity profiles or the short-term toxigenomic data, with chemical structural features is able to predict rodent acute liver toxicity with reasonable accuracy. Therefore, the second part of this proposal will be devoted to develop novel computational models for hepatotoxicity prediction by integrating qHTS toxicity profiles and chemical structural information In Aim 1, we will curate, preprocess, and normalize collected public liver toxicity datasets. In ths study, we will model toxicity effects using multiple large public datasets such as HTS and qHTS bioassay data (Tox21[1] and ToxCast[2]), hepatotoxicity side effect reports on marketed failed drugs[3], the Liver Toxicity Knowledge Base Benchmark Dataset (LTKB-BD[4]), etc. Statistical methods for cross-study validation and quality control will be applied to the collected datasets to ensure computational compatibility and to select the appropriate datasets for analysis. In Aim 2, we will develop predictive models for chemicals' liver toxicity based on an integrative modeling workflow that will make use of both structural and in vitro toxicity profiles of a chemical. Our previous studies [5] showed that models using both in vitro toxicity profiles and chemical structural data have better accuracy for rodent acute liver toxicity than models using either data type alone. Here, we will develop a novel modeling workflow that start with defining the functional clusters of chemicals via curated qHTS toxicity profiles, and is followed by developing computational models to correlate chemical and biological data with overall toxicity risks in humans. The predictive models will be validated using independent datasets with over 800 compounds. In Aim 3, we propose to prioritize the qHTS profiling assays used in the model for future toxicity testing. We will evaluate all the in vitro assays as biological descriptors from thee perspectives, including descriptor importance in the integrative toxicity model, correlation with i vivo DILI outcomes, and level of information content estimated by a novel approach based on network analysis. PUBLIC HEALTH RELEVANCE: In this study we aim to develop computational models that can identify potential liver toxicants. Liver toxicity is a significant contributor to the high attition rate in drug development. Moreover, toxic chemicals in food, water, and consumer products all pose serious risks for liver toxicity. As a result, there is great interest in developing high-throughput, high-content experimental and computational tools to evaluate the liver toxicity of thousands of pharmaceutical and environmental chemicals. This study focuses on developing novel informatics tools that enable the extraction and integration of chemical concentration-response information from multiple quantitative high-throughput screening databases for model development, and developing statistical models that are able to integrate this concentration-response information with chemical structural features to predict their risk of liver toxicity.  ",Development of integrative models for early liver toxicity assessment,9333370,R03ES026397,"['Acute', 'Address', 'Adverse effects', 'Algorithms', 'Animals', 'Area', 'Benchmarking', 'Biological', 'Biological Assay', 'Chemical Models', 'Chemicals', 'Communities', 'Complex', 'Computer Simulation', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Dose', 'Dreams', 'Ensure', 'Ethics', 'Food', 'Future', 'Gene Expression', 'Genomics', 'Goals', 'Gold', 'Hepatotoxicity', 'Human', 'In Vitro', 'Informatics', 'International', 'Liver', 'Machine Learning', 'Marketing', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Environmental Health Sciences', 'Nature', 'Network-based', 'North Carolina', 'Outcome', 'Pathway Analysis', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Poison', 'Process', 'Productivity', 'Quality Control', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research', 'Resources', 'Risk', 'Rodent', 'Scientist', 'Shapes', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicogenetics', 'Toxicology', 'Translational Research', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'Universities', 'Variant', 'Water', 'base', 'computational toxicology', 'computerized tools', 'consumer product', 'cost', 'cost effective', 'cytotoxicity', 'data modeling', 'drug development', 'drug market', 'drug withdrawal', 'environmental chemical', 'experience', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'interest', 'knowledge base', 'liver injury', 'model development', 'novel', 'novel strategies', 'preclinical study', 'predictive modeling', 'programs', 'public health relevance', 'response', 'screening', 'success', 'tool', 'toxicant', 'validation studies']",NIEHS,UT SOUTHWESTERN MEDICAL CENTER,R03,2017,81000,-0.017126633096621692
"Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy DESCRIPTION (provided by applicant): Our understanding of cervical remodeling during pregnancy and labor is incomplete, partly due to the lack of in vivo studies on the biochemical changes that occur in the cervix over the course of pregnancy. Elucidation of the mechanisms for cervical ripening could be used to predict the onset of preterm labor. Until recently, in vivo research methods were too invasive to be used as discovery tools, particularly in women who present with preterm labor. This proposal will use in vivo Raman spectroscopy, an optical technique that is sensitive to collagen content, collagen structure, hydration, lipids, proteins, ad other biomolecules to non-invasively investigate the biochemistry of the cervix throughout pregnancy. Using fiber optic in vivo Raman spectroscopy, we recently found significant differences in Raman spectra in at least four important peaks during the course of pregnancy in mice, including discrete signatures for lipids, collagen, amide bonds, and enriched amino acids (proline, tyrosine). Computational analysis of these spectra yielded predictive algorithms with 94% classification accuracy for stage of pregnancy. Studies performed in 2-hour windows at the end of pregnancy identified spectra predictive for the timing of parturition. This approach provides a detailed real-time biomolecular map of cervical ripening that is currently unavailable by other means. In this proposal, we hypothesize that the different mechanisms of premature cervical ripening have unique Raman spectral signatures that correspond to underlying biochemical and mechanical changes that precede preterm birth, which can be detected in vivo. Two Specific Aims are proposed: 1) Determine spectral changes in the cervix of mice with normal and abnormal pregnancy and parturition; 2) Identify specific mediators of cervical remodeling by comparing Raman spectra to mechanical and biochemical changes in the ex vivo cervix during normal and abnormal parturition. Raman spectroscopy has primarily been used for detection of disease. Collaboration between our reproductive biology and bioengineering groups will capitalize on our expertise in Raman analysis of cervical tissues to study dynamic changes in cervix composition during pregnancy. Key elements in cervical biochemistry will be identified. In vivo Raman spectroscopy will be combined with biomechanical studies and imaging mass spectrometry, a powerful tool for in situ proteomic analysis, to examine mice with premature or delayed cervical remodeling. Together, these highly innovative approaches will generate in-depth profiles of cervical biology that will translate into novel non-invasive methods to detect impending premature birth in women. PUBLIC HEALTH RELEVANCE: This proposal will use Raman Spectroscopy, a non-invasive, optical scattering technique, to investigate the composition of the cervix throughout pregnancy and provide detailed real-time information on cervical ripening. These studies will identify spectral differences in the cervix during normal and abnormal cervical maturation; optical and biochemical markers will be identified to help monitor pregnancy non-invasively, as the fiber optic probe only requires brief contact with the external surface of the cervix to obtain measurements. Elucidating the mechanisms that initiate cervical ripening will provide a critical step for early detection and treatment of preterm birth, which is the leading cause of infant morbidity and mortality.",Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy,9443649,R01HD081121,"['Address', 'Algorithms', 'Alprostadil', 'Amides', 'Amino Acids', 'Applications Grants', 'Biochemical', 'Biochemical Markers', 'Biochemistry', 'Biological', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biomechanics', 'Biomedical Engineering', 'Birth', 'Cervical', 'Cervical Ripening', 'Cervix Uteri', 'Classification', 'Clinical', 'Collaborations', 'Collagen', 'Computational algorithm', 'Computer Analysis', 'Data', 'Detection', 'Development', 'Disease', 'Early Diagnosis', 'Early treatment', 'Elasticity', 'Elements', 'Emerging Technologies', 'Etiology', 'Fetal Development', 'Fiber Optics', 'Foundations', 'Generations', 'Goals', 'Health', 'High-Risk Pregnancy', 'Hormonal', 'Hour', 'Hydration status', 'Image', 'Immunohistochemistry', 'Impairment', 'In Situ', 'In Situ Hybridization', 'Infant Mortality', 'Interdisciplinary Study', 'Laboratories', 'Lead', 'Link', 'Lipids', 'Machine Learning', 'Maintenance', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Mechanics', 'Mediator of activation protein', 'Medical', 'Methods', 'Mifepristone', 'Modality', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mus', 'Optics', 'Periodicity', 'Phenotype', 'Physiological', 'Pregnancy', 'Premature Birth', 'Premature Labor', 'Prevention', 'Process', 'Proline', 'Property', 'Proteins', 'Proteomics', 'Raman Spectrum Analysis', 'Reproductive Biology', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Spectrum Analysis', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Translating', 'Translations', 'Tyrosine', 'Woman', 'base', 'clinical application', 'in vivo', 'infant morbidity', 'innovation', 'insight', 'learning strategy', 'mouse model', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'physical science', 'prediction algorithm', 'predictive modeling', 'pregnant', 'premature', 'public health relevance', 'response', 'tool']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2017,376620,-0.015637589233675298
"Kynurenine metabolites and depression: An in vitro and ex vivo study ﻿    DESCRIPTION (provided by applicant): The kynurenine catabolic pathway for tryptophan degradation in humans produces metabolites that are neurologically active and affect neurotransmission and neuronal integrity. Quinolinic acid (QUIN) is a product of the pathway which has been implicated in numerous neuropsychiatric disorders and has been correlated with depression in patients administered the inflammatory cytokine, interferon (IFN)-α. Besides, activation of the kynurenine pathway by inflammation leads to elevated levels of QUIN, which have been shown to cause depressive-like behavior in mice. The mechanism by which QUIN acts in the brain is to activate excitotoxic neurotransmitter pathways involving the amino acid glutamate and by leading to lipid peroxidation. Following metabolism by indoleamine 2,3-dioxygenase and subsequent steps, three consecutive enzymes of the kynurenine pathway determine the metabolic partitioning of tryptophan metabolites to QUIN, picolinic acid (PIC), or further metabolism. We hypothesize that the shift of QUIN and PIC levels is linked to chronic inflammation and major depression. To test this hypothesis, we will, in the first aim, study the structure and action of the enzymes and the mechanisms controlling QUIN and PIC production. In the second aim, a new, rapid and efficient analytical method will be developed for in vitro and ex vivo quantitation of QUIN and PIC levels and will be applied to samples obtained from patient populations expected to exhibit high levels of QUIN as a result of increased inflammation and from assays using purified protein to simulate similar distributions. Through these studies, the correlation between kynurenine metabolites and major depression will be investigated, and small molecule inhibitors will be used as probes for modulating the production of QUIN and PIC levels. n/a",Kynurenine metabolites and depression: An in vitro and ex vivo study,9272010,R21MH107985,"['AIDS Dementia Complex', 'Acids', 'Affect', 'Agonist', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Anabolism', 'Anxiety', 'Behavior', 'Biochemistry', 'Biological Assay', 'Biological Markers', 'Brain', 'Carboxy-Lyases', 'Cerebrospinal Fluid', 'Chronic', 'Clinical', 'Complex', 'Detection', 'Development', 'Diagnostic Procedure', 'Dioxygenases', 'Disease', 'Enzyme Inhibitor Drugs', 'Enzymes', 'Epilepsy', 'Essential Amino Acids', 'Exhibits', 'Genes', 'Glutamates', 'Goals', 'Human', 'Huntington Disease', 'In Vitro', 'Individual', 'Inflammation', 'Inflammatory', 'Interferon-alpha', 'Kynurenic Acid', 'Kynurenine', 'Label', 'Lead', 'Link', 'Lipid Peroxidation', 'Lipid Peroxides', 'Liquid substance', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Mental Depression', 'Metabolic', 'Metabolic Control', 'Metabolism', 'Molecular', 'Mus', 'N-Methyl-D-Aspartate Receptors', 'Neurologic', 'Neurons', 'Neurotoxins', 'Neurotransmitters', 'Oxidoreductase', 'Pathologic', 'Pathway interactions', 'Patients', 'Phase', 'Physicians', 'Picolinic Acids', 'Plasma', 'Positioning Attribute', 'Production', 'Proteins', 'Protocols documentation', 'Quinolinic Acid', 'Reaction', 'Records', 'Regulation', 'Research', 'Research Personnel', 'Route', 'Sampling', 'Scheme', 'Scientist', 'Side', 'Solid', 'Source', 'Structure', 'Testing', 'Tissues', 'Tryptophan', 'Tryptophan 2,3 Dioxygenase', 'analytical method', 'base', 'chemical reaction', 'cost', 'cost effective', 'cytokine', 'depressive symptoms', 'design', 'enzyme mechanism', 'enzyme pathway', 'excitotoxicity', 'improved', 'innovation', 'interest', 'metabolic profile', 'neuroinflammation', 'neuropsychiatric disorder', 'neurotransmission', 'novel', 'patient population', 'small molecule', 'small molecule inhibitor', 'stem', 'three dimensional structure']",NIMH,UNIVERSITY OF TEXAS SAN ANTONIO,R21,2017,222688,-0.014892170267451203
"Software Platform to Stratify Patients for Treatment Arm Randomization in Human Clinical Trials using Patient-Level Predictive Models ABSTRACT Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease of brain and spinal cord motor neurons. Since Riluzole was approved in 1995, over 30 late-phase clinical trials have failed and no additional medications have been approved for ALS. While death from ALS averages 3 to 5 years from onset of symptoms, disease progression displays wide heterogeneity. A typical year-long clinical trial can have 20 to 25% of the patients die from the disease while another similar percentage progresses very slowly if at all. Several meta-analyses of ALS trial data indicate that ALS clinical trials are prone to statistical uncertainty and would benefit from tools that increase statistical sensitivity. It is clear that current statistical tools are inadequate to address the drug development challenges posed by this disease and many other diseases that characteristically exhibit high heterogeneity in disease progression. Using the recently available ALS PRO-ACT data set, our team was recently declared a winner of the DREAM Phil Bowen ALS Prediction Prize4Life Challenge. Since the contest, we have significantly improved the algorithm, built several additional models and begun to create drug development tools. The goal of this grant is to validate our clinical trial randomization tool and develop a prototype interface for use and testing at clinical trial sites. This prototype will serve as a platform for building a suite of tools based on disease progression predictions of individual patients that will eventually be used for drug development in multiple indications. The key innovation of this work, as it applies to randomizing patients for inclusion in different arms of a clinical trial is that it stratifies patients not by a set of features at the beginning of a trial, but rather by predicted outcome at the end of the trial as if patients in the treatment arm had not received the intervention being tested. An improved trial arm balance will provide a better test of the efficacy of the intervention. This work will focus on the following Specific Aims: Aim 1: Demonstrate that, compared to traditional randomization strata, randomization strata defined by predictive algorithms significantly improve the balance of outcome features at the end of a trial period. Aim 2: Work with our clinical partner to develop a prototype platform that will enable an ALS predictive algorithm to be used by on-site investigators for randomization in future clinical trials. The randomization tool is the first in a series of planned tools based on patient level disease progression predictions. These tools will radically change the way early ALS clinical trials are enrolled, simulated and analyzed and will enable the development of similar tools, not only for other neurodegenerative diseases such as Parkinson’s and Alzheimer’s, but also for multiple other diseases including diabetes, hospital-acquired infections, heart disease and cancer. NARRATIVE This work will develop a prototype to test the use of patient disease progression predictions made by machine learning models as a new way of randomizing clinical trials. The prototype will serve as a platform for the inclusion of a range of drug development tools based on individual patient predictions",Software Platform to Stratify Patients for Treatment Arm Randomization in Human Clinical Trials using Patient-Level Predictive Models,9347506,R43TR002047,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Amyotrophic Lateral Sclerosis', 'Big Data', 'Biotechnology', 'Brain Diseases', 'Cessation of life', 'Characteristics', 'Client', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Disease model', 'Drug usage', 'Enrollment', 'Equilibrium', 'Exhibits', 'Failure', 'Future', 'Goals', 'Grant', 'Healthcare', 'Heart Diseases', 'Heterogeneity', 'Human', 'Huntington Disease', 'Intervention', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Motor Neurons', 'Neurodegenerative Disorders', 'Nosocomial Infections', 'Onset of illness', 'Outcome', 'Outcome Measure', 'Parkinson Disease', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Placebos', 'Predictive Analytics', 'Principal Investigator', 'Production', 'Protocols documentation', 'Randomized', 'Randomized Clinical Trials', 'Records', 'Research', 'Research Personnel', 'Riluzole', 'Scientist', 'Series', 'Site', 'Small Business Innovation Research Grant', 'Software Tools', 'Spinal Diseases', 'Statistical Data Interpretation', 'Statistical sensitivity', 'Stratification', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Treatment Efficacy', 'Uncertainty', 'Work', 'Writing', 'analytical tool', 'arm', 'base', 'design', 'drug development', 'drug discovery', 'efficacy testing', 'experience', 'experimental study', 'field study', 'improved', 'individual patient', 'innovation', 'outcome prediction', 'patient stratification', 'prediction algorithm', 'predictive modeling', 'prognostic', 'prototype', 'randomized trial', 'research and development', 'simulation', 'survival prediction', 'tool', 'tool development', 'willingness']",NCATS,"ORIGENT DATA SCIENCES, INC.",R43,2017,224796,-0.02203792784270005
"Validation of Survey Questions to Distinguish Type 1 and Type 2 Diabetes Among Adults With Diabetes PROJECT SUMMARY/ABSTRACT Type 1 and Type 2 diabetes are distinct clinical conditions with different etiologies, ages of onset, management strategies, risk factors, and outcomes. Currently, the data sources that the Centers for Disease Control and Prevention (CDC) relies upon to monitor trends in diabetes prevalence and incidence are unable to reliably distinguish between types of diabetes. Most of the large federal surveys used for diabetes surveillance have not included questions on diabetes type, and few studies have reported survey-based algorithms for identifying diabetes type. None have compared survey-based identification algorithms with a gold standard case ascertainment in order to validate survey-based assignment of diabetes type. Methods for distinguishing between diabetes types in electronic health records (EHR) data have been tested for children and adults, but additional validation work is needed. Our approach to improving diabetes surveillance in these two areas is based on an integrated study design whereby survey data for diabetes patients are linked with data from their EHR and a gold standard case ascertainment derived from chart review. We will select a diverse sample of diabetes patients that is designed and powered to assess algorithm validity for subpopulations defined by age, sex, and race/ethnicity. We will use rigorous questionnaire development methods to build on items used in previous surveys, cognitively test the new survey module to optimize wording and question order, field the survey using data collection methods similar to established CDC surveys, and analyze the responses relative to a gold standard classification of diabetes type. In addition, we will develop and validate against the gold standard EHR-based algorithms, including a modeling approach that produces a parsimonious rule-based algorithm for determining diabetes type based on the most important clinical variables, and a machine learning approach that uses the gold standard dataset as a starting point to identify implicit patterns that distinguish T1DM and T2DM. PROJECT NARRATIVE The proposed project will result in a set of validated survey questions for distinguishing Type 1 diabetes (T1DM) and Type 2 diabetes (T2DM). It will also produce validated survey-based and EHR-based algorithms for identifying diabetes type. T1DM and T2DM are distinct clinical conditions with different etiologies, ages of onset, management strategies, risk factors, and outcomes. Currently, the data sources that CDC relies upon to monitor trends in diabetes prevalence and incidence are unable to reliably distinguish between types of diabetes. It is important for surveillance systems to be able to distinguish diabetes type to support type-specific analyses of morbidity, mortality, medical care costs, and health-related quality of life. Improved surveillance data by diabetes type can help guide and monitor federal, state, and local diabetes programs, enrich diabetes research, and support people with diabetes.",Validation of Survey Questions to Distinguish Type 1 and Type 2 Diabetes Among Adults With Diabetes,9438672,U01DP006327,[' '],NCCDPHP,"WESTAT, INC.",U01,2017,498977,-0.017028284805405244
"Quantitative microscopy-based rapid phenotyping and screening ﻿    DESCRIPTION:  Synapses are most fundamental to the function of a nervous system. C. elegans is an excellent genetic model system for finding genes and elucidating pathways because of its sequenced genome and the abundance of molecular biology tools and mutants. Due to the simplicity of its nervous system, many breakthroughs have been made in C. elegans for understanding molecular mechanisms in the patterning of the nervous system and synapse development. The current bottlenecks are in the manual and non-quantitative techniques such as visual screens, limiting both the throughput of the experiments and the phenotypes one can examine. Our long-term objective is to develop technologies and to understand how genes, age, and the environment together define and continue to remodel the nervous system of an organism. In the last funding period, we have made large progress in hardware system design (including microtechnologies and automation technologies) and software for quantitative characterization of phenotypes. The objective of this continuation project is to further engineer superior micro devices for large-scale live imaging and quantitative imaging technologies, and combine with the power of genetic and genomic approaches to study synapse development in this in vivo system; genes and pathways emerging from this study could potentially become targets of therapeutics in neurological disorders.  We have shown in the previous phase of the project that quantitative microscopy-based approaches can indeed enable identification of novel genes and pathways that conventional approaches cannot. In the continuation phase, we will further optimize on-chip rapid and high-content in vivo imaging techniques, and in parallel further develop algorithms and quantitative measures for the analysis of such high-content data; we will screen based on novel synthetic phenotype unobservable by eye; we will also exploit powerful genomic techniques to identify loci and potential multigenic interactions that shape the synapse morphology. These experimental approaches will identify genes that cannot have been identified otherwise because of the difficulties associated with the phenotypical profiling, but addressed using our engineered techniques here. The approach is innovative because the technology developed here dramatically increases the throughput, sensitivity, and accuracy of the experiments, and truly enables the utility of extremely powerful genetic and genomic methods. The proposed research is significant because it fills the urgent need in high-throughput and high-content screens as well as identifying novel genes and pathways. In addition, besides the contribution to the specific neurobiology, the technologies are widely applicable to areas such as developmental cell biology, and to other small organisms such as fly larvae and zebrafish embryos. PUBLIC HEALTH RELEVANCE:   Synapse development is an important and active area of research linking genes and environments to the formation and maintenance of synapses in the nervous system. It has direct implications in many human diseases developmental and psychiatric diseases such as Autism Spectrum Disorder and Schizophrenia.",Quantitative microscopy-based rapid phenotyping and screening,9301591,R01GM088333,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alleles', 'Animals', 'Area', 'Automation', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Chromosome Mapping', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Development', 'Developmental Cell Biology', 'Devices', 'Disease', 'Drosophila genus', 'Embryo', 'Engineering', 'Environment', 'Event', 'Eye', 'Fill-It', 'Fluorescence', 'Funding', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Research', 'Genetic Screening', 'Genome', 'Genomic approach', 'Genomics', 'Goals', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Imaging technology', 'Inbreeding', 'Larva', 'Lead', 'Link', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Microfluidics', 'Microscopy', 'Molecular', 'Molecular Biology', 'Morphology', 'Nematoda', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurons', 'Organism', 'Pathway interactions', 'Pattern', 'Phase', 'Phenotype', 'Phylogeny', 'Positioning Attribute', 'Quantitative Microscopy', 'Quantitative Trait Loci', 'Regulatory Pathway', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Speed', 'Synapses', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Vision', 'Visual', 'Work', 'Zebrafish', 'autism spectrum disorder', 'base', 'design', 'experience', 'experimental study', 'fly', 'forward genetics', 'genetic approach', 'high dimensionality', 'high throughput screening', 'human disease', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'mutant', 'nerve injury', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'quantitative imaging', 'screening', 'success', 'synaptogenesis', 'targeted treatment', 'technology development', 'tool', 'trait']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2017,305864,-0.040350359363126954
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper.         PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.            ",Deep learning for representation of codes used for SEER-Medicare claims research,9023921,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medical Surveillance', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'abstracting', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2016,178072,-0.04063979110965981
"Machine Learning and Personalized Prognosis for Depression Treatment Abstract Depression treatment is effective for approximately 50-60% of patients who receive treatment, but the probability of a successful response is typically unknown before treatment begins. As a result, depression treatment is routinely delivered in a trial-and-error fashion until a satisfactory response is achieved. Our objective is to provide a personalized prognosis by applying ensemble machine learning techniques to discover novel, non-linear combinations of multiple weak predictors that collectively yield accurate predictions of treatment outcome. This statistical approach considers many prediction variables simultaneously and iteratively constructs a complex prediction model that often dramatically outperforms traditional statistical methods. Aim 1 is to apply stochastic gradient boosted decision trees to predict response to citalopram using archival data from the STAR*D clinical trial. In preliminary analyses, we randomly selected 1223 patients to train the model and another 407 patients to independently test the model (a 75-25 split), with tuning parameters selected by cross-validation to minimize log-loss. The resulting prediction on the independent test sample was superior to the no-information rate (p < 0.001), with an overall predictive accuracy of 66%. Although this level of prediction is significantly better than a no information model, we plan to improve the model's prognostication by 1) adding features that capture the “pharmacological noise” of concurrent (non- study) medication use and 2) updating model predictions based on early signs of response. Aim 2 is to use a similar machine learning approach to examine response to internet-based CBT for depression. Internet-based treatments for depression are growing in popularity, provide efficient access to health care, reduce treatment costs, and have good evidence for treatment efficacy. Importantly, we have a large dataset (N = 1,013) within which to develop treatment-matching algorithms that predict treatment response based on patient attributes. Study Impact: The overarching goal of this project is to use machine learning methods to develop treatment matching algorithms. In the long term, we can envision a system that evaluates a patient on a number of important predictor variables and provides a personalized probability of treatment success. These probabilities would then be used to guide treatment selection or modify current treatment if a poor response is predicted. Developing algorithms that successfully predict whether a particular form of treatment is likely to be successful for a patient with a given set of attributes would be a tremendous step towards efficient and personalized depression treatment.   Public Health Narrative Depression treatment is effective for roughly half of the patients who receive treatment, but it is usually unknown how a specific patient with a particular set of attributes will respond to a given treatment. Our objective is to provide a personalized prognosis by applying ensemble machine learning techniques to discover novel combinations of multiple weak predictors that collectively yield accurate predictions of treatment outcome. Developing algorithms that successfully predict whether a particular form of treatment is likely to be successful for a patient would be a tremendous step towards efficient and personalized depression treatment.  ",Machine Learning and Personalized Prognosis for Depression Treatment,9168157,R21MH110758,"['Advocate', 'Algorithms', 'Citalopram', 'Clinical', 'Clinical Trials', 'Coin', 'Complex', 'Data', 'Data Set', 'Databases', 'Decision Trees', 'Development', 'Goals', 'Health', 'Internet', 'Intervention', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Mental Depression', 'Mental Health', 'Methods', 'Modeling', 'Multi-Institutional Clinical Trial', 'Noise', 'Online Systems', 'Patients', 'Pharmaceutical Preparations', 'Probability', 'Public Health', 'Recommendation', 'Sample Size', 'Sampling', 'Selection for Treatments', 'Statistical Methods', 'Symptoms', 'System', 'Techniques', 'Testing', 'Training', 'Treatment Cost', 'Treatment Efficacy', 'Treatment outcome', 'United States', 'United States National Institutes of Health', 'Update', 'Validation', 'abstracting', 'base', 'clinical practice', 'effective therapy', 'health care availability', 'improved', 'information model', 'learning strategy', 'novel', 'outcome forecast', 'personalized medicine', 'precision medicine', 'predicting response', 'psychologic', 'response', 'success', 'treatment response']",NIMH,"UNIVERSITY OF TEXAS, AUSTIN",R21,2016,234375,-0.021890258579224095
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9180344,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoking', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2016,188040,-0.07061272998642412
"Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA ﻿    DESCRIPTION (provided by applicant): Osteoporosis is a disease characterized by loss of bone mass and structural deterioration leading to increased risk of fracture. Currently, osteoporosis is diagnosed by measurement of areal bone mineral density by dual- energy x-ray absorptiometry (DXA). However, the majority of fractures occur in both women and men who are not classified as osteoporotic by current DXA criteria (T-score = -2.5). As a 2-dimensional (2D) technology, DXA does not provide information about 3-dimensional (3D) bone structure, shape and geometry, which substantially contribute to bone strength and resistance to fracture. Finite element (FE) analysis of quantitative computed tomography (QCT) images can provide 3D structure and strength measurements but QCT is impractical for widespread clinical use because of high radiation exposure and expense. In contrast, DXA is widely available, inexpensive and has low radiation exposure. What is needed is a method by which DXA images can be used to generate 3D shape models that incorporate bone structure and geometry. However, fractures are complex events influenced by other factors including age, race, body mass index, risk of falls, and prior medical and fracture history. Even sophisticated measurements of bone density, structure, and strength may not be able to predict fractures accurately. Machine learning is an emerging field in which models are created by ""learning"" from previous data. These models can incorporate various factors and be used to classify or predict outcomes for new data. The overall hypothesis of this proposal is that advanced analyses of widely available DXA images that incorporate structural and strength information and statistical modeling using machine learning to incorporate additional risk factors will better identify patient at high risk of osteoporotic fracture. This hypothesis will be tested using QCT and DXA data from previous studies to generate 3D statistical shape models that describe variability in proximal femur morphology. By aligning 2D DXA images to the models, patient-specific 3D models will be reconstructed for quantitative analyses and combined with FE analysis to estimate bone strength. Machine learning models will be used to incorporate these novel measurements, demographics, and various risk factors for fracture to predict incident fractures in two very large, prospective studies. The ultimate goal of this proposal is to increase the diagnostic utility of DXA, a safe, non-invasive, and widely available technology, by applying novel image processing and statistical techniques to predict fractures more accurately. PUBLIC HEALTH RELEVANCE: Approximately 50% of women and 25% of men over age 50 are destined to suffer an osteoporotic fracture during their remaining lifetime. Unfortunately, the current standard for the diagnosis of osteoporosis, DXA, does not predict most fractures. This research will develop advanced analyses of DXA images and use machine learning to improve individualized fracture risk assessment.",Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA,9068774,K99AR067883,"['3-Dimensional', 'Age', 'American', 'Body mass index', 'Bone Density', 'Cadaver', 'Characteristics', 'Clinical', 'Complex', 'Data', 'Data Set', 'Deterioration', 'Diagnosis', 'Diagnostic', 'Disease', 'Elements', 'Epidemiology', 'Event', 'Femur', 'Finite Element Analysis', 'Fracture', 'Future', 'Geometry', 'Goals', 'Gold', 'Health', 'Height', 'Hip Fractures', 'Image', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Morphology', 'Osteopenia', 'Osteoporosis', 'Osteoporotic', 'Outcome', 'Patients', 'Peripheral', 'Postmenopause', 'Prospective Studies', 'Race', 'Radiation', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Resistance', 'Risk', 'Risk Assessment', 'Risk Factors', 'Scanning', 'Shapes', 'Specimen', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Weight', 'Woman', 'X-Ray Computed Tomography', 'base', 'bone', 'bone geometry', 'bone mass', 'bone strength', 'cohort', 'demographics', 'density', 'diagnosis standard', 'fall risk', 'high risk', 'image processing', 'improved', 'in vivo', 'information model', 'learning strategy', 'men', 'novel', 'osteoporosis with pathological fracture', 'prospective', 'three dimensional structure', 'tool', 'two-dimensional']",NIAMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,K99,2016,91800,-0.029176884196257427
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,9015770,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'genomic data', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2016,71329,-0.03241828459893793
"Building Multistage Treatment Regimens for Depression after Acute Coronary SyndromeSyndrome Building Multistage Treatment Policy for Depression after Acute Coronary Syndrome Project Summary Depression is not only commonly observed among patients who experienced acute coronary syndrome (ACS), but also has been shown to increase risks for recurrent ACS and mortality. Despite its high prevalence and serious impact, management of post-ACS depression remains poor because of ineciencies in depression screen- ing, limited treatment options of depression after ACS, and lack of e ective procedure if initial treatment fails. To address these issues, clinical researchers have tried to develop personalized stepped care procedures for post-ACS depression patients; this involves o ering patients the choice of receiving psychotherapy and/or antidepressant treatment and adjusting treatment as needed. The treatment decisions are usually based on patient demographics, treatment preference, medical history, progress of disease, and comorbid conditions. With the development of modern technologies, the number of available treatments increases, and more pa- tient information are collected in clinical research. Thus excavating useful information for treatment decisions is becoming more challenging. In this project, we propose to develop a principled way to construct simple interpretable multistage treatment policies from high-dimensional data, that can be used to guide treatment selection throughout the course of the disease. Aim 1 of the project is devoted to the development of vari- able selection methodology for constructing multistage treatment policies using statistical machine learning techniques. The proposed research seeks to incorporate the popular variable selection technique (LASSO) into existing treatment policy search approaches, namely Q-learning and A-Learning, for developing optimal treatment policies and for identifying patient response status to initial treatment { an important factor for tailoring treatment in the subsequent stages. Aim 2 evaluates the proposed methods, applies the methods to post-ACS depression data, and addresses some computational challenges. Statistical research in this area has been focused on the development of evidence-based treatment policies using pre-chosen models and variables; few if any discuss how to select models or variables in a principled way. The proposed work aims to ll this gap in methodology using modern machine learning techniques. Project Narratives The proposed research aims to answer the following question:\How to excavate simple interpretable multistage treatment policies from high-dimensional, longitudinal medical data in a principled way?"" This is a crucial step in the management of chronic disease, such as depression, for which a large number of variables are collected over time, by facilitating the construction of a parsimonious clinical decision system.",Building Multistage Treatment Regimens for Depression after Acute Coronary SyndromeSyndrome,9180305,R21MH108999,"['Acute', 'Address', 'Adopted', 'Antidepressive Agents', 'Area', 'Caring', 'Characteristics', 'Chronic', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Comorbidity', 'Coronary', 'Data', 'Decision Making', 'Depression screen', 'Development', 'Disease', 'Disease remission', 'Evaluation Studies', 'Evidence based treatment', 'Goals', 'Guidelines', 'Heterogeneity', 'High Prevalence', 'Intervention', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Medical History', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Physicians', 'Policies', 'Procedures', 'Process', 'Property', 'Psychotherapy', 'Recurrence', 'Relapse', 'Research', 'Research Personnel', 'Risk', 'Selection for Treatments', 'Staging', 'System', 'Techniques', 'Technology', 'Time', 'Treatment Protocols', 'Weight', 'Work', 'acute coronary syndrome', 'base', 'clinical practice', 'cost', 'demographics', 'evidence base', 'experience', 'individualized medicine', 'mortality', 'preference', 'prevent', 'psychosocial', 'response', 'simulation', 'treatment response']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2016,200000,-0.01350736340565048
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates ﻿    DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9120824,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Joints', 'Knowledge', 'Leisure Activities', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,504687,-0.07157800972935863
"A Machine-Learning Based Software Widget for Resolving Metabolite Identities Owing to recent technological advances in measurement platforms, it is now possible to simultaneously detect and characterize a very large number of metabolites covering a substantial fraction of the small molecules present in a biological sample. This presents an exciting opportunity to develop potentially transformative approaches to study cells and organisms. One major challenge in realizing this potential lies in processing and analyzing the data. A typical dataset from an untargeted experiment contains many of thousands of “features,” each of which could correspond to a unique metabolite. Analyzing such datasets to obtain meaningful biological information depends on reliably and efficiently resolving the chemical identities of the detected features. Currently, in silico fragmentation methods predict candidate metabolites that are scored and ranked based on how well the fragmentation explains the observed MS/MS spectrum, and on other factors influencing fragmentation such as bond dissociation energies and ionization conditions. Deciding which candidate metabolites is the best match for a particular feature in the context of the biological sample, however, is a daunting task. Extensive testing of candidate metabolites against chemical standards library may be prohibitive in terms of cost and efforts. We seek to develop software-enabled workflows centered on resolving metabolite identities. Our approach is to exploit knowledge of the biological context of a sample to identify the metabolites. Recognizing that the metabolites present in a sample result from enzyme-catalyzed biochemical reactions active in the corresponding biological system, we employ topological analysis and inference to best map the metabolites implied by the detected features to metabolic pathways that are feasible based on the genome(s) of cells in the biological system. Aim 1 develops a computational method based on Bayesian-inference to enhance candidate metabolite rankings that are obtained via in silico fragmentation analysis. Our method utilizes all available information (database lookups, in silico fragmentation analysis, and network/pathway context) to maximally inform and adjust the rankings. Aim 2 will build software widgets to implement the metabolite identification workflow within a data-analytics framework. As the analytics framework, we will use Orange, which allows the user to create interactive data analysis pipelines through a plug-and-play graphical user interface (GUI). Aim 3 will validate the computational method and software widget implementation. Experimental validation will utilize high-purity standards to confirm (or reject) the computationally assigned metabolite identities. Widget implementation will be evaluated through a focus group discussion with the widget users in the labs directed by the PIs. As project outcomes, we anticipate both a methodological advance in analyzing mass signature data as well as a suite of easily accessible software in the form of widgets. Metabolomics is concerned with the comprehensive characterization of the small molecule metabolites in biological systems. Owing to recent technological advances in measurement platforms, it is now possible to simultaneously detect and characterize a very large number of metabolites. Prospectively, advanced computational tools and software for metabolomics data analysis can aid discovery efforts aimed at identifying novel bioactive metabolites that could be developed into diagnostic indicators or therapeutic agents. ",A Machine-Learning Based Software Widget for Resolving Metabolite Identities,9223450,R03CA211839,"['Address', 'Algorithms', 'Attention', 'Automatic Data Processing', 'Bayesian Analysis', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Cells', 'Chemicals', 'Classification', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Databases', 'Diagnostic', 'Dissociation', 'Environment', 'Enzymes', 'Feedback', 'Focus Groups', 'Genes', 'Genome', 'Goals', 'Human', 'Knowledge', 'Libraries', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Nuclear Magnetic Resonance', 'Oranges', 'Organism', 'Outcome', 'Pathway Analysis', 'Pathway interactions', 'Pattern', 'Play', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Surveys', 'Testing', 'Therapeutic Agents', 'Time', 'Uncertainty', 'Validation', 'Visual', 'base', 'biological systems', 'chemical standard', 'computerized tools', 'cost', 'database query', 'flexibility', 'functional outcomes', 'graphical user interface', 'heuristics', 'inhibitor/antagonist', 'instrument', 'ionization', 'mass spectrometer', 'member', 'metabolomics', 'novel', 'programs', 'protein expression', 'research study', 'small molecule', 'software development']",NCI,TUFTS UNIVERSITY MEDFORD,R03,2016,147569,-0.055607812503752546
"IGF::OT::IGF ARTIFICIAL INTELLIGENCE IN MEDICINE INC: HHSN261201500033I- TASK ORDER 2; POP 9/26/2016-9/25/2017 The purpose of this acquisition is to 1) maintain and update the existing Surveillance Epidemiology and End Results (SEER) ePath network; 2) expand the SEER network to additional pathology laboratories; 3) expand electronic data capture to include electronic reports from diagnostic imaging, and 4) install cancer data forwarding module in previously installed ePath laboratories n/a",IGF::OT::IGF ARTIFICIAL INTELLIGENCE IN MEDICINE INC: HHSN261201500033I- TASK ORDER 2; POP 9/26/2016-9/25/2017,9361217,61201500033I,"['Artificial Intelligence', 'Data', 'Diagnostic Imaging', 'Electronics', 'Laboratories', 'Malignant Neoplasms', 'Medical Surveillance', 'Medicine', 'Pathology', 'Reporting', 'Update', 'electronic data']",NCI,"ARTIFICIAL INTELLIGENCE IN MEDICINE, INC",N03,2016,1999910,-0.02801503093339977
"Computational modeling of semantic decline in Alzheimer's disease Project Summary To interact, communicate, and navigate the world successfully, people must retrieve relevant information from their semantic memory (memory for facts and general knowledge). Individuals with Alzheimer's disease have difficulty retrieving such knowledge from early in the course of the disease and progressively gets worse as the disease spreads, a process known as semantic decline. This project examines the mechanisms underlying semantic decline in individuals with Alzheimer's disease by developing and applying novel computational tools. The extent to which semantic memory is impaired in individuals with Alzheimer's disease can be probed using behavioral experiments. Individuals with Alzheimer's as well as those at-risk for the disease display a pattern of behavior on these tasks distinct from healthy individuals. Despite decades of research, explanations of these behavioral impairments focus almost exclusively on cognitive mechanisms that may explain a patient's current behavior at a given time point, but without an account of the transition from normal, pre-symptomatic behavior to fully impaired behavior. Existing models fail to explain the mechanisms by which semantic memory and memory retrieval processes degrade over time due to Alzheimer's, limiting our understanding of the development of the disease, as well as hindering our ability for prognosis, early detection measures, and possible interventions. This project will test computational models of how the disease spreads, making specific quantitative predictions about the decline of semantic memory. Additionally, we will develop a novel machine learning method that can be used to map the structure of an individual's semantic memory, creating opportunities for individualized behavioral interventions to improve semantic memory and improve the quality of life for individuals with Alzheimer's disease. Project Narrative This project examines the mechanisms underlying decline of semantic memory (memory for knowledge) in individuals with Alzheimer's disease by developing and applying novel computational models. This research will develop methods for testing neurocognitive theories of Alzheimer's and other neurodegenerative diseases, inform interventions for mitigating decline, and further our understanding of how the disease spreads.",Computational modeling of semantic decline in Alzheimer's disease,9164777,R21AG053467,"['Accounting', 'Affect', 'Alzheimer&apos', 's Disease', 'American', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Categories', 'Clinical', 'Cognitive', 'Computer Simulation', 'Data', 'Data Set', 'Dementia', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Emotional', 'Family', 'Free Association', 'Huntington Disease', 'Impairment', 'Individual', 'Intervention', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Maps', 'Measures', 'Memory', 'Methods', 'Modeling', 'Neurodegenerative Disorders', 'Neurologic', 'Patients', 'Pattern', 'Performance', 'Phenotype', 'Population', 'Process', 'Quality of life', 'Research', 'Retrieval', 'Semantic memory', 'Semantics', 'Staging', 'Structure', 'Techniques', 'Testing', 'Time', 'Walking', 'Work', 'behavioral impairment', 'computerized tools', 'contagion', 'disorder risk', 'improved', 'learning strategy', 'memory retrieval', 'neurocognitive test', 'novel', 'outcome forecast', 'pre-clinical', 'research study', 'theories', 'tool', 'trait', 'transmission process']",NIA,UNIVERSITY OF WISCONSIN-MADISON,R21,2016,206644,-0.07659552268924417
"Identifying Huntington's disease markers by modern statistical learning methods. DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD. PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.",Identifying Huntington's disease markers by modern statistical learning methods.,9119862,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Health', 'Huntington Disease', 'Huntington gene', 'Image', 'Individual', 'Intervention', 'Investigational Therapies', 'Knowledge', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'cognitive testing', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'imaging biomarker', 'improved', 'indexing', 'interest', 'learning strategy', 'meetings', 'nervous system disorder', 'novel', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2016,356244,-0.015531066100047139
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis.         PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.            ",Developing Classification Criteria for the Uveitides,9081760,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Population', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'Translational Research', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'Work', 'age group', 'age related', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428590,-0.0043274276226662476
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9099858,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2016,306754,-0.03532517398271782
"Predictive Model of Chronic Kidney Disease in a Hispanic Population DESCRIPTION (provided by applicant): This career transition award will provide the applicant with protected research time to develop an independent research career in biomedical informatics. Dr. Starkey received his M.D. in 2003 from UT Southwestern, completed the NLM Biomedical Informatics Training Program post-doctoral fellowship in 2011 and earned a Ph.D. focusing on clinical informatics from the Clinical Sciences Degree Program at the University of Texas Medical Branch (UTMB) at Galveston in 2012. Dr. Starkey accepted a faculty position at the Institute for Translational Sciences at UTMB that is currently supporting his transition to independent investigator. Dr. Starkey will utilize the career transition award for novel applications of advanced machine learning methods to create predictive models of disease. Predictive models of disease allow for the stratification of risk and prevention of disease and have been successfully implemented in the assessment of cardiovascular disease risk. Predictive models of chronic kidney disease (CKD) are an active area of research since CKD is a risk factor for all-cause mortality, cardiovascular death and end-stage renal disease. However, there is not a single predictive model of CKD created for application to Hispanics and the external validity of existing models is poor. The objective of this project is to create predictive models of chronic kidney disease in a Hispanic population and quantify biomarkers of chronic kidney disease using multiple reaction monitoring (MRM) proteomics in this minority population. MRM proteomics will utilize the UTMB Novel Methodologies core and its array of resources to create an inter-institutional collaboration. Heterogeneous ensemble machine learning methods will be used to create the predictive model of chronic kidney disease in the Cameron County Hispanic Cohort (CCHC). Established in 2004, the CCHC is a random population sample in Brownsville, Texas created to evaluate the determinants of health in a US/Mexico border population that is primarily of Hispanic ethnicity. The CCHC has an extremely high prevalence of obesity and diabetes at 49.7% and 30.3%, respectively. Both obesity and diabetes are independent risk factors for the development of CKD and represent significant health disparities in the Hispanic population. Archived serum samples of CCHC participants and the CCHC database will be utilized to complete the following aims: 1) Create predictive models of CKD in a Hispanic population 2) Refine the predictive models by including clinical laboratory data and a selective reaction monitoring mass spectrometry panel of biomarkers. At the completion of this project, predictive models of CKD applicable to a Hispanic population will be created and MRM proteomics will demonstrate utility of biomarkers identified in other populations. The application of the results will be used to create clinical tools for CKD risk and guide future studies in this Hispanic population with health disparities. Importantly, it will provide a career transition for D. Starkey to demonstrate the knowledge gained during his training that results in impactful research and publications. PROJECT NARRATIVE:  The proposed research is relevant to public health because it allows stratification of chronic kidney disease risk and quantifies biomarkers of chronic kidney disease in the Hispanic minority whom has significant health disparities with regard to kidney disease. The translation of selective reaction monitoring of biomarkers and development of heterogeneous ensemble machine learning methods for application to the prediction of chronic kidney disease in a minority population supports the NLM mission.",Predictive Model of Chronic Kidney Disease in a Hispanic Population,9121614,K22LM011869,"['Age', 'Albumins', 'Archives', 'Area', 'Atherosclerosis', 'Biological Markers', 'Cardiovascular system', 'Career Mobility', 'Career Transition Award', 'Cessation of life', 'Chronic Kidney Failure', 'Clinical', 'Clinical Informatics', 'Clinical Sciences', 'Collaborations', 'Communities', 'County', 'Creatinine', 'Data', 'Data Set', 'Databases', 'Degree program', 'Development', 'Diabetes Mellitus', 'Dialysis procedure', 'Disease', 'Disease Outcome', 'Doctor of Medicine', 'Doctor of Philosophy', 'End stage renal failure', 'Ethnic Origin', 'Faculty', 'Fellowship', 'Funding', 'Future', 'General Population', 'Goals', 'Health', 'High Prevalence', 'Hispanics', 'Hospitalization', 'Household', 'Incidence', 'Individual', 'Institutes', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Mass Spectrum Analysis', 'Medical', 'Methodology', 'Methods', 'Mexican', 'Mexico', 'Minority', 'Mission', 'Modeling', 'Monitor', 'Obesity', 'Participant', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Prevalence', 'Probability', 'Process', 'Proteomics', 'Public Health', 'Publications', 'Reaction', 'Recruitment Activity', 'Renal function', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Sampling', 'Serum', 'Specimen', 'Stratification', 'Texas', 'Time', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transplantation', 'Universities', 'Validation', 'Visit', 'Weight', 'Work', 'biomarker development', 'biomarker panel', 'biomedical informatics', 'cardiovascular disorder risk', 'career', 'cohort', 'cost effective', 'design', 'disorder prevention', 'disorder risk', 'follow-up', 'forest', 'health disparity', 'improved', 'informatics training', 'innovation', 'inter-institutional', 'learning strategy', 'medically underserved', 'mortality', 'multiple reaction monitoring', 'novel', 'novel marker', 'predictive modeling', 'programs', 'prospective', 'screening', 'skills', 'tool', 'urinary']",NLM,UNIVERSITY OF TEXAS MED BR GALVESTON,K22,2016,135270,-0.04306064279083741
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ﻿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices. PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,9109642,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Health', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'therapeutic biomarker', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2016,271743,-0.02528897326579329
"Advanced morphological analysis of cerebral blood flow for acute concussion diagnosis and return-to-play determination Project Summary / Abstract Between 1.6 and 3.8 million people each year suffer a mild TBI in the US alone. Reliable diagnosis and prompt treatments are vital to managing the often-serious short and long-term sequelae resulting from mild TBI. However, a reliable objective and accurate method for mild TBI diagnosis outside of a hospital setting, and in particular for determining RTP readiness, has eluded the clinical community. Current diagnosis and RTP assessments are based on patient symptoms, neurocognitive evaluations, and / or physical performance testing. Use of symptom scales are problematic for several reasons including subjectivity and reliability. Neurocognitive evaluations and physical tests (such as balance tests), although less subjective, require pre- injury baseline testing of subjects due to inherently large subject-to-subject variations in evaluation performances. Due to these reasons, current mild TBI diagnostic methods have limited applications and are not suitable for a significant majority of patients who suffer mild TBI. This project is aimed at developing an objective diagnosis of mild traumatic brain injury (mild TBI) based on physiologic changes in a patient after injury and providing a platform capable of RTP guidance. The method is based on quantification of well-known physiologic changes after a concussion, i.e. the impairment of autonomic function and altered cerebral blood flow (CBF) as measured with transcranial Doppler (TCD). The novelty of the proposed approach is the use of a recently-developed analytical machine learning framework for the analysis of the CBF velocity (CBFV) waveforms. In contrast to previous methods used before, the proposed approach utilizes the entire shape of the complex CBFV waveform, thus obtaining subtle changes in blood flow that are lost in other analysis methods. Additionally, comprehensive verification between our platform and MRI will be performed following injury resulting in the first scientific experiments of this kind. The ultimate goal of this Phase II SBIR is to commercialize an objective and accurate software algorithm for reliable diagnosis and management of sports concussions which does not currently exist. The outcome will be a software suite integrated into existing TCD and will be marketed to emergency departments, neurology clinics, and other healthcare providers involved in mild TBI diagnosis and RTP management. Project Narrative Traumatic brain injury (TBI) is a serious public health problem in the United States contributing to a substantial number of deaths and cases of permanent disability. Mild TBI concussions account for over 80% of all TBIs sustained and a major problem is the high rate of mis-diagnosis due to lack of objective measures and delayed onset of symptoms. This project aims to develop the first objective concussion evaluation method using a novel analysis platform that can obtain subtle, physiologic changes in cerebral hemodynamics. Successful completion of this project will result in a portable diagnostic device suitable for use in many scenarios where concussion diagnosis is inaccurate or unavailable today.",Advanced morphological analysis of cerebral blood flow for acute concussion diagnosis and return-to-play determination,9202982,R44NS092209,"['Accident and Emergency department', 'Accounting', 'Acute', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area Under Curve', 'Assessment tool', 'Blood Flow Velocity', 'Blood flow', 'Brain Concussion', 'Cerebrovascular Circulation', 'Cessation of life', 'Classification', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Controlled Study', 'Core-Binding Factor', 'Data', 'Data Analytics', 'Data Collection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Evaluation', 'Functional disorder', 'Future', 'Goals', 'Gold', 'Guidelines', 'Health Personnel', 'Hospitals', 'Image', 'Impairment', 'Injury', 'Letters', 'Licensing', 'Machine Learning', 'Magnetic Resonance Imaging', 'Marketing', 'Measures', 'Methods', 'Modeling', 'Neurocognitive', 'Neurologist', 'Neurology', 'Outcome', 'Patients', 'Pediatric Neurology', 'Performance', 'Persons', 'Phase', 'Physical Performance', 'Physicians', 'Physiological', 'Play', 'Public Health', 'Publications', 'Readiness', 'Recovery', 'Research', 'Resolution', 'Risk', 'Severities', 'Shapes', 'Site', 'Small Business Innovation Research Grant', 'Spin Labels', 'Sports', 'Sports Medicine', 'Symptoms', 'Syndrome', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Traumatic Brain Injury', 'Ultrasonography', 'United States', 'Variant', 'abstracting', 'balance testing', 'base', 'brain health', 'cerebral hemodynamics', 'clinical Diagnosis', 'diagnostic accuracy', 'disability', 'hemodynamics', 'high school', 'injured', 'innovation', 'mild traumatic brain injury', 'novel', 'performance tests', 'prevent', 'programs', 'relating to nervous system', 'research study', 'success', 'tool']",NINDS,"NEURAL ANALYTICS, INC.",R44,2016,1500000,-0.009207297759885755
"Low Cost Mobile Platform for Pulmonary Disease Screening ﻿    DESCRIPTION (provided by applicant): Pulmonary disease (including primarily asthma, Chronic Obstructive Pulmonary Disease (COPD), pneumonia, lung cancer, and tuberculosis) is an increasingly large portion of the global health burden. Developing countries with large low-income populations are disproportionately affected, due to increased risk factors (e.g. biomass cooking stoves) as well as poor access to health care and lack of affordable screening tools for early detection. Chronic Obstructive Pulmonary Disease (COPD) alone is currently the third leading cause of death in the world and second leading cause of death in India after ischemic heart disease. In the younger population, pneumonia is a particular concern, being the leading cause of death for children under 5 years of age. Tuberculosis (TB) has also reached alarming proportions in India (24% of all cases worldwide). Despite this great prevalence of pulmonary disease in India, access to modern diagnostics instruments is not possible; furthermore, approximately 60% of general practice (GP) clinic doctors in India are primarily trained in Ayurvedic medicine with little or no training for diagnosing respiratory disease. As a result, many of the patients with lung disease are underdiagnosed or misdiagnosed (often confused with cardiovascular disease). As a result, there is a great need to provide health workers in India with simple tools that can be used to diagnose or screen for respiratory disease in the primary care setting. Addressing this need, our team has been developing a mobile diagnostic platform consisting of a digital stethoscope, peak flow meter, and mobile phone that can be used to screen for symptoms of lung disease and provide a guide for diagnosis. The present study extends this work and has the following aims: (1) To validate and test a low-cost mobile diagnostic platform for the purpose of identifying symptoms of lung disease and providing diagnostic guidance. (2) To assess the acceptance and usability of the mobile diagnostic platform by the local general practitioner (GP) doctors in rural India. We propose to deploy and test a low-cost mobile diagnostic platform, making use of machine learning algorithms that will detect specific symptoms of lung disease and help guide diagnosis. In the first year of the project, we shall create and train the mobile software algorithm using data collected in the field from patients (N=250) that have been previously diagnosed with specific lung diseases. Then year 2, we shall evaluate and test our mobile platform with Indian patients (N=250) recruited from four GP clinic sites in the Pune, India region. The automated mobile phone diagnosis result shall then be compared with the diagnosis from trained pulmonologists, using a traditional stethoscope as well as standard lung function testing instruments. A preliminary diagnosis and qualitative feedback shall also be collected from non-trained GP doctors using the mobile tools in order to ascertain usability and diagnostic value.         PUBLIC HEALTH RELEVANCE: Pulmonary disease is a very large public health concern in India, as well as in most developing countries. Relatively few general practitioner (GP) doctors are trained to properly diagnose pulmonary disease, and affordable tools for diagnostic support simply do not exist. This proposal seeks to validate and test the use of low-cost mobile phone-based diagnostic tools for pulmonary disease.            ",Low Cost Mobile Platform for Pulmonary Disease Screening,9018346,R21TW010245,"['5 year old', 'Address', 'Adult', 'Affect', 'Age', 'Air Pollution', 'Algorithmic Software', 'Algorithms', 'Area', 'Asthma', 'Ayurvedic Medicine', 'Bacteria', 'Behavioral Medicine', 'Biomass', 'Bronchitis', 'Car Phone', 'Cardiovascular Diseases', 'Categories', 'Cause of Death', 'Child', 'Chronic', 'Chronic Obstructive Airway Disease', 'Chronic lung disease', 'Clinic', 'Clinical', 'Communicable Diseases', 'Communities', 'Cookstove', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Feedback', 'General Practices', 'General Practitioners', 'Health', 'Household Air Pollution', 'India', 'Interstitial Lung Diseases', 'Intervention', 'Life', 'Low Income Population', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant neoplasm of lung', 'Medical', 'Myocardial Ischemia', 'Patients', 'Pharmaceutical Preparations', 'Pneumonia', 'Population', 'Prevalence', 'Public Health', 'Pulmonary Emphysema', 'Pulmonary function tests', 'Quality of life', 'Recruitment Activity', 'Research', 'Risk Factors', 'Rural', 'Sampling', 'Site', 'Staging', 'Stethoscopes', 'Symptoms', 'Testing', 'Training', 'Tuberculosis', 'Work', 'accurate diagnosis', 'base', 'cigarette smoking', 'cost', 'digital', 'global health', 'health care availability', 'improved', 'inclusion criteria', 'instrument', 'mHealth', 'meter', 'mobile computing', 'primary care setting', 'public health relevance', 'respiratory', 'screening', 'tool', 'usability']",FIC,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2016,181365,-0.08973489569226449
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9057057,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Health', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'research study', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2016,445349,-0.03083753134581628
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9099895,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2016,418572,-0.012266676431651328
"Repurposing pyronaridine as a treatment for the Ebola virus Summary In 2014, the outbreak of the Ebola virus (EBOV) in West Africa highlighted the need for broad-spectrum antiviral drugs for this and other emerging viruses. Several groups had previously performed high throughput screens in 2013 and identified FDA approved drugs (amodiaquine, chloroquine, clomiphene and toremifene) with in vitro growth inhibitory activities against EBOV. We used these compounds to create a computational pharmacophore to identify additional compounds to test in vitro. In addition, data from a published large scale high throughput screen performed by SRI International and Texas Biomedical Research Institute was used to create machine learning models and then subsequently used to score clinical compounds for testing. We have published on how these combined methods identified 3 compounds for testing which were ultimately found to be nM in vitro. One of these compounds is an antimalarial approved in Europe called pyronaridine. We propose to characterize the ADME and PK properties of this compound prior to determining its efficacy in a mouse model of the Ebola virus infection. Therefore the Aims of this R21 proposal will fill some of the gaps inherent in the published data on pyronaridine so far: Aim 1. Perform preclinical in vitro characterization of pyronaridine. Aim 2. Formulate pyronaridine and perform PK studies in mouse. Aim 3. In vitro characterization of pyronaridine against multiple EBOV strains and in vivo efficacy in the mouse model of Ebola virus infection. The results of these aims will determine go/no go criteria for pursuing larger animal studies in non-human primates prior to clinical studies. In the light of a recent paper in the New England Journal of Medicine showing a clinical observation that EBOV patients treated with artesunate-amodiaquine had a 31% higher survival rate than those treated with artemether- lumefantrine 2, there will be considerable interest in evaluating antimalarials against Ebola. Our proposal to consider testing the efficacy in the mouse EBOV model using pyronaridine (which is used as artesunate- pyronaridine (Pyramax) and would be readily accessible in the clinic), presents a rapid approach to leverage the aforementioned clinical observations with a more potent compound. Pyronaridine also has additional benefits of tolerability which may be important in this patient population. Narrative Preliminary clinical data showed that Ebola virus (EBOV) patients treated with the antimalarials artesunate- amodiaquine had a higher survival rate than those treated with artemether-lumefantrine, in agreement with the in vitro EC50 for amodiaquine EC50 of 2.6µM. The antimalarial pyronaridine, a structural analog of amodiaquine, was identified by a computational repurposing strategy and further shown to have an EC50 of 420 nM against EBOV in vitro. We now propose to fully characterize this compound using standard preclinical ADME assays prior to mouse pharmacokinetic analysis, determine broad-spectrum applicability against multiple EBOV strains and ultimately in vivo efficacy testing in the mouse Ebola virus model prior to testing in a non-human primate model. Our aim is to show whether Pyronaridine is a viable clinical candidate to treat patients infected with EBOV.",Repurposing pyronaridine as a treatment for the Ebola virus,9204673,R21TR001718,"['Africa', 'Agreement', 'Amodiaquine', 'Animal Model', 'Animals', 'Antimalarials', 'Antiviral Agents', 'Area', 'Babesia', 'Behavioral', 'Binding Proteins', 'Biological Assay', 'Biological Availability', 'Biomedical Research', 'Blood specimen', 'Body Weight decreased', 'Bolus Infusion', 'Cessation of life', 'China', 'Chloroquine', 'Chloroquine resistance', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clomiphene', 'Control Groups', 'Data', 'Data Set', 'Disease', 'Disease Outbreaks', 'Dose', 'Drug Kinetics', 'Ebola virus', 'Ensure', 'Enzymes', 'Erythrocytes', 'Europe', 'European', 'FDA approved', 'Family', 'Female', 'Filovirus', 'Formulation', 'Growth', 'Half-Life', 'Hour', 'In Vitro', 'Infection', 'International', 'Intestines', 'Journals', 'Knowledge', 'Lethal Dose 50', 'Libraries', 'Liver', 'Machine Learning', 'Malaria', 'Mannich Bases', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Mus', 'Natural Product Drug', 'New England', 'Oral', 'Paper', 'Patients', 'Permeability', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Plasma', 'Plasma Proteins', 'Plasmodium falciparum', 'Plasmodium vivax', 'Property', 'PubChem', 'Publishing', 'Research Institute', 'Route', 'Solubility', 'Staging', 'Survival Rate', 'Techniques', 'Testing', 'Texas', 'Time', 'Toremifene', 'Toxic effect', 'Trypanosoma cruzi', 'Virus', 'Virus Diseases', 'Virus Inhibitors', 'Vivax Malaria', 'Whole Blood', 'analog', 'artemether', 'artesunate', 'base', 'benflumetol', 'bioactive natural products', 'design', 'efficacy testing', 'high throughput screening', 'in vitro testing', 'in vivo', 'inhibitor/antagonist', 'interest', 'male', 'mouse model', 'neurotoxicity', 'nonhuman primate', 'novel therapeutics', 'patient population', 'pharmacophore', 'pre-clinical', 'preclinical study', 'prevent', 'pyronaridine', 'research clinical testing', 'response', 'treatment duration']",NCATS,"COLLABORATIONS PHARMACEUTICALS, INC.",R21,2016,288801,-0.0060230504643804805
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,-0.012084691517933972
"Optimization of small molecule triazine antituberculars for in vivo efficacy ﻿    DESCRIPTION (provided by applicant): Tuberculosis (TB) is due to infection with the pathogen Mycobacterium tuberculosis (Mtb). This disease represents a global health pandemic as based on WHO statistics it claims the lives of approximately 1.5 million people per year, while infecting nearly 9 million. New drugs are urgently needed with novel mechanisms of action that treat this disease while also addressing an important need to reduce the lengthy course of treatment that is at best 6 months in duration. We have a primary goal of discovering novel classes of antibacterials. (E)-6-(2-((5-nitrofuran-2-yl)methylene)hydrazinyl)-N2,N4-diphenyl-1,3,5-triazine-2,4-diamine (JSF-2019) [1], was rediscovered by us using Bayesian machine learning models in 2013. It represents a class of antitubercular agents reported only once in 1969 [2]. While JSF-2019 did not exhibit in vivo efficacy in an acute model in our hands, a close analog, ((E)-N2,N4-diisopropyl-6-(2-((5-nitrofuran-2-yl)methylene)hydrazinyl)- 1,3,5-triazine-2,4-diamine (JSF-2032) [2] was reported in 1969 to exhibit in vivo activity. Our preliminary data on solubility in PBS, mouse liver microsomal stability, Caco-2 cell permeability, and mouse snapshot pharmacokinetic (PK) profiles demonstrate that the diaminotriazine class of antituberculars holds significant promise for seeding a novel therapeutic. We aim to further improve upon the in vitro efficacy, in vitro Absorption, Distribution, Metabolism and Excretion (ADME) and in vivo pharmacokinetic (PK) profiles of these early compounds. The Specific Aims of this proposed research are: Utilize medicinal chemistry and predictive ADME models to optimize the initial triazine hit family as antitubercular agents. Apply transcriptional profiling nd resistant mutant/whole-genome sequencing methods to identify potential drug targets and mechanism of action of the triazine antitubercular class. Phase I would, therefore, seek to deliver an in vivo active small molecule triazine with information as to potential target/s through complimentary methods. A Phase II program would leverage this information to further optimize this series towards a preclinical candidate of significant interest to foundations and/or biotech/pharmaceutical companies. PUBLIC HEALTH RELEVANCE: Tuberculosis represents a global health pandemic, which claims the lives of approximately 1.5 million people per year. Starting from a series of triazines active compounds we will optimize ADME properties to afford a novel in vivo active lead compound by combining our unique expertise in medicinal chemistry and predictive ADME models. We will also probe the potential target/s of the triazine series. We will further leverage the exceptional facilities at Rutgers to ultimately lead to a clinical candidate by the end of phas II.",Optimization of small molecule triazine antituberculars for in vivo efficacy,9045992,R41AI122434,"['AIDS/HIV problem', 'Acute', 'Address', 'Affect', 'Anti-Bacterial Agents', 'Antitubercular Agents', 'Area', 'Biological Assay', 'Biotechnology', 'Caco-2 Cells', 'Cause of Death', 'Clinical', 'Clinical Trials', 'Collaborations', 'Colony-forming units', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Databases', 'Diagnosis', 'Diamines', 'Disease', 'Disease Reservoirs', 'Drug Kinetics', 'Drug Targeting', 'Drug resistance', 'Drug resistance in tuberculosis', 'Excretory function', 'Exhibits', 'Extreme drug resistant tuberculosis', 'Family', 'Foundations', 'Frequencies', 'Genome', 'Goals', 'HIV', 'Health', 'In Vitro', 'Infection', 'Lead', 'Licensing', 'Light', 'Liver', 'Lung', 'Machine Learning', 'Medicine', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Multi-Drug Resistance', 'Multidrug-Resistant Tuberculosis', 'Mus', 'Mutation', 'Mycobacterium tuberculosis', 'Nitrofurans', 'Paper', 'Patients', 'Permeability', 'Pharmaceutical Chemistry', 'Pharmacologic Substance', 'Phase', 'Phosphate Buffer', 'Population', 'Predisposition', 'Property', 'Reporting', 'Research', 'Resistance', 'Resistance profile', 'Saline', 'Series', 'Solubility', 'South Africa', 'Structure-Activity Relationship', 'Techniques', 'Time', 'Triazines', 'Tuberculosis', 'Validation', 'absorption', 'analog', 'aqueous', 'base', 'carbene', 'cytotoxicity', 'diphenyl', 'drug candidate', 'drug mechanism', 'experience', 'extensive drug resistance', 'genome sequencing', 'global health', 'improved', 'in vitro activity', 'in vivo', 'interest', 'killings', 'mouse model', 'mutant', 'novel', 'novel therapeutics', 'pandemic disease', 'pathogen', 'pre-clinical', 'process optimization', 'programs', 'small molecule', 'statistics', 'tuberculosis drugs', 'whole genome']",NIAID,"COLLABORATIONS PHARMACEUTICALS, INC.",R41,2016,149388,-0.02580726665559572
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases. PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,9123353,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'skills', 'social media', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2016,107469,-0.02467302379297667
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9041640,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'genomic data', 'hazard', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,255295,-0.019286249884763913
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,-0.0035388565779969163
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,9081248,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2016,404000,-0.01736577666274286
"In vivo Characterization of Stents using Intravascular OCT Imaging DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel. PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,9097737,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Health', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shapes', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Surrogate Markers', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'preclinical trial', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'stent thrombosis', 'tool', 'treatment choice']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2016,447440,-0.015109290650945049
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9106828,R01AR068456,"['Accounting', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cost', 'density', 'experience', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2016,31678,-0.006452803120542414
"Aptamer-Based Arrays for Detection of Pathogenic IgA1 O-Glycoforms in IgA Nephropathy ﻿    DESCRIPTION (provided by applicant): The glycosylation patterns on IgA1 antibodies are highly complex and heterogeneous. When there are dysregulations in activities of glycosylation enzymes, the hinge domain of IgA1's, i.e., the peptide domain connecting constant and variable regions, can undergo shifts in glycosylation patterns and become galactose- deficient (Gd). The galactose deficiency, together with other triggers, can lead to an auto-immune response in which patients' own antibodies form complexes with Gd-IgA1's; these complexes precipitate and cause damage in glomeruli, eventually leading to IgA1 nephropathy (IgAN).  It is of a great interest to monitor regularly Gd-IgA1's of patients who are predisposed to develop nephropathy. Further, increased understanding of correlations between dynamic variations in glycosylation patterns and the natural development of the disease in individual patients is expected to lead to improved interventions, including individually optimized therapies that could block the formation of offending complexes. None of the current approaches to assess microheterogeneity in glycosylation patterns is completely satisfactory; while elegant, these are also arduous and indirect, limited to highly specialized laboratories and difficult to reproduce in actual patients' samples, in large part due to lack of precise, well-characterized, molecular-level analytical tools  We propose to address this issue by systematic isolation of oligonucleotide-based molecular receptors or aptamers that will interact with clusters of different O-glycosides displayed in the hinge subregions. Aptameric receptors will be isolated from large oligonucleotide pools through the process of an in vitro selection and amplification coupled to the affinity separation via interactions with IgA1 hinge regions isolated from both Gd- IgAN patients and healthy controls. Individual aptamers will interact with substructures within the hinge domain, that is, with shorter peptides displaying one or more oligosaccharides. A large number of identified aptamers will be screened for their ability to interact with fractions of polyclonal IgA1's, and a variety of those aptamers that show a quantitatively different response to IgA1s from patients and matched controls will be selected for a more detailed characterization and incorporation in ""classification sensor arrays"" (CSAs).  As the result of our work, we will have immediately a set of aptamers that would together form a classification sensor array, an artificial ""nose"" capable of distinguishing samples belonging to patients with Gd- IgAN from healthy controls, as well as quantifying the extent of shifts in glycosylation patterns. Further molecular-level characterizatio of epitopes (subdomains) that these aptamers recognize is expected to enable studies towards identification of structures that are responsible for auto-immune responses in individual patients, and correlation with secondary triggers of diseases, which are the key step in the rational design of targeted inhibitors of the complex formation.         PUBLIC HEALTH RELEVANCE: Complex saccharide structures ('glycans') play crucial roles in a wide range of biological functions and diseases such as autoimmunity, cancer, and nephropathy. We will help elucidate the role of complex glycans in the pathogenesis of IgA nephropathy, one of the most common causes of kidney failure worldwide, by systematically and exhaustively generating oligonucleotide-based receptors (aptamers) for glycan clusters on IgA1 antibodies. This will allow new diagnostic protocols based on pattern recognition with classification arrays and, in the long-term, personalized therapies to prevent antigen-autoantibody interaction and nephropathy.            ",Aptamer-Based Arrays for Detection of Pathogenic IgA1 O-Glycoforms in IgA Nephropathy,9128230,R21DK109690,"['Address', 'Affinity', 'Alkaloids', 'Amino Acids', 'Antibodies', 'Antigens', 'Autoantibodies', 'Autoimmunity', 'Biological Process', 'Biology', 'Biopsy', 'Chemicals', 'Chemistry', 'Classification', 'Complex', 'Coupled', 'Deposition', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Dopamine', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epidemiology', 'Epitopes', 'Galactose', 'Glycosides', 'IgA1', 'Immune response', 'Immunoglobulin A', 'Immunoglobulin Constant Region', 'Immunoglobulin Variable Region', 'Immunology', 'In Vitro', 'Individual', 'Intervention', 'Kidney', 'Kidney Diseases', 'Kidney Failure', 'Laboratories', 'Lead', 'Lectin', 'Machine Learning', 'Malignant Neoplasms', 'Medical Genetics', 'Methodology', 'Methods', 'Molecular', 'Monitor', 'Nephrology', 'Neurotransmitters', 'Nose', 'Nucleic Acids', 'Olfactory Pathways', 'Oligonucleotides', 'Oligosaccharides', 'Pathogenesis', 'Patients', 'Pattern', 'Pattern Recognition', 'Peptides', 'Play', 'Polysaccharides', 'Process', 'Proteins', 'Protocols documentation', 'Reagent', 'Receiver Operating Characteristics', 'Regulation', 'Resolution', 'Role', 'Sampling', 'Sensitivity and Specificity', 'Series', 'Serum', 'Specificity', 'Sphingosine', 'Steroids', 'Structure', 'Tertiary Protein Structure', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Variant', 'Work', 'analytical tool', 'aptamer', 'base', 'combinatorial', 'cost effective', 'design', 'disorder control', 'glycosylation', 'improved', 'individual patient', 'inhibitor/antagonist', 'inorganic phosphate', 'interest', 'multidisciplinary', 'noninvasive diagnosis', 'novel', 'novel diagnostics', 'patient stratification', 'personalized medicine', 'prevent', 'public health relevance', 'receptor', 'response', 'screening', 'sensor', 'sugar', 'tool']",NIDDK,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2016,251130,-0.020878004099322903
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,9097814,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Genetic', 'Genetic Structures', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Internships', 'K-12 student', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Staging', 'Statistical Data Interpretation', 'Statistical Models', 'Structure', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'fluorescence imaging', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'neuroinformatics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'quantitative imaging', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2016,323032,-0.01064382066116746
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9115248,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'study population', 'terabyte', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2016,347156,-0.02024329832363095
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9135552,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2016,324169,-0.01536988530083576
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,9052846,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Health', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Lateral', 'Lead', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'individual patient', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'response', 'screening', 'spinal tract', 'treatment response', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2016,1,-0.01672907458400965
"Development of integrative models for early liver toxicity assessment ﻿    DESCRIPTION (provided by applicant): Computational toxicology has become a critical area of research due to the burgeoning need to evaluate thousands of pharmaceutical and environmental chemicals with unknown toxicity profiles, the high demand in time and resources by current experimental toxicity testing, and the growing ethical concerns over animal use in toxicity studies. Despite tremendous efforts, little success has been attained thus far in the development of predictive computational models for toxicity, primarily due to the complexity of toxicity mechanisms as well as the lack of high-quality experimental data for model development.  A critical challenge in toxicity testing of chemicals is that toxicity effects are doe-dependent: the true toxic hits may show no toxicity at all at low dose level. Therefore, traditiona high-throughput screening (HTS) that test chemicals only at a single concentration is not suitable for toxicity screening. On the contrary, the recently developed quantitative high-throughput screening (qHTS) platforms can evaluate each chemical across a broad range of concentrations, and is gaining ever-increasing popularity as a tool for in vitro toxicity profiling The concentration-response information generated by qHTS are expected to provide more accurate and comprehensive information of the toxicity effects of chemicals, offering promising data that can be mined to estimate in vivo toxicities of chemicals. However, our previous studies showed that if processed inappropriately, such concentration-response information contribute little to improve the toxicity prediction. This is especially true when multiple types of qHTS data are used together. Therefore, in this study, we will extend our previous approaches to develop novel statistical and computational tools that can curate, preprocess, and normalize the concentration-response information from multiple different qHTS databases.  Traditionally, toxicity models are based on either the chemical data (such as the quantitative structure- activity relationship analysis), or the in vitro toxicity profiling data (such as the in vitro-in vivo extrapolations). Our previous experiences suggested that integrating biological descriptors such as the in vitro cytotoxicity profiles or the short-term toxigenomic data, with chemical structural features is able to predict rodent acute liver toxicity with reasonable accuracy. Therefore, the second part of this proposal will be devoted to develop novel computational models for hepatotoxicity prediction by integrating qHTS toxicity profiles and chemical structural information In Aim 1, we will curate, preprocess, and normalize collected public liver toxicity datasets. In ths study, we will model toxicity effects using multiple large public datasets such as HTS and qHTS bioassay data (Tox21[1] and ToxCast[2]), hepatotoxicity side effect reports on marketed failed drugs[3], the Liver Toxicity Knowledge Base Benchmark Dataset (LTKB-BD[4]), etc. Statistical methods for cross-study validation and quality control will be applied to the collected datasets to ensure computational compatibility and to select the appropriate datasets for analysis. In Aim 2, we will develop predictive models for chemicals' liver toxicity based on an integrative modeling workflow that will make use of both structural and in vitro toxicity profiles of a chemical. Our previous studies [5] showed that models using both in vitro toxicity profiles and chemical structural data have better accuracy for rodent acute liver toxicity than models using either data type alone. Here, we will develop a novel modeling workflow that start with defining the functional clusters of chemicals via curated qHTS toxicity profiles, and is followed by developing computational models to correlate chemical and biological data with overall toxicity risks in humans. The predictive models will be validated using independent datasets with over 800 compounds. In Aim 3, we propose to prioritize the qHTS profiling assays used in the model for future toxicity testing. We will evaluate all the in vitro assays as biological descriptors from thee perspectives, including descriptor importance in the integrative toxicity model, correlation with i vivo DILI outcomes, and level of information content estimated by a novel approach based on network analysis.    PUBLIC HEALTH RELEVANCE: In this study we aim to develop computational models that can identify potential liver toxicants. Liver toxicity is a significant contributor to the high attition rate in drug development. Moreover, toxic chemicals in food, water, and consumer products all pose serious risks for liver toxicity. As a result, there is great interest in developing high-throughput, high-content experimental and computational tools to evaluate the liver toxicity of thousands of pharmaceutical and environmental chemicals. This study focuses on developing novel informatics tools that enable the extraction and integration of chemical concentration-response information from multiple quantitative high-throughput screening databases for model development, and developing statistical models that are able to integrate this concentration-response information with chemical structural features to predict their risk of liver toxicity.  ",Development of integrative models for early liver toxicity assessment,9017336,R03ES026397,"['Acute', 'Address', 'Adverse effects', 'Algorithms', 'Animals', 'Area', 'Benchmarking', 'Biological', 'Biological Assay', 'Chemicals', 'Communities', 'Complex', 'Computer Simulation', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Dose', 'Dreams', 'Ensure', 'Ethics', 'Food', 'Future', 'Gene Expression', 'Genomics', 'Goals', 'Gold', 'Health', 'Hepatotoxicity', 'Human', 'In Vitro', 'Informatics', 'International', 'Liver', 'Machine Learning', 'Marketing', 'Mining', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Environmental Health Sciences', 'Nature', 'North Carolina', 'Outcome', 'Pathway Analysis', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Poison', 'Process', 'Productivity', 'Quality Control', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research', 'Resources', 'Risk', 'Rodent', 'Scientist', 'Shapes', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicogenetics', 'Toxicology', 'Translational Research', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'Universities', 'Variant', 'Water', 'base', 'computerized tools', 'consumer product', 'cost', 'cost effective', 'cytotoxicity', 'data modeling', 'drug development', 'drug market', 'drug withdrawal', 'environmental chemical', 'experience', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'interest', 'knowledge base', 'liver injury', 'model building', 'model development', 'novel', 'novel strategies', 'post-market', 'preclinical study', 'predictive modeling', 'programs', 'response', 'screening', 'success', 'tool', 'toxicant', 'validation studies']",NIEHS,UT SOUTHWESTERN MEDICAL CENTER,R03,2016,81000,-0.017126633096621692
"Kynurenine metabolites and depression: An in vitro and ex vivo study ﻿    DESCRIPTION (provided by applicant): The kynurenine catabolic pathway for tryptophan degradation in humans produces metabolites that are neurologically active and affect neurotransmission and neuronal integrity. Quinolinic acid (QUIN) is a product of the pathway which has been implicated in numerous neuropsychiatric disorders and has been correlated with depression in patients administered the inflammatory cytokine, interferon (IFN)-α. Besides, activation of the kynurenine pathway by inflammation leads to elevated levels of QUIN, which have been shown to cause depressive-like behavior in mice. The mechanism by which QUIN acts in the brain is to activate excitotoxic neurotransmitter pathways involving the amino acid glutamate and by leading to lipid peroxidation. Following metabolism by indoleamine 2,3-dioxygenase and subsequent steps, three consecutive enzymes of the kynurenine pathway determine the metabolic partitioning of tryptophan metabolites to QUIN, picolinic acid (PIC), or further metabolism. We hypothesize that the shift of QUIN and PIC levels is linked to chronic inflammation and major depression. To test this hypothesis, we will, in the first aim, study the structure and action of the enzymes and the mechanisms controlling QUIN and PIC production. In the second aim, a new, rapid and efficient analytical method will be developed for in vitro and ex vivo quantitation of QUIN and PIC levels and will be applied to samples obtained from patient populations expected to exhibit high levels of QUIN as a result of increased inflammation and from assays using purified protein to simulate similar distributions. Through these studies, the correlation between kynurenine metabolites and major depression will be investigated, and small molecule inhibitors will be used as probes for modulating the production of QUIN and PIC levels.         n/a",Kynurenine metabolites and depression: An in vitro and ex vivo study,9112097,R21MH107985,"['AIDS Dementia Complex', 'Acids', 'Affect', 'Agonist', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Anabolism', 'Anxiety', 'Behavior', 'Biochemistry', 'Biological Assay', 'Biological Markers', 'Brain', 'Carboxy-Lyases', 'Cerebrospinal Fluid', 'Chronic', 'Clinical', 'Complex', 'Correlation Studies', 'Detection', 'Development', 'Diagnostic Procedure', 'Dioxygenases', 'Disease', 'Enzyme Inhibitor Drugs', 'Enzyme Inhibitors', 'Enzymes', 'Epilepsy', 'Essential Amino Acids', 'Exhibits', 'Genes', 'Glutamates', 'Goals', 'Human', 'Huntington Disease', 'In Vitro', 'Inflammation', 'Inflammatory', 'Interferon-alpha', 'Kynurenic Acid', 'Kynurenine', 'Label', 'Lead', 'Link', 'Lipid Peroxidation', 'Liquid substance', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Mental Depression', 'Metabolic', 'Metabolic Control', 'Metabolism', 'Molecular', 'Mus', 'N-Methyl-D-Aspartate Receptors', 'Neurons', 'Neurotoxins', 'Neurotransmitters', 'Oxidoreductase', 'Pathway interactions', 'Patients', 'Phase', 'Physicians', 'Picolinic Acids', 'Plasma', 'Positioning Attribute', 'Production', 'Proteins', 'Protocols documentation', 'Quinolinic Acid', 'Reaction', 'Records', 'Regulation', 'Research', 'Research Personnel', 'Route', 'Sampling', 'Scheme', 'Scientist', 'Side', 'Solid', 'Source', 'Structure', 'Testing', 'Tissues', 'Tryptophan', 'Tryptophan 2,3 Dioxygenase', 'analytical method', 'base', 'chemical reaction', 'cost', 'cost effective', 'cytokine', 'depressive symptoms', 'design', 'enzyme mechanism', 'enzyme pathway', 'improved', 'innovation', 'interest', 'metabolic profile', 'neuroinflammation', 'neuropsychiatric disorder', 'neurotransmission', 'novel', 'patient population', 'small molecule', 'small molecule inhibitor', 'stem', 'three dimensional structure']",NIMH,UNIVERSITY OF TEXAS SAN ANTONIO,R21,2016,197688,-0.014892170267451203
"SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic* DESCRIPTION (provided by applicant): Existing studies suggest that tissue elasticity is possibly correlated with the aggressiveness of cancers. Based on results from deformable image registration, the proposed project investigates the possibility of tracking the organs movement subject to external forces and geometric constraints, thereby deducing patient-specific tissue elasticity parameters for proactive health monitoring. The objectives of this exploratory research project are (1) to develop a computational framework based on extensive studies of a large cohort of cancer patients, in order to accurately estimate patent-specific tissu elasticity using a coupled biomechanical simulation-optimization framework on a pair of medical images (possibly from ultrasound, mammography, computed tomography scan, magnetic resonance imaging, or other imaging technologies); (2) to examine potential association between tissue elasticity in different regions with aggressiveness of know/diagnosed cancer in the corresponding regions; (3) to derive predictive models for cancer staging/grading based on recovered patient-specific tissue elasticity and other explanatory variables; (4) to design a health monitoring system based on individualized analysis of tissue elasticity for 'at-risk' groups  who are more likely to develop cancers. This proposal describes a truly ambitious effort and a bold vision that is built upon the investigators' prior scientific accomplishments and strong credentials to potentially transform existing practice to more proactive, preventive, evidence-based health monitoring for individuals at risk of developing cancers. This research is expected to make several major scientific advances. These include new algorithms for non-invasive, image-based techniques for automatic extraction of tissue elasticity parameters without force applications and/or force sensing devices, novel regression models and inference procedures for survival analysis, new force sensing devices, novel regression models and inference procedures for survival analysis, new predictive models for cancer staging and grading based on patient-specific tissue elasticity parameters, and a health monitoring system for at-risk groups based on individual tissue elasticity along with other variables. PUBLIC HEALTH RELEVANCE: Other than health monitoring, the patient-specific tissue parameters can be incorporated into medical simulators to perform patient-specific surgical planning, compute desired force-feedback for tele-surgery, design and prototype medical devices, and conduct virtual surgical training. The statistical inference techniques developed can be applicable to genetic epidemiology, health economics, and bioinformatics.",SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic*,9103112,R01EB020426,"['Age', 'Aging', 'Algorithms', 'Behavioral', 'Bioinformatics', 'Biopsy', 'Blood Tests', 'Cancer Etiology', 'Cancer Patient', 'Cancerous', 'Cause of Death', 'Cessation of life', 'Chronic', 'Clinical', 'Colorectal Cancer', 'Coupled', 'Data', 'Databases', 'Developed Countries', 'Devices', 'Diagnosis', 'Diagnostic Neoplasm Staging', 'Elastic Tissue', 'Elasticity', 'Family', 'Feedback', 'Future', 'Gleason Grade for Prostate Cancer', 'Health', 'Hepatitis', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Knowledge', 'Learning', 'Legal patent', 'Life Style', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Malignant neoplasm of lung', 'Malignant neoplasm of prostate', 'Mammary Gland Parenchyma', 'Mammary Ultrasonography', 'Mammography', 'Medical', 'Medical Device', 'Medical Imaging', 'Modeling', 'Monitor', 'Movement', 'Operative Surgical Procedures', 'Organ', 'Patient Monitoring', 'Patients', 'Personal Satisfaction', 'Physicians', 'Preventive', 'Procedures', 'Property', 'Prostate', 'Radiation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'Scientific Advances and Accomplishments', 'Screening for cancer', 'Second Primary Cancers', 'Societies', 'Staging', 'Survival Analysis', 'System', 'Techniques', 'Test Result', 'Time', 'Tissues', 'Training', 'Tumor stage', 'Ultrasonography', 'United States', 'Virus Diseases', 'Vision', 'Woman', 'X-Ray Computed Tomography', 'aging population', 'base', 'biomechanical model', 'cancer diagnosis', 'cancer type', 'cohort', 'computer framework', 'design', 'evidence base', 'genetic epidemiology', 'health economics', 'image registration', 'insight', 'longitudinal analysis', 'malignant breast neoplasm', 'malignant stomach neoplasm', 'men', 'non-invasive imaging', 'novel', 'predictive modeling', 'prostate biopsy', 'prototype', 'tumor', 'virtual']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2016,60647,-0.0371042896556393
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,-0.00751186796659364
"Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis ﻿    DESCRIPTION (provided by applicant): Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis Idiopathic pulmonary fibrosis (IPF) is a disorder characterized by unrelenting scarring and stiffening of the lungs that leads to the death of an estimated 34,000 individuals in the U.S. each year. Unfortunately, individuals with IPF have extremely limited treatment options, as no effective drugs have been identified to halt the progression of fibrosis. Despite the importance of collagens to the structural organization both normal and remodeled ECM, little is known about how collagen structure in IPF differs from that of normal tissue architecture. There is a clear need to develop highly specific/sensitive techniques to probe collagen structure and organization in IPF tissues. In this project we will implement new collagen specific analyses using the high resolution microscopy technique of Second Harmonic Generation (SHG). This method is sensitive to both the fibrillar organization and also sub-resolution aspects of macro and supramolecular assembly. Here we will utilize SHG microscopy to: 1) determine the how pathologic collagen organization (seen in IPF) differs from normal tissue; 2) identify and quantify areas of active fibrosis (enriched in collagen III) from ""old"" or mature fibrosis (high in collagen I) in IPF lung specimens; 3) assess changes in elastin and collagen distribution during disease progression; and 4) correlate areas of high collagen III/I signal in IPF histologic samples with clinical markers of disease activity. As part of the project, we will develop customized automated machine vision routines to automatically classify tissues in terms of severity. We will specifically focus all of our efforts on studying structure around fibroblastic foci, which will be identified by other microscope modalities. These foci are thought to be at the leading edge of ECM remodeling but the dynamics of their formation in relationship to the overall fibrotic process remain unclear. We hypothesize that these structural changes will serve as label- free biomarkers of IPF and further hypothesize that the collagen is altered specifically around foci in a manner which is associated with disease progression. The information gained may form the basis of future prognostic/diagnostic schemes. We propose 2 Aims: Aim 1 Polarization resolved SHG to determine distribution of Col I/III and other ECM changes in different stages of IPF.  Aim 2. Develop classification system of morphological changes in IPF visualized by SHG. PUBLIC HEALTH RELEVANCE: Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis Narrative Idiopathic fibrosis (IPF) patients have poor survival rates and there is also a lack of diagnostic/prognostic tools that have sufficient sensitivity and specificity to evaluate changes in collagen in the extracellular matrix. The methods developed here will improve upon these limitations and may lay the groundwork for eventual non- invasive in vivo imaging.",Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis,9122490,R21HL126190,"['Architecture', 'Area', 'Biological Markers', 'Biomedical Engineering', 'Cessation of life', 'Cicatrix', 'Classification', 'Clinical', 'Clinical Markers', 'Collagen', 'Computer Vision Systems', 'Computers', 'Diagnostic', 'Disease', 'Disease Progression', 'Elastin', 'Equilibrium', 'Extracellular Matrix', 'Fibrosis', 'Future', 'Generations', 'Hamman-Rich syndrome', 'Health', 'Histologic', 'Image', 'Individual', 'Label', 'Lung', 'Maintenance', 'Methods', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'Normal tissue morphology', 'Pathologic', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protein Isoforms', 'Resolution', 'Sampling', 'Scheme', 'Scientist', 'Sensitivity and Specificity', 'Severities', 'Signal Transduction', 'Specimen', 'Staging', 'Structure', 'Survival Rate', 'System', 'Techniques', 'Texture', 'Thick', 'Three-Dimensional Imaging', 'Tissue Sample', 'Tissues', 'Vision', 'base', 'improved', 'in vivo imaging', 'interdisciplinary approach', 'prognostic', 'prognostic tool', 'second harmonic', 'tool']",NHLBI,UNIVERSITY OF WISCONSIN-MADISON,R21,2016,182770,-0.00466059205287717
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,-0.011636753720283652
"Forecasting pulmonary inflammation from in vitro assay results for nanoparticles ﻿    DESCRIPTION (provided by applicant):  The rapidly developing field of nanotechnology shows promise by allowing designers to specifically select unique combinations of material properties as needed increasing the effectiveness of applications in medicine, coatings, lubrication, semiconductors, composites, and many others. These materials with their unique combinations of properties on exposure to humans may result in unanticipated hazards, however, putting workers in nanotechnology-related industries at risk. Traditional animal testing is expensive and too slow to evaluate potential risks for the current pace of new nanomaterial development. Both technology developers and regulators need more rapid methods to evaluate new nanomaterial configurations for their risk potential. Much hope is placed in high-throughput in vitro screening assays, but the relevance of these results to the potential for human disease or even the observed toxic effects in animal exposures is unclear. Some research has proposed Quantitative Structure Activity Relationships (QSARs) to predict in vitro nanomaterial toxicity in a few specific assays, but the applicability of these models to a wider group of materials, alternative in vitro assays, or in vivo toxicity has not been explored. If the primary exposure pathway for workers in the near term is inhalation, which in vitro assays will provide the most reliable risk information for that scenario? Two recently available data sources will permit this study to investigate this question: the Environmental Protection Agency's (EPA) ToxCast data for nanomaterials and the Nanomaterial Pulmonary Toxicity Database (NTDB), a collection of published peer reviewed studies observing pulmonary inflammation in rodents upon exposures to nanomaterials. This study will pursue the following specific aims: (1.) identify combinations of in vitro assay results that can reliably forecast the results of pulmonary inflammation results in rodents; (2.) evaluate whether existing proposed QSARs for nanomaterial toxicity apply to a wider array of in vitro toxicity assays; and (3.) evaluate whether existing proposed QSARs for nanomaterial toxicity apply to in vivo pulmonary inflammation results. This study will employ machine learning methods to cluster similar nanomaterials between the various in vitro and in vivo results, and to identify combinations of in vitro assays that rank order the toxicity of nanomaterials most similarly to pulmonary inflammation results in rodents considering also how changes in specific chemical and physical particle properties exacerbate or mitigate observed toxicity. This study addresses documented research needs in the National Occupational Research Agenda (NORA) cross- sector Nanotechnology program including specific goals in the Human Health and Informatics categories. Implementation complies with the Research to Practice (r2p) Initiative in its formulation, design, and implementation plan including industry an public outreach. The insight generated by this study will improve nanomaterial risk screening capabilities and focus attention and effort on those measurements and techniques proven to be most effective and reliable enabling better management and control of the risks faced by workers. PUBLIC HEALTH RELEVANCE:  Although toxicity risk information for nanoparticles is accumulating rapidly, the development of new nanomaterial configurations is proceeding too fast for our best risk assessment tools (i.e. animal testing) to keep up. The new availability of two large databases of in vitro assay results and pulmonary inflammation results in rodents will permit this study to investigate which in vitro assays provide the most predictive information about the results from in vivo exposures, and thus speed up the risk screening process for nanomaterials. The results of this study will have important implications for more quickly identifying new nanomaterial-related risks to workers.",Forecasting pulmonary inflammation from in vitro assay results for nanoparticles,9144793,R03OH010956,[' '],NIOSH,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R03,2016,66564,-0.015611572205090529
"A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans ﻿    DESCRIPTION (provided by applicant): Exercise is arguably the most potent approach we can take to defer physical decline associated with aging and to protect against late onset diseases such as diabetes, cancer, and Alzheimer's disease. Molecular understanding of how exercise benefits translate into healthy aging is thus of definitive medical interest. We study fundamental processes relevant to healthy aging in the 959-celled nematode C. elegans. Recently we made a fascinating discovery-C. elegans can exercise (swim) to exhibit training benefits, and appear to gain benefits by molecular pathways conserved in humans. Our initial model development opens up a new research area for understanding how tissue-specific and organism-wide health benefits are induced by exercise, and creates a novel paradigm for identifying exercise mimetic drugs that might promote healthy aging. To really harvest the potential of this model, we need to measure the strength of the tiny C. elegans. We collaborated to develop a strength test in which trained animals thread through a matrix of deformable pillars, and the extent of pillar deflection is used to calculate force. Our ""NemaFlex"" force detection device is the quantitative foundation with which we expect to break new ground in understanding exercise impact on healthy aging. Here we propose required development to enhance assay throughput and pursue applications that will not only anchor this technology as an essential component of C. elegans exercise evaluation but also accelerate studies on exercise biology and healthy aging in this powerful model. Aim 1 is to develop a novel high throughput tool for direct strength evaluation in C. elegans.  This aim will generate an essential tool for analysis of C. elegans strength at multiple life stages, define the exercise regimen that will become the anchor protocol in the field, and reveal features of training in this model. Aim 2 is to use NemaFlex to evaluate exercise mimetic drugs & to facilitate focused pilot genetic screens. This aim will establish critical proof-of-principle for genetic and drug discovery using the NemaFlex. Aim 3 is to initiate dissection of the functional and molecular relationship between exercise and healthy aging, grounded in NemaFlex force measures of training benefits.  To begin, we will test how optimized strength training tracks with a broad spectrum of healthspan indicators that decline with age, we will investigate impact of cessation of training on aging quality, and we will ask if exercise mimetic drugs extend healthspan in the absence of training. Our goals will create novel technology that for the first time permits facile quantitativ analysis of exercise adaptations in the powerful C. elegans genetic model. Accomplishment of our tractable aims will anchor a new subfield of genetic investigation of exercise and healthy aging that may influence design of interventions that broadly promote health and defer aging. PUBLIC HEALTH RELEVANCE: Exercise has a profound positive impact on health of the aging population in that it protects against age-associated diseases including cancer, diabetes, and cardiovascular disease, at the same time it maintains muscle, immune system, and nervous system function in aging. We are developing the first exercise model in the simple animal C. elegans, in which training benefits appear mediated by conserved mechanisms and exercise promotes healthy aging. We will optimize a novel tool for direct strength measurement of these tiny 959-celled animals and show how our device can facilitate searches for exercise mimetic drugs and genes that are associated with training adaptations, and can also help define exercise impact on a broad range of healthy aging measures. The experimental advantages of C. elegans may yield unexpected insights that inspire development of novel interventions that protect against age-associated disease and age-associated decline.",A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans,9116734,R21AG050503,"['Address', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Animals', 'Area', 'Automation', 'Biological Assay', 'Biology', 'Biology of Aging', 'Caenorhabditis elegans', 'Cardiac', 'Cardiovascular Diseases', 'Cells', 'Collaborations', 'Computer Vision Systems', 'Detection', 'Development', 'Device Designs', 'Devices', 'Diabetes Mellitus', 'Disease', 'Dissection', 'Elderly', 'Engineering', 'Evaluation', 'Exercise', 'Exhibits', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Engineering', 'Genetic Models', 'Genetic Screening', 'Goals', 'Harvest', 'Health', 'Health Benefit', 'Human', 'Immune system', 'Intervention', 'Investigation', 'Late-Onset Disorder', 'Life', 'Longevity', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediating', 'Mediator of activation protein', 'Medical', 'Modeling', 'Molecular', 'Molecular Genetics', 'Muscle', 'Muscle function', 'Nematoda', 'Nervous System Physiology', 'Organism', 'Outcome', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protocols documentation', 'Pump', 'Regimen', 'Reporting', 'Research', 'Staging', 'Swimming', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translating', 'Work', 'age-related muscle loss', 'aging population', 'anti aging', 'base', 'cognitive function', 'design', 'drug discovery', 'exercise regimen', 'exercise training', 'experience', 'fascinate', 'healthy aging', 'immune function', 'improved', 'insight', 'interest', 'mimetics', 'model development', 'new technology', 'novel', 'programs', 'strength training', 'therapy design', 'tool']",NIA,TEXAS TECH UNIVERSITY,R21,2016,184503,-0.0360399032990401
"Quantitative microscopy-based rapid phenotyping and screening ﻿    DESCRIPTION:  Synapses are most fundamental to the function of a nervous system. C. elegans is an excellent genetic model system for finding genes and elucidating pathways because of its sequenced genome and the abundance of molecular biology tools and mutants. Due to the simplicity of its nervous system, many breakthroughs have been made in C. elegans for understanding molecular mechanisms in the patterning of the nervous system and synapse development. The current bottlenecks are in the manual and non-quantitative techniques such as visual screens, limiting both the throughput of the experiments and the phenotypes one can examine. Our long-term objective is to develop technologies and to understand how genes, age, and the environment together define and continue to remodel the nervous system of an organism. In the last funding period, we have made large progress in hardware system design (including microtechnologies and automation technologies) and software for quantitative characterization of phenotypes. The objective of this continuation project is to further engineer superior micro devices for large-scale live imaging and quantitative imaging technologies, and combine with the power of genetic and genomic approaches to study synapse development in this in vivo system; genes and pathways emerging from this study could potentially become targets of therapeutics in neurological disorders.  We have shown in the previous phase of the project that quantitative microscopy-based approaches can indeed enable identification of novel genes and pathways that conventional approaches cannot. In the continuation phase, we will further optimize on-chip rapid and high-content in vivo imaging techniques, and in parallel further develop algorithms and quantitative measures for the analysis of such high-content data; we will screen based on novel synthetic phenotype unobservable by eye; we will also exploit powerful genomic techniques to identify loci and potential multigenic interactions that shape the synapse morphology. These experimental approaches will identify genes that cannot have been identified otherwise because of the difficulties associated with the phenotypical profiling, but addressed using our engineered techniques here. The approach is innovative because the technology developed here dramatically increases the throughput, sensitivity, and accuracy of the experiments, and truly enables the utility of extremely powerful genetic and genomic methods. The proposed research is significant because it fills the urgent need in high-throughput and high-content screens as well as identifying novel genes and pathways. In addition, besides the contribution to the specific neurobiology, the technologies are widely applicable to areas such as developmental cell biology, and to other small organisms such as fly larvae and zebrafish embryos.         PUBLIC HEALTH RELEVANCE:   Synapse development is an important and active area of research linking genes and environments to the formation and maintenance of synapses in the nervous system. It has direct implications in many human diseases developmental and psychiatric diseases such as Autism Spectrum Disorder and Schizophrenia.                ",Quantitative microscopy-based rapid phenotyping and screening,9116662,R01GM088333,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alleles', 'Animals', 'Area', 'Automation', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Chromosome Mapping', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Development', 'Developmental Cell Biology', 'Devices', 'Disease', 'Drosophila genus', 'Embryo', 'Engineering', 'Environment', 'Event', 'Eye', 'Fill-It', 'Fluorescence', 'Funding', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Research', 'Genetic Screening', 'Genomic approach', 'Genomics', 'Goals', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Imaging technology', 'Inbreeding', 'Larva', 'Lead', 'Life', 'Link', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Microfluidics', 'Microscopy', 'Molecular', 'Molecular Biology', 'Morphology', 'Nematoda', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurons', 'Organism', 'Pathway interactions', 'Pattern', 'Phase', 'Phenotype', 'Phylogeny', 'Positioning Attribute', 'Quantitative Microscopy', 'Quantitative Trait Loci', 'Regulatory Pathway', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Speed', 'Synapses', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Vision', 'Visual', 'Work', 'Zebrafish', 'autism spectrum disorder', 'base', 'design', 'experience', 'fly', 'forward genetics', 'genetic approach', 'genome sequencing', 'high throughput screening', 'human disease', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'mutant', 'nerve injury', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'quantitative imaging', 'research study', 'screening', 'success', 'synaptogenesis', 'targeted treatment', 'technology development', 'tool', 'trait']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2016,306503,-0.040350359363126954
"Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA ﻿    DESCRIPTION (provided by applicant): Osteoporosis is a disease characterized by loss of bone mass and structural deterioration leading to increased risk of fracture. Currently, osteoporosis is diagnosed by measurement of areal bone mineral density by dual- energy x-ray absorptiometry (DXA). However, the majority of fractures occur in both women and men who are not classified as osteoporotic by current DXA criteria (T-score = -2.5). As a 2-dimensional (2D) technology, DXA does not provide information about 3-dimensional (3D) bone structure, shape and geometry, which substantially contribute to bone strength and resistance to fracture. Finite element (FE) analysis of quantitative computed tomography (QCT) images can provide 3D structure and strength measurements but QCT is impractical for widespread clinical use because of high radiation exposure and expense. In contrast, DXA is widely available, inexpensive and has low radiation exposure. What is needed is a method by which DXA images can be used to generate 3D shape models that incorporate bone structure and geometry. However, fractures are complex events influenced by other factors including age, race, body mass index, risk of falls, and prior medical and fracture history. Even sophisticated measurements of bone density, structure, and strength may not be able to predict fractures accurately. Machine learning is an emerging field in which models are created by ""learning"" from previous data. These models can incorporate various factors and be used to classify or predict outcomes for new data. The overall hypothesis of this proposal is that advanced analyses of widely available DXA images that incorporate structural and strength information and statistical modeling using machine learning to incorporate additional risk factors will better identify patient at high risk of osteoporotic fracture. This hypothesis will be tested using QCT and DXA data from previous studies to generate 3D statistical shape models that describe variability in proximal femur morphology. By aligning 2D DXA images to the models, patient-specific 3D models will be reconstructed for quantitative analyses and combined with FE analysis to estimate bone strength. Machine learning models will be used to incorporate these novel measurements, demographics, and various risk factors for fracture to predict incident fractures in two very large, prospective studies. The ultimate goal of this proposal is to increase the diagnostic utility of DXA, a safe, non-invasive, and widely available technology, by applying novel image processing and statistical techniques to predict fractures more accurately.         PUBLIC HEALTH RELEVANCE: Approximately 50% of women and 25% of men over age 50 are destined to suffer an osteoporotic fracture during their remaining lifetime. Unfortunately, the current standard for the diagnosis of osteoporosis, DXA, does not predict most fractures. This research will develop advanced analyses of DXA images and use machine learning to improve individualized fracture risk assessment.            ",Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA,8869145,K99AR067883,"['3-Dimensional', 'Age', 'American', 'Body mass index', 'Bone Density', 'Cadaver', 'Characteristics', 'Clinical', 'Complex', 'Data', 'Data Set', 'Deterioration', 'Diagnosis', 'Diagnostic', 'Disease', 'Elements', 'Epidemiology', 'Event', 'Femur', 'Finite Element Analysis', 'Fracture', 'Future', 'Geometry', 'Goals', 'Gold', 'Height', 'Hip Fractures', 'Image', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Morphology', 'Osteopenia', 'Osteoporosis', 'Outcome', 'Patients', 'Peripheral', 'Postmenopause', 'Prospective Studies', 'Race', 'Radiation', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Resistance', 'Risk', 'Risk Assessment', 'Risk Factors', 'Scanning', 'Shapes', 'Specimen', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Weight', 'Woman', 'X-Ray Computed Tomography', 'base', 'bone', 'bone geometry', 'bone mass', 'bone strength', 'cohort', 'demographics', 'density', 'diagnosis standard', 'fall risk', 'high risk', 'image processing', 'improved', 'in vivo', 'information model', 'men', 'novel', 'osteoporosis with pathological fracture', 'prospective', 'public health relevance', 'three dimensional structure', 'tool', 'two-dimensional']",NIAMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,K99,2015,91800,-0.029176884196257427
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8825472,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis', 'transcriptome sequencing']",NCI,BROWN UNIVERSITY,R01,2015,71329,-0.03241828459893793
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,-0.015379854278088767
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates ﻿    DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques.         PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.            ",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,8955720,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Individual', 'Instruction', 'Joints', 'Knowledge', 'Leisure Activities', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2015,544862,-0.07157800972935863
"Identifying Huntington's disease markers by modern statistical learning methods. DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD. PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.",Identifying Huntington's disease markers by modern statistical learning methods.,8896079,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Health', 'Huntington Disease', 'Image', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Relative (related person)', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'cognitive testing', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'human Huntingtin protein', 'improved', 'indexing', 'interest', 'meetings', 'nervous system disorder', 'novel', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2015,339691,-0.015531066100047139
"Predictive Model of Chronic Kidney Disease in a Hispanic Population DESCRIPTION (provided by applicant): This career transition award will provide the applicant with protected research time to develop an independent research career in biomedical informatics. Dr. Starkey received his M.D. in 2003 from UT Southwestern, completed the NLM Biomedical Informatics Training Program post-doctoral fellowship in 2011 and earned a Ph.D. focusing on clinical informatics from the Clinical Sciences Degree Program at the University of Texas Medical Branch (UTMB) at Galveston in 2012. Dr. Starkey accepted a faculty position at the Institute for Translational Sciences at UTMB that is currently supporting his transition to independent investigator. Dr. Starkey will utilize the career transition award for novel applications of advanced machine learning methods to create predictive models of disease. Predictive models of disease allow for the stratification of risk and prevention of disease and have been successfully implemented in the assessment of cardiovascular disease risk. Predictive models of chronic kidney disease (CKD) are an active area of research since CKD is a risk factor for all-cause mortality, cardiovascular death and end-stage renal disease. However, there is not a single predictive model of CKD created for application to Hispanics and the external validity of existing models is poor. The objective of this project is to create predictive models of chronic kidney disease in a Hispanic population and quantify biomarkers of chronic kidney disease using multiple reaction monitoring (MRM) proteomics in this minority population. MRM proteomics will utilize the UTMB Novel Methodologies core and its array of resources to create an inter-institutional collaboration. Heterogeneous ensemble machine learning methods will be used to create the predictive model of chronic kidney disease in the Cameron County Hispanic Cohort (CCHC). Established in 2004, the CCHC is a random population sample in Brownsville, Texas created to evaluate the determinants of health in a US/Mexico border population that is primarily of Hispanic ethnicity. The CCHC has an extremely high prevalence of obesity and diabetes at 49.7% and 30.3%, respectively. Both obesity and diabetes are independent risk factors for the development of CKD and represent significant health disparities in the Hispanic population. Archived serum samples of CCHC participants and the CCHC database will be utilized to complete the following aims: 1) Create predictive models of CKD in a Hispanic population 2) Refine the predictive models by including clinical laboratory data and a selective reaction monitoring mass spectrometry panel of biomarkers. At the completion of this project, predictive models of CKD applicable to a Hispanic population will be created and MRM proteomics will demonstrate utility of biomarkers identified in other populations. The application of the results will be used to create clinical tools for CKD risk and guide future studies in this Hispanic population with health disparities. Importantly, it will provide a career transition for D. Starkey to demonstrate the knowledge gained during his training that results in impactful research and publications. PROJECT NARRATIVE:  The proposed research is relevant to public health because it allows stratification of chronic kidney disease risk and quantifies biomarkers of chronic kidney disease in the Hispanic minority whom has significant health disparities with regard to kidney disease. The translation of selective reaction monitoring of biomarkers and development of heterogeneous ensemble machine learning methods for application to the prediction of chronic kidney disease in a minority population supports the NLM mission.",Predictive Model of Chronic Kidney Disease in a Hispanic Population,8929292,K22LM011869,"['Age', 'Albumins', 'Archives', 'Area', 'Atherosclerosis', 'Biological Markers', 'Cardiovascular system', 'Career Transition Award', 'Cessation of life', 'Chronic Kidney Failure', 'Clinical', 'Clinical Informatics', 'Clinical Sciences', 'Collaborations', 'Communities', 'County', 'Creatinine', 'Data', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Dialysis procedure', 'Disease', 'Disease Outcome', 'Doctor of Medicine', 'Doctor of Philosophy', 'End stage renal failure', 'Ethnic Origin', 'Faculty', 'Fellowship', 'Funding', 'Future', 'General Population', 'Goals', 'Health', 'High Prevalence', 'Hispanics', 'Hospitalization', 'Household', 'Incidence', 'Individual', 'Institutes', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Mass Spectrum Analysis', 'Medical', 'Methodology', 'Methods', 'Mexican', 'Mexico', 'Minority', 'Mission', 'Modeling', 'Monitor', 'Obesity', 'Participant', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Prevalence', 'Probability', 'Process', 'Proteomics', 'Public Health', 'Publications', 'Reaction', 'Recruitment Activity', 'Renal function', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Sampling', 'Serum', 'Specimen', 'Stratification', 'Texas', 'Time', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transplantation', 'Universities', 'Validation', 'Visit', 'Weight', 'Work', 'biomedical informatics', 'cardiovascular disorder risk', 'career', 'cohort', 'cost effective', 'design', 'disorder prevention', 'disorder risk', 'follow-up', 'forest', 'health disparity', 'improved', 'informatics training', 'innovation', 'medically underserved', 'mortality', 'multiple reaction monitoring', 'novel', 'predictive modeling', 'programs', 'prospective', 'screening', 'skills', 'tool', 'urinary']",NLM,UNIVERSITY OF TEXAS MED BR GALVESTON,K22,2015,132348,-0.04306064279083741
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages.         PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.            ",Multi-Resolution Docking Methods for Electron Microscopy,8964685,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Image', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Relative (related person)', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2015,307928,-0.03532517398271782
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ﻿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices.         PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.            ",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,8984813,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'public health relevance', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2015,273816,-0.02528897326579329
"Protect Privacy of Healthcare Data in the Cloud DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier. Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8925916,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Laplacian', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'privacy protection', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2015,192613,-0.014040351914158564
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases.         PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.            ",Integration and Visualization of Diverse Biological Data,8886554,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Drug Targeting', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'drug development', 'drug discovery', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'public health relevance', 'research study', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2015,473642,-0.03083753134581628
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",8927659,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2015,434391,-0.012266676431651328
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8935819,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2015,107469,-0.02467302379297667
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,8858662,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,248912,-0.019286249884763913
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,-0.0035388565779969163
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,8915740,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2015,397940,-0.01736577666274286
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8792219,R01EY022039,"['Address', 'Algorithm Design', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2015,379750,-0.020468810283714278
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,8808769,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Health', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2015,324063,-0.06880505697344363
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,8929328,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'genetic makeup', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'personalized medicine', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2015,329422,-0.01536988530083576
"In vivo Characterization of Stents using Intravascular OCT Imaging DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel. PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,8885879,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Health', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'stent thrombosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2015,457216,-0.015109290650945049
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8920676,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'quantitative imaging', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2015,323033,-0.01064382066116746
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,8890255,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2015,347156,-0.02024329832363095
IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS. Surveillance Epidemiology and End Results (SEER) Electronic Data Capture Software Support and Installations. n/a,IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS.,9162089,61201500033I,"['Artificial Intelligence', 'Award', 'Computer software', 'Contracts', 'Data', 'Diagnostic Imaging', 'Electronics', 'Hour', 'Laboratories', 'Maintenance', 'Malignant Neoplasms', 'Medical Surveillance', 'Medicine', 'Pathology', 'Reporting', 'Update', 'electronic data']",NCI,"ARTIFICIAL INTELLIGENCE IN MEDICINE, INC",N03,2015,1135265,-0.032423319470570526
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,8839318,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Health', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Individual', 'Lateral', 'Lead', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'response', 'screening', 'spinal tract', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2015,577490,-0.01672907458400965
"Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy DESCRIPTION (provided by applicant): Our understanding of cervical remodeling during pregnancy and labor is incomplete, partly due to the lack of in vivo studies on the biochemical changes that occur in the cervix over the course of pregnancy. Elucidation of the mechanisms for cervical ripening could be used to predict the onset of preterm labor. Until recently, in vivo research methods were too invasive to be used as discovery tools, particularly in women who present with preterm labor. This proposal will use in vivo Raman spectroscopy, an optical technique that is sensitive to collagen content, collagen structure, hydration, lipids, proteins, ad other biomolecules to non-invasively investigate the biochemistry of the cervix throughout pregnancy. Using fiber optic in vivo Raman spectroscopy, we recently found significant differences in Raman spectra in at least four important peaks during the course of pregnancy in mice, including discrete signatures for lipids, collagen, amide bonds, and enriched amino acids (proline, tyrosine). Computational analysis of these spectra yielded predictive algorithms with 94% classification accuracy for stage of pregnancy. Studies performed in 2-hour windows at the end of pregnancy identified spectra predictive for the timing of parturition. This approach provides a detailed real-time biomolecular map of cervical ripening that is currently unavailable by other means. In this proposal, we hypothesize that the different mechanisms of premature cervical ripening have unique Raman spectral signatures that correspond to underlying biochemical and mechanical changes that precede preterm birth, which can be detected in vivo. Two Specific Aims are proposed: 1) Determine spectral changes in the cervix of mice with normal and abnormal pregnancy and parturition; 2) Identify specific mediators of cervical remodeling by comparing Raman spectra to mechanical and biochemical changes in the ex vivo cervix during normal and abnormal parturition. Raman spectroscopy has primarily been used for detection of disease. Collaboration between our reproductive biology and bioengineering groups will capitalize on our expertise in Raman analysis of cervical tissues to study dynamic changes in cervix composition during pregnancy. Key elements in cervical biochemistry will be identified. In vivo Raman spectroscopy will be combined with biomechanical studies and imaging mass spectrometry, a powerful tool for in situ proteomic analysis, to examine mice with premature or delayed cervical remodeling. Together, these highly innovative approaches will generate in-depth profiles of cervical biology that will translate into novel non-invasive methods to detect impending premature birth in women. PUBLIC HEALTH RELEVANCE: This proposal will use Raman Spectroscopy, a non-invasive, optical scattering technique, to investigate the composition of the cervix throughout pregnancy and provide detailed real-time information on cervical ripening. These studies will identify spectral differences in the cervix during normal and abnormal cervical maturation; optical and biochemical markers will be identified to help monitor pregnancy non-invasively, as the fiber optic probe only requires brief contact with the external surface of the cervix to obtain measurements. Elucidating the mechanisms that initiate cervical ripening will provide a critical step for early detection and treatment of preterm birth, which is the leading cause of infant morbidity and mortality.",Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy,8925122,R01HD081121,"['Address', 'Algorithms', 'Alprostadil', 'Amides', 'Amino Acids', 'Applications Grants', 'Biochemical', 'Biochemical Markers', 'Biochemistry', 'Biological', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biomechanics', 'Biomedical Engineering', 'Birth', 'Cervical', 'Cervical Ripening', 'Cervix Uteri', 'Classification', 'Clinical', 'Collaborations', 'Collagen', 'Computational algorithm', 'Computer Analysis', 'Data', 'Detection', 'Development', 'Disease', 'Early Diagnosis', 'Early treatment', 'Elasticity', 'Emerging Technologies', 'Etiology', 'Fetal Development', 'Fiber Optics', 'Foundations', 'Generations', 'Goals', 'Health', 'High-Risk Pregnancy', 'Hormonal', 'Hour', 'Hydration status', 'Image', 'Immunohistochemistry', 'In Situ', 'In Situ Hybridization', 'Indium', 'Interdisciplinary Study', 'Laboratories', 'Lead', 'Link', 'Lipids', 'Machine Learning', 'Maintenance', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Mechanics', 'Mediator of activation protein', 'Medical', 'Methods', 'Mifepristone', 'Modality', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mus', 'Optics', 'Phenotype', 'Physiological', 'Pregnancy', 'Premature Birth', 'Premature Labor', 'Prevention', 'Process', 'Proline', 'Property', 'Proteins', 'Proteomics', 'RU-486', 'Raman Spectrum Analysis', 'Reproductive Biology', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Spectrum Analysis', 'Staging', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Translating', 'Translations', 'Tyrosine', 'Woman', 'base', 'clinical application', 'in vivo', 'infant morbidity/mortality', 'innovation', 'insight', 'mouse model', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'physical science', 'predictive modeling', 'pregnant', 'premature', 'response', 'tool']",NICHD,VANDERBILT UNIVERSITY,R01,2015,362979,-0.015637589233675298
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic   expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,9023076,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'anxious', 'base', 'biosignature', 'clinical phenotype', 'clinical predictors', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'personalized medicine', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2015,457863,-0.003959831710535706
"SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic* DESCRIPTION (provided by applicant): Existing studies suggest that tissue elasticity is possibly correlated with the aggressiveness of cancers. Based on results from deformable image registration, the proposed project investigates the possibility of tracking the organs movement subject to external forces and geometric constraints, thereby deducing patient-specific tissue elasticity parameters for proactive health monitoring. The objectives of this exploratory research project are (1) to develop a computational framework based on extensive studies of a large cohort of cancer patients, in order to accurately estimate patent-specific tissu elasticity using a coupled biomechanical simulation-optimization framework on a pair of medical images (possibly from ultrasound, mammography, computed tomography scan, magnetic resonance imaging, or other imaging technologies); (2) to examine potential association between tissue elasticity in different regions with aggressiveness of know/diagnosed cancer in the corresponding regions; (3) to derive predictive models for cancer staging/grading based on recovered patient-specific tissue elasticity and other explanatory variables; (4) to design a health monitoring system based on individualized analysis of tissue elasticity for 'at-risk' groups  who are more likely to develop cancers. This proposal describes a truly ambitious effort and a bold vision that is built upon the investigators' prior scientific accomplishments and strong credentials to potentially transform existing practice to more proactive, preventive, evidence-based health monitoring for individuals at risk of developing cancers. This research is expected to make several major scientific advances. These include new algorithms for non-invasive, image-based techniques for automatic extraction of tissue elasticity parameters without force applications and/or force sensing devices, novel regression models and inference procedures for survival analysis, new force sensing devices, novel regression models and inference procedures for survival analysis, new predictive models for cancer staging and grading based on patient-specific tissue elasticity parameters, and a health monitoring system for at-risk groups based on individual tissue elasticity along with other variables. PUBLIC HEALTH RELEVANCE: Other than health monitoring, the patient-specific tissue parameters can be incorporated into medical simulators to perform patient-specific surgical planning, compute desired force-feedback for tele-surgery, design and prototype medical devices, and conduct virtual surgical training. The statistical inference techniques developed can be applicable to genetic epidemiology, health economics, and bioinformatics.",SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic*,8934115,R01EB020426,"['Age', 'Aging', 'Algorithms', 'Behavioral', 'Bioinformatics', 'Biopsy', 'Blood Tests', 'Cancer Etiology', 'Cancer Patient', 'Cancerous', 'Cause of Death', 'Cessation of life', 'Chronic', 'Clinical', 'Colorectal Cancer', 'Coupled', 'Data', 'Databases', 'Developed Countries', 'Devices', 'Diagnosis', 'Diagnostic Neoplasm Staging', 'Elastic Tissue', 'Elasticity', 'Family', 'Feedback', 'Future', 'Gleason Grade for Prostate Cancer', 'Health', 'Hepatitis', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Knowledge', 'Learning', 'Legal patent', 'Life Style', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Malignant neoplasm of lung', 'Malignant neoplasm of prostate', 'Mammary Gland Parenchyma', 'Mammary Ultrasonography', 'Mammography', 'Medical', 'Medical Device', 'Medical Imaging', 'Modeling', 'Monitor', 'Movement', 'Operative Surgical Procedures', 'Organ', 'Patient Monitoring', 'Patients', 'Personal Satisfaction', 'Physicians', 'Preventive', 'Procedures', 'Property', 'Prostate', 'Radiation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'Scientific Advances and Accomplishments', 'Screening for cancer', 'Second Primary Cancers', 'Societies', 'Staging', 'Survival Analysis', 'System', 'Techniques', 'Test Result', 'Time', 'Tissues', 'Training', 'Tumor stage', 'Ultrasonography', 'United States', 'Virus Diseases', 'Vision', 'Woman', 'X-Ray Computed Tomography', 'aging population', 'base', 'biomechanical model', 'cancer diagnosis', 'cancer type', 'cohort', 'computer framework', 'design', 'evidence base', 'genetic epidemiology', 'health economics', 'image registration', 'insight', 'longitudinal analysis', 'malignant breast neoplasm', 'malignant stomach neoplasm', 'men', 'non-invasive imaging', 'novel', 'predictive modeling', 'prostate biopsy', 'prototype', 'tumor', 'virtual']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2015,214237,-0.0371042896556393
"Informatic tools for predicting an ordinal response for high-dimensional data DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale. Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8900334,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Breast Cancer Patient', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2015,121902,-0.017673093149009916
"Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis ﻿    DESCRIPTION (provided by applicant): Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis Idiopathic pulmonary fibrosis (IPF) is a disorder characterized by unrelenting scarring and stiffening of the lungs that leads to the death of an estimated 34,000 individuals in the U.S. each year. Unfortunately, individuals with IPF have extremely limited treatment options, as no effective drugs have been identified to halt the progression of fibrosis. Despite the importance of collagens to the structural organization both normal and remodeled ECM, little is known about how collagen structure in IPF differs from that of normal tissue architecture. There is a clear need to develop highly specific/sensitive techniques to probe collagen structure and organization in IPF tissues. In this project we will implement new collagen specific analyses using the high resolution microscopy technique of Second Harmonic Generation (SHG). This method is sensitive to both the fibrillar organization and also sub-resolution aspects of macro and supramolecular assembly. Here we will utilize SHG microscopy to: 1) determine the how pathologic collagen organization (seen in IPF) differs from normal tissue; 2) identify and quantify areas of active fibrosis (enriched in collagen III) from ""old"" or mature fibrosis (high in collagen I) in IPF lung specimens; 3) assess changes in elastin and collagen distribution during disease progression; and 4) correlate areas of high collagen III/I signal in IPF histologic samples with clinical markers of disease activity. As part of the project, we will develop customized automated machine vision routines to automatically classify tissues in terms of severity. We will specifically focus all of our efforts on studying structure around fibroblastic foci, which will be identified by other microscope modalities. These foci are thought to be at the leading edge of ECM remodeling but the dynamics of their formation in relationship to the overall fibrotic process remain unclear. We hypothesize that these structural changes will serve as label- free biomarkers of IPF and further hypothesize that the collagen is altered specifically around foci in a manner which is associated with disease progression. The information gained may form the basis of future prognostic/diagnostic schemes. We propose 2 Aims: Aim 1 Polarization resolved SHG to determine distribution of Col I/III and other ECM changes in different stages of IPF.  Aim 2. Develop classification system of morphological changes in IPF visualized by SHG.         PUBLIC HEALTH RELEVANCE: Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis Narrative Idiopathic fibrosis (IPF) patients have poor survival rates and there is also a lack of diagnostic/prognostic tools that have sufficient sensitivity and specificity to evaluate changes in collagen in the extracellular matrix. The methods developed here will improve upon these limitations and may lay the groundwork for eventual non- invasive in vivo imaging.            ",Second Harmonic Generation analysis of the ECM in idiopathic pulmonary fibrosis,8969087,R21HL126190,"['Architecture', 'Area', 'Biological Markers', 'Biomedical Engineering', 'Cessation of life', 'Cicatrix', 'Classification', 'Clinical', 'Clinical Markers', 'Collagen', 'Computer Vision Systems', 'Computers', 'Diagnostic', 'Disease', 'Disease Progression', 'Elastin', 'Equilibrium', 'Extracellular Matrix', 'Fibrosis', 'Future', 'Generations', 'Hamman-Rich syndrome', 'Histologic', 'Image', 'Individual', 'Label', 'Lung', 'Maintenance', 'Methods', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'Normal tissue morphology', 'Pathologic', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protein Isoforms', 'Resolution', 'Sampling', 'Scheme', 'Scientist', 'Sensitivity and Specificity', 'Severities', 'Signal Transduction', 'Specimen', 'Staging', 'Structure', 'Survival Rate', 'System', 'Techniques', 'Texture', 'Thick', 'Three-Dimensional Imaging', 'Tissue Sample', 'Tissues', 'Vision', 'base', 'improved', 'in vivo imaging', 'interdisciplinary approach', 'prognostic', 'prognostic tool', 'public health relevance', 'second harmonic', 'tool']",NHLBI,UNIVERSITY OF WISCONSIN-MADISON,R21,2015,218370,-0.00466059205287717
"Forecasting pulmonary inflammation from in vitro assay results for nanoparticles ﻿    DESCRIPTION (provided by applicant):  The rapidly developing field of nanotechnology shows promise by allowing designers to specifically select unique combinations of material properties as needed increasing the effectiveness of applications in medicine, coatings, lubrication, semiconductors, composites, and many others. These materials with their unique combinations of properties on exposure to humans may result in unanticipated hazards, however, putting workers in nanotechnology-related industries at risk. Traditional animal testing is expensive and too slow to evaluate potential risks for the current pace of new nanomaterial development. Both technology developers and regulators need more rapid methods to evaluate new nanomaterial configurations for their risk potential. Much hope is placed in high-throughput in vitro screening assays, but the relevance of these results to the potential for human disease or even the observed toxic effects in animal exposures is unclear. Some research has proposed Quantitative Structure Activity Relationships (QSARs) to predict in vitro nanomaterial toxicity in a few specific assays, but the applicability of these models to a wider group of materials, alternative in vitro assays, or in vivo toxicity has not been explored. If the primary exposure pathway for workers in the near term is inhalation, which in vitro assays will provide the most reliable risk information for that scenario? Two recently available data sources will permit this study to investigate this question: the Environmental Protection Agency's (EPA) ToxCast data for nanomaterials and the Nanomaterial Pulmonary Toxicity Database (NTDB), a collection of published peer reviewed studies observing pulmonary inflammation in rodents upon exposures to nanomaterials. This study will pursue the following specific aims: (1.) identify combinations of in vitro assay results that can reliably forecast the results of pulmonary inflammation results in rodents; (2.) evaluate whether existing proposed QSARs for nanomaterial toxicity apply to a wider array of in vitro toxicity assays; and (3.) evaluate whether existing proposed QSARs for nanomaterial toxicity apply to in vivo pulmonary inflammation results. This study will employ machine learning methods to cluster similar nanomaterials between the various in vitro and in vivo results, and to identify combinations of in vitro assays that rank order the toxicity of nanomaterials most similarly to pulmonary inflammation results in rodents considering also how changes in specific chemical and physical particle properties exacerbate or mitigate observed toxicity. This study addresses documented research needs in the National Occupational Research Agenda (NORA) cross- sector Nanotechnology program including specific goals in the Human Health and Informatics categories. Implementation complies with the Research to Practice (r2p) Initiative in its formulation, design, and implementation plan including industry an public outreach. The insight generated by this study will improve nanomaterial risk screening capabilities and focus attention and effort on those measurements and techniques proven to be most effective and reliable enabling better management and control of the risks faced by workers.         PUBLIC HEALTH RELEVANCE:  Although toxicity risk information for nanoparticles is accumulating rapidly, the development of new nanomaterial configurations is proceeding too fast for our best risk assessment tools (i.e. animal testing) to keep up. The new availability of two large databases of in vitro assay results and pulmonary inflammation results in rodents will permit this study to investigate which in vitro assays provide the most predictive information about the results from in vivo exposures, and thus speed up the risk screening process for nanomaterials. The results of this study will have important implications for more quickly identifying new nanomaterial-related risks to workers.            ",Forecasting pulmonary inflammation from in vitro assay results for nanoparticles,8953935,R03OH010956,[' '],NIOSH,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R03,2015,66904,-0.015611572205090529
"A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans ﻿    DESCRIPTION (provided by applicant): Exercise is arguably the most potent approach we can take to defer physical decline associated with aging and to protect against late onset diseases such as diabetes, cancer, and Alzheimer's disease. Molecular understanding of how exercise benefits translate into healthy aging is thus of definitive medical interest. We study fundamental processes relevant to healthy aging in the 959-celled nematode C. elegans. Recently we made a fascinating discovery-C. elegans can exercise (swim) to exhibit training benefits, and appear to gain benefits by molecular pathways conserved in humans. Our initial model development opens up a new research area for understanding how tissue-specific and organism-wide health benefits are induced by exercise, and creates a novel paradigm for identifying exercise mimetic drugs that might promote healthy aging. To really harvest the potential of this model, we need to measure the strength of the tiny C. elegans. We collaborated to develop a strength test in which trained animals thread through a matrix of deformable pillars, and the extent of pillar deflection is used to calculate force. Our ""NemaFlex"" force detection device is the quantitative foundation with which we expect to break new ground in understanding exercise impact on healthy aging. Here we propose required development to enhance assay throughput and pursue applications that will not only anchor this technology as an essential component of C. elegans exercise evaluation but also accelerate studies on exercise biology and healthy aging in this powerful model. Aim 1 is to develop a novel high throughput tool for direct strength evaluation in C. elegans.  This aim will generate an essential tool for analysis of C. elegans strength at multiple life stages, define the exercise regimen that will become the anchor protocol in the field, and reveal features of training in this model. Aim 2 is to use NemaFlex to evaluate exercise mimetic drugs & to facilitate focused pilot genetic screens. This aim will establish critical proof-of-principle for genetic and drug discovery using the NemaFlex. Aim 3 is to initiate dissection of the functional and molecular relationship between exercise and healthy aging, grounded in NemaFlex force measures of training benefits.  To begin, we will test how optimized strength training tracks with a broad spectrum of healthspan indicators that decline with age, we will investigate impact of cessation of training on aging quality, and we will ask if exercise mimetic drugs extend healthspan in the absence of training. Our goals will create novel technology that for the first time permits facile quantitativ analysis of exercise adaptations in the powerful C. elegans genetic model. Accomplishment of our tractable aims will anchor a new subfield of genetic investigation of exercise and healthy aging that may influence design of interventions that broadly promote health and defer aging.         PUBLIC HEALTH RELEVANCE: Exercise has a profound positive impact on health of the aging population in that it protects against age-associated diseases including cancer, diabetes, and cardiovascular disease, at the same time it maintains muscle, immune system, and nervous system function in aging. We are developing the first exercise model in the simple animal C. elegans, in which training benefits appear mediated by conserved mechanisms and exercise promotes healthy aging. We will optimize a novel tool for direct strength measurement of these tiny 959-celled animals and show how our device can facilitate searches for exercise mimetic drugs and genes that are associated with training adaptations, and can also help define exercise impact on a broad range of healthy aging measures. The experimental advantages of C. elegans may yield unexpected insights that inspire development of novel interventions that protect against age-associated disease and age-associated decline.              ",A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans,8936078,R21AG050503,"['Address', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Animals', 'Area', 'Automation', 'Biological Assay', 'Biology', 'Biology of Aging', 'Caenorhabditis elegans', 'Cardiac', 'Cardiovascular Diseases', 'Cells', 'Collaborations', 'Computer Vision Systems', 'Detection', 'Development', 'Device Designs', 'Devices', 'Diabetes Mellitus', 'Disease', 'Dissection', 'Elderly', 'Engineering', 'Evaluation', 'Exercise', 'Exhibits', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Engineering', 'Genetic Models', 'Genetic Screening', 'Goals', 'Harvest', 'Health', 'Health Benefit', 'Human', 'Immune system', 'Intervention', 'Investigation', 'Late-Onset Disorder', 'Life', 'Longevity', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediating', 'Mediator of activation protein', 'Medical', 'Modeling', 'Molecular', 'Molecular Genetics', 'Muscle', 'Muscle function', 'Nematoda', 'Nervous System Physiology', 'Organism', 'Outcome', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protocols documentation', 'Pump', 'Regimen', 'Reporting', 'Research', 'Staging', 'Swimming', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translating', 'Work', 'aging population', 'anti aging', 'base', 'cognitive function', 'design', 'drug discovery', 'experience', 'fascinate', 'healthy aging', 'immune function', 'improved', 'insight', 'interest', 'mimetics', 'model development', 'new technology', 'novel', 'programs', 'public health relevance', 'strength training', 'therapy design', 'tool']",NIA,TEXAS TECH UNIVERSITY,R21,2015,235630,-0.0360399032990401
"Quantitative microscopy-based rapid phenotyping and screening ﻿    DESCRIPTION:  Synapses are most fundamental to the function of a nervous system. C. elegans is an excellent genetic model system for finding genes and elucidating pathways because of its sequenced genome and the abundance of molecular biology tools and mutants. Due to the simplicity of its nervous system, many breakthroughs have been made in C. elegans for understanding molecular mechanisms in the patterning of the nervous system and synapse development. The current bottlenecks are in the manual and non-quantitative techniques such as visual screens, limiting both the throughput of the experiments and the phenotypes one can examine. Our long-term objective is to develop technologies and to understand how genes, age, and the environment together define and continue to remodel the nervous system of an organism. In the last funding period, we have made large progress in hardware system design (including microtechnologies and automation technologies) and software for quantitative characterization of phenotypes. The objective of this continuation project is to further engineer superior micro devices for large-scale live imaging and quantitative imaging technologies, and combine with the power of genetic and genomic approaches to study synapse development in this in vivo system; genes and pathways emerging from this study could potentially become targets of therapeutics in neurological disorders.  We have shown in the previous phase of the project that quantitative microscopy-based approaches can indeed enable identification of novel genes and pathways that conventional approaches cannot. In the continuation phase, we will further optimize on-chip rapid and high-content in vivo imaging techniques, and in parallel further develop algorithms and quantitative measures for the analysis of such high-content data; we will screen based on novel synthetic phenotype unobservable by eye; we will also exploit powerful genomic techniques to identify loci and potential multigenic interactions that shape the synapse morphology. These experimental approaches will identify genes that cannot have been identified otherwise because of the difficulties associated with the phenotypical profiling, but addressed using our engineered techniques here. The approach is innovative because the technology developed here dramatically increases the throughput, sensitivity, and accuracy of the experiments, and truly enables the utility of extremely powerful genetic and genomic methods. The proposed research is significant because it fills the urgent need in high-throughput and high-content screens as well as identifying novel genes and pathways. In addition, besides the contribution to the specific neurobiology, the technologies are widely applicable to areas such as developmental cell biology, and to other small organisms such as fly larvae and zebrafish embryos.         PUBLIC HEALTH RELEVANCE:   Synapse development is an important and active area of research linking genes and environments to the formation and maintenance of synapses in the nervous system. It has direct implications in many human diseases developmental and psychiatric diseases such as Autism Spectrum Disorder and Schizophrenia.                ",Quantitative microscopy-based rapid phenotyping and screening,8964929,R01GM088333,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alleles', 'Animals', 'Area', 'Automation', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Chromosome Mapping', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Development', 'Developmental Cell Biology', 'Devices', 'Disease', 'Drosophila genus', 'Embryo', 'Engineering', 'Environment', 'Event', 'Eye', 'Fill-It', 'Fluorescence', 'Funding', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Research', 'Genetic Screening', 'Genomic approach', 'Genomics', 'Goals', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Imaging technology', 'Inbreeding', 'Larva', 'Lead', 'Life', 'Link', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Microfluidics', 'Microscopy', 'Molecular', 'Molecular Biology', 'Morphology', 'Nematoda', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurons', 'Organism', 'Pathway interactions', 'Pattern', 'Phase', 'Phenotype', 'Phylogeny', 'Positioning Attribute', 'Quantitative Microscopy', 'Quantitative Trait Loci', 'Regulatory Pathway', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Speed', 'Synapses', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Vision', 'Visual', 'Work', 'Zebrafish', 'autism spectrum disorder', 'base', 'design', 'experience', 'fly', 'forward genetics', 'genetic approach', 'genome sequencing', 'high throughput screening', 'human disease', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'mutant', 'nerve injury', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'quantitative imaging', 'research study', 'screening', 'success', 'synaptogenesis', 'technology development', 'therapeutic target', 'tool', 'trait']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2015,307094,-0.040350359363126954
"Translating from Rats to Humans: A Human Foraging Model of Decision-Making No abstract available PUBLIC HEALTH RELEVANCE:  Animal models of addiction are some of the most highly regarded models of psychopathology; however, there remains a disconnection between these pre-clinical models and treatment outcomes.  This division may be, in part, due to untested assumptions that different species recruit the same cognitive systems in the compared tasks.  The novel research outlined in this proposal will help improve the translation between human and nonhuman animal models, and in turn, improve treatment efficacy for these disorders.                ",Translating from Rats to Humans: A Human Foraging Model of Decision-Making,8977868,F31DA040335,"['Accounting', 'Advanced Development', 'Aggressive behavior', 'Alcohol or Other Drugs use', 'Animal Model', 'Animals', 'Back', 'Basic Science', 'Behavior', 'Behavioral', 'Behavioral Mechanisms', 'Categories', 'Chronic', 'Clinical', 'Clinical Psychology', 'Clinical Research', 'Clinical Sciences', 'Clip', 'Cognitive', 'Data', 'Decision Making', 'Development', 'Disease', 'Disinhibition', 'Economics', 'Emotional', 'Evaluation', 'Event', 'Exhibits', 'Faculty', 'Food', 'Human', 'Impulsivity', 'Individual Differences', 'Internet', 'Intervention', 'Investigation', 'Linear Models', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Maps', 'Measures', 'Mentorship', 'Methods', 'Minnesota', 'Modeling', 'Neurosciences', 'Participant', 'Patient Self-Report', 'Pattern', 'Pre-Clinical Model', 'Process', 'Psychology', 'Psychopathology', 'Rattus', 'Recruitment Activity', 'Regrets', 'Research', 'Research Personnel', 'Resources', 'Restaurants', 'Rewards', 'Sampling', 'Series', 'Staging', 'Stimulus', 'Students', 'Substance abuse problem', 'System', 'Task Performances', 'Testing', 'Time', 'Translating', 'Translations', 'Travel', 'Treatment Efficacy', 'Treatment outcome', 'Universities', 'Variant', 'Ventral Striatum', 'Work', 'addiction', 'analog', 'base', 'behavior test', 'cognitive process', 'cognitive system', 'design', 'effective therapy', 'experience', 'graduate student', 'human ethology', 'human subject', 'improved', 'neuroimaging', 'neurophysiology', 'novel', 'pleasure', 'pre-clinical', 'preference', 'public health relevance', 'relating to nervous system', 'research study', 'stem', 'trait', 'trait impulsivity', 'willingness']",NIDA,UNIVERSITY OF MINNESOTA,F31,2015,38875,-0.010564313251465084
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8685211,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2014,69189,-0.029380829107733143
"Computer-aided detection of non-calcified plaques in coronary CT angiograms   Cardiovascular disease is the leading cause of death in both men and women in the United States. Over 16 million Americans have coronary heart disease (CHD), causing about 0.5 million deaths each year. The most common CHD is coronary artery disease which is mainly caused by atherosclerosis. Clinical evidence in recent years shows that noncalcified plaques (NCPs) are more vulnerable to rupture than calcified plaques. Plaque rupture and the thrombosis that follows is the main cause of acute myocardial infarction. Multidetector coronary CT angiography (cCTA) has the potential to help clinicians in early detection and in quantification of NCPs. cCTA may thus be useful for CHD detection, risk stratification, monitoring, and evaluation of the effectiveness of risk reduction treatment. However, many of these potential applications have not been utilized clinically.  The goal of this project is to develop a computer-aided detection (CADe) system to serve as a second reader for assisting clinicians in detection and quantification of NCPs in cCTA exams. Our specific aims are to (1) develop machine learning methods for detection of NCPs causing stenosis and/or positive remodeling along coronary arteries, and (2) evaluate the effect of CADe on radiologists' detection of NCPs on cCTA by observer ROC study. To achieve these aims, we will collect a database of cCTA cases for training and testing the CADe system, define the search space by designing 3D multiscale coronary artery response enhancement, segmentation, and dynamic balloon vessel tracking methods, develop a unique vessel- stitching method to automatically identify the best-quality phase for each individual artery segment from all available phases in prospectively or retrospectively gated cCTA exams, develop innovative vessel-sector- profile analysis and vessel lumen analysis to detect NCPs that cause stenosis or positive remodeling, estimate the total NCP volume, and explore calibration method to quantify plaque density by phantom studies. To demonstrate the usefulness of CADe, a preclinical reader study will be conducted to compare radiologists' detection accuracy of NCPs with and without CADe.  The major innovations of this project include (1) being the first CADe system to automatically detect non-calcified plaques including those cause positive remodeling or stenosis in cCTA, (2) development of new machine learning techniques including the vessel-stitching method, vessel-sector-profile analysis, multiscale enhancement response, and dynamic balloon tracking specifically suited for coronary arterial trees, and (3) conducting the first ROC study to evaluate the effect of CADe on radiologists' detection of NCPs.  Narrative:  Cardiovascular disease is the leading cause of death in both men and women in the United States. If the proposed CADe system is successfully developed, it will (1) provide an accurate, efficient, and consistent tool to assist clinicians in detecting and quantifying non-calcified plaques (NCP) including those causing positive remodeling and/or stenosis for an individual patient, (2) help clinicians estimate the total NCP burden and study its significance, in analogy to that of the total calcium score, which may lead to improved clinical management of the vulnerable plaques, and (3) accelerate studies to develop new treatment options of coronary heart disease by providing a monitoring tool of treatment response. The proposed CADe system can thus serve as a foundation for these broader future applications and improve the efficacy of cCTA. Improved detection and management of NCPs may reduce the risk of myocardial infarctions and will have strong and long-lasting impact on health care.",Computer-aided detection of non-calcified plaques in coronary CT angiograms,8586273,R01HL106545,"['Acute myocardial infarction', 'American', 'Angiography', 'Arterial Fatty Streak', 'Arteries', 'Atherosclerosis', 'Calcified', 'Calcium', 'Calibration', 'Cardiovascular Diseases', 'Catheters', 'Cause of Death', 'Cessation of life', 'Clinical', 'Clinical Management', 'Computer Vision Systems', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Coronary heart disease', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Dose', 'Early Diagnosis', 'Effectiveness', 'Electrocardiogram', 'Evaluation', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Lead', 'Machine Learning', 'Methods', 'Modality', 'Monitor', 'Myocardial Infarction', 'Patients', 'Performance', 'Phase', 'Procedures', 'Radiation', 'Reader', 'Receiver Operating Characteristics', 'Recording of previous events', 'Resolution', 'Risk', 'Risk Reduction', 'Rupture', 'Scanning', 'Stenosis', 'Stratification', 'System', 'Techniques', 'Testing', 'Thrombosis', 'Time', 'Training', 'Trees', 'Ultrasonography', 'United States', 'Visual', 'Woman', 'computer aided detection', 'density', 'design', 'detector', 'improved', 'innovation', 'men', 'pre-clinical', 'prospective', 'radiologist', 'response', 'tool', 'treatment response', 'virtual']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2014,610289,-0.032321710309991404
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8669000,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'inquiry-based learning', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2014,293329,-0.03143554284507681
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,-0.017540050136525164
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8657051,R01GM053163,"['Address', 'Algorithms', 'Biochemical', 'Budgets', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Development', 'Drug Design', 'Evaluation', 'Funding', 'Geometry', 'Goals', 'Image', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Output', 'Pattern', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Relative (related person)', 'Research', 'Shapes', 'Signal Transduction', 'Solutions', 'Specimen', 'Spottings', 'Structure', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2014,320096,-0.01902264064424372
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,-0.01542602740227815
"Automated Web-Based Behavioral Diagnostics of Cognitive Impairment    DESCRIPTION (provided by applicant): Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based method for early diagnosis of cognitive decline and memory loss. We will build on current research that has demonstrated a clear link between performance on the Visual Paired- Comparison Task (VPC) and diagnosis of MCI, and we will extend the impact of this finding by developing a suite of software tools and analysis methods that will improve the accuracy and extend the accessibility of cognitive diagnostics with a web-based VPC task that can be widely deployed. Although the VPC task is promising as a diagnostic aid, clinical application is severely limited by the need to use an eye tracker to precisely monitor subjects' eye movements. Unfortunately, eye trackers are expensive, require trained personnel, and are not widely available. However, our preliminary findings show that it is possible to produce picture examination behavior that is similar to eye-movements, using modified versions of the VPC task that could be administered by anyone, on any computer with an internet connection. In addition, powerful machine learning techniques from computer science can help accurately analyze and diagnose the resulting behavior. An important contribution from this work will be the possibility of predicting oncoming cognitive decline in MCI patients sooner than is now possible, that is, at a time when the nervous system is less compromised and more likely to benefit from therapeutic intervention. In support of this objective, we propose three aims: 1) Develop and investigate appropriate representation of eye movement characteristics and corresponding machine learning-based classification techniques for effective identification of the patient status, 2) Develop and validate a web-based version of the VPC task, and 3) explore the feasibility of our task as an automatic screening instrument for the general elderly population. Successful completion of these aims has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable thousands and potentially millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.        Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based, and widely accessible behavioral screening test for early diagnosis of cognitive decline and memory loss. Successful completion of this project has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.         ",Automated Web-Based Behavioral Diagnostics of Cognitive Impairment,8704932,R01EB014266,"['Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Behavior', 'Behavioral', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Computer software', 'Computers', 'Data', 'Dementia', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Elderly', 'Exhibits', 'Eye', 'Eye Movements', 'Future', 'General Practitioners', 'Goals', 'Human Resources', 'Image', 'Impaired cognition', 'Individual', 'Internet', 'Length', 'Link', 'Machine Learning', 'Measures', 'Memory', 'Memory Loss', 'Memory impairment', 'Methods', 'Monitor', 'Movement', 'Mus', 'Nervous system structure', 'Neurodegenerative Disorders', 'Online Systems', 'Paired Comparison', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Proxy', 'Recruitment Activity', 'Research', 'Research Subjects', 'Saccades', 'Software Tools', 'Specificity', 'Stimulus', 'Structure', 'Supervision', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Translational Research', 'Visual', 'Work', 'aged', 'base', 'checkup examination', 'clinical application', 'clinical practice', 'cognitive neuroscience', 'computer science', 'cost', 'cost effective', 'gaze', 'improved', 'instrument', 'memory recognition', 'mild cognitive impairment', 'novel', 'older patient', 'preference', 'sample fixation', 'screening', 'tool', 'tool development', 'visual adaptation']",NIBIB,EMORY UNIVERSITY,R01,2014,338287,-0.037641819229788466
"Hand Sensorimotor Function and Carpal Tunnel Syndrome    DESCRIPTION (provided by applicant): The median nerve is susceptible to compression in the wrist, leading to carpal tunnel syndrome (CTS). CTS is the most common compression neuropathy and have an immense impact on national health care, worker productivity, and quality of life. Despite its high prevalence and public health cost, our understanding of CTS is limited, and the management of CTS awaits improvement. The central notion of this project is that hand sensorimotor function is sensitive to peripheral median neuropathy and that the central nervous system is affected by CTS, causing the associated sensorimotor deficit. We will investigate this notion with quantifiable sensorimotor data from novel biomechanical and neurophysiological studies. This project has three aims consisting of biomechanical, neurophysiological and translational research. The first aim is to investigate CTS-induced pathokinematic and pathokinetic performance using dexterous manual tasks of thumb opposition, reach-to-pinch, precision grip, and finger pressing. The second aim is to investigate the neurophysiological implications of chronic peripheral neuropathy (i.e., CTS) on the central nervous system by evaluating corticomuscular coupling and stretch reflex. The third aim is to identify novel biomechanical and neurophysiological markers for CTS cases using machine learning and classification algorithms. The results of this project will elucidate the pathological mechanisms and behavioral manifestations of CTS and aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder. More generally, CTS as a chronic neuropathy serves as an effective model to study sensorimotor mechanisms of the peripheral and central nervous systems. In addition, the methodology developed in this project is applicable to other neuromuscular disorders.       PUBLIC HEALTH RELEVANCE:   Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.               ",Hand Sensorimotor Function and Carpal Tunnel Syndrome,8627111,R01AR056964,"['Abnormal coordination', 'Affect', 'Algorithms', 'Behavioral Mechanisms', 'Biomechanics', 'Carpal Tunnel Syndrome', 'Carpometacarpal joint structure', 'Chronic', 'Classification', 'Clinical', 'Coupling', 'Data', 'Development', 'Disease', 'Drops', 'Electroencephalography', 'Electromyography', 'Exertion', 'Eye', 'Fingers', 'Hand', 'Health Care Costs', 'Health Personnel', 'High Prevalence', 'Human', 'Individual', 'Joints', 'Lasso', 'Machine Learning', 'Manuals', 'Measures', 'Median Neuropathy', 'Metacarpophalangeal joint structure', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neural Conduction', 'Neuraxis', 'Neuromuscular Diseases', 'Neuropathy', 'Patients', 'Performance', 'Peripheral', 'Peripheral Nervous System Diseases', 'Production', 'Productivity', 'Pronation', 'Public Health', 'Quality of life', 'Questionnaires', 'Reaction', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Response Latencies', 'Sensorimotor functions', 'Sensory', 'Techniques', 'Testing', 'Thumb structure', 'Time', 'Translational Research', 'Trees', 'Validation', 'Wrist', 'data mining', 'diagnosis evaluation', 'experience', 'grasp', 'improved', 'indexing', 'median nerve', 'motor control', 'neurophysiology', 'novel', 'programs', 'public health relevance', 'research study', 'response', 'stretch reflex', 'tool', 'vector']",NIAMS,CLEVELAND CLINIC LERNER COM-CWRU,R01,2014,346185,-0.007767572347284102
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8689173,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2014,202714,-0.010585404869905822
"Scalable Biomedical Pattern Recognition Via Deep Learning DESCRIPTION (provided by applicant): Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.  The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is  being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.  Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.  In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes. Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,9302040,R21LM011664,[' '],NLM,VANDERBILT UNIVERSITY MEDICAL CENTER,R21,2014,7696,-0.010585404869905822
"Identifying Huntington's disease markers by modern statistical learning methods.     DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD.         PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.            ",Identifying Huntington's disease markers by modern statistical learning methods.,8721037,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Huntington Disease', 'Image', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Relative (related person)', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'human Huntingtin protein', 'improved', 'indexing', 'interest', 'meetings', 'nervous system disorder', 'novel', 'public health relevance', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2014,359539,-0.015531066100047139
"Predictive Model of Chronic Kidney Disease in a Hispanic Population     DESCRIPTION (provided by applicant): This career transition award will provide the applicant with protected research time to develop an independent research career in biomedical informatics. Dr. Starkey received his M.D. in 2003 from UT Southwestern, completed the NLM Biomedical Informatics Training Program post-doctoral fellowship in 2011 and earned a Ph.D. focusing on clinical informatics from the Clinical Sciences Degree Program at the University of Texas Medical Branch (UTMB) at Galveston in 2012. Dr. Starkey accepted a faculty position at the Institute for Translational Sciences at UTMB that is currently supporting his transition to independent investigator. Dr. Starkey will utilize the career transition award for novel applications of advanced machine learning methods to create predictive models of disease. Predictive models of disease allow for the stratification of risk and prevention of disease and have been successfully implemented in the assessment of cardiovascular disease risk. Predictive models of chronic kidney disease (CKD) are an active area of research since CKD is a risk factor for all-cause mortality, cardiovascular death and end-stage renal disease. However, there is not a single predictive model of CKD created for application to Hispanics and the external validity of existing models is poor. The objective of this project is to create predictive models of chronic kidney disease in a Hispanic population and quantify biomarkers of chronic kidney disease using multiple reaction monitoring (MRM) proteomics in this minority population. MRM proteomics will utilize the UTMB Novel Methodologies core and its array of resources to create an inter-institutional collaboration. Heterogeneous ensemble machine learning methods will be used to create the predictive model of chronic kidney disease in the Cameron County Hispanic Cohort (CCHC). Established in 2004, the CCHC is a random population sample in Brownsville, Texas created to evaluate the determinants of health in a US/Mexico border population that is primarily of Hispanic ethnicity. The CCHC has an extremely high prevalence of obesity and diabetes at 49.7% and 30.3%, respectively. Both obesity and diabetes are independent risk factors for the development of CKD and represent significant health disparities in the Hispanic population. Archived serum samples of CCHC participants and the CCHC database will be utilized to complete the following aims: 1) Create predictive models of CKD in a Hispanic population 2) Refine the predictive models by including clinical laboratory data and a selective reaction monitoring mass spectrometry panel of biomarkers. At the completion of this project, predictive models of CKD applicable to a Hispanic population will be created and MRM proteomics will demonstrate utility of biomarkers identified in other populations. The application of the results will be used to create clinical tools for CKD risk and guide future studies in this Hispanic population with health disparities. Importantly, it will provide a career transition for D. Starkey to demonstrate the knowledge gained during his training that results in impactful research and publications.              PROJECT NARRATIVE:  The proposed research is relevant to public health because it allows stratification of chronic kidney disease risk and quantifies biomarkers of chronic kidney disease in the Hispanic minority whom has significant health disparities with regard to kidney disease. The translation of selective reaction monitoring of biomarkers and development of heterogeneous ensemble machine learning methods for application to the prediction of chronic kidney disease in a minority population supports the NLM mission.",Predictive Model of Chronic Kidney Disease in a Hispanic Population,8700987,K22LM011869,"['Age', 'Albumins', 'Archives', 'Area', 'Atherosclerosis', 'Biological Markers', 'Cardiovascular system', 'Career Transition Award', 'Cessation of life', 'Chronic Kidney Failure', 'Clinical', 'Clinical Informatics', 'Clinical Sciences', 'Collaborations', 'Communities', 'County', 'Creatinine', 'Data', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Dialysis procedure', 'Disease', 'Disease Outcome', 'Doctor of Medicine', 'Doctor of Philosophy', 'End stage renal failure', 'Ethnic Origin', 'Faculty', 'Fellowship', 'Funding', 'Future', 'General Population', 'Goals', 'Health', 'High Prevalence', 'Hispanics', 'Hospitalization', 'Household', 'Incidence', 'Individual', 'Institutes', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Mass Spectrum Analysis', 'Medical', 'Methodology', 'Methods', 'Mexican', 'Mexico', 'Minority', 'Mission', 'Modeling', 'Monitor', 'Obesity', 'Participant', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Prevalence', 'Probability', 'Process', 'Proteomics', 'Public Health', 'Publications', 'Reaction', 'Recruitment Activity', 'Renal function', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Sampling', 'Serum', 'Specimen', 'Stratification', 'Texas', 'Time', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transplantation', 'Universities', 'Validation', 'Visit', 'Weight', 'Work', 'biomedical informatics', 'cardiovascular disorder risk', 'career', 'cohort', 'cost effective', 'design', 'disorder prevention', 'disorder risk', 'follow-up', 'forest', 'health disparity', 'improved', 'informatics training', 'innovation', 'medically underserved', 'mortality', 'multiple reaction monitoring', 'novel', 'predictive modeling', 'programs', 'prospective', 'screening', 'skills', 'tool', 'urinary']",NLM,UNIVERSITY OF TEXAS MED BR GALVESTON,K22,2014,132121,-0.04306064279083741
"Protect Privacy of Healthcare Data in the Cloud     DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier.             Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8810023,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2014,183155,-0.014040351914158564
"Models for synthesising molecular, clinical and epidemiological data, and transla     DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)?         PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.            ","Models for synthesising molecular, clinical and epidemiological data, and transla",8703195,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2014,427668,-0.012266676431651328
"Heterogeneous and Robust Survival Analysis in Genomic Studies     DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed.         PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.            ",Heterogeneous and Robust Survival Analysis in Genomic Studies,8696520,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,255295,-0.019286249884763913
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8829434,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Coronavirus', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency Situation', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syndrome', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'respiratory', 'response', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,BOSTON CHILDREN'S HOSPITAL,K01,2014,42469,-0.02467302379297667
"Automated Assessment of Structural Changes & Functional Recovery Post Spinal Inju    DESCRIPTION (provided by applicant): Establishing structure-function correlations is fundamental to understanding how information is processed in the central nervous system (CNS). Axonal connectivity is a key relationship that facilitates information transmission and reception within the CNS. Recently, diffusion weighted magnetic resonance imaging (DW-MRI) methods have been shown to provide fundamental information required for viewing structural connectivity and have allowed visualization of fiber bundles in the CNS in vivo. In this project, we propose to develop methods for extraction and analysis of these patterns from high angular resolution diffusion weighted images (HARDI) that is known to have better resolving power over diffusion tensor imaging (DTI). To this end, a biologically relevant and clinically important model has been chosen to study changes in the organization of fibers in the intact and injured spinal cord. Our hypothesis is that, changes in geometrical properties of the anatomical substrate, identifying the region of injury and neuroplastic changes in distant spinal segments, correlate with different magnitudes of injury and levels of locomotor recovery following spinal cord injury (SCI). Prior to hypothesis testing, we will denoise the HARDI data and then construct a normal atlas cord. Deformable registration and tensor morphometry between a normal atlas and an injured cord would be performed to provide a distinct signature for each type of behavior recovery associated with the SCI substrate. Validation of the hypothesis will be performed through systematic histological analysis of cord samples following acquisition of the HARDI data. Spinal cords will be cut and stained with fiber and cell stains to verify changes in anatomical organization that result from contusive injury (common in humans as well) to the spinal cord. A comparison between anatomical characteristics obtained from histological versus HARDI analysis will provide validation for the image analysis and the hypothesis. Three severities of spinal cord injuries will be produced (light, mild and moderate contusions) based upon normed injury device parameters. The structural signatures of these labeled data subsets will then be identified. Automatic classification of novel & injured cord HARDI data sets will then be achieved using a large margin classifier. Finally, HARDI data acquired over time will be analyzed in order to learn and predict the level of locomotor recovery by studying the structural changes over time and developing a dynamic model of structural transformations corresponding to each chosen class. We will use an auto-regressive model in the feature space to track and predict structural changes in SCI and correlate it to functional recovery.        PUBLIC HEALTH RELEVANCE: This project involves the development of automated methods to extract morphological signatures that characterize changes in spinal cord injury (SCI) substrate estimated from Diffusion MRI scans of rats, and predict the functional recovery by correlating to behavioral studies. Although the various algorithms developed here are for analysis of SCI, they can be used in other applications such as traumatic brain injury, in tracking and predicting developmental changes etc. from diffusion MRI scans.               ",Automated Assessment of Structural Changes & Functional Recovery Post Spinal Inju,8628880,R01NS066340,"['Algebraic Geometry', 'Algorithms', 'Atlases', 'Behavior', 'Behavioral', 'Categories', 'Characteristics', 'Classification', 'Communities', 'Contusions', 'Data', 'Data Collection', 'Data Set', 'Development', 'Devices', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Distant', 'Fiber', 'Geometry', 'Goals', 'Human', 'Image Analysis', 'Imagery', 'In Vitro', 'Injury', 'Label', 'Learning', 'Left', 'Light', 'Literature', 'Locomotion', 'Locomotor Recovery', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Metric', 'Modeling', 'Motor', 'Neuraxis', 'Pattern', 'Population', 'Population Control', 'Population Registers', 'Probability', 'Process', 'Property', 'Rattus', 'Recovery', 'Recovery of Function', 'Research', 'Resolution', 'Sampling', 'Sensory', 'Severities', 'Solutions', 'Sorting - Cell Movement', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Staging', 'Staining method', 'Stains', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Time', 'Traumatic Brain Injury', 'Validation', 'Water', 'Weight', 'base', 'clinically relevant', 'density', 'expectation', 'fiber cell', 'imaging modality', 'in vivo', 'injured', 'interest', 'member', 'morphometry', 'novel', 'public health relevance', 'sensor', 'statistics', 'transmission process']",NINDS,UNIVERSITY OF FLORIDA,R01,2014,494546,-0.03147870516214184
"Statistical methods for biosignals with varying domains     DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2         PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4                ",Statistical methods for biosignals with varying domains,8742367,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'public health relevance', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2014,419500,-0.01736577666274286
"Data-Driven Statistical Learning with Applications to Genomics     DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software.         PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.            ",Data-Driven Statistical Learning with Applications to Genomics,8796068,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Simulate', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'high throughput analysis', 'interest', 'novel', 'patient population', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2014,361063,-0.01536988530083576
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.        The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8601076,R01EY022039,"['Address', 'Algorithm Design', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,379750,-0.020468810283714278
"Statistical Methods for Selection and Evaluation of Biomarkers     DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particulr, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pan- creatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public.         PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.            ",Statistical Methods for Selection and Evaluation of Biomarkers,8660307,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Pancreas', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2014,324364,-0.06914620178211436
"In vivo Characterization of Stents using Intravascular OCT Imaging     DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel.         PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,8724992,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Simulate', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thrombosis', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'public health relevance', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2014,402534,-0.015109290650945049
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.          The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8650344,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2014,536206,-0.01254290092766011
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler     DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS.         PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.            ",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,8695570,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Individual', 'Lateral', 'Lead', 'Life', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Metric', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'prevent', 'prognostic', 'public health relevance', 'response', 'screening', 'spinal tract', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2014,616625,-0.01672907458400965
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development     DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation.             n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8697162,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Simulate', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2014,333533,-0.01064382066116746
"Statistical methods for large and complex databases of ultra-high-dimensional     DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences.         PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8738735,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2014,343683,-0.02024329832363095
"Scalable Assays for Morphological Analysis of Mammalian Neurons    DESCRIPTION (provided by applicant): Medium and high-throughput assays (i.e., screens) have generally not been applied to mammalian neurons because of the difficulties in culturing them in large numbers and because of the low efficiency with which the genetic makeup of neurons can be altered. Furthermore, because many aspects of neuronal function can only be assayed with electrophysiological assays, follow-up analysis and validation of screening hits is difficult. We propose to use automated imaging approaches to analyze synapse number and neuronal structure in vitro in a scalable format. We have implemented tissue culture and immunostaining approaches to monitor the number and types of synapses formed onto neurons in multi-well plates. We will couple this analysis with lentivirus mediated introduction of short-hairpin RNAs to induce RNA interference against genes expressed in neurons. This will be performed in concert with transcriptional analysis of neurons to determine the key changes in gene expression that correlate with structural and synaptic changes. The proposal represents a significant collaboration between several groups with expertise in functional analysis of neurons, automated analysis of images, viral mediated manipulation of gene expression, and whole-genome transcriptional analysis. We hope that our work will lead, for the first time, to a turn-key and robust method of analysis of neuron and synapse structure suitable for scalable, whole-genome analysis. Such a system will permit the unbiased and systematic analysis of pathways involved in neuropsychiatric diseases including neurodegenerative diseases such as Alzheimer's and Parkinson's as well as neurodevelopmental disorders such as mental retardation and autism.        Massively parallel analysis of cells in many conditions has allowed the discovery of key pathways that control cell function. Unfortunately, these techniques have not been applied to neurons due to difficulties in handling, manipulating, and analyzing large numbers of brain cells. We propose to develop imaging-based techniques to analyze neurons in dishes at a high throughput in order to find pathways that control their development and susceptibility to disease.            ",Scalable Assays for Morphological Analysis of Mammalian Neurons,8657123,R01NS077907,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Antibodies', 'Autistic Disorder', 'Axon', 'Benchmarking', 'Biochemical Pathway', 'Biological Assay', 'Cell Line', 'Cell physiology', 'Cells', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Dendrites', 'Development', 'Disease', 'Ensure', 'Excitatory Synapse', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Image', 'Image Analysis', 'In Vitro', 'Inhibitory Synapse', 'Institutes', 'Label', 'Lead', 'Machine Learning', 'Measurement', 'Mediating', 'Mental Retardation', 'Methods', 'Mining', 'Mitochondria', 'Monitor', 'Morphology', 'Mus', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuroglia', 'Neurons', 'Organelles', 'Parkinson Disease', 'Pathway Analysis', 'Pathway interactions', 'Phenotype', 'Predisposition', 'Procedures', 'Process', 'Protocols documentation', 'RNA Interference', 'Relative (related person)', 'Structure', 'Subfamily lentivirinae', 'Synapses', 'System', 'Techniques', 'Time', 'Transfection', 'Validation', 'Viral', 'Work', 'base', 'bioimaging', 'brain cell', 'density', 'design', 'fluorescence imaging', 'follow-up', 'genome analysis', 'genome-wide', 'high throughput screening', 'medical schools', 'neuropsychiatry', 'open source', 'prototype', 'scale up', 'screening', 'small hairpin RNA', 'synaptogenesis', 'tissue culture', 'tool']",NINDS,HARVARD MEDICAL SCHOOL,R01,2014,419513,-0.039514906304702134
"Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy     DESCRIPTION (provided by applicant): Our understanding of cervical remodeling during pregnancy and labor is incomplete, partly due to the lack of in vivo studies on the biochemical changes that occur in the cervix over the course of pregnancy. Elucidation of the mechanisms for cervical ripening could be used to predict the onset of preterm labor. Until recently, in vivo research methods were too invasive to be used as discovery tools, particularly in women who present with preterm labor. This proposal will use in vivo Raman spectroscopy, an optical technique that is sensitive to collagen content, collagen structure, hydration, lipids, proteins, ad other biomolecules to non-invasively investigate the biochemistry of the cervix throughout pregnancy. Using fiber optic in vivo Raman spectroscopy, we recently found significant differences in Raman spectra in at least four important peaks during the course of pregnancy in mice, including discrete signatures for lipids, collagen, amide bonds, and enriched amino acids (proline, tyrosine). Computational analysis of these spectra yielded predictive algorithms with 94% classification accuracy for stage of pregnancy. Studies performed in 2-hour windows at the end of pregnancy identified spectra predictive for the timing of parturition. This approach provides a detailed real-time biomolecular map of cervical ripening that is currently unavailable by other means. In this proposal, we hypothesize that the different mechanisms of premature cervical ripening have unique Raman spectral signatures that correspond to underlying biochemical and mechanical changes that precede preterm birth, which can be detected in vivo. Two Specific Aims are proposed: 1) Determine spectral changes in the cervix of mice with normal and abnormal pregnancy and parturition; 2) Identify specific mediators of cervical remodeling by comparing Raman spectra to mechanical and biochemical changes in the ex vivo cervix during normal and abnormal parturition. Raman spectroscopy has primarily been used for detection of disease. Collaboration between our reproductive biology and bioengineering groups will capitalize on our expertise in Raman analysis of cervical tissues to study dynamic changes in cervix composition during pregnancy. Key elements in cervical biochemistry will be identified. In vivo Raman spectroscopy will be combined with biomechanical studies and imaging mass spectrometry, a powerful tool for in situ proteomic analysis, to examine mice with premature or delayed cervical remodeling. Together, these highly innovative approaches will generate in-depth profiles of cervical biology that will translate into novel non-invasive methods to detect impending premature birth in women.         PUBLIC HEALTH RELEVANCE: This proposal will use Raman Spectroscopy, a non-invasive, optical scattering technique, to investigate the composition of the cervix throughout pregnancy and provide detailed real-time information on cervical ripening. These studies will identify spectral differences in the cervix during normal and abnormal cervical maturation; optical and biochemical markers will be identified to help monitor pregnancy non-invasively, as the fiber optic probe only requires brief contact with the external surface of the cervix to obtain measurements. Elucidating the mechanisms that initiate cervical ripening will provide a critical step for early detection and treatment of preterm birth, which is the leading cause of infant morbidity and mortality.            ",Detecting biochemical changes in the pregnant mouse cervix by Raman spectroscopy,8766404,R01HD081121,"['Address', 'Algorithms', 'Alprostadil', 'Amides', 'Amino Acids', 'Applications Grants', 'Biochemical', 'Biochemical Markers', 'Biochemistry', 'Biological', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biomechanics', 'Biomedical Engineering', 'Birth', 'Cervical', 'Cervical Ripening', 'Cervix Uteri', 'Classification', 'Clinical', 'Collaborations', 'Collagen', 'Computational algorithm', 'Computer Analysis', 'Data', 'Detection', 'Development', 'Disease', 'Early Diagnosis', 'Early treatment', 'Elasticity', 'Emerging Technologies', 'Etiology', 'Fetal Development', 'Fiber Optics', 'Foundations', 'Generations', 'Goals', 'Health', 'High-Risk Pregnancy', 'Hormonal', 'Hour', 'Hydration status', 'Image', 'Immunohistochemistry', 'In Situ', 'In Situ Hybridization', 'Indium', 'Interdisciplinary Study', 'Laboratories', 'Lead', 'Link', 'Lipids', 'Machine Learning', 'Maintenance', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Mechanics', 'Mediator of activation protein', 'Medical', 'Methods', 'Mifepristone', 'Modality', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mus', 'Optics', 'Phenotype', 'Physiological', 'Pregnancy', 'Premature Birth', 'Premature Labor', 'Prevention', 'Process', 'Proline', 'Property', 'Proteins', 'Proteomics', 'RU-486', 'Raman Spectrum Analysis', 'Reproductive Biology', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Spectrum Analysis', 'Staging', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Translating', 'Translations', 'Tyrosine', 'Woman', 'base', 'clinical application', 'in vivo', 'infant morbidity/mortality', 'innovation', 'insight', 'mouse model', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'physical science', 'predictive modeling', 'pregnant', 'premature', 'public health relevance', 'response', 'tool']",NICHD,VANDERBILT UNIVERSITY,R01,2014,387356,-0.015637589233675298
"SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic* DESCRIPTION (provided by applicant): Existing studies suggest that tissue elasticity is possibly correlated with the aggressiveness of cancers. Based on results from deformable image registration, the proposed project investigates the possibility of tracking the organs movement subject to external forces and geometric constraints, thereby deducing patient-specific tissue elasticity parameters for proactive health monitoring. The objectives of this exploratory research project are (1) to develop a computational framework based on extensive studies of a large cohort of cancer patients, in order to accurately estimate patent-specific tissu elasticity using a coupled biomechanical simulation-optimization framework on a pair of medical images (possibly from ultrasound, mammography, computed tomography scan, magnetic resonance imaging, or other imaging technologies); (2) to examine potential association between tissue elasticity in different regions with aggressiveness of know/diagnosed cancer in the corresponding regions; (3) to derive predictive models for cancer staging/grading based on recovered patient-specific tissue elasticity and other explanatory variables; (4) to design a health monitoring system based on individualized analysis of tissue elasticity for 'at-risk' groups  who are more likely to develop cancers. This proposal describes a truly ambitious effort and a bold vision that is built upon the investigators' prior scientific accomplishments and strong credentials to potentially transform existing practice to more proactive, preventive, evidence-based health monitoring for individuals at risk of developing cancers. This research is expected to make several major scientific advances. These include new algorithms for non-invasive, image-based techniques for automatic extraction of tissue elasticity parameters without force applications and/or force sensing devices, novel regression models and inference procedures for survival analysis, new force sensing devices, novel regression models and inference procedures for survival analysis, new predictive models for cancer staging and grading based on patient-specific tissue elasticity parameters, and a health monitoring system for at-risk groups based on individual tissue elasticity along with other variables. PUBLIC HEALTH RELEVANCE: Other than health monitoring, the patient-specific tissue parameters can be incorporated into medical simulators to perform patient-specific surgical planning, compute desired force-feedback for tele-surgery, design and prototype medical devices, and conduct virtual surgical training. The statistical inference techniques developed can be applicable to genetic epidemiology, health economics, and bioinformatics.",SCH: Proactive Health Monitoring Using Individualized Analysis of Tissue Elastic*,8788148,R01EB020426,"['Age', 'Aging', 'Algorithms', 'Behavioral', 'Bioinformatics', 'Biomechanics', 'Biopsy', 'Blood Tests', 'Cancer Etiology', 'Cancer Patient', 'Cancerous', 'Cause of Death', 'Cessation of life', 'Chronic', 'Clinical', 'Colorectal Cancer', 'Coupled', 'Data', 'Databases', 'Developed Countries', 'Devices', 'Diagnosis', 'Diagnostic Neoplasm Staging', 'Elastic Tissue', 'Elasticity', 'Family', 'Feedback', 'Future', 'Gleason Grade for Prostate Cancer', 'Health', 'Hepatitis', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Knowledge', 'Learning', 'Legal patent', 'Life Style', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Malignant neoplasm of lung', 'Malignant neoplasm of prostate', 'Mammary Gland Parenchyma', 'Mammary Ultrasonography', 'Mammography', 'Medical', 'Medical Device', 'Medical Imaging', 'Modeling', 'Monitor', 'Movement', 'Operative Surgical Procedures', 'Organ', 'Patient Monitoring', 'Patients', 'Personal Satisfaction', 'Physicians', 'Preventive', 'Procedures', 'Property', 'Prostate', 'Radiation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'Scientific Advances and Accomplishments', 'Screening for cancer', 'Second Primary Cancers', 'Societies', 'Staging', 'Survival Analysis', 'System', 'Techniques', 'Test Result', 'Time', 'Tissues', 'Training', 'Tumor stage', 'Ultrasonography', 'United States', 'Virus Diseases', 'Vision', 'Woman', 'X-Ray Computed Tomography', 'aging population', 'base', 'cancer diagnosis', 'cancer type', 'cohort', 'computer framework', 'design', 'evidence base', 'genetic epidemiology', 'health economics', 'image registration', 'insight', 'longitudinal analysis', 'malignant breast neoplasm', 'malignant stomach neoplasm', 'men', 'non-invasive imaging', 'novel', 'predictive modeling', 'prototype', 'simulation', 'tumor', 'virtual']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2014,223808,-0.0371042896556393
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8714054,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2014,227026,-0.017673093149009916
"Improving the Detection of Activation in High Resolution fMRI using Multivariate No abstract available PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8920855,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,CLEVELAND CLINIC LERNER COM-CWRU,R01,2014,130281,-0.028656103655820084
"Analytical Approaches to Massive Data Computation with Applications to Genomics     DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.              n/a",Analytical Approaches to Massive Data Computation with Applications to Genomics,8599823,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics']",NCI,BROWN UNIVERSITY,R01,2013,71329,-0.02943011981254032
"Computer-aided detection of non-calcified plaques in coronary CT angiograms   Cardiovascular disease is the leading cause of death in both men and women in the United States. Over 16 million Americans have coronary heart disease (CHD), causing about 0.5 million deaths each year. The most common CHD is coronary artery disease which is mainly caused by atherosclerosis. Clinical evidence in recent years shows that noncalcified plaques (NCPs) are more vulnerable to rupture than calcified plaques. Plaque rupture and the thrombosis that follows is the main cause of acute myocardial infarction. Multidetector coronary CT angiography (cCTA) has the potential to help clinicians in early detection and in quantification of NCPs. cCTA may thus be useful for CHD detection, risk stratification, monitoring, and evaluation of the effectiveness of risk reduction treatment. However, many of these potential applications have not been utilized clinically.  The goal of this project is to develop a computer-aided detection (CADe) system to serve as a second reader for assisting clinicians in detection and quantification of NCPs in cCTA exams. Our specific aims are to (1) develop machine learning methods for detection of NCPs causing stenosis and/or positive remodeling along coronary arteries, and (2) evaluate the effect of CADe on radiologists' detection of NCPs on cCTA by observer ROC study. To achieve these aims, we will collect a database of cCTA cases for training and testing the CADe system, define the search space by designing 3D multiscale coronary artery response enhancement, segmentation, and dynamic balloon vessel tracking methods, develop a unique vessel- stitching method to automatically identify the best-quality phase for each individual artery segment from all available phases in prospectively or retrospectively gated cCTA exams, develop innovative vessel-sector- profile analysis and vessel lumen analysis to detect NCPs that cause stenosis or positive remodeling, estimate the total NCP volume, and explore calibration method to quantify plaque density by phantom studies. To demonstrate the usefulness of CADe, a preclinical reader study will be conducted to compare radiologists' detection accuracy of NCPs with and without CADe.  The major innovations of this project include (1) being the first CADe system to automatically detect non-calcified plaques including those cause positive remodeling or stenosis in cCTA, (2) development of new machine learning techniques including the vessel-stitching method, vessel-sector-profile analysis, multiscale enhancement response, and dynamic balloon tracking specifically suited for coronary arterial trees, and (3) conducting the first ROC study to evaluate the effect of CADe on radiologists' detection of NCPs.  Narrative:  Cardiovascular disease is the leading cause of death in both men and women in the United States. If the proposed CADe system is successfully developed, it will (1) provide an accurate, efficient, and consistent tool to assist clinicians in detecting and quantifying non-calcified plaques (NCP) including those causing positive remodeling and/or stenosis for an individual patient, (2) help clinicians estimate the total NCP burden and study its significance, in analogy to that of the total calcium score, which may lead to improved clinical management of the vulnerable plaques, and (3) accelerate studies to develop new treatment options of coronary heart disease by providing a monitoring tool of treatment response. The proposed CADe system can thus serve as a foundation for these broader future applications and improve the efficacy of cCTA. Improved detection and management of NCPs may reduce the risk of myocardial infarctions and will have strong and long-lasting impact on health care.",Computer-aided detection of non-calcified plaques in coronary CT angiograms,8392109,R01HL106545,"['Acute myocardial infarction', 'American', 'Angiography', 'Arterial Fatty Streak', 'Arteries', 'Atherosclerosis', 'Calcified', 'Calcium', 'Calibration', 'Cardiovascular Diseases', 'Catheters', 'Cause of Death', 'Cessation of life', 'Clinical', 'Clinical Management', 'Computer Vision Systems', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Coronary heart disease', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Dose', 'Early Diagnosis', 'Effectiveness', 'Electrocardiogram', 'Evaluation', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Lead', 'Machine Learning', 'Methods', 'Modality', 'Monitor', 'Myocardial Infarction', 'Patients', 'Performance', 'Phase', 'Procedures', 'Radiation', 'Reader', 'Receiver Operating Characteristics', 'Recording of previous events', 'Resolution', 'Risk', 'Risk Reduction', 'Rupture', 'Scanning', 'Stenosis', 'Stratification', 'System', 'Techniques', 'Testing', 'Thrombosis', 'Time', 'Training', 'Trees', 'Ultrasonography', 'United States', 'Visual', 'Woman', 'computer aided detection', 'density', 'design', 'detector', 'improved', 'innovation', 'men', 'pre-clinical', 'prospective', 'radiologist', 'response', 'tool', 'treatment response', 'virtual']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2013,552793,-0.032321710309991404
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8501535,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2013,283518,-0.03143554284507681
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8470172,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2013,309127,-0.01902264064424372
"Automated Web-Based Behavioral Diagnostics of Cognitive Impairment    DESCRIPTION (provided by applicant): Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based method for early diagnosis of cognitive decline and memory loss. We will build on current research that has demonstrated a clear link between performance on the Visual Paired- Comparison Task (VPC) and diagnosis of MCI, and we will extend the impact of this finding by developing a suite of software tools and analysis methods that will improve the accuracy and extend the accessibility of cognitive diagnostics with a web-based VPC task that can be widely deployed. Although the VPC task is promising as a diagnostic aid, clinical application is severely limited by the need to use an eye tracker to precisely monitor subjects' eye movements. Unfortunately, eye trackers are expensive, require trained personnel, and are not widely available. However, our preliminary findings show that it is possible to produce picture examination behavior that is similar to eye-movements, using modified versions of the VPC task that could be administered by anyone, on any computer with an internet connection. In addition, powerful machine learning techniques from computer science can help accurately analyze and diagnose the resulting behavior. An important contribution from this work will be the possibility of predicting oncoming cognitive decline in MCI patients sooner than is now possible, that is, at a time when the nervous system is less compromised and more likely to benefit from therapeutic intervention. In support of this objective, we propose three aims: 1) Develop and investigate appropriate representation of eye movement characteristics and corresponding machine learning-based classification techniques for effective identification of the patient status, 2) Develop and validate a web-based version of the VPC task, and 3) explore the feasibility of our task as an automatic screening instrument for the general elderly population. Successful completion of these aims has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable thousands and potentially millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.        Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based, and widely accessible behavioral screening test for early diagnosis of cognitive decline and memory loss. Successful completion of this project has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.         ",Automated Web-Based Behavioral Diagnostics of Cognitive Impairment,8514601,R01EB014266,"['Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Behavior', 'Behavioral', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Computer software', 'Computers', 'Data', 'Dementia', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Elderly', 'Exhibits', 'Eye', 'Eye Movements', 'Future', 'General Practitioners', 'Goals', 'Human Resources', 'Image', 'Impaired cognition', 'Individual', 'Internet', 'Length', 'Link', 'Machine Learning', 'Measures', 'Memory', 'Memory Loss', 'Memory impairment', 'Methods', 'Monitor', 'Movement', 'Mus', 'Nervous system structure', 'Neurodegenerative Disorders', 'Online Systems', 'Paired Comparison', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Proxy', 'Recruitment Activity', 'Research', 'Research Subjects', 'Saccades', 'Software Tools', 'Specificity', 'Stimulus', 'Structure', 'Supervision', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Translational Research', 'Visual', 'Work', 'aged', 'base', 'checkup examination', 'clinical application', 'clinical practice', 'cognitive neuroscience', 'computer science', 'cost', 'cost effective', 'gaze', 'improved', 'instrument', 'memory recognition', 'mild cognitive impairment', 'novel', 'older patient', 'preference', 'sample fixation', 'screening', 'tool', 'tool development', 'visual adaptation']",NIBIB,EMORY UNIVERSITY,R01,2013,328871,-0.037641819229788466
"Hand Sensorimotor Function and Carpal Tunnel Syndrome    DESCRIPTION (provided by applicant): The median nerve is susceptible to compression in the wrist, leading to carpal tunnel syndrome (CTS). CTS is the most common compression neuropathy and have an immense impact on national health care, worker productivity, and quality of life. Despite its high prevalence and public health cost, our understanding of CTS is limited, and the management of CTS awaits improvement. The central notion of this project is that hand sensorimotor function is sensitive to peripheral median neuropathy and that the central nervous system is affected by CTS, causing the associated sensorimotor deficit. We will investigate this notion with quantifiable sensorimotor data from novel biomechanical and neurophysiological studies. This project has three aims consisting of biomechanical, neurophysiological and translational research. The first aim is to investigate CTS-induced pathokinematic and pathokinetic performance using dexterous manual tasks of thumb opposition, reach-to-pinch, precision grip, and finger pressing. The second aim is to investigate the neurophysiological implications of chronic peripheral neuropathy (i.e., CTS) on the central nervous system by evaluating corticomuscular coupling and stretch reflex. The third aim is to identify novel biomechanical and neurophysiological markers for CTS cases using machine learning and classification algorithms. The results of this project will elucidate the pathological mechanisms and behavioral manifestations of CTS and aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder. More generally, CTS as a chronic neuropathy serves as an effective model to study sensorimotor mechanisms of the peripheral and central nervous systems. In addition, the methodology developed in this project is applicable to other neuromuscular disorders.       PUBLIC HEALTH RELEVANCE:   Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.               ",Hand Sensorimotor Function and Carpal Tunnel Syndrome,8452605,R01AR056964,"['Abnormal coordination', 'Affect', 'Algorithms', 'Behavioral Mechanisms', 'Biomechanics', 'Carpal Tunnel Syndrome', 'Carpometacarpal joint structure', 'Chronic', 'Classification', 'Clinical', 'Coupling', 'Data', 'Development', 'Disease', 'Drops', 'Electroencephalography', 'Electromyography', 'Exertion', 'Eye', 'Fingers', 'Hand', 'Health Care Costs', 'Health Personnel', 'High Prevalence', 'Human', 'Individual', 'Joints', 'Lasso', 'Machine Learning', 'Manuals', 'Measures', 'Median Neuropathy', 'Metacarpophalangeal joint structure', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neural Conduction', 'Neuraxis', 'Neuromuscular Diseases', 'Neuropathy', 'Patients', 'Performance', 'Peripheral', 'Peripheral Nervous System Diseases', 'Production', 'Productivity', 'Pronation', 'Public Health', 'Quality of life', 'Questionnaires', 'Reaction', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Response Latencies', 'Sensorimotor functions', 'Sensory', 'Techniques', 'Testing', 'Thumb structure', 'Time', 'Translational Research', 'Trees', 'Validation', 'Wrist', 'data mining', 'diagnosis evaluation', 'experience', 'grasp', 'improved', 'indexing', 'median nerve', 'motor control', 'neurophysiology', 'novel', 'programs', 'public health relevance', 'research study', 'response', 'stretch reflex', 'tool', 'vector']",NIAMS,CLEVELAND CLINIC LERNER COM-CWRU,R01,2013,335588,-0.007767572347284102
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8566062,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2013,175500,-0.010585404869905822
"Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging    DESCRIPTION (provided by applicant): Multiplexed biomarker analysis is more powerful in reflecting the biological behaviors of a tumor than single biomarker analysis, but its standardization and quantification is still a challenge. Furthermore, most computer software does not provide methods for imaging and analyzing subcellular localization of biomarkers and correlating them with biological and clinical information. The objective of this project is to develop a platform which combines imaging and quantification of multiplexed immunostaining plus bioinformatics for the prediction of lymph node metastases (LNM) from the primary tumor (PT) of squamous cell carcinoma of the head and neck (SCCHN). LNM of SCCHN is a precisely defined biological phenomenon which is an ideal model to be utilized to develop this multiplexed biomarker platform (MBP). Based on our preliminary studies, we aim to test the hypothesis that that the MBP can be developed to identify the subcellular distribution and expression of multiple metastasis-related biomarkers simultaneously in PTs. Accurate quantification of these biomarkers will facilitate the prediction of metastasis from PTs. Three emerging technologies, quantum dot (QD)-based immunohistofluorescence (IHF), multispectral imaging, and machine learning will be used to test this hypothesis. Using these approaches, a platform that combines quantifying multiplexed immunostaining with biostatistics will be developed and tested for its sensitivity, specificity, and prediction power for use in the clinic. Therefore, this project fits appropriately to the scope of the NCI program announcement ""Developmental Research in Cancer Prognosis and Prediction"" (PA-09-159).  Three aims are proposed in the study. (1) To develop a multiplexed biomarker system and method based on a bulk tissue model for prediction of LNM in SCCHN PT tissues. This Aim will establish and validate an analysis methodology for multiplexed quantification of membrane and cytoplasmic staining using a new function in InForm software where subcellular localization of certain biomarkers will be specifically analyzed. Prediction of LNM based on this bulk tissue model will be achieved. (2) To develop a per-cell quantification method based on a sub-population model for prediction of LNM in SCCHN PT tissue.  The per-cell analysis results will quantified as the percentage of high risk cells from the multiplexed biomarker analyses in the same PTs. The high risk population will be correlated with LNM. The sensitivity and specificity of the prediction by the sub-population model will be compared with that of the bulk tissue model. (3) To develop and validate a nomogram with software combining clinical characterizations of metastasis as a working platform for the prediction of LNM. While the primary endpoint of Aim 1 and 2 is to correlate the three biomarkers with metastasis, other clinical factors such as differentiation status, tumor stage, and site, etc. may also correlate with LNM. The most predictive biomarker set combined with relevant clinical factors will constitute a platform with computer software that will be validated in an additional 100 SCCHN samples for prediction of LNM.        Imaging and quantifying expression and subcellular localization of multiplexed biomarkers is currently a challenge in cancer research and clinical application. This project aims to develop a platform which combines imaging and quantifying multiplexed biomarkers plus their correlation with lymph node metastasis of squamous cell carcinoma of the head and neck. This platform can be used for an assessment of LNM which is paramount for appropriate treatment planning.         ",Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging,8504823,R33CA161873,"['Behavior', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Biometry', 'Breast Cancer Cell', 'Cancer Prognosis', 'Cell membrane', 'Cells', 'Clinic', 'Clinical', 'Color', 'Colorectal Cancer', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disseminated Malignant Neoplasm', 'E-Cadherin', 'Emerging Technologies', 'Epidermal Growth Factor Receptor', 'Epithelial', 'Flow Cytometry', 'Head and Neck Squamous Cell Carcinoma', 'Human', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Literature', 'Lung', 'Machine Learning', 'Mesenchymal', 'Methodology', 'Methods', 'Modeling', 'NIH Program Announcements', 'Neoplasm Metastasis', 'Nomograms', 'Operative Surgical Procedures', 'Outcome', 'Population', 'Primary Neoplasm', 'Production', 'Quantum Dots', 'Reaction', 'Receiver Operating Characteristics', 'Research', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Specificity', 'Specimen', 'Staining method', 'Stains', 'Standardization', 'System', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Tumor Tissue', 'Tumor stage', 'Work', 'aldehyde dehydrogenases', 'anticancer research', 'base', 'cancer cell', 'cancer stem cell', 'clinical application', 'high risk', 'lymph nodes', 'model development', 'nanoparticle', 'neoplastic cell', 'novel', 'outcome forecast', 'tool', 'treatment planning', 'tumor']",NCI,EMORY UNIVERSITY,R33,2013,296962,-0.044415488839420696
"Automated Assessment of Structural Changes & Functional Recovery Post Spinal Inju    DESCRIPTION (provided by applicant): Establishing structure-function correlations is fundamental to understanding how information is processed in the central nervous system (CNS). Axonal connectivity is a key relationship that facilitates information transmission and reception within the CNS. Recently, diffusion weighted magnetic resonance imaging (DW-MRI) methods have been shown to provide fundamental information required for viewing structural connectivity and have allowed visualization of fiber bundles in the CNS in vivo. In this project, we propose to develop methods for extraction and analysis of these patterns from high angular resolution diffusion weighted images (HARDI) that is known to have better resolving power over diffusion tensor imaging (DTI). To this end, a biologically relevant and clinically important model has been chosen to study changes in the organization of fibers in the intact and injured spinal cord. Our hypothesis is that, changes in geometrical properties of the anatomical substrate, identifying the region of injury and neuroplastic changes in distant spinal segments, correlate with different magnitudes of injury and levels of locomotor recovery following spinal cord injury (SCI). Prior to hypothesis testing, we will denoise the HARDI data and then construct a normal atlas cord. Deformable registration and tensor morphometry between a normal atlas and an injured cord would be performed to provide a distinct signature for each type of behavior recovery associated with the SCI substrate. Validation of the hypothesis will be performed through systematic histological analysis of cord samples following acquisition of the HARDI data. Spinal cords will be cut and stained with fiber and cell stains to verify changes in anatomical organization that result from contusive injury (common in humans as well) to the spinal cord. A comparison between anatomical characteristics obtained from histological versus HARDI analysis will provide validation for the image analysis and the hypothesis. Three severities of spinal cord injuries will be produced (light, mild and moderate contusions) based upon normed injury device parameters. The structural signatures of these labeled data subsets will then be identified. Automatic classification of novel & injured cord HARDI data sets will then be achieved using a large margin classifier. Finally, HARDI data acquired over time will be analyzed in order to learn and predict the level of locomotor recovery by studying the structural changes over time and developing a dynamic model of structural transformations corresponding to each chosen class. We will use an auto-regressive model in the feature space to track and predict structural changes in SCI and correlate it to functional recovery.        PUBLIC HEALTH RELEVANCE: This project involves the development of automated methods to extract morphological signatures that characterize changes in spinal cord injury (SCI) substrate estimated from Diffusion MRI scans of rats, and predict the functional recovery by correlating to behavioral studies. Although the various algorithms developed here are for analysis of SCI, they can be used in other applications such as traumatic brain injury, in tracking and predicting developmental changes etc. from diffusion MRI scans.               ",Automated Assessment of Structural Changes & Functional Recovery Post Spinal Inju,8432789,R01NS066340,"['Algorithms', 'Atlases', 'Behavior', 'Behavioral', 'Categories', 'Characteristics', 'Classification', 'Communities', 'Contusions', 'Data', 'Data Collection', 'Data Set', 'Development', 'Devices', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Distant', 'Fiber', 'Goals', 'Human', 'Image Analysis', 'Imagery', 'In Vitro', 'Injury', 'Label', 'Learning', 'Left', 'Light', 'Literature', 'Locomotion', 'Locomotor Recovery', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Metric', 'Modeling', 'Motor', 'Neuraxis', 'Pattern', 'Population', 'Population Control', 'Population Registers', 'Probability', 'Process', 'Property', 'Rattus', 'Recovery', 'Recovery of Function', 'Research', 'Resolution', 'Sampling', 'Sensory', 'Severities', 'Solutions', 'Sorting - Cell Movement', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Staging', 'Staining method', 'Stains', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Time', 'Traumatic Brain Injury', 'Validation', 'Water', 'Weight', 'base', 'clinically relevant', 'density', 'expectation', 'fiber cell', 'imaging modality', 'in vivo', 'injured', 'interest', 'member', 'morphometry', 'novel', 'public health relevance', 'sensor', 'statistics', 'transmission process']",NINDS,UNIVERSITY OF FLORIDA,R01,2013,480486,-0.03147870516214184
"Statistical Methods for Selection and Evaluation of Biomarkers     DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particulr, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pan- creatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public.         PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.            ",Statistical Methods for Selection and Evaluation of Biomarkers,8483561,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Pancreas', 'Patients', 'Performance', 'Population', 'Probability', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2013,324656,-0.06914620178211436
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.        The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8410578,R01EY022039,"['Address', 'Algorithms', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'design', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,368125,-0.020468810283714278
"In vivo Characterization of Stents using Intravascular OCT Imaging     DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel.         PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.            ",In vivo Characterization of Stents using Intravascular OCT Imaging,8529140,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Goals', 'Graph', 'Healed', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Simulate', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thrombosis', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'public health relevance', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2013,412883,-0.015109290650945049
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development     DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation.             n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8644396,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Simulate', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGE MASON UNIVERSITY,R01,2013,322109,-0.01064382066116746
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.          The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8456053,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2013,531082,-0.01254290092766011
"Statistical methods for large and complex databases of ultra-high-dimensional  Abstract Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8614974,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2013,373406,-0.01983163905648392
"Scalable Assays for Morphological Analysis of Mammalian Neurons    DESCRIPTION (provided by applicant): Medium and high-throughput assays (i.e., screens) have generally not been applied to mammalian neurons because of the difficulties in culturing them in large numbers and because of the low efficiency with which the genetic makeup of neurons can be altered. Furthermore, because many aspects of neuronal function can only be assayed with electrophysiological assays, follow-up analysis and validation of screening hits is difficult. We propose to use automated imaging approaches to analyze synapse number and neuronal structure in vitro in a scalable format. We have implemented tissue culture and immunostaining approaches to monitor the number and types of synapses formed onto neurons in multi-well plates. We will couple this analysis with lentivirus mediated introduction of short-hairpin RNAs to induce RNA interference against genes expressed in neurons. This will be performed in concert with transcriptional analysis of neurons to determine the key changes in gene expression that correlate with structural and synaptic changes. The proposal represents a significant collaboration between several groups with expertise in functional analysis of neurons, automated analysis of images, viral mediated manipulation of gene expression, and whole-genome transcriptional analysis. We hope that our work will lead, for the first time, to a turn-key and robust method of analysis of neuron and synapse structure suitable for scalable, whole-genome analysis. Such a system will permit the unbiased and systematic analysis of pathways involved in neuropsychiatric diseases including neurodegenerative diseases such as Alzheimer's and Parkinson's as well as neurodevelopmental disorders such as mental retardation and autism.        Massively parallel analysis of cells in many conditions has allowed the discovery of key pathways that control cell function. Unfortunately, these techniques have not been applied to neurons due to difficulties in handling, manipulating, and analyzing large numbers of brain cells. We propose to develop imaging-based techniques to analyze neurons in dishes at a high throughput in order to find pathways that control their development and susceptibility to disease.            ",Scalable Assays for Morphological Analysis of Mammalian Neurons,8467069,R01NS077907,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Antibodies', 'Autistic Disorder', 'Axon', 'Benchmarking', 'Biochemical Pathway', 'Biological Assay', 'Cell Line', 'Cell physiology', 'Cells', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Dendrites', 'Development', 'Disease', 'Ensure', 'Excitatory Synapse', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Image', 'Image Analysis', 'In Vitro', 'Inhibitory Synapse', 'Institutes', 'Label', 'Lead', 'Machine Learning', 'Measurement', 'Mediating', 'Mental Retardation', 'Methods', 'Mining', 'Mitochondria', 'Monitor', 'Morphology', 'Mus', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuroglia', 'Neurons', 'Organelles', 'Parkinson Disease', 'Pathway Analysis', 'Pathway interactions', 'Phenotype', 'Predisposition', 'Procedures', 'Process', 'Protocols documentation', 'RNA Interference', 'Relative (related person)', 'Structure', 'Subfamily lentivirinae', 'Synapses', 'System', 'Techniques', 'Time', 'Transfection', 'Validation', 'Viral', 'Work', 'base', 'bioimaging', 'brain cell', 'density', 'design', 'fluorescence imaging', 'follow-up', 'genome analysis', 'genome-wide', 'high throughput screening', 'medical schools', 'neuropsychiatry', 'open source', 'prototype', 'scale up', 'screening', 'small hairpin RNA', 'synaptogenesis', 'tissue culture', 'tool']",NINDS,HARVARD MEDICAL SCHOOL,R01,2013,408818,-0.039514906304702134
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic   expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8512790,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2013,1792950,-0.003959831710535706
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic   expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8689306,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2013,141638,-0.003959831710535706
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8538496,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2013,234332,-0.017673093149009916
"Development of vector-specific, resistance-breaking insecticides to reduce malari    DESCRIPTION (provided by applicant): Malaria exacts a terrible toll in sub-Saharan Africa, killing an estimated 1-2 million persons each year, mostly children. Pyrethroid-based insecticide treated nets (pyrethroid ITNs) provide the first line of defense against disease transmission, but emerging resistant strains of the disease vector (Anopheles gambiae) threaten to render these ITNs ineffective. Our broad objective is to develop a new class of acetylcholinesterase (AChE)-targeting insecticide for deployment on ITNs, that is safe for use, effective against current pyrethroid- and AChE- resistant strains, and is less likely to foster emergence of new AChE-resistant strains. Thus our goal is consistent with the focus of the solicitation on novel interventions for the control of Malaria. FNIH-sponsored research from 2005-2008 enabled us to make significant progress towards our long-term goal. Further support from NIH will allow us to establish proof of concept that our novel AChE-based insecticide, deployed on an ITN, would constitute a superior intervention to manage the disease vector. Thus our goal is also consistent with the stated aim of the solicitation to fund translational research.       To achieve our goal we have assembled a team of chemists, structural biologists, entomologists, and toxicologists. Our specific aims are to 1)improve stability of An. gambiae AChE (AgAChE)-selective carbamates to oxidative detoxification; 2)acquire 3D structural information on AgAChE to optimize inhibition potency and selectivity; 3)develop bivalent carbamates for resilience to target-site mutation; 4)identify strategies to mitigate against carboxylesterase-mediated detoxification; and 5)make a preliminary assessment of mammalian toxicity of the most promising insecticides to emerge from these studies. To guide us through the proposed five years of research we have prepared a detailed timeline and decision tree that incorporate five integrated streams of insecticide discovery for optimizing field performance and human safety. Moreover the built-in complementarity of the chemical synthesis routes and the optimization approaches (e.g. resilience to both target-site and metabolic resistance mechanisms) means that unexpected difficulty in one stream need not slow progress in the other streams. These multiple approaches increase the probability of project success.       Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment, effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.              Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment , effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.","Development of vector-specific, resistance-breaking insecticides to reduce malari",8445236,R01AI082581,"['Acetylcholinesterase', 'Acetylcholinesterase Inhibitors', 'Acute', 'Address', 'Africa South of the Sahara', 'African', 'Agriculture', 'Amines', 'Anopheles Genus', 'Anopheles gambiae', 'Antidotes', 'Binding', 'Binding Sites', 'Biological Assay', 'Carbamates', 'Carboxylic Ester Hydrolases', 'Catalytic Domain', 'Child', 'Cholinesterase Inhibitors', 'Computer Simulation', 'Crystallization', 'Culicidae', 'Decision Trees', 'Development', 'Disease Vectors', 'Drug Metabolic Detoxication', 'Enzymes', 'Fostering', 'Funding', 'Goals', 'Human', 'Insecticide Resistance', 'Insecticides', 'Intervention', 'Length', 'Life', 'Ligands', 'Malaria', 'Measures', 'Mediating', 'Metabolic', 'Mus', 'Mutation', 'Oral', 'Paper', 'Performance', 'Peripheral', 'Permethrin', 'Persons', 'Phenylcarbamates', 'Probability', 'Propoxur', 'Relative (related person)', 'Research', 'Research Design', 'Research Methodology', 'Resistance', 'Risk', 'Roentgen Rays', 'Route', 'Safety', 'Site', 'Stream', 'Structural Biologist', 'Structure', 'Testing', 'TimeLine', 'Toxic effect', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Variant', 'base', 'carboxylesterase', 'chemical synthesis', 'design', 'disease transmission', 'improved', 'in vitro Assay', 'inhibitor/antagonist', 'innovation', 'killings', 'mutant', 'novel', 'pharmacophore', 'pyrethroid', 'research study', 'resilience', 'resistance mechanism', 'resistant strain', 'screening', 'success', 'transmission process', 'vector', 'vector control', 'vector mosquito', 'virtual']",NIAID,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2013,621519,-0.008161672171836745
Regulation of Alternative Cleavage and Polyadenylation No abstract available n/a,Regulation of Alternative Cleavage and Polyadenylation,8720197,R01GM084089,"['3&apos', ' Untranslated Regions', 'Address', 'Affect', 'Biochemical', 'Biological', 'Biological Assay', 'Cell Line', 'Classification', 'Code', 'Complementary DNA', 'Computational Molecular Biology', 'DNA Microarray Chip', 'Data', 'Databases', 'Elements', 'Event', 'Evolution', 'Exons', 'Expressed Sequence Tags', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Gene Mutation', 'Genes', 'Genetic Polymorphism', 'Genome', 'Goals', 'Human', 'Indium', 'Introns', 'Light', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Messenger RNA', 'Metabolism', 'MicroRNAs', 'Modeling', 'Molecular Biology', 'Molecular Biology Techniques', 'Mus', 'Mutagenesis', 'Mutation', 'Pattern', 'Phylogenetic Analysis', 'Poly A', 'Polyadenylation', 'Polyadenylation Pathway', 'Proteins', 'RNA', 'RNA Binding', 'RNA Interference', 'RNA Splicing', 'RNA-Binding Proteins', 'Regulation', 'Regulatory Element', 'Relative (related person)', 'Reporter', 'Site', 'Statistical Models', 'Structure', 'System', 'Technology', 'Tissues', 'Trees', 'Untranslated Regions', 'Validation', 'Variant', 'base', 'human disease', 'improved', 'preference', 'research study', 'serial analysis of gene expression', 'tool']",NIGMS,RBHS-NEW JERSEY MEDICAL SCHOOL,R01,2013,259660,-0.022969457166921878
"Computer-aided detection of non-calcified plaques in coronary CT angiograms   Cardiovascular disease is the leading cause of death in both men and women in the United States. Over 16 million Americans have coronary heart disease (CHD), causing about 0.5 million deaths each year. The most common CHD is coronary artery disease which is mainly caused by atherosclerosis. Clinical evidence in recent years shows that noncalcified plaques (NCPs) are more vulnerable to rupture than calcified plaques. Plaque rupture and the thrombosis that follows is the main cause of acute myocardial infarction. Multidetector coronary CT angiography (cCTA) has the potential to help clinicians in early detection and in quantification of NCPs. cCTA may thus be useful for CHD detection, risk stratification, monitoring, and evaluation of the effectiveness of risk reduction treatment. However, many of these potential applications have not been utilized clinically.  The goal of this project is to develop a computer-aided detection (CADe) system to serve as a second reader for assisting clinicians in detection and quantification of NCPs in cCTA exams. Our specific aims are to (1) develop machine learning methods for detection of NCPs causing stenosis and/or positive remodeling along coronary arteries, and (2) evaluate the effect of CADe on radiologists' detection of NCPs on cCTA by observer ROC study. To achieve these aims, we will collect a database of cCTA cases for training and testing the CADe system, define the search space by designing 3D multiscale coronary artery response enhancement, segmentation, and dynamic balloon vessel tracking methods, develop a unique vessel- stitching method to automatically identify the best-quality phase for each individual artery segment from all available phases in prospectively or retrospectively gated cCTA exams, develop innovative vessel-sector- profile analysis and vessel lumen analysis to detect NCPs that cause stenosis or positive remodeling, estimate the total NCP volume, and explore calibration method to quantify plaque density by phantom studies. To demonstrate the usefulness of CADe, a preclinical reader study will be conducted to compare radiologists' detection accuracy of NCPs with and without CADe.  The major innovations of this project include (1) being the first CADe system to automatically detect non-calcified plaques including those cause positive remodeling or stenosis in cCTA, (2) development of new machine learning techniques including the vessel-stitching method, vessel-sector-profile analysis, multiscale enhancement response, and dynamic balloon tracking specifically suited for coronary arterial trees, and (3) conducting the first ROC study to evaluate the effect of CADe on radiologists' detection of NCPs.  Narrative:  Cardiovascular disease is the leading cause of death in both men and women in the United States. If the proposed CADe system is successfully developed, it will (1) provide an accurate, efficient, and consistent tool to assist clinicians in detecting and quantifying non-calcified plaques (NCP) including those causing positive remodeling and/or stenosis for an individual patient, (2) help clinicians estimate the total NCP burden and study its significance, in analogy to that of the total calcium score, which may lead to improved clinical management of the vulnerable plaques, and (3) accelerate studies to develop new treatment options of coronary heart disease by providing a monitoring tool of treatment response. The proposed CADe system can thus serve as a foundation for these broader future applications and improve the efficacy of cCTA. Improved detection and management of NCPs may reduce the risk of myocardial infarctions and will have strong and long-lasting impact on health care.",Computer-aided detection of non-calcified plaques in coronary CT angiograms,8206668,R01HL106545,"['Acute myocardial infarction', 'American', 'Angiography', 'Arterial Fatty Streak', 'Arteries', 'Atherosclerosis', 'Calcified', 'Calcium', 'Calibration', 'Cardiovascular Diseases', 'Catheters', 'Cause of Death', 'Cessation of life', 'Clinical', 'Clinical Management', 'Computer Vision Systems', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Coronary heart disease', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Dose', 'Early Diagnosis', 'Effectiveness', 'Electrocardiogram', 'Evaluation', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Lead', 'Machine Learning', 'Methods', 'Modality', 'Monitor', 'Myocardial Infarction', 'Patients', 'Performance', 'Phase', 'Procedures', 'Radiation', 'Reader', 'Receiver Operating Characteristics', 'Recording of previous events', 'Resolution', 'Risk', 'Risk Reduction', 'Rupture', 'Scanning', 'Stenosis', 'Stratification', 'System', 'Techniques', 'Testing', 'Thrombosis', 'Time', 'Training', 'Trees', 'Ultrasonography', 'United States', 'Visual', 'Woman', 'computer aided detection', 'density', 'design', 'detector', 'improved', 'innovation', 'men', 'pre-clinical', 'prospective', 'radiologist', 'response', 'tool', 'treatment response', 'virtual']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,585438,-0.032321710309991404
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8281439,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2012,294260,-0.03143554284507681
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8494858,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2012,371054,-0.011013903556950932
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8269876,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2012,323305,-0.01902264064424372
"SmartTool for Anomaly Detection in Radiotherapy Treatment Plan Data    DESCRIPTION (provided by applicant):  Adverse events and medical errors result in thousands of accidental deaths and over one million excess injuries each year. To avoid medical errors in radiation cancer treatment, careful attention needs to be made to ensure accurate implementation of the intended treatment plan. We propose a SmartTool to automatically detect and highlight potential errors in a radiotherapy treatment plan, in real time and before its execution. SmartTool will double check all the treatment parameters in the background against a previously built Predictive Model of a Medical Error (PMME) and flag the operator, [post human QA,] if there is a discrepancy in the treatment plan, by stopping execution, highlighting the outlier treatment parameter and prompting human intervention. To build the PMME we will mine the dataset of previously treated cancer patients, by clustering the data in the groups based on treatment parameter similarity, labeling the clusters and using an innovative algorithm to build a highly accurate anomaly detection tool. PMME will also be dynamically updated [to include new treatment data instances coming in to the system, and updating the model should any treatment flags be identified as false positive or false negative]. The vastly innovative aspect of SmartTool is in the novel use of machine learning techniques to automatically build an anomaly prediction model on unlabeled data (customarily a labeled data is required to build a predictive model) and provide an automatic, real time and unobtrusive intelligent computational treatment checking algorithm. Moreover, having an analytical model of an outlier/anomaly offers the capability to describe the conditions of the outlier being created and is the essential in gaining investigative (and medical) insight in what went wrong and how to improve the process in the future. SmartTool can also be applied in a variety of other medical areas (e.g. predicting errors in pharmacy, laboratory data, and treatment procedure data), to detect anomalies and describe them, offering potential novel medical discoveries and a prospect of saving thousands more lives, with a vast commercialization aspect.      PUBLIC HEALTH RELEVANCE:  The proposal is aimed at promoting research and development in biomedical computational science and technology that is consistent with the objective of the NIH and NCI to support rapid progress in areas of scientific opportunity in biomedical research, and enhancing the public health. If the project is successfully completed, this proof of concept study will result in a valuable health information technology tool for automatic detection of catastrophic errors in cancer radiotherapy, which adds another safeguard for patient safety.            The proposal is aimed at promoting research and development in biomedical computational science and technology that is consistent with the objective of the NIH and NCI to support rapid progress in areas of scientific opportunity in biomedical research, and enhancing the public health. If the project is successfully completed, this proof of concept study will result in a valuable health information technology tool for automatic detection of catastrophic errors in cancer radiotherapy, which adds another safeguard for patient safety.         ",SmartTool for Anomaly Detection in Radiotherapy Treatment Plan Data,8250930,R43TR000629,"['Adverse event', 'Algorithms', 'Area', 'Attention', 'Biomedical Research', 'California', 'Cancer Center', 'Cancer Patient', 'Cessation of life', 'Classification', 'Computational Science', 'Computer software', 'Data', 'Data Set', 'Detection', 'Ensure', 'Evaluation', 'Event', 'Funding', 'Future', 'Healthcare', 'Human', 'Information Systems', 'Injury', 'Intervention', 'Label', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medical Errors', 'Methods', 'Mining', 'Modeling', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phase', 'Procedures', 'Process', 'Public Health', 'Radiation', 'Radiation therapy', 'Research', 'Severities', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'Update', 'Work', 'base', 'cancer radiation therapy', 'cancer therapy', 'commercialization', 'design', 'health information technology', 'improved', 'innovation', 'insight', 'novel', 'patient safety', 'predictive modeling', 'prevent', 'research and development', 'tool', 'treatment planning']",NCATS,"SCIBERQUEST, INC.",R43,2012,148660,-0.036711762663315065
"Automated Web-Based Behavioral Diagnostics of Cognitive Impairment    DESCRIPTION (provided by applicant): Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based method for early diagnosis of cognitive decline and memory loss. We will build on current research that has demonstrated a clear link between performance on the Visual Paired- Comparison Task (VPC) and diagnosis of MCI, and we will extend the impact of this finding by developing a suite of software tools and analysis methods that will improve the accuracy and extend the accessibility of cognitive diagnostics with a web-based VPC task that can be widely deployed. Although the VPC task is promising as a diagnostic aid, clinical application is severely limited by the need to use an eye tracker to precisely monitor subjects' eye movements. Unfortunately, eye trackers are expensive, require trained personnel, and are not widely available. However, our preliminary findings show that it is possible to produce picture examination behavior that is similar to eye-movements, using modified versions of the VPC task that could be administered by anyone, on any computer with an internet connection. In addition, powerful machine learning techniques from computer science can help accurately analyze and diagnose the resulting behavior. An important contribution from this work will be the possibility of predicting oncoming cognitive decline in MCI patients sooner than is now possible, that is, at a time when the nervous system is less compromised and more likely to benefit from therapeutic intervention. In support of this objective, we propose three aims: 1) Develop and investigate appropriate representation of eye movement characteristics and corresponding machine learning-based classification techniques for effective identification of the patient status, 2) Develop and validate a web-based version of the VPC task, and 3) explore the feasibility of our task as an automatic screening instrument for the general elderly population. Successful completion of these aims has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable thousands and potentially millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.        Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based, and widely accessible behavioral screening test for early diagnosis of cognitive decline and memory loss. Successful completion of this project has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.         ",Automated Web-Based Behavioral Diagnostics of Cognitive Impairment,8294581,R01EB014266,"['Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Behavior', 'Behavioral', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Computer software', 'Computers', 'Data', 'Dementia', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Elderly', 'Exhibits', 'Eye', 'Eye Movements', 'Future', 'General Practitioners', 'Goals', 'Human Resources', 'Image', 'Impaired cognition', 'Individual', 'Internet', 'Length', 'Link', 'Machine Learning', 'Measures', 'Memory', 'Memory Loss', 'Memory impairment', 'Methods', 'Monitor', 'Movement', 'Mus', 'Nervous system structure', 'Neurodegenerative Disorders', 'Online Systems', 'Paired Comparison', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Proxy', 'Recruitment Activity', 'Research', 'Research Subjects', 'Saccades', 'Screening procedure', 'Software Tools', 'Specificity', 'Stimulus', 'Structure', 'Supervision', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Translational Research', 'Visual', 'Work', 'aged', 'base', 'checkup examination', 'clinical application', 'clinical practice', 'cognitive neuroscience', 'computer science', 'cost', 'cost effective', 'gaze', 'improved', 'instrument', 'memory recognition', 'mild neurocognitive impairment', 'novel', 'older patient', 'preference', 'sample fixation', 'tool', 'tool development', 'visual adaptation']",NIBIB,EMORY UNIVERSITY,R01,2012,348750,-0.037641819229788466
"Hand Sensorimotor Function and Carpal Tunnel Syndrome    DESCRIPTION (provided by applicant): The median nerve is susceptible to compression in the wrist, leading to carpal tunnel syndrome (CTS). CTS is the most common compression neuropathy and have an immense impact on national health care, worker productivity, and quality of life. Despite its high prevalence and public health cost, our understanding of CTS is limited, and the management of CTS awaits improvement. The central notion of this project is that hand sensorimotor function is sensitive to peripheral median neuropathy and that the central nervous system is affected by CTS, causing the associated sensorimotor deficit. We will investigate this notion with quantifiable sensorimotor data from novel biomechanical and neurophysiological studies. This project has three aims consisting of biomechanical, neurophysiological and translational research. The first aim is to investigate CTS-induced pathokinematic and pathokinetic performance using dexterous manual tasks of thumb opposition, reach-to-pinch, precision grip, and finger pressing. The second aim is to investigate the neurophysiological implications of chronic peripheral neuropathy (i.e., CTS) on the central nervous system by evaluating corticomuscular coupling and stretch reflex. The third aim is to identify novel biomechanical and neurophysiological markers for CTS cases using machine learning and classification algorithms. The results of this project will elucidate the pathological mechanisms and behavioral manifestations of CTS and aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder. More generally, CTS as a chronic neuropathy serves as an effective model to study sensorimotor mechanisms of the peripheral and central nervous systems. In addition, the methodology developed in this project is applicable to other neuromuscular disorders.      PUBLIC HEALTH RELEVANCE:   Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.                 Project Narrative Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.",Hand Sensorimotor Function and Carpal Tunnel Syndrome,8249044,R01AR056964,"['Abnormal coordination', 'Affect', 'Algorithms', 'Behavioral Mechanisms', 'Biomechanics', 'Carpal Tunnel Syndrome', 'Carpometacarpal joint structure', 'Chronic', 'Classification', 'Clinical', 'Coupling', 'Data', 'Development', 'Disease', 'Drops', 'Electroencephalography', 'Electromyography', 'Exertion', 'Eye', 'Fingers', 'Hand', 'Health Care Costs', 'Health Personnel', 'High Prevalence', 'Human', 'Individual', 'Joints', 'Lasso', 'Machine Learning', 'Manuals', 'Measures', 'Median Neuropathy', 'Metacarpophalangeal joint structure', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neural Conduction', 'Neuraxis', 'Neuromuscular Diseases', 'Neuropathy', 'Patients', 'Performance', 'Peripheral', 'Peripheral Nervous System Diseases', 'Production', 'Productivity', 'Pronation', 'Public Health', 'Quality of life', 'Questionnaires', 'Reaction', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Response Latencies', 'Sensorimotor functions', 'Sensory', 'Techniques', 'Testing', 'Thumb structure', 'Time', 'Translational Research', 'Trees', 'Validation', 'Wrist', 'data mining', 'diagnosis evaluation', 'experience', 'grasp', 'improved', 'indexing', 'median nerve', 'motor control', 'neurophysiology', 'novel', 'programs', 'public health relevance', 'research study', 'response', 'stretch reflex', 'tool', 'vector']",NIAMS,CLEVELAND CLINIC LERNER COM-CWRU,R01,2012,353250,-0.0075277716792353936
"Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging    DESCRIPTION (provided by applicant): Multiplexed biomarker analysis is more powerful in reflecting the biological behaviors of a tumor than single biomarker analysis, but its standardization and quantification is still a challenge. Furthermore, most computer software does not provide methods for imaging and analyzing subcellular localization of biomarkers and correlating them with biological and clinical information. The objective of this project is to develop a platform which combines imaging and quantification of multiplexed immunostaining plus bioinformatics for the prediction of lymph node metastases (LNM) from the primary tumor (PT) of squamous cell carcinoma of the head and neck (SCCHN). LNM of SCCHN is a precisely defined biological phenomenon which is an ideal model to be utilized to develop this multiplexed biomarker platform (MBP). Based on our preliminary studies, we aim to test the hypothesis that that the MBP can be developed to identify the subcellular distribution and expression of multiple metastasis-related biomarkers simultaneously in PTs. Accurate quantification of these biomarkers will facilitate the prediction of metastasis from PTs. Three emerging technologies, quantum dot (QD)-based immunohistofluorescence (IHF), multispectral imaging, and machine learning will be used to test this hypothesis. Using these approaches, a platform that combines quantifying multiplexed immunostaining with biostatistics will be developed and tested for its sensitivity, specificity, and prediction power for use in the clinic. Therefore, this project fits appropriately to the scope of the NCI program announcement ""Developmental Research in Cancer Prognosis and Prediction"" (PA-09-159).  Three aims are proposed in the study. (1) To develop a multiplexed biomarker system and method based on a bulk tissue model for prediction of LNM in SCCHN PT tissues. This Aim will establish and validate an analysis methodology for multiplexed quantification of membrane and cytoplasmic staining using a new function in InForm software where subcellular localization of certain biomarkers will be specifically analyzed. Prediction of LNM based on this bulk tissue model will be achieved. (2) To develop a per-cell quantification method based on a sub-population model for prediction of LNM in SCCHN PT tissue.  The per-cell analysis results will quantified as the percentage of high risk cells from the multiplexed biomarker analyses in the same PTs. The high risk population will be correlated with LNM. The sensitivity and specificity of the prediction by the sub-population model will be compared with that of the bulk tissue model. (3) To develop and validate a nomogram with software combining clinical characterizations of metastasis as a working platform for the prediction of LNM. While the primary endpoint of Aim 1 and 2 is to correlate the three biomarkers with metastasis, other clinical factors such as differentiation status, tumor stage, and site, etc. may also correlate with LNM. The most predictive biomarker set combined with relevant clinical factors will constitute a platform with computer software that will be validated in an additional 100 SCCHN samples for prediction of LNM.        Imaging and quantifying expression and subcellular localization of multiplexed biomarkers is currently a challenge in cancer research and clinical application. This project aims to develop a platform which combines imaging and quantifying multiplexed biomarkers plus their correlation with lymph node metastasis of squamous cell carcinoma of the head and neck. This platform can be used for an assessment of LNM which is paramount for appropriate treatment planning.         ",Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging,8307808,R33CA161873,"['Behavior', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Biometry', 'Breast Cancer Cell', 'Cancer Prognosis', 'Cell membrane', 'Cells', 'Clinic', 'Clinical', 'Color', 'Colorectal Cancer', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disseminated Malignant Neoplasm', 'E-Cadherin', 'Emerging Technologies', 'Epidermal Growth Factor Receptor', 'Epithelial', 'Flow Cytometry', 'Head and Neck Squamous Cell Carcinoma', 'Human', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Literature', 'Lung', 'Machine Learning', 'Mesenchymal', 'Methodology', 'Methods', 'Modeling', 'NIH Program Announcements', 'Neoplasm Metastasis', 'Nomograms', 'Operative Surgical Procedures', 'Outcome', 'Population', 'Primary Neoplasm', 'Production', 'Quantum Dots', 'Reaction', 'Receiver Operating Characteristics', 'Research', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Specificity', 'Specimen', 'Staining method', 'Stains', 'Standardization', 'System', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Tumor Tissue', 'Tumor stage', 'Work', 'aldehyde dehydrogenases', 'anticancer research', 'base', 'cancer cell', 'cancer stem cell', 'clinical application', 'high risk', 'lymph nodes', 'model development', 'nanoparticle', 'neoplastic cell', 'novel', 'outcome forecast', 'tool', 'treatment planning', 'tumor']",NCI,EMORY UNIVERSITY,R33,2012,354843,-0.044415488839420696
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.      PUBLIC HEALTH RELEVANCE: The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.              The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8216617,R01EY022039,"['Address', 'Algorithms', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'design', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,386771,-0.028680255961891936
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.        PUBLIC HEALTH RELEVANCE: The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.              The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8315044,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2012,620921,-0.01559224933207968
"Scalable Assays for Morphological Analysis of Mammalian Neurons    DESCRIPTION (provided by applicant): Medium and high-throughput assays (i.e., screens) have generally not been applied to mammalian neurons because of the difficulties in culturing them in large numbers and because of the low efficiency with which the genetic makeup of neurons can be altered. Furthermore, because many aspects of neuronal function can only be assayed with electrophysiological assays, follow-up analysis and validation of screening hits is difficult. We propose to use automated imaging approaches to analyze synapse number and neuronal structure in vitro in a scalable format. We have implemented tissue culture and immunostaining approaches to monitor the number and types of synapses formed onto neurons in multi-well plates. We will couple this analysis with lentivirus mediated introduction of short-hairpin RNAs to induce RNA interference against genes expressed in neurons. This will be performed in concert with transcriptional analysis of neurons to determine the key changes in gene expression that correlate with structural and synaptic changes. The proposal represents a significant collaboration between several groups with expertise in functional analysis of neurons, automated analysis of images, viral mediated manipulation of gene expression, and whole-genome transcriptional analysis. We hope that our work will lead, for the first time, to a turn-key and robust method of analysis of neuron and synapse structure suitable for scalable, whole-genome analysis. Such a system will permit the unbiased and systematic analysis of pathways involved in neuropsychiatric diseases including neurodegenerative diseases such as Alzheimer's and Parkinson's as well as neurodevelopmental disorders such as mental retardation and autism.        Massively parallel analysis of cells in many conditions has allowed the discovery of key pathways that control cell function. Unfortunately, these techniques have not been applied to neurons due to difficulties in handling, manipulating, and analyzing large numbers of brain cells. We propose to develop imaging-based techniques to analyze neurons in dishes at a high throughput in order to find pathways that control their development and susceptibility to disease.            ",Scalable Assays for Morphological Analysis of Mammalian Neurons,8306192,R01NS077907,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Antibodies', 'Autistic Disorder', 'Axon', 'Benchmarking', 'Biochemical Pathway', 'Biological Assay', 'Cell Line', 'Cell physiology', 'Cells', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Dendrites', 'Development', 'Disease', 'Ensure', 'Excitatory Synapse', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Image', 'Image Analysis', 'In Vitro', 'Inhibitory Synapse', 'Institutes', 'Label', 'Lead', 'Machine Learning', 'Measurement', 'Mediating', 'Mental Retardation', 'Methods', 'Mining', 'Mitochondria', 'Monitor', 'Morphology', 'Mus', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuroglia', 'Neurons', 'Organelles', 'Parkinson Disease', 'Pathway Analysis', 'Pathway interactions', 'Phenotype', 'Predisposition', 'Procedures', 'Process', 'Protocols documentation', 'RNA Interference', 'Relative (related person)', 'Screening procedure', 'Structure', 'Subfamily lentivirinae', 'Synapses', 'System', 'Techniques', 'Time', 'Transfection', 'Validation', 'Viral', 'Work', 'base', 'bioimaging', 'brain cell', 'density', 'design', 'fluorescence imaging', 'follow-up', 'genome-wide', 'high throughput screening', 'medical schools', 'neuropsychiatry', 'open source', 'prototype', 'scale up', 'small hairpin RNA', 'synaptogenesis', 'tissue culture', 'tool']",NINDS,HARVARD MEDICAL SCHOOL,R01,2012,422500,-0.039514906304702134
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8499527,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2012,29926,-0.003959831710535706
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8499526,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2012,548996,-0.003959831710535706
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic   expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8332359,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2012,1778562,-0.003959831710535706
"Biosignatures of Treatment Remission in Major Depression DESCRIPTION (provided by applicant): Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic   expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness   dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving   supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. PUBLIC HEALTH RELEVANCE: This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8448431,U01MH092250,"['Accounting', 'Address', 'Adverse event', 'Affinity', 'Alcoholism', 'Algorithms', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Markers', 'Brain', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease remission', 'Electroencephalography', 'Emotions', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Gene Expression Profile', 'Genetic', 'Genetic Crossing Over', 'Gonadal Steroid Hormones', 'Health', 'Individual', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteomics', 'Qualifying', 'Randomized', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Serotonin Receptor 5-HT1A', 'Specific qualifier value', 'Structure', 'Techniques', 'Thick', 'Training', 'Treatment outcome', 'base', 'biosignature', 'clinical phenotype', 'depressive symptoms', 'disability', 'executive function', 'gene environment interaction', 'immortalized cell', 'interest', 'meetings', 'monoamine', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'novel', 'predictive modeling', 'psychosocial', 'receptor binding', 'repository', 'response', 'sex', 'stressor', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2012,41673,-0.003959831710535706
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8216289,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2012,255679,-0.017673093149009916
"Development of vector-specific, resistance-breaking insecticides to reduce malari    DESCRIPTION (provided by applicant): Malaria exacts a terrible toll in sub-Saharan Africa, killing an estimated 1-2 million persons each year, mostly children. Pyrethroid-based insecticide treated nets (pyrethroid ITNs) provide the first line of defense against disease transmission, but emerging resistant strains of the disease vector (Anopheles gambiae) threaten to render these ITNs ineffective. Our broad objective is to develop a new class of acetylcholinesterase (AChE)-targeting insecticide for deployment on ITNs, that is safe for use, effective against current pyrethroid- and AChE- resistant strains, and is less likely to foster emergence of new AChE-resistant strains. Thus our goal is consistent with the focus of the solicitation on novel interventions for the control of Malaria. FNIH-sponsored research from 2005-2008 enabled us to make significant progress towards our long-term goal. Further support from NIH will allow us to establish proof of concept that our novel AChE-based insecticide, deployed on an ITN, would constitute a superior intervention to manage the disease vector. Thus our goal is also consistent with the stated aim of the solicitation to fund translational research.       To achieve our goal we have assembled a team of chemists, structural biologists, entomologists, and toxicologists. Our specific aims are to 1)improve stability of An. gambiae AChE (AgAChE)-selective carbamates to oxidative detoxification; 2)acquire 3D structural information on AgAChE to optimize inhibition potency and selectivity; 3)develop bivalent carbamates for resilience to target-site mutation; 4)identify strategies to mitigate against carboxylesterase-mediated detoxification; and 5)make a preliminary assessment of mammalian toxicity of the most promising insecticides to emerge from these studies. To guide us through the proposed five years of research we have prepared a detailed timeline and decision tree that incorporate five integrated streams of insecticide discovery for optimizing field performance and human safety. Moreover the built-in complementarity of the chemical synthesis routes and the optimization approaches (e.g. resilience to both target-site and metabolic resistance mechanisms) means that unexpected difficulty in one stream need not slow progress in the other streams. These multiple approaches increase the probability of project success.       Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment, effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.              Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment , effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.","Development of vector-specific, resistance-breaking insecticides to reduce malari",8237040,R01AI082581,"['Acetylcholinesterase', 'Acetylcholinesterase Inhibitors', 'Acute', 'Address', 'Africa South of the Sahara', 'African', 'Agriculture', 'Amines', 'Anopheles Genus', 'Anopheles gambiae', 'Antidotes', 'Binding', 'Binding Sites', 'Biological Assay', 'Carbamates', 'Carboxylic Ester Hydrolases', 'Catalytic Domain', 'Child', 'Cholinesterase Inhibitors', 'Computer Simulation', 'Crystallization', 'Culicidae', 'Decision Trees', 'Development', 'Disease Vectors', 'Drug Metabolic Detoxication', 'Enzymes', 'Fostering', 'Funding', 'Goals', 'Human', 'Insecticide Resistance', 'Insecticides', 'Intervention', 'Length', 'Life', 'Ligands', 'Malaria', 'Measures', 'Mediating', 'Metabolic', 'Mus', 'Mutation', 'Oral', 'Paper', 'Performance', 'Peripheral', 'Permethrin', 'Persons', 'Phenylcarbamates', 'Probability', 'Propoxur', 'Relative (related person)', 'Research', 'Research Design', 'Research Methodology', 'Resistance', 'Risk', 'Roentgen Rays', 'Route', 'Safety', 'Screening procedure', 'Site', 'Stream', 'Structural Biologist', 'Structure', 'Testing', 'TimeLine', 'Toxic effect', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Variant', 'base', 'carboxylesterase', 'chemical synthesis', 'design', 'disease transmission', 'improved', 'in vitro Assay', 'inhibitor/antagonist', 'innovation', 'killings', 'mutant', 'novel', 'pharmacophore', 'pyrethroid', 'research study', 'resilience', 'resistance mechanism', 'resistant strain', 'success', 'transmission process', 'vector', 'vector control', 'vector mosquito', 'virtual']",NIAID,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2012,661190,-0.008161672171836745
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,8322813,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Clinical assessments', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Health', 'Health Personnel', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Imagery', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'data modeling', 'graphical user interface', 'hemodynamics', 'imaging informatics', 'improved', 'innovation', 'insight', 'mortality', 'network models', 'patient population', 'prognostic', 'prospective', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2012,570954,-0.030462471411229108
"CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING    DESCRIPTION (provided by applicant): Case Studies in Bayesian Statistics and Machine Learning I continues in the tradition of the Case Studies in Bayesian Statistics series. The original series of workshops were held in odd years at Carnegie Mellon University in the early fall. The first edition of the new workshop will be held at Carnegie Mellon University on October 14-15, 2011. The highest level goal of the workshop series is to generate and present successful solutions to difficult substantive problems in a wide variety of areas. The specific objectives of the workshop are to 1. Present and discuss solutions to challenging scientific problems that illustrate the potential for statistical machine learning approaches in substantive research; 2. Present an opportunity for statisticians and computer scientists to present applications-oriented research  that changes the way that data are analyzed in scientific fields; 3. Stimulate discussion of the challenges of the analysis of high-dimensional and complex datasets in a scientifically useful manner; 4. Encourage young researchers, including graduate students, to present their applied work; 5. Provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; 6. Expose young researchers to important challenges and opportunities in collaborative research; 7. Include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment; 8. Encourage dissemination of the findings presented at the workshop via well-documented and peer- reviewed journal articles.      PUBLIC HEALTH RELEVANCE: Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.           Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.         ",CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING,8203089,R13CA144626,"['Area', 'Bioinformatics', 'Case Study', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Data Analyses', 'Data Set', 'Disabled Persons', 'Discipline', 'Disease', 'Educational workshop', 'Environment', 'Fostering', 'Goals', 'Hand', 'Health Sciences', 'Image', 'Institutes', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Human Genome Research Institute', 'Participant', 'Peer Review', 'Research', 'Research Personnel', 'Scientist', 'Series', 'Solutions', 'Underrepresented Minority', 'Universities', 'Woman', 'Work', 'anticancer research', 'computer science', 'data modeling', 'falls', 'graduate student', 'interest', 'journal article', 'meetings', 'peer', 'planetary Atmosphere', 'statistics', 'symposium']",NCI,CARNEGIE-MELLON UNIVERSITY,R13,2011,7500,-0.033339701397249906
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.      PUBLIC HEALTH RELEVANCE: This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.           This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8101478,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,290523,-0.03260291787806552
"Computer-aided detection of non-calcified plaques in coronary CT angiograms    DESCRIPTION (provided by applicant):  Cardiovascular disease is the leading cause of death in both men and women in the United States. Over 16 million Americans have coronary heart disease (CHD), causing about 0.5 million deaths each year. The most common CHD is coronary artery disease which is mainly caused by atherosclerosis. Clinical evidence in recent years shows that noncalcified plaques (NCPs) are more vulnerable to rupture than calcified plaques. Plaque rupture and the thrombosis that follows is the main cause of acute myocardial infarction. Multidetector coronary CT angiography (cCTA) has the potential to help clinicians in early detection and in quantification of NCPs. cCTA may thus be useful for CHD detection, risk stratification, monitoring, and evaluation of the effectiveness of risk reduction treatment. However, many of these potential applications have not been utilized clinically.  The goal of this project is to develop a computer-aided detection (CADe) system to serve as a second reader for assisting clinicians in detection and quantification of NCPs in cCTA exams. Our specific aims are to (1) develop machine learning methods for detection of NCPs causing stenosis and/or positive remodeling along coronary arteries, and (2) evaluate the effect of CADe on radiologists' detection of NCPs on cCTA by observer ROC study. To achieve these aims, we will collect a database of cCTA cases for training and testing the CADe system, define the search space by designing 3D multiscale coronary artery response enhancement, segmentation, and dynamic balloon vessel tracking methods, develop a unique vessel- stitching method to automatically identify the best-quality phase for each individual artery segment from all available phases in prospectively or retrospectively gated cCTA exams, develop innovative vessel-sector- profile analysis and vessel lumen analysis to detect NCPs that cause stenosis or positive remodeling, estimate the total NCP volume, and explore calibration method to quantify plaque density by phantom studies. To demonstrate the usefulness of CADe, a preclinical reader study will be conducted to compare radiologists' detection accuracy of NCPs with and without CADe.  The major innovations of this project include (1) being the first CADe system to automatically detect non-calcified plaques including those cause positive remodeling or stenosis in cCTA, (2) development of new machine learning techniques including the vessel-stitching method, vessel-sector-profile analysis, multiscale enhancement response, and dynamic balloon tracking specifically suited for coronary arterial trees, and (3) conducting the first ROC study to evaluate the effect of CADe on radiologists' detection of NCPs.      PUBLIC HEALTH RELEVANCE: Cardiovascular disease is the leading cause of death in both men and women in the United States. If the proposed CADe system is successfully developed, it will (1) provide an accurate, efficient, and consistent tool to assist clinicians in detecting and quantifying non-calcified plaques (NCP) including those causing positive remodeling and/or stenosis for an individual patient, (2) help clinicians estimate the total NCP burden and study its significance, in analogy to that of the total calcium score, which may lead to improved clinical management of the vulnerable plaques, and (3) accelerate studies to develop new treatment options of coronary heart disease by providing a monitoring tool of treatment response. The proposed CADe system can thus serve as a foundation for these broader future applications and improve the efficacy of cCTA. Improved detection and management of NCPs may reduce the risk of myocardial infarctions and will have strong and long-lasting impact on health care.           Cardiovascular disease is the leading cause of death in both men and women in the United States. If the proposed CADe system is successfully developed, it will (1) provide an accurate, efficient, and consistent tool to assist clinicians in detecting and quantifying non-calcified plaques (NCP) including those causing positive remodeling and/or stenosis for an individual patient, (2) help clinicians estimate the total NCP burden and study its significance, in analogy to that of the total calcium score, which may lead to improved clinical management of the vulnerable plaques, and (3) accelerate studies to develop new treatment options of coronary heart disease by providing a monitoring tool of treatment response. The proposed CADe system can thus serve as a foundation for these broader future applications and improve the efficacy of cCTA. Improved detection and management of NCPs may reduce the risk of myocardial infarctions and will have strong and long-lasting impact on health care.         ",Computer-aided detection of non-calcified plaques in coronary CT angiograms,8032999,R01HL106545,"['Acute myocardial infarction', 'American', 'Angiography', 'Arterial Fatty Streak', 'Arteries', 'Atherosclerosis', 'Calcified', 'Calcium', 'Calibration', 'Cardiovascular Diseases', 'Catheters', 'Cause of Death', 'Cessation of life', 'Clinical', 'Clinical Management', 'Computer Assisted', 'Computer Vision Systems', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Coronary heart disease', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Dose', 'Early Diagnosis', 'Effectiveness', 'Electrocardiogram', 'Evaluation', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Lead', 'Machine Learning', 'Methods', 'Modality', 'Monitor', 'Myocardial Infarction', 'Patients', 'Performance', 'Phase', 'Procedures', 'Radiation', 'Reader', 'Receiver Operating Characteristics', 'Recording of previous events', 'Resolution', 'Risk', 'Risk Reduction', 'Rupture', 'Scanning', 'Stenosis', 'Stratification', 'System', 'Techniques', 'Testing', 'Thrombosis', 'Time', 'Training', 'Trees', 'Ultrasonography', 'United States', 'Visual', 'Woman', 'density', 'design', 'detector', 'improved', 'innovation', 'men', 'pre-clinical', 'prospective', 'radiologist', 'response', 'tool', 'treatment response', 'virtual']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,583681,-0.036543616029955484
"Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T    DESCRIPTION (provided by applicant):  Peripheral nerve injuries are common diseases that affect a large amount of patients every year.  Tissue engineering has emerged as a powerful approach for developing alternative nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  Some research groups have applied artificial neural networks and decision trees to obtain the best model configuration for the prediction of the tissue engineering strategies.  For the decision trees based methods, it is hard to tell which classification tree is better than the other.  Furthermore, the prediction system using the decision tree algorithm lacks the capability of accumulating the learning experience over time.  On the other hand, Artificial Neural Networks (ANNs) exhibit some remarkable properties, but only the connection weights are trained with fixed topology.  It is hard to find the best fixed topology in advance for each specific tissue engineering strategy.  In this proposal, swarm intelligence (SI) based evolving ANNs technique is proposed to tackle this challenge.  Two swarm intelligence based methods, Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO), will be applied in this project to train the ANN model.  More specifically, ACO will be used to optimize the topology structure of the ANN models, while the PSO is used to adjust the connection weights of the ANN models based on the optimized topology structure.  For this SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) system, both topology and connection weight of artificial neural networks can be evolved automatically and simultaneously so that an optimal classifier for tissue engineering strategies in peripheral nerve regeneration can be achieved.  The research project will include the following phases:  Aim 1:  Predict tissue engineering strategies in peripheral nerve regeneration using SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) analytical and prediction system.  Aim 2:  Validate the efficacy of novel unknown tissue engineered nerve grafts as predicted by using SWIRL-ANN based analytical and prediction system for bridging peripheral nerve gaps in rat sciatic nerve injury model in vivo.      PUBLIC HEALTH RELEVANCE:  Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.               Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.            ",Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T,8232817,R15NS074404,"['Advanced Development', 'Affect', 'Algorithms', 'Animal Experiments', 'Animals', 'Ants', 'Area', 'Artificial Intelligence', 'Autologous Transplantation', 'Biocompatible Materials', 'Biological', 'Biological Neural Networks', 'Biomedical Research', 'Breathing', 'Cells', 'Characteristics', 'Classification', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Defect', 'Disease', 'Exhibits', 'Foundations', 'Future', 'Generic Drugs', 'Individual', 'Insecta', 'Intelligence', 'Knowledge', 'Learning', 'Maps', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nerve', 'Nerve Regeneration', 'Network-based', 'Neural Network Simulation', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Perception', 'Peripheral Nerves', 'Peripheral nerve injury', 'Phase', 'Physicians', 'Process', 'Property', 'Psychological reinforcement', 'Rattus', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Social Behavior', 'Structure', 'Surgical incisions', 'System', 'Techniques', 'Time', 'Tissue Engineering', 'Tissues', 'Training', 'Treatment Protocols', 'Trees', 'Vertebrates', 'Weight', 'Work', 'base', 'experience', 'in vivo', 'in vivo Model', 'insight', 'nerve gap', 'nerve injury', 'novel', 'particle', 'relating to nervous system', 'sciatic nerve', 'social', 'tool', 'vector']",NINDS,STEVENS INSTITUTE OF TECHNOLOGY,R15,2011,423840,-0.007786910137094036
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8107695,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2011,108418,-0.011013903556950932
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8062031,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Health', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2011,359403,-0.01233503867323192
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.      PUBLIC HEALTH RELEVANCE: The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.             The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8108523,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,341852,-0.015802146540033715
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,8133946,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computational algorithm', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2011,297497,-0.014834418641739596
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,8112574,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'spatiotemporal', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2011,176524,-0.011365437999994479
"Automated Web-Based Behavioral Diagnostics of Cognitive Impairment    DESCRIPTION (provided by applicant): Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based method for early diagnosis of cognitive decline and memory loss. We will build on current research that has demonstrated a clear link between performance on the Visual Paired- Comparison Task (VPC) and diagnosis of MCI, and we will extend the impact of this finding by developing a suite of software tools and analysis methods that will improve the accuracy and extend the accessibility of cognitive diagnostics with a web-based VPC task that can be widely deployed. Although the VPC task is promising as a diagnostic aid, clinical application is severely limited by the need to use an eye tracker to precisely monitor subjects' eye movements. Unfortunately, eye trackers are expensive, require trained personnel, and are not widely available. However, our preliminary findings show that it is possible to produce picture examination behavior that is similar to eye-movements, using modified versions of the VPC task that could be administered by anyone, on any computer with an internet connection. In addition, powerful machine learning techniques from computer science can help accurately analyze and diagnose the resulting behavior. An important contribution from this work will be the possibility of predicting oncoming cognitive decline in MCI patients sooner than is now possible, that is, at a time when the nervous system is less compromised and more likely to benefit from therapeutic intervention. In support of this objective, we propose three aims: 1) Develop and investigate appropriate representation of eye movement characteristics and corresponding machine learning-based classification techniques for effective identification of the patient status, 2) Develop and validate a web-based version of the VPC task, and 3) explore the feasibility of our task as an automatic screening instrument for the general elderly population. Successful completion of these aims has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable thousands and potentially millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.      PUBLIC HEALTH RELEVANCE: Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based, and widely accessible behavioral screening test for early diagnosis of cognitive decline and memory loss. Successful completion of this project has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.           Alzheimer's disease affects an estimated 5.3 million Americans currently, and this number is expected to grow significantly in the coming decade. A critical goal of Alzheimer's disease research is to improve current methods of diagnosis so that patients can be identified sooner and, therefore, obtain greater advantage from available therapies. The major goal of this project is to develop and validate an automated, web-based, and widely accessible behavioral screening test for early diagnosis of cognitive decline and memory loss. Successful completion of this project has the potential to dramatically alter the current practice of clinical translational research as well as the current methods used for diagnosing cognitive deficits. This would enable millions of patients and potential research subjects to take the test as part of their routine checkup, requiring nothing more than a computer with an internet connection.         ",Automated Web-Based Behavioral Diagnostics of Cognitive Impairment,8116342,R01EB014266,"['Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Behavior', 'Behavioral', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Computer software', 'Computers', 'Data', 'Dementia', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Elderly', 'Exhibits', 'Eye', 'Eye Movements', 'Future', 'General Practitioners', 'Goals', 'Human Resources', 'Image', 'Impaired cognition', 'Individual', 'Internet', 'Length', 'Link', 'Machine Learning', 'Measures', 'Memory', 'Memory Loss', 'Memory impairment', 'Methods', 'Monitor', 'Movement', 'Mus', 'Nervous system structure', 'Neurodegenerative Disorders', 'Online Systems', 'Paired Comparison', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Proxy', 'Recruitment Activity', 'Research', 'Research Subjects', 'Saccades', 'Screening procedure', 'Software Tools', 'Specificity', 'Stimulus', 'Structure', 'Supervision', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Translational Research', 'Visual', 'Work', 'aged', 'base', 'checkup examination', 'clinical application', 'clinical practice', 'cognitive neuroscience', 'computer science', 'cost', 'cost effective', 'gaze', 'improved', 'instrument', 'memory recognition', 'mild neurocognitive impairment', 'novel', 'older patient', 'preference', 'sample fixation', 'tool', 'tool development', 'visual adaptation']",NIBIB,EMORY UNIVERSITY,R01,2011,324000,-0.04504805632231261
"Hand Sensorimotor Function and Carpal Tunnel Syndrome    DESCRIPTION (provided by applicant): The median nerve is susceptible to compression in the wrist, leading to carpal tunnel syndrome (CTS). CTS is the most common compression neuropathy and have an immense impact on national health care, worker productivity, and quality of life. Despite its high prevalence and public health cost, our understanding of CTS is limited, and the management of CTS awaits improvement. The central notion of this project is that hand sensorimotor function is sensitive to peripheral median neuropathy and that the central nervous system is affected by CTS, causing the associated sensorimotor deficit. We will investigate this notion with quantifiable sensorimotor data from novel biomechanical and neurophysiological studies. This project has three aims consisting of biomechanical, neurophysiological and translational research. The first aim is to investigate CTS-induced pathokinematic and pathokinetic performance using dexterous manual tasks of thumb opposition, reach-to-pinch, precision grip, and finger pressing. The second aim is to investigate the neurophysiological implications of chronic peripheral neuropathy (i.e., CTS) on the central nervous system by evaluating corticomuscular coupling and stretch reflex. The third aim is to identify novel biomechanical and neurophysiological markers for CTS cases using machine learning and classification algorithms. The results of this project will elucidate the pathological mechanisms and behavioral manifestations of CTS and aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder. More generally, CTS as a chronic neuropathy serves as an effective model to study sensorimotor mechanisms of the peripheral and central nervous systems. In addition, the methodology developed in this project is applicable to other neuromuscular disorders.      PUBLIC HEALTH RELEVANCE:   Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.                   Carpal tunnel syndrome is highly prevalent and costly. One of the distinct consequences of carpal tunnel syndrome is that patients experience inexplicable dropping of objects and clumsiness while performing simple daily tasks. In this project, we propose to study the sensorimotor deficit using novel biomechanical and neurophysiological experiments. The results of this project will elucidate the pathological mechanisms and manifestations of carpal tunnel syndrome. This project is clinically translational to aid in the development of new strategies for diagnosis, evaluation, rehabilitation, and treatment of this disorder.               ",Hand Sensorimotor Function and Carpal Tunnel Syndrome,8102684,R01AR056964,"['Abnormal coordination', 'Affect', 'Algorithms', 'Behavioral Mechanisms', 'Biomechanics', 'Carpal Tunnel Syndrome', 'Carpometacarpal joint structure', 'Chronic', 'Classification', 'Clinical', 'Coupling', 'Data', 'Development', 'Disease', 'Drops', 'Electroencephalography', 'Electromyography', 'Exertion', 'Eye', 'Fingers', 'Hand', 'Health Care Costs', 'Health Personnel', 'High Prevalence', 'Human', 'Individual', 'Joints', 'Lasso', 'Machine Learning', 'Manuals', 'Measures', 'Median Neuropathy', 'Metacarpophalangeal joint structure', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Cortex', 'Neural Conduction', 'Neuraxis', 'Neuromuscular Diseases', 'Neuropathy', 'Patients', 'Performance', 'Peripheral', 'Peripheral Nervous System Diseases', 'Production', 'Productivity', 'Pronation', 'Public Health', 'Quality of life', 'Questionnaires', 'Reaction', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Response Latencies', 'Sensorimotor functions', 'Sensory', 'Techniques', 'Testing', 'Thumb structure', 'Time', 'Translational Research', 'Trees', 'Validation', 'Wrist', 'data mining', 'diagnosis evaluation', 'experience', 'grasp', 'improved', 'indexing', 'median nerve', 'motor control', 'neurophysiology', 'novel', 'programs', 'research study', 'response', 'stretch reflex', 'tool', 'vector']",NIAMS,CLEVELAND CLINIC LERNER COM-CWRU,R01,2011,353250,-0.007369930566367146
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,8055907,R01LM009731,"['Address', 'Algorithms', 'Area', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'International', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Population', 'Prevention', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'disorder prevention', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2011,325956,-0.004505079728883449
"Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging    DESCRIPTION (provided by applicant): Multiplexed biomarker analysis is more powerful in reflecting the biological behaviors of a tumor than single biomarker analysis, but its standardization and quantification is still a challenge. Furthermore, most computer software does not provide methods for imaging and analyzing subcellular localization of biomarkers and correlating them with biological and clinical information. The objective of this project is to develop a platform which combines imaging and quantification of multiplexed immunostaining plus bioinformatics for the prediction of lymph node metastases (LNM) from the primary tumor (PT) of squamous cell carcinoma of the head and neck (SCCHN). LNM of SCCHN is a precisely defined biological phenomenon which is an ideal model to be utilized to develop this multiplexed biomarker platform (MBP). Based on our preliminary studies, we aim to test the hypothesis that that the MBP can be developed to identify the subcellular distribution and expression of multiple metastasis-related biomarkers simultaneously in PTs. Accurate quantification of these biomarkers will facilitate the prediction of metastasis from PTs. Three emerging technologies, quantum dot (QD)-based immunohistofluorescence (IHF), multispectral imaging, and machine learning will be used to test this hypothesis. Using these approaches, a platform that combines quantifying multiplexed immunostaining with biostatistics will be developed and tested for its sensitivity, specificity, and prediction power for use in the clinic. Therefore, this project fits appropriately to the scope of the NCI program announcement ""Developmental Research in Cancer Prognosis and Prediction"" (PA-09-159).  Three aims are proposed in the study. (1) To develop a multiplexed biomarker system and method based on a bulk tissue model for prediction of LNM in SCCHN PT tissues. This Aim will establish and validate an analysis methodology for multiplexed quantification of membrane and cytoplasmic staining using a new function in InForm software where subcellular localization of certain biomarkers will be specifically analyzed. Prediction of LNM based on this bulk tissue model will be achieved. (2) To develop a per-cell quantification method based on a sub-population model for prediction of LNM in SCCHN PT tissue.  The per-cell analysis results will quantified as the percentage of high risk cells from the multiplexed biomarker analyses in the same PTs. The high risk population will be correlated with LNM. The sensitivity and specificity of the prediction by the sub-population model will be compared with that of the bulk tissue model. (3) To develop and validate a nomogram with software combining clinical characterizations of metastasis as a working platform for the prediction of LNM. While the primary endpoint of Aim 1 and 2 is to correlate the three biomarkers with metastasis, other clinical factors such as differentiation status, tumor stage, and site, etc. may also correlate with LNM. The most predictive biomarker set combined with relevant clinical factors will constitute a platform with computer software that will be validated in an additional 100 SCCHN samples for prediction of LNM.      PUBLIC HEALTH RELEVANCE: Imaging and quantifying expression and subcellular localization of multiplexed biomarkers is currently a challenge in cancer research and clinical application. This project aims to develop a platform which combines imaging and quantifying multiplexed biomarkers plus their correlation with lymph node metastasis of squamous cell carcinoma of the head and neck. This platform can be used for an assessment of LNM which is paramount for appropriate treatment planning.           Imaging and quantifying expression and subcellular localization of multiplexed biomarkers is currently a challenge in cancer research and clinical application. This project aims to develop a platform which combines imaging and quantifying multiplexed biomarkers plus their correlation with lymph node metastasis of squamous cell carcinoma of the head and neck. This platform can be used for an assessment of LNM which is paramount for appropriate treatment planning.         ",Developing a Platform for Prediction of Metastasis Using Multiplexed QD-imaging,8177540,R33CA161873,"['Behavior', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Biometry', 'Breast Cancer Cell', 'Cancer Prognosis', 'Cell membrane', 'Cells', 'Clinic', 'Clinical', 'Color', 'Colorectal Cancer', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disseminated Malignant Neoplasm', 'E-Cadherin', 'Emerging Technologies', 'Epidermal Growth Factor Receptor', 'Epithelial', 'Flow Cytometry', 'Head and Neck Squamous Cell Carcinoma', 'Human', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Literature', 'Lung', 'Machine Learning', 'Mesenchymal', 'Methodology', 'Methods', 'Modeling', 'NIH Program Announcements', 'Neoplasm Metastasis', 'Nomograms', 'Operative Surgical Procedures', 'Outcome', 'Population', 'Primary Neoplasm', 'Production', 'Quantum Dots', 'Reaction', 'Receiver Operating Characteristics', 'Research', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Specificity', 'Specimen', 'Staining method', 'Stains', 'Standardization', 'System', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Tumor Tissue', 'Tumor stage', 'Work', 'aldehyde dehydrogenases', 'anticancer research', 'base', 'cancer cell', 'cancer stem cell', 'clinical application', 'high risk', 'lymph nodes', 'model development', 'nanoparticle', 'neoplastic cell', 'novel', 'outcome forecast', 'tool', 'treatment planning', 'tumor']",NCI,EMORY UNIVERSITY,R33,2011,308241,-0.04563357442031696
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,-0.031209714835308273
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,8143297,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'animal tissue', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2011,132786,-0.02095514567583906
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,8079474,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,250488,-0.038604794493349075
"Replacement Ocular Battery (ROBatt)    DESCRIPTION (provided by applicant): The objective of this grant project is the development and pre-validation of the Replacement Ocular Battery (ROBatt), a tiered testing strategy consisting of a battery of four alternative ocular irritancy assays, which will replace regulatory mandated acute ocular irritation testing using the Draize Rabbit Eye test. ROBatt consists of the Bovine Corneal Opacity and Permeability Assay (BCOP), the Chorioallantoic Membrane Vascular Assay using 10-day fertile chicken eggs (CAMVA), the Porcine Corneal Reversibility Assay (PorCORA) and the Porcine Confocal Assay (PorFocal). This tiered strategy follows a decision tree that allows for a thorough interrogation of possible ocular irritants. Although four assays are recommended, in most cases only two or three will be used depending on the degree of irritation. The Specific Aim of the ROBatt project is to validate the decision tree variables using at least 50 chemicals listed in the European Center for Ecotoxicology and Toxicology of Chemicals (ECETOC) data bank, including Corrosive (EEC R41, GHS/EPA Cat 1), Severe (EEC R36, GHS CaL 2, HMIS 2), Moderate (EPA Cat. 3, HMIS 2), Mild (HMIS 1) and Non-irritating (EPA Cat 4, HMIS 0). The long-term project goal is to submit the ROBatt testing strategy to iCCVAM/ECVAM for consideration as a standalone alternative to the Draize Rabbit Eye test. Validation and acceptance of the ROBatt testing strategy will significantly reduce the number of rabbits used in the toxicological assessment of consumer products, chemicals and raw materials by replacing rabbits with four robust alternative assays.       PUBLIC HEALTH RELEVANCE: Ocular irritation testing is extremely relevant to assuring adequate safety levels of public health as new formulations of chemicals and products are introduced. In most cases, these safety assessments are performed using the Draize Rabbit Eye test, resulting in thousands of rabbits used in testing every year. Alternatives have been discussed since the 80s without any appreciable acceptance from regulators.           n/a",Replacement Ocular Battery (ROBatt),8150947,U01NS073481,"['Acute', 'Biological Assay', 'Blood Vessels', 'Cattle', 'Chemicals', 'Chickens', 'Cornea', 'Corneal Opacity', 'Corrosives', 'Databases', 'Decision Trees', 'Development', 'Drug Formulations', 'European', 'Eye', 'Family suidae', 'Felis catus', 'Goals', 'Grant', 'Irritants', 'Oryctolagus cuniculus', 'Permeability', 'Public Health', 'Safety', 'Test Result', 'Testing', 'Toxicology', 'Validation', 'chorioallantoic membrane', 'consumer product', 'egg', 'irritation', 'public health relevance']",NINDS,"MB RESEARCH LABORATORIES, INC.",U01,2011,483139,-0.011864118826564865
"An implantable device to predict and prevent seizures    DESCRIPTION (provided by applicant): After the first four years of our Bioengineering Research Partnership, implantable devices for epilepsy are now a reality. This is due, in part, to translation of technology developed by our group to industry. Data from multi-center clinical trials of first generation responsive antiepileptic devices indicate that this new technology is safe, and that there is promise of significant benefit to patients. They also demonstrate that 1st-generation devices rarely make patients seizure free. This is because we do not yet understand when, where and how to deliver electrical stimulation to pre-empt seizures, or the mechanisms underlying seizure generation in epileptic networks. These challenges, and translating them into more effective second-generation devices, are the focus of this proposal. Specifically, our aims are: (1) To understand mechanisms underlying seizure generation in two well characterized, spontaneously seizing animal models of epilepsy with documented similarities to refractory human epilepsy, (2) To map seizure generation in the epileptic network to determine where to place sensing electrodes and when to stimulate to maximize seizure suppression and minimize side effects. (3) To develop more effective closed loop stimulation algorithms for controlling seizures. These Aims will be accomplished through a series of projects led by established collaborators in neurology, neuroscience, bioengineering and industry, at Penn, CHOP, Georgia Tech, and BioQuantix, Inc. Teams will focus on improving upon results from first-generation human devices through detailed animal experiments on multiple temporal and spatial scales. These include: (1) the cellular level, through broad-band unit recording and biophysically accurate computational modeling; (2) the network level, with in vitro experiments on hippocampal slices using voltage sensitive dyes and multi-electrode arrays; and (3) the whole brain level, through simultaneous micro and macroelectrode field recordings and responsive brain stimulation in vivo. These experiments will build upon the substantial progress made during the first cycle of our Bioengineering Research Partnership grant. The unique composition of our group, its track record of successful technology transfer, and our ability to learn from and immediately convey our discoveries to existing programmable devices, provide an unprecedented opportunity to perform cutting-edge neuroscience and bioengineering research and immediately translate it into better treatment for patients.           n/a",An implantable device to predict and prevent seizures,8059576,R01NS041811,"['Adverse effects', 'Algorithms', 'Animal Experimentation', 'Animal Experiments', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Biological Models', 'Biomedical Engineering', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Devices', 'Drops', 'Dyes', 'Eels', 'Electric Stimulation', 'Electrodes', 'Elements', 'Engineering', 'Epilepsy', 'Event', 'Evolution', 'Feedback', 'Frequencies', 'Generations', 'Genetic', 'Grant', 'Hippocampus (Brain)', 'Human', 'In Vitro', 'Industry', 'Interneurons', 'Learning', 'Link', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Metabotropic Glutamate Receptors', 'Methods', 'Multi-Institutional Clinical Trial', 'Neurology', 'Neurosciences', 'Organism', 'Outcome', 'Patients', 'Pattern', 'Phase', 'Physiological', 'Prevention', 'Probability', 'Process', 'Protocols documentation', 'Refractory', 'Research', 'Research Personnel', 'Resolution', 'Rodent Model', 'Seizures', 'Series', 'Signal Transduction', 'Simulate', 'Site', 'Slice', 'Statistical Methods', 'Statistical Models', 'Subclinical Seizures', 'Synaptic plasticity', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Translating', 'Translations', 'base', 'computerized data processing', 'experience', 'gamma-Aminobutyric Acid', 'implantable device', 'improved', 'in vitro Model', 'in vivo', 'insight', 'interest', 'markov model', 'neurosurgery', 'new technology', 'novel', 'prevent', 'programs', 'research study', 'sensor', 'voltage']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2011,1244329,-0.013005309326476857
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,-0.01429938606532231
"Biosignatures of Treatment Remission in Major Depression Major Depressive Disorder (MDD) is associated with structural, functional, and neurochemical alterations in key interrelated brain circuits involved in emotion, reward, and executive functioning. Current models of its etiology, including genetic expression, gene environment interactions, the monoamine hypothesis, and neurogenesis guided our choice of biomarkers. We propose to use biomarkers from several levels of organization that address one or more of these models and examine their ability to predict treatment remission. At the genetic level, we will examine epigenetic measures and the transcriptome. At the molecular level, the utility of measures of 5HT1a neuroreceptor binding using Position Emission Tomography and proteomics will be investigated. At the anatomical level, we will examine white matter tract integrity and regional decreases in cortical thickness. Functional assessments include electroencephalography, loudness dependent auditory evoked potentials, and neurocognitive performance. Clinical features will be studied as well, e.g. presence of anxious depression, family history of depression, and others. While receiving supportive clinical management, 300 patients will be observed medication free for 3 weeks, to diminish the influence of placebo response and minimize effects on biosignature assays. Those still meeting criteria after the 3 weeks will receive all aforementioned assessments. Patients then will be randomized in a doublemasked fashion to bupropion or escitalopram, two of the most commonly prescribed treatments for depression, with putatively distinct mechanisms of action. Treatment will be for 12-14 weeks. Treatment outcome will be remission, measures of symptomatic improvement, and assessment of adverse events. Non-remitters will be crossed over. Outcomes will be measured with both traditional and contemporary clinical assessments. Patients will be followed for 6 months after randomization to assess maintenance of response and remission. We will also use a comprehensive analysis algorithm, using novel statistical techniques for high dimensional data to develop an optimal predictive model of treatment outcome that includes all data recorded from all modalities. The statistical team will develop new strategies to address the complex data set to be generated by this study. The resulting optimized algorithm for predicting remission can serve as the basis for a new study intended to validate this tool for personalized treatment of depression. Data and biological materials collected in this project would become part of a repository, open to qualified individuals for additional analysis. This application is in response to RFA MH-10-040: Biosignature Discovery for Personalized Treatment in Depression.",Biosignatures of Treatment Remission in Major Depression,8332889,U01MH092250,"['Accounting', 'Address', 'Adverse effects', 'Adverse event', 'Affect', 'Affinity', 'Algorithms', 'Anatomy', 'Anisotropy', 'Antidepressive Agents', 'Auditory Evoked Potentials', 'Binding', 'Biocompatible Materials', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Markers', 'Blood', 'Brain', 'Brain imaging', 'Brain-Derived Neurotrophic Factor', 'Bupropion', 'Cell Line', 'Characteristics', 'Child Abuse', 'Chronic', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Complex', 'Constitution', 'Data', 'Data Set', 'Databases', 'Dependency', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Disease remission', 'Disease susceptibility', 'Early-life trauma', 'Electroencephalography', 'Emotions', 'Ensure', 'Environment', 'Epigenetic Process', 'Escitalopram', 'Etiology', 'Family history of', 'Female', 'Future', 'Gender', 'Gene Expression Profile', 'Generations', 'Genes', 'Genetic', 'Genetic Crossing Over', 'Goals', 'Gonadal Steroid Hormones', 'Grant', 'Image Analysis', 'Individual', 'Intervention', 'Leadership', 'Literature', 'Loudness', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Masks', 'Measurement', 'Measures', 'Melancholic Depression', 'Menopausal Status', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Modification', 'Molecular', 'Morbidity - disease rate', 'National Institute of Mental Health', 'Neurocognitive', 'Neurons', 'Outcome', 'Outcome Measure', 'Panic Attack', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Placebo Effect', 'Positioning Attribute', 'Positron-Emission Tomography', 'Prefrontal Cortex', 'Proteome', 'Proteomics', 'Protocols documentation', 'Publishing', 'Qualifying', 'Quality of life', 'Randomized', 'Randomized Clinical Trials', 'Recording of previous events', 'Relapse', 'Reporting', 'Research Personnel', 'Resources', 'Rest', 'Rewards', 'Risk', 'Role', 'Sampling', 'Selective Serotonin Reuptake Inhibitor', 'Sensory Receptors', 'Series', 'Serotonin', 'Serotonin Receptor 5-HT1A', 'Signaling Protein', 'Specific qualifier value', 'Structure', 'Techniques', 'Testing', 'Thick', 'Thinking', 'Training', 'Trauma', 'Treatment outcome', 'Validation', 'Variant', 'base', 'biosignature', 'clinical phenotype', 'cost', 'depressive symptoms', 'design', 'disability', 'early-onset alcoholism', 'executive function', 'experience', 'follow-up', 'gene environment interaction', 'immortalized cell', 'in vivo', 'interest', 'meetings', 'monoamine', 'multi-site trial', 'neurochemistry', 'neurocognitive test', 'neurogenesis', 'neurotransmission', 'novel', 'pediatric trauma', 'predictive modeling', 'psychosocial', 'receptor', 'receptor binding', 'repository', 'response', 'restoration', 'sex', 'stressor', 'success', 'symptomatic improvement', 'tomography', 'tool', 'transcriptomics', 'treatment response', 'white matter']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2011,614421,-0.0036950204048786183
"Development of vector-specific, resistance-breaking insecticides to reduce malari    DESCRIPTION (provided by applicant): Malaria exacts a terrible toll in sub-Saharan Africa, killing an estimated 1-2 million persons each year, mostly children. Pyrethroid-based insecticide treated nets (pyrethroid ITNs) provide the first line of defense against disease transmission, but emerging resistant strains of the disease vector (Anopheles gambiae) threaten to render these ITNs ineffective. Our broad objective is to develop a new class of acetylcholinesterase (AChE)-targeting insecticide for deployment on ITNs, that is safe for use, effective against current pyrethroid- and AChE- resistant strains, and is less likely to foster emergence of new AChE-resistant strains. Thus our goal is consistent with the focus of the solicitation on novel interventions for the control of Malaria. FNIH-sponsored research from 2005-2008 enabled us to make significant progress towards our long-term goal. Further support from NIH will allow us to establish proof of concept that our novel AChE-based insecticide, deployed on an ITN, would constitute a superior intervention to manage the disease vector. Thus our goal is also consistent with the stated aim of the solicitation to fund translational research.       To achieve our goal we have assembled a team of chemists, structural biologists, entomologists, and toxicologists. Our specific aims are to 1)improve stability of An. gambiae AChE (AgAChE)-selective carbamates to oxidative detoxification; 2)acquire 3D structural information on AgAChE to optimize inhibition potency and selectivity; 3)develop bivalent carbamates for resilience to target-site mutation; 4)identify strategies to mitigate against carboxylesterase-mediated detoxification; and 5)make a preliminary assessment of mammalian toxicity of the most promising insecticides to emerge from these studies. To guide us through the proposed five years of research we have prepared a detailed timeline and decision tree that incorporate five integrated streams of insecticide discovery for optimizing field performance and human safety. Moreover the built-in complementarity of the chemical synthesis routes and the optimization approaches (e.g. resilience to both target-site and metabolic resistance mechanisms) means that unexpected difficulty in one stream need not slow progress in the other streams. These multiple approaches increase the probability of project success.       Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment, effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.              Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment , effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.","Development of vector-specific, resistance-breaking insecticides to reduce malari",8049059,R01AI082581,"['Acetylcholinesterase', 'Acetylcholinesterase Inhibitors', 'Acute', 'Address', 'Africa South of the Sahara', 'African', 'Agriculture', 'Amines', 'Anopheles Genus', 'Anopheles gambiae', 'Antidotes', 'Binding', 'Binding Sites', 'Biological Assay', 'Carbamates', 'Carboxylic Ester Hydrolases', 'Catalytic Domain', 'Child', 'Cholinesterase Inhibitors', 'Computer Simulation', 'Crystallization', 'Culicidae', 'Decision Trees', 'Development', 'Disease Vectors', 'Drug Metabolic Detoxication', 'Enzymes', 'Fostering', 'Funding', 'Goals', 'Human', 'Insecticide Resistance', 'Insecticides', 'Intervention', 'Length', 'Life', 'Ligands', 'Malaria', 'Measures', 'Mediating', 'Metabolic', 'Mus', 'Mutation', 'Oral', 'Paper', 'Performance', 'Peripheral', 'Permethrin', 'Persons', 'Phenylcarbamates', 'Probability', 'Propoxur', 'Relative (related person)', 'Research', 'Research Design', 'Research Methodology', 'Resistance', 'Risk', 'Roentgen Rays', 'Route', 'Safety', 'Screening procedure', 'Site', 'Stream', 'Structural Biologist', 'Structure', 'Testing', 'TimeLine', 'Toxic effect', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Variant', 'base', 'carboxylesterase', 'chemical synthesis', 'design', 'disease transmission', 'improved', 'in vitro Assay', 'inhibitor/antagonist', 'innovation', 'killings', 'mutant', 'novel', 'pharmacophore', 'pyrethroid', 'research study', 'resilience', 'resistance mechanism', 'resistant strain', 'success', 'transmission process', 'vector', 'vector control', 'vector mosquito', 'virtual']",NIAID,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2011,677523,-0.008161672171836745
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,8137721,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Clinical assessments', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Health', 'Health Personnel', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Imagery', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'data modeling', 'graphical user interface', 'hemodynamics', 'imaging informatics', 'improved', 'innovation', 'insight', 'mortality', 'network models', 'patient population', 'prognostic', 'programs', 'prospective', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2011,581841,-0.030462471411229108
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,8049892,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2010,5742,-0.002605419330863029
"Predicting Patient Outcomes from Clinical and Genome-Wide Data    DESCRIPTION (provided by applicant):       Clinical classification and prediction are key components of clinical care. Even modest improvements in classification and predictive performance may have significant healthcare consequences in terms of improved patient outcomes and reduced healthcare costs. This proposal describes a new, efficient Bayesian machine-learning method for performing clinical predictions. The method is designed to use both clinical and genome-wide data in predicting patient outcomes.       The method's performance will be evaluated using existing clinical and genome-wide data from the Framingham Heart Study that is being made available to qualified researchers through the SHARe resource of the National Center for Biotechnology Information. The outcomes to be predicted in individuals include the onset of major cardiovascular events and death from all causes. The study will evaluate how well these predictions can be made with a new Bayesian method when using traditional clinical data alone, genome-wide data alone, and both types of data together. The method's performance will also be compared that of existing models and methods.       The main hypothesis to be tested is that the proposed machine-learning method will be an advancement over existing methods in that it will be computationally feasible to apply it using a combination of traditional clinical data and genome-wide data, and it will yield better predictive performance than do existing predictive models and methods. If shown to be so, this new method is anticipated to provide substantial benefit in future electronic patient-care systems, including applications to computer-based decision support that have available both traditional clinical data and genome-wide data.            Predicting whether an individual will acquire a disease is an important clinical task. This project will develop and evaluate a new computer-based method to predict diseases in individuals based on the use of both traditional clinical data and data about an individual's genetic make up. Advances obtained in predictive performance may have significant healthcare consequences in terms of improved patient care and outcomes.",Predicting Patient Outcomes from Clinical and Genome-Wide Data,7860710,R01LM010020,"['Address', 'Bayesian Method', 'Biotechnology', 'Calibration', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Discrimination', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Framingham Heart Study', 'Future', 'Generations', 'Genetic', 'Health Care Costs', 'Healthcare', 'Individual', 'Internet', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Methods', 'Modeling', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Performance', 'Publishing', 'Qualifying', 'Research Personnel', 'Resource Sharing', 'Resources', 'Source Code', 'Stroke', 'Testing', 'Work', 'base', 'care systems', 'clinical care', 'data sharing', 'design', 'genome-wide', 'improved', 'insight', 'novel strategies', 'predictive modeling']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,582590,-0.03146836202638411
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7913074,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,1248287,-0.011013903556950932
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8121894,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,300000,-0.011013903556950932
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8144973,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,113520,-0.011013903556950932
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8147585,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,151816,-0.011013903556950932
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8068069,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,51400,-0.01233503867323192
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7828142,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,338802,-0.01233503867323192
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7912919,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,304151,-0.014834418641739596
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,7961101,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2010,221192,-0.011365437999994479
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7945336,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'abstracting', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs']",NICHD,SEQUOIA FOUNDATION,RC2,2010,1397584,-0.012494522576585404
"Prediction of influenza antigenic variants using a novel sparse multitask learnin    DESCRIPTION (provided by applicant): Influenza and influenza related complication lead to more than 200,000 hospitalizations and approximately 36,000 deaths in the United States each year, and vaccination is the primary option for reducing influenza effect. A large amount of global efforts has to be made each year to identify antigenic variants and decide whether new vaccine strains are needed. Current laboratory based antigenic characterization processes are labor intensive and time consuming, and it has been the bottleneck for generating an effective influenza vaccination program. A robust method without such a laboratory characterization is demanding for rapid identification of influenza antigenic variants. This project proposes to develop a novel sparse multitask learning method in predicting influenza antigenic variants solely based on the input of protein sequences, and further to apply this method in mapping antigenic drift pathway of A/H3N2 influenza viruses and studying antigenic drift patterns leading to influenza outbreaks. This method is based on the assumption that influenza antigenicity would be determined by certain features in hemagglutinin (HA) protein sequence and tertiary structure. This assumption was well evidenced that the viruses with conserved HAs generated cross-reactions in serological reactions and also provided cross- protection in both laboratory experiments and field practices. The proposed method is novel since it combines multitask learning and sparse learning. Therefore not only this project will develop significant technology for antigenic variant screen, but also new machine learning methods. This project will facilitate vaccine strain selection since the proposed method can potentially reduce and even eliminate serological assay, one of the most labor intensive procedures, in influenza surveillance. In addition, the antigenicity specific features and the drift patterns causing influenza outbreaks to be identified in this study will enhance our understanding about antigen-antibody interaction thus enhance our knowledge in influenza immunology and serology. Furthermore, the proposed method is potentially applicable in characterizing antigenic properties of other pathogens with significant antigenic variations, for example, rotavirus. The specific aims are the following: (1) Development of a novel sparse multitask learning method in generating antigenic distance matrix using hemagglutinin inhibition (HI) data; (2) Development of a quantitative method for predicting antigenic variants in silicon; (3) Application of this method in studying seasonal influenza antigenic drift pathway and antigenic drift patterns leading to influenza outbreaks. This nature of this study is to address a novel predictive method for measuring antigenic divergence between influenza viruses, which is critical in influenza vaccine strain selection. Thus, we are submitting this project to the broad challenge area (06) Enabling Technologies and fit for the Specific Challenge 06-GM-103: development of predictive method for molecular structure, recognition, and ligand interaction.       PUBLIC HEALTH RELEVANCE: This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.               Project Narrative This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.",Prediction of influenza antigenic variants using a novel sparse multitask learnin,7835340,RC1AI086830,"['Address', 'Amino Acid Sequence', 'Antibodies', 'Antigenic Variation', 'Antigens', 'Area', 'Biological Assay', 'Cessation of life', 'Communities', 'Complication', 'Computing Methodologies', 'Cross Reactions', 'Data', 'Development', 'Epidemic', 'Hemagglutinin', 'Hospitalization', 'Immunology', 'Influenza', 'Influenza A Virus, H3N2 Subtype', 'Influenza vaccination', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Ligands', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Membrane Glycoproteins', 'Methods', 'Molecular Structure', 'Mutation', 'Nature', 'Online Systems', 'Pathway interactions', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Procedures', 'Process', 'Property', 'Reaction', 'Research', 'Rotavirus', 'Seasons', 'Serologic tests', 'Serological', 'Silicon', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Trees', 'United States', 'Vaccination', 'Vaccines', 'Variant', 'Viral', 'Virus', 'base', 'genetic analysis', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'novel', 'novel vaccines', 'pathogen', 'programs', 'public health relevance', 'research study', 'seasonal influenza', 'tool', 'vector']",NIAID,MISSISSIPPI STATE UNIVERSITY,RC1,2010,412913,-0.027965492762541913
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7805478,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2010,339537,-0.004505079728883449
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,-0.010096297488627929
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,8115481,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'operation', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,99989,-0.013162946245757095
"A Novel Computational Framework for Individualized Clinical Decision-Making    DESCRIPTION (provided by applicant):       Personalized therapy requires innovative genome-scale studies for identifying expression patterns in disease progression. Since each individual feature selection algorithm has different strengths, hybrid models, combining multiple algorithms, have become necessary for identifying clinically relevant biomarkers. Furthermore, it is important to reveal disease-mediated biomarker interactions, including feedback circuits, for more effective therapy. This proposal will develop a novel bioinformatics framework by combining genomics, proteomics, and clinical approaches for more informed clinical decision-making.       In Aim 1, we will develop a feature selection system by integrating multiple algorithms for biomarker identification. Combinations of several feature selection methods in different stages of gene filtering will be investigated. The optimal combination scheme for generating the highest prediction accuracy with the minimum number of biomarkers will be determined for several cancer types. The identified biomarkers will be validated by extensive public data sets. In Aim 2, we will develop a novel methodology for modeling biomarker interaction patterns for clinical classification. Based on the expression profiles of the biomarkers selected in Aim 1, Dempster-Shafer belief networks will be employed for predicting individual clinical outcome. The network structure will elucidate molecular interactions among the biomarker proteins in disease progression. Algorithms will be developed to optimize the performance of the Dempster-Shafer network formalism. Different combination rules of Dempster-Shafer theory will be implemented in the belief networks to handle various real- life clinical applications. In Aim 3, stringent criteria will be applied to compare Dempster-Shafer networks with Bayesian networks and other machine learning methods, using the same data sets. The best molecular classifiers will be identified and evaluated with respect to traditional prognostic factors. This strategy will allow patient stratification based on risk of tumor recurrence and the need for adjuvant chemotherapy. The biomarker interactions derived in Dempster-Shafer networks and Bayesian networks will be evaluated for providing useful biological insights. A web-based infrastructure for clinical decision-making will be developed and validated.       This project will focus on predicting metastasis and relapse in non-small cell lung cancer and colorectal cancer. This multidisciplinary research will involve collaborations among bioinformaticians, clinicians, and biomedical researchers for algorithm development and evaluation with respect to strategies for biomarker- based patient stratification and assessment of therapeutic outcomes in different prognostic groups. The project results will be evaluated in prospective clinical trials for colorectal cancer treatment intervention. Our long-term goals are to identify biomarkers that reveal important molecular mechanisms and/or therapeutic targets underlying disease and to make accurate clinical predictions for personalized therapy. This study will advance the computational modeling of human genome data in disease for clinical decision-making.           Local and distant recurrence is the major cause of cancer-related deaths. This research will develop an innovative computational system with implications for early detection of cancer as well as potential recurrence and metastasis, leading to increased survival rates. This study will advance biomedical informatics for applications in clinical decision- making.",A Novel Computational Framework for Individualized Clinical Decision-Making,7918914,R01LM009500,"['Address', 'Adjuvant Chemotherapy', 'Algorithms', 'Arts', 'Belief', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Cancer Etiology', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Trials', 'Collaborations', 'Colorectal Cancer', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Distant', 'Early Diagnosis', 'Evaluation', 'Feedback', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Hybrids', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Life', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Molecular Profiling', 'Neoplasm Metastasis', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Prevention', 'Preventive Medicine', 'Prognostic Factor', 'Proteins', 'Proteomics', 'Recurrence', 'Relapse', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Scheme', 'Screening for cancer', 'Staging', 'Stratification', 'Structure', 'Survival Rate', 'System', 'Testing', 'Therapeutic', 'base', 'biomedical informatics', 'cancer recurrence', 'cancer therapy', 'cancer type', 'clinical application', 'clinical care', 'clinical decision-making', 'clinically relevant', 'combinatorial', 'computer based statistical methods', 'computer framework', 'data mining', 'design', 'effective therapy', 'forest', 'innovation', 'insight', 'network models', 'novel', 'outcome forecast', 'prognostic', 'prospective', 'statistics', 'theories', 'therapeutic target', 'tumor']",NLM,WEST VIRGINIA UNIVERSITY,R01,2010,321216,-0.04799414545476073
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7825424,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2010,185505,0.009444958478312306
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7798186,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'allograft rejection', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2010,289814,-0.013327860403782433
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7941984,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2010,303186,-0.02095514567583906
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7858165,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,253269,-0.038604794493349075
"An implantable device to predict and prevent seizures    DESCRIPTION (provided by applicant): After the first four years of our Bioengineering Research Partnership, implantable devices for epilepsy are now a reality. This is due, in part, to translation of technology developed by our group to industry. Data from multi-center clinical trials of first generation responsive antiepileptic devices indicate that this new technology is safe, and that there is promise of significant benefit to patients. They also demonstrate that 1st-generation devices rarely make patients seizure free. This is because we do not yet understand when, where and how to deliver electrical stimulation to pre-empt seizures, or the mechanisms underlying seizure generation in epileptic networks. These challenges, and translating them into more effective second-generation devices, are the focus of this proposal. Specifically, our aims are: (1) To understand mechanisms underlying seizure generation in two well characterized, spontaneously seizing animal models of epilepsy with documented similarities to refractory human epilepsy, (2) To map seizure generation in the epileptic network to determine where to place sensing electrodes and when to stimulate to maximize seizure suppression and minimize side effects. (3) To develop more effective closed loop stimulation algorithms for controlling seizures. These Aims will be accomplished through a series of projects led by established collaborators in neurology, neuroscience, bioengineering and industry, at Penn, CHOP, Georgia Tech, and BioQuantix, Inc. Teams will focus on improving upon results from first-generation human devices through detailed animal experiments on multiple temporal and spatial scales. These include: (1) the cellular level, through broad-band unit recording and biophysically accurate computational modeling; (2) the network level, with in vitro experiments on hippocampal slices using voltage sensitive dyes and multi-electrode arrays; and (3) the whole brain level, through simultaneous micro and macroelectrode field recordings and responsive brain stimulation in vivo. These experiments will build upon the substantial progress made during the first cycle of our Bioengineering Research Partnership grant. The unique composition of our group, its track record of successful technology transfer, and our ability to learn from and immediately convey our discoveries to existing programmable devices, provide an unprecedented opportunity to perform cutting-edge neuroscience and bioengineering research and immediately translate it into better treatment for patients.           n/a",An implantable device to predict and prevent seizures,7808779,R01NS041811,"['Adverse effects', 'Algorithms', 'Animal Experimentation', 'Animal Experiments', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Biological', 'Biomedical Engineering', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Devices', 'Drops', 'Dyes', 'Eels', 'Electric Stimulation', 'Electrodes', 'Elements', 'Engineering', 'Epilepsy', 'Event', 'Evolution', 'Feedback', 'Frequencies', 'Generations', 'Genetic', 'Grant', 'Hippocampus (Brain)', 'Human', 'In Vitro', 'Industry', 'Interneurons', 'Learning', 'Link', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Metabotropic Glutamate Receptors', 'Methods', 'Multi-Institutional Clinical Trial', 'Neurology', 'Neurosciences', 'Organism', 'Outcome', 'Patients', 'Pattern', 'Phase', 'Physiological', 'Prevention', 'Probability', 'Process', 'Protocols documentation', 'Refractory', 'Research', 'Research Personnel', 'Resolution', 'Rodent Model', 'Seizures', 'Series', 'Signal Transduction', 'Simulate', 'Site', 'Slice', 'Statistical Methods', 'Statistical Models', 'Subclinical Seizures', 'Synaptic plasticity', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Translating', 'Translations', 'base', 'computerized data processing', 'experience', 'implantable device', 'improved', 'in vitro Model', 'in vivo', 'insight', 'interest', 'markov model', 'neurosurgery', 'new technology', 'novel', 'prevent', 'programs', 'research study', 'sensor', 'voltage']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2010,1249881,-0.013005309326476857
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,-0.031209714835308273
"Replacement Ocular Battery (ROBatt)    DESCRIPTION (provided by applicant): The objective of this grant project is the development and pre-validation of the Replacement Ocular Battery (ROBatt), a tiered testing strategy consisting of a battery of four alternative ocular irritancy assays, which will replace regulatory mandated acute ocular irritation testing using the Draize Rabbit Eye test. ROBatt consists of the Bovine Corneal Opacity and Permeability Assay (BCOP), the Chorioallantoic Membrane Vascular Assay using 10-day fertile chicken eggs (CAMVA), the Porcine Corneal Reversibility Assay (PorCORA) and the Porcine Confocal Assay (PorFocal). This tiered strategy follows a decision tree that allows for a thorough interrogation of possible ocular irritants. Although four assays are recommended, in most cases only two or three will be used depending on the degree of irritation. The Specific Aim of the ROBatt project is to validate the decision tree variables using at least 50 chemicals listed in the European Center for Ecotoxicology and Toxicology of Chemicals (ECETOC) data bank, including Corrosive (EEC R41, GHS/EPA Cat 1), Severe (EEC R36, GHS CaL 2, HMIS 2), Moderate (EPA Cat. 3, HMIS 2), Mild (HMIS 1) and Non-irritating (EPA Cat 4, HMIS 0). The long-term project goal is to submit the ROBatt testing strategy to iCCVAM/ECVAM for consideration as a standalone alternative to the Draize Rabbit Eye test. Validation and acceptance of the ROBatt testing strategy will significantly reduce the number of rabbits used in the toxicological assessment of consumer products, chemicals and raw materials by replacing rabbits with four robust alternative assays.       PUBLIC HEALTH RELEVANCE: Ocular irritation testing is extremely relevant to assuring adequate safety levels of public health as new formulations of chemicals and products are introduced. In most cases, these safety assessments are performed using the Draize Rabbit Eye test, resulting in thousands of rabbits used in testing every year. Alternatives have been discussed since the 80s without any appreciable acceptance from regulators.           n/a",Replacement Ocular Battery (ROBatt),8068095,U01NS073481,"['Acute', 'Biological Assay', 'Blood Vessels', 'Cattle', 'Chemicals', 'Chickens', 'Cornea', 'Corneal Opacity', 'Corrosives', 'Databases', 'Decision Trees', 'Development', 'Drug Formulations', 'European', 'Eye', 'Family suidae', 'Felis catus', 'Goals', 'Grant', 'Instruction', 'Irritants', 'Oryctolagus cuniculus', 'Permeability', 'Public Health', 'Safety', 'Test Result', 'Testing', 'Toxicology', 'Validation', 'chorioallantoic membrane', 'consumer product', 'egg', 'irritation']",NINDS,"MB RESEARCH LABORATORIES, INC.",U01,2010,562935,-0.011864118826564865
"Automated Biodosimetry The Unites States Government and Federal agencies have estimated that over 1 million people may seek information on their personal risk (radiation exposure level), if an event (such as a 10 kiloton detonation of an improvised nuclear device) occurs in a large metropolitan area. There will be a need for immediate triage and rapid cytogenetic biodosimetry for estimation of whole-body dose (<2Gy, 2-4Gy, >4Gy) of individuals within 48 hours, to determine and categorize the exposed cohorts for dose based stratification and effective medical management. Individuals exposed at levels exceeding 2 Gy will need immediate treatment and further evaluation with more sophisticated and precise dosimetry tools (such as devices, bioassays, biomarkers etc) to ascertain their absorbed dose.    Several of these established available assays are very labor intensive (demands skilled personnel; cytogeneticist) and time consuming (days to weeks), which serves to be a bane during a mass-casualty event. Effective medical management, response and treatment to the exposed cohort (within a short time window) can only be maximized and achieved with the aid of robotic tools and automated system. Automated cytogenetic systems can also reduce significant level of human error caused by fatigue due to the magnitude of samples to be processed. The Automated Cytogenetics Laboratory at AFRRI, focuses on automation of various classical biodosimerty assays (Dicentric Chromosomes, Micronuclei, PCC assays etc) to effectively enhance the throughput of sample analysis, thereby medical management. n/a",Automated Biodosimetry,8172179,1OD0505,"['Area', 'Artificial Intelligence', 'Automation', 'Biological Assay', 'Biological Markers', 'Biological Neural Networks', 'Cytogenetics', 'Devices', 'Dicentric chromosome', 'Documentation', 'Dose', 'Evaluation', 'Event', 'Fatigue', 'Hour', 'Human', 'Human Resources', 'Image Analysis', 'Individual', 'Laboratories', 'Medical', 'Metaphase', 'Modeling', 'Nuclear', 'Process', 'Radiation', 'Risk', 'Robotics', 'Sampling', 'State Government', 'Stratification', 'System', 'Systems Analysis', 'Testing', 'Time', 'Triage', 'United States', 'Variant', 'base', 'biodosimetry', 'cohort', 'dosimetry', 'handheld mobile device', 'metropolitan', 'micronucleus', 'tool', 'treatment response']",OD,"OFFICE OF THE DIRECTOR, NATIONAL INSTITUTES OF HEALTH",Y01,2010,698597,-0.026141410169079126
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,-0.01429938606532231
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,-0.018176313649747946
"Development of vector-specific, resistance-breaking insecticides to reduce malari    DESCRIPTION (provided by applicant): Malaria exacts a terrible toll in sub-Saharan Africa, killing an estimated 1-2 million persons each year, mostly children. Pyrethroid-based insecticide treated nets (pyrethroid ITNs) provide the first line of defense against disease transmission, but emerging resistant strains of the disease vector (Anopheles gambiae) threaten to render these ITNs ineffective. Our broad objective is to develop a new class of acetylcholinesterase (AChE)-targeting insecticide for deployment on ITNs, that is safe for use, effective against current pyrethroid- and AChE- resistant strains, and is less likely to foster emergence of new AChE-resistant strains. Thus our goal is consistent with the focus of the solicitation on novel interventions for the control of Malaria. FNIH-sponsored research from 2005-2008 enabled us to make significant progress towards our long-term goal. Further support from NIH will allow us to establish proof of concept that our novel AChE-based insecticide, deployed on an ITN, would constitute a superior intervention to manage the disease vector. Thus our goal is also consistent with the stated aim of the solicitation to fund translational research.       To achieve our goal we have assembled a team of chemists, structural biologists, entomologists, and toxicologists. Our specific aims are to 1)improve stability of An. gambiae AChE (AgAChE)-selective carbamates to oxidative detoxification; 2)acquire 3D structural information on AgAChE to optimize inhibition potency and selectivity; 3)develop bivalent carbamates for resilience to target-site mutation; 4)identify strategies to mitigate against carboxylesterase-mediated detoxification; and 5)make a preliminary assessment of mammalian toxicity of the most promising insecticides to emerge from these studies. To guide us through the proposed five years of research we have prepared a detailed timeline and decision tree that incorporate five integrated streams of insecticide discovery for optimizing field performance and human safety. Moreover the built-in complementarity of the chemical synthesis routes and the optimization approaches (e.g. resilience to both target-site and metabolic resistance mechanisms) means that unexpected difficulty in one stream need not slow progress in the other streams. These multiple approaches increase the probability of project success.       Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment, effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.              Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment , effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.","Development of vector-specific, resistance-breaking insecticides to reduce malari",7796794,R01AI082581,"['Acetylcholinesterase', 'Acetylcholinesterase Inhibitors', 'Ache', 'Acute', 'Address', 'Africa South of the Sahara', 'African', 'Agriculture', 'Amines', 'Anopheles Genus', 'Anopheles gambiae', 'Antidotes', 'Binding', 'Binding Sites', 'Biological Assay', 'Carbamates', 'Carboxylic Ester Hydrolases', 'Catalytic Domain', 'Child', 'Cholinesterase Inhibitors', 'Computer Simulation', 'Crystallization', 'Culicidae', 'Decision Trees', 'Development', 'Disease Vectors', 'Drug Metabolic Detoxication', 'Enzymes', 'Fostering', 'Funding', 'Goals', 'Human', 'Insecticide Resistance', 'Insecticides', 'Intervention', 'Length', 'Life', 'Ligands', 'Malaria', 'Measures', 'Mediating', 'Metabolic', 'Methods', 'Mus', 'Mutation', 'Oral', 'Paper', 'Performance', 'Peripheral', 'Permethrin', 'Persons', 'Phenylcarbamates', 'Probability', 'Propoxur', 'Relative (related person)', 'Research', 'Research Design', 'Resistance', 'Risk', 'Roentgen Rays', 'Route', 'Safety', 'Screening procedure', 'Site', 'Stream', 'Structural Biologist', 'Structure', 'Testing', 'TimeLine', 'Toxic effect', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Variant', 'base', 'carboxylesterase', 'chemical synthesis', 'design', 'disease transmission', 'improved', 'in vitro Assay', 'inhibitor/antagonist', 'innovation', 'killings', 'mutant', 'novel', 'pharmacophore', 'pyrethroid', 'research study', 'resilience', 'resistance mechanism', 'resistant strain', 'success', 'transmission process', 'vector', 'vector control', 'vector mosquito', 'virtual']",NIAID,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2010,722567,-0.008161672171836745
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,-0.024283636176862026
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,7928213,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Stroke', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'computer based statistical methods', 'data modeling', 'graphical user interface', 'hemodynamics', 'improved', 'innovation', 'insight', 'mortality', 'nervous system disorder', 'network models', 'prognostic', 'programs', 'prospective', 'public health relevance', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2010,600856,-0.030462471411229108
"Integrated Cardio-Renal Risk Prediction Models in Type 1 Diabetes    DESCRIPTION (provided by applicant):  This application addresses broad Challenge Area (03) Biomarker Discovery and Validation, and the specific Challenge Topic that this application addresses is: 03-DK-101 Discovery of biomarkers for disease risk, progression or response to therapy in diseases of interest to NIDDK. The Challenge: Despite tremendous progress in treatment, patients with type 1 diabetes continue to die 15 years earlier, experience excess morbidity and have medical costs over 10 times higher than the general population. The risk of coronary artery disease and diabetic nephropathy is still greatly increased and responsible for 80% of deaths in these patients. Diabetic nephropathy and coronary artery disease are intertwined, suggesting common pathways, and yet current risk prediction is inadequate. Study Rationale: Nearly 25% of T1D patients develop end-stage renal disease. The conventional theory is that the sequence of events leading to diabetic nephropathy begins from microalbuminuria, progressing to overt proteinuria and eventual end-stage renal disease. Primary prevention with ACE/ARB treatment usually begins when persistent microalbuminuria is found. However, recent prospective studies using serial measurements of kidney function estimated from serum cystatin C have changed this paradigm by demonstrating that decline in glomerular filtration may begin in the absence of microalbuminuria or continue despite remission of microalbuminuria. By their mid 40's, over 70% of men and 50% of women with type 1 diabetes develop coronary artery calcification (CAC) - a marker of significant atherosclerotic plaque burden. Cardiovascular disease is particularly prevalent among patients with renal disease, but most cardiac events occur in patients with normal albumin excretion rates. Evidence has accumulated that diabetic nephropathy and CAC are parallel, rather than sequential, complications of type 1 diabetes, sharing a number of important predictors. Accurate prediction of the individual's global cardio-renal risk is needed to guide treatment choices. As a result, there is a recognized need for better risk partitioning, either from the discovery of additional biomarkers or the application of superior methods for discrimination of disease states. Our Approach: We are proposing to use the biobank of 652 adults with type 1 diabetes who have been thoroughly genotyped and prospectively followed for development of diabetic nephropathy and coronary artery disease by the Coronary Artery Calcification in Type 1 Diabetes study (CACTI, R01 HL61753, 1999- 2009) to develop and validate integrated biomarker prediction models for cardio-renal complications in type 1 diabetes. Our study aims are the investigation of biomarkers within the well-characterized CACTI cohort for 1) early progressive renal function decline, 2) the presence and progression of coronary artery calcium, and the 3) development of combined renal and cardiovascular disease burden. Renal disease and coronary artery disease (CAD) are the leading causes of death among adults with type 1 diabetes, but traditional screening tests (urinary albumin and serum creatinine) for renal disease are inadequate, and traditional CAD risk factors (dyslipidemia, hypertension, smoking, obesity) do not fully explain the increased burden of disease associated with type 1 diabetes. Renal and cardiovascular complications are intertwined, suggesting shared pathways. Increased systemic inflammation is present in type 1 diabetes and inflammatory mediators such as uric acid, vitamin D and bone metabolism markers have been suggested to play a role in both renal and cardiovascular diseases. As a result, it is of critical public health importance to discover and validate new biomarkers for cardio-renal disease among adults with type 1 diabetes.       PUBLIC HEALTH RELEVANCE:  Renal disease and coronary artery disease (CAD) are the leading causes of death among adults with type 1 diabetes, but traditional screening tests (urinary albumin and serum creatinine) for renal disease are inadequate, and traditional CAD risk factors (dyslipidemia, hypertension, smoking, obesity) do not fully explain the increased burden of disease associated with type 1 diabetes. Renal and cardiovascular complications are intertwined, suggesting shared pathways. Increased systemic inflammation is present in type 1 diabetes and inflammatory mediators such as uric acid, vitamin D and bone metabolism markers have been suggested to play a role in both renal and cardiovascular diseases. As a result, it is of critical public health importance to discover and validate new biomarkers for cardio-renal disease among adults with type 1 diabetes.          Project Narrative  Renal disease and coronary artery disease (CAD) are the leading causes of death among adults with type 1 diabetes, but traditional screening tests (urinary albumin and serum creatinine) for renal disease are inadequate, and traditional CAD risk factors (dyslipidemia, hypertension, smoking, obesity) do not fully explain the increased burden of disease associated with type 1 diabetes. Renal and cardiovascular complications are intertwined, suggesting shared pathways. Increased systemic inflammation is present in type 1 diabetes, and inflammatory mediators such as uric acid, vitamin D and bone metabolism markers have been suggested to play a role in both renal and cardiovascular diseases. As a result, it is of critical public health importance to discover and validate new biomarkers for cardio-renal disease among adults with type 1 diabetes.",Integrated Cardio-Renal Risk Prediction Models in Type 1 Diabetes,7829755,RC1DK086958,"['Address', 'Adult', 'Age', 'Albumins', 'Albuminuria', 'Area', 'Arterial Fatty Streak', 'Artificial Intelligence', 'Biological Markers', 'Biological Neural Networks', 'Blood', 'C-reactive protein', 'Calcium', 'Candidate Disease Gene', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cause of Death', 'Cessation of life', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Creatinine', 'Data', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Discrimination', 'Disease', 'Disease remission', 'Dyslipidemias', 'End stage renal failure', 'Estradiol', 'Event', 'Excretory function', 'General Population', 'Genes', 'Genetic Markers', 'Genetic Polymorphism', 'Genotype', 'Glomerular Filtration Rate', 'Glycosylated hemoglobin A', 'Goals', 'High Density Lipoprotein Cholesterol', 'Homocysteine', 'Homocystine', 'Hypertension', 'Hyperuricemia', 'IL6 gene', 'Individual', 'Inflammation', 'Inflammation Mediators', 'Inflammatory', 'Insulin-Dependent Diabetes Mellitus', 'Interleukin 2 Receptor', 'Interleukin-18', 'Investigation', 'Kidney', 'Kidney Diseases', 'Measurement', 'Medical', 'Methods', 'Microalbuminuria', 'Modeling', 'Morbidity - disease rate', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Obesity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Phenotype', 'Play', 'Primary Prevention', 'Prospective Studies', 'Proteinuria', 'Public Health', 'Renal function', 'Risk', 'Risk Factors', 'Role', 'Screening procedure', 'Serum', 'Serum Albumin', 'Sex Hormone-Binding Globulin', 'Smoke', 'Smoking', 'Staging', 'Techniques', 'Testing', 'Testosterone', 'Time', 'Tumor necrosis factor receptor 11b', 'Uric Acid', 'Validation', 'Vitamin D', 'Vitamin D Deficiency', 'Woman', 'adaptive immunity', 'adiponectin', 'base', 'biobank', 'bone metabolism', 'burden of illness', 'calcification', 'chemokine', 'cohort', 'coronary artery calcification', 'cost', 'cytokine', 'disorder risk', 'early experience', 'follow-up', 'genome wide association study', 'glomerular filtration', 'inflammatory marker', 'interest', 'lipoprotein-associated phospholipase A(2)', 'men', 'novel', 'post gamma-globulins', 'predictive modeling', 'public health relevance', 'response', 'statistics', 'theories', 'urinary']",NIDDK,UNIVERSITY OF COLORADO DENVER,RC1,2010,496938,-0.02973062720470292
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7670456,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,74750,-0.002605419330863029
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7560409,R01NS051826,"['Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Area', 'Back', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Modeling', 'Neonatal', 'Normal Range', 'Operative Surgical Procedures', 'Patients', 'Population', 'Population Study', 'Process', 'Research', 'Schizophrenia', 'Shapes', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'base', 'computer studies', 'disease classification', 'feeding', 'improved', 'neonate', 'nervous system disorder', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,280500,-0.019617763661010812
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,-0.011109811062455753
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant): This proposal is submitted in response to NOT-OD-09-058 NIH Announces the Availability of Recovery Act Funds for Competitive Revision Applications. Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Our currently funded R03 grant, ""Recursive partitioning and ensemble methods for classifying an ordinal response,"" consists of the following three specific aims (SA.1) extend the recursive partitioning and random forest classification methodologies for predicting an ordinal response by developing computational tools for the R programming environment including implementing our ordinal impurity criteria in rpart and implementing the ordinal impurity criteria in randomForest; (SA.2) evaluate the proposed ordinal classification methods in comparison to existing nominal and continuous response methods using simulated, benchmark, and gene expression datasets; and (SA.3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been described. Herein we propose to extend the L1 penalized method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing a model-based ordinal classification methodology applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies considered in the parent grant. The specific aims of this competitive revision application are to: Aim 1) Extend the L1 penalized methodology to enable predicting an ordinal response by developing computational tools for the R programming environment; Aim 2) Using simulated, benchmark, and gene expression datasets, evaluate L1 penalized ordinal response models by comparing error rates from our L1 fitting algorithm to those obtained when using a forward variable selection modeling strategy and our ordinal random forest approach; and Aim 3) Evaluate methods for assessing important covariates from L1 penalized ordinal response models.           This project will develop L1 penalized ordinal response models and implement them in the R programming environment. By conducting extensive comparisons of various ordinal response modeling methods using simulated, benchmark, and gene expression datasets, we will be able to make a recommendation regarding ordinal response modeling to the scientific community. This research is significant since the ordinal response modeling methods developed during the project period will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.",Recursive partitioning and ensemble methods for classifying an ordinal response,7805045,R03LM009347,"['Advocate', 'Algorithms', 'Applications Grants', 'Area', 'Behavioral Research', 'Benchmarking', 'Bioconductor', 'Biopsy Specimen', 'Cancer Patient', 'Chronic Hepatitis', 'Classification', 'Clinical', 'Communities', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Drug toxicity', 'Economics', 'Education', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Gene Expression', 'Genomics', 'Grant', 'Health', 'Health Status', 'Health Surveys', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Literature', 'Location', 'Logistics', 'Machine Learning', 'Mathematics', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Neoplasm Metastasis', 'Occupations', 'Outcome', 'Patients', 'Performance', 'Positioning Attribute', 'Progressive Disease', 'Recommendation', 'Recovery', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Sampling', 'Science', 'Scoring Method', 'Simulate', 'Solid Neoplasm', 'Stable Disease', 'Staging', 'Technology', 'Translational Research', 'Travel', 'Trees', 'United States National Institutes of Health', 'base', 'computerized tools', 'cost', 'forest', 'heuristics', 'improved', 'indexing', 'interest', 'liver biopsy', 'meetings', 'neglect', 'novel', 'parent grant', 'partial response', 'preference', 'programs', 'research study', 'response', 'simulation', 'social', 'software development', 'symposium', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,75000,-0.019798169839607756
"Predicting Patient Outcomes from Clinical and Genome-Wide Data    DESCRIPTION (provided by applicant):       Clinical classification and prediction are key components of clinical care. Even modest improvements in classification and predictive performance may have significant healthcare consequences in terms of improved patient outcomes and reduced healthcare costs. This proposal describes a new, efficient Bayesian machine-learning method for performing clinical predictions. The method is designed to use both clinical and genome-wide data in predicting patient outcomes.       The method's performance will be evaluated using existing clinical and genome-wide data from the Framingham Heart Study that is being made available to qualified researchers through the SHARe resource of the National Center for Biotechnology Information. The outcomes to be predicted in individuals include the onset of major cardiovascular events and death from all causes. The study will evaluate how well these predictions can be made with a new Bayesian method when using traditional clinical data alone, genome-wide data alone, and both types of data together. The method's performance will also be compared that of existing models and methods.       The main hypothesis to be tested is that the proposed machine-learning method will be an advancement over existing methods in that it will be computationally feasible to apply it using a combination of traditional clinical data and genome-wide data, and it will yield better predictive performance than do existing predictive models and methods. If shown to be so, this new method is anticipated to provide substantial benefit in future electronic patient-care systems, including applications to computer-based decision support that have available both traditional clinical data and genome-wide data.            Predicting whether an individual will acquire a disease is an important clinical task. This project will develop and evaluate a new computer-based method to predict diseases in individuals based on the use of both traditional clinical data and data about an individual's genetic make up. Advances obtained in predictive performance may have significant healthcare consequences in terms of improved patient care and outcomes.",Predicting Patient Outcomes from Clinical and Genome-Wide Data,7634045,R01LM010020,"['Address', 'Bayesian Method', 'Biotechnology', 'Calibration', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Discrimination', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Framingham Heart Study', 'Future', 'Generations', 'Genetic', 'Health Care Costs', 'Healthcare', 'Individual', 'Internet', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Methods', 'Modeling', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Performance', 'Publishing', 'Qualifying', 'Research Personnel', 'Resource Sharing', 'Resources', 'Source Code', 'Stroke', 'Testing', 'Work', 'base', 'care systems', 'clinical care', 'data sharing', 'design', 'genome-wide', 'improved', 'insight', 'novel strategies', 'predictive modeling']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,579698,-0.03146836202638411
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7622614,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2009,1224323,-0.011013903556950932
"Computational Annotation of Orphan Metabolic Activities    DESCRIPTION (provided by applicant):  Even state-of-the-art homology methods cannot annotate metabolic genes with no or remote sequence identity to known enzymes. This presents a significant obstacle to network reconstruction, as about 30%- 40% (>1500) of known metabolic activities remain orphan, i.e. there are no known proteins catalyzing these activities in any organism. The scale of the orphan activities problem makes it arguably the single biggest challenge of modern biochemistry. We propose to develop, experimentally validate, and make available to the scientific community an efficient computational approach to fill the remaining gaps in metabolic networks. The main idea of the proposed method is to use genes assigned to the network neighbors of the remaining gaps as constraints in assigning genes for orphan activities. We demonstrate that this approach significantly outperforms simpler or existing methods. Our cross-validated results in model organisms demonstrate that the proposed method can predict the correct genes in more than 50% of the cases, without any sequence homology information. The calculations indicate that the prediction accuracy will also remain high in less studied organisms. Using the developed method we have already identified and validated a gene responsible for an E. coli metabolic activity which remained orphan for more than 25 years. There are four specific aims of the proposal: 1.) We will calculate the appropriate context-based descriptors of protein function for the majority of sequenced organisms. Many new functional descriptors will be developed and used for the predictions. 2.) We will investigate the ability of various machine learning approaches and fitness functions to integrate context-based descriptors. Based on the developed methodology we will make predictions for all orphan activities in sequenced organisms. 3.) The predictions will be available through a searchable and constantly updated Web server. We will also develop a method to detect functional misannotations and apply it to all public metabolic databases. 4.) In collaboration with the laboratories of Dr. Uwe Sauer (ETH Zurich) and Dr. George Church (Harvard) we will experimentally test at least 50 of the predicted genes without close sequence homologs in E. coli, B. subtilis, S. cerevisiae.           n/a",Computational Annotation of Orphan Metabolic Activities,7653790,R01GM079759,"['Animal Model', 'Arts', 'Base Sequence', 'Biochemical', 'Biochemical Pathway', 'Biochemistry', 'Biological Neural Networks', 'Church', 'Collaborations', 'Communities', 'Databases', 'Decision Trees', 'Descriptor', 'Enzymes', 'Escherichia coli', 'Evolution', 'Gene Fusion', 'Genes', 'Genetic', 'Internet', 'Laboratories', 'Link', 'Machine Learning', 'Metabolic', 'Methodology', 'Methods', 'Operon', 'Organism', 'Orphan', 'Pathway interactions', 'Performance', 'Positioning Attribute', 'Proteins', 'Research Personnel', 'Saccharomyces cerevisiae', 'Sequence Homologs', 'Sequence Homology', 'Specific qualifier value', 'Structure', 'Testing', 'Update', 'Validation', 'base', 'computer based statistical methods', 'fitness', 'gene correction', 'gene function', 'genome sequencing', 'metabolomics', 'protein function', 'reconstruction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,397297,-0.033006217663972985
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7685518,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Series', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'new therapeutic target', 'novel', 'pathway tools', 'programs', 'reconstruction']",NLM,SRI INTERNATIONAL,R01,2009,175647,-0.017473350950006156
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7577491,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2009,342223,-0.01233503867323192
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7670408,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,311541,-0.014834418641739596
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7853378,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs', 'public health relevance']",NICHD,SEQUOIA FOUNDATION,RC2,2009,2003191,-0.012494522576585404
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7848604,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170861,-0.004505079728883449
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7901729,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170789,-0.004505079728883449
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7612766,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,342967,-0.004505079728883449
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,-0.010096297488627929
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7633119,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2009,18437,-0.01311839654305209
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7652508,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,363929,-0.013162946245757095
"A Novel Computational Framework for Individualized Clinical Decision-Making    DESCRIPTION (provided by applicant):       Personalized therapy requires innovative genome-scale studies for identifying expression patterns in disease progression. Since each individual feature selection algorithm has different strengths, hybrid models, combining multiple algorithms, have become necessary for identifying clinically relevant biomarkers. Furthermore, it is important to reveal disease-mediated biomarker interactions, including feedback circuits, for more effective therapy. This proposal will develop a novel bioinformatics framework by combining genomics, proteomics, and clinical approaches for more informed clinical decision-making.       In Aim 1, we will develop a feature selection system by integrating multiple algorithms for biomarker identification. Combinations of several feature selection methods in different stages of gene filtering will be investigated. The optimal combination scheme for generating the highest prediction accuracy with the minimum number of biomarkers will be determined for several cancer types. The identified biomarkers will be validated by extensive public data sets. In Aim 2, we will develop a novel methodology for modeling biomarker interaction patterns for clinical classification. Based on the expression profiles of the biomarkers selected in Aim 1, Dempster-Shafer belief networks will be employed for predicting individual clinical outcome. The network structure will elucidate molecular interactions among the biomarker proteins in disease progression. Algorithms will be developed to optimize the performance of the Dempster-Shafer network formalism. Different combination rules of Dempster-Shafer theory will be implemented in the belief networks to handle various real- life clinical applications. In Aim 3, stringent criteria will be applied to compare Dempster-Shafer networks with Bayesian networks and other machine learning methods, using the same data sets. The best molecular classifiers will be identified and evaluated with respect to traditional prognostic factors. This strategy will allow patient stratification based on risk of tumor recurrence and the need for adjuvant chemotherapy. The biomarker interactions derived in Dempster-Shafer networks and Bayesian networks will be evaluated for providing useful biological insights. A web-based infrastructure for clinical decision-making will be developed and validated.       This project will focus on predicting metastasis and relapse in non-small cell lung cancer and colorectal cancer. This multidisciplinary research will involve collaborations among bioinformaticians, clinicians, and biomedical researchers for algorithm development and evaluation with respect to strategies for biomarker- based patient stratification and assessment of therapeutic outcomes in different prognostic groups. The project results will be evaluated in prospective clinical trials for colorectal cancer treatment intervention. Our long-term goals are to identify biomarkers that reveal important molecular mechanisms and/or therapeutic targets underlying disease and to make accurate clinical predictions for personalized therapy. This study will advance the computational modeling of human genome data in disease for clinical decision-making.           Local and distant recurrence is the major cause of cancer-related deaths. This research will develop an innovative computational system with implications for early detection of cancer as well as potential recurrence and metastasis, leading to increased survival rates. This study will advance biomedical informatics for applications in clinical decision- making.",A Novel Computational Framework for Individualized Clinical Decision-Making,7685444,R01LM009500,"['Address', 'Adjuvant Chemotherapy', 'Algorithms', 'Arts', 'Belief', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Cancer Etiology', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Trials', 'Collaborations', 'Colorectal Cancer', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Decision Making', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Distant', 'Early Diagnosis', 'Evaluation', 'Feedback', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Hybrids', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Life', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Molecular Profiling', 'Neoplasm Metastasis', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Prevention', 'Preventive Medicine', 'Prognostic Factor', 'Proteins', 'Proteomics', 'Recurrence', 'Relapse', 'Research', 'Research Personnel', 'Risk', 'Scheme', 'Screening for cancer', 'Staging', 'Stratification', 'Structure', 'Survival Rate', 'System', 'Testing', 'Therapeutic', 'base', 'biomedical informatics', 'cancer recurrence', 'cancer therapy', 'cancer type', 'clinical application', 'clinical care', 'clinical infrastructure', 'clinically relevant', 'combinatorial', 'computer based statistical methods', 'computer framework', 'data mining', 'design', 'effective therapy', 'forest', 'innovation', 'insight', 'network models', 'novel', 'outcome forecast', 'prognostic', 'prospective', 'statistics', 'theories', 'therapeutic target', 'tumor']",NLM,WEST VIRGINIA UNIVERSITY,R01,2009,323471,-0.04799414545476073
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7620994,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,263148,0.009444958478312306
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7809191,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,140804,0.00916604263781605
"Sequence Induction in Infancy: A systematic approach    DESCRIPTION (provided by applicant): Thirteen experiments investigate the early development of the ability to learn patterns in sequential input. Such evidence would seem likely to provide a substantial contribution to our knowledge of how infants identify both concrete and abstract patterns, a fundamental aspect of cognitive development. The proposed experiments are organized into three broad studies. In the first study, we will examine ""ancillary"" factors that may influence sequence learning in infancy. We emphasize the importance of two kinds of relations between elements in any multi-element group or string: itemwise relations, between specific instances (e.g., statistical learning), and variablewise relations, between algebraic placeholders (e.g., abstract pattern or rule learning). We argue that a first step toward any programmatic series of experiments on these kinds of learning-must involve an understanding of processing limitations that may constrain performance. In the second study, we will investigate the different kinds of patterns infants may be able to learn, using past and pilot data as a guide for our theorizing. In the final study, we will explore the intriguing possibility that infants may learn more than one pattern in any single set of inputs, a classic question that has received little empirical attention in the literature on cognitive development. The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the ability to detect abstract visual and auditory structure. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, how perceptual skills impact knowledge acquisition and structure, and how to best characterize early development. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.           n/a",Sequence Induction in Infancy: A systematic approach,7684605,R01HD048733,"['Address', 'Age', 'Attention', 'Auditory', 'Be++ element', 'Beds', 'Beryllium', 'Body of uterus', 'Child', 'Childhood', 'Classification', 'Cognition', 'Cognitive', 'Conceptual Domain', 'Conflict (Psychology)', 'Data', 'Dependency', 'Development', 'Diagnostic', 'Drug Formulations', 'Elements', 'Fostering', 'Future', 'Goals', 'Hand', 'Human', 'Indium', 'Infant', 'Investigation', 'Knowledge', 'Knowledge acquisition', 'Language Development', 'Learning', 'Life', 'Light', 'Literature', 'Machine Learning', 'Medial', 'Nature', 'Pattern', 'Pattern Recognition', 'Performance', 'Process', 'Research', 'Risk', 'Schools', 'Series', 'Shapes', 'Sorting - Cell Movement', 'Staging', 'Structure', 'Techniques', 'Testing', 'Time', 'Vegetables', 'Visual', 'Work', 'abstracting', 'age related', 'base', 'cognitive change', 'coping', 'developmental disease', 'infancy', 'neglect', 'object perception', 'postnatal', 'research study', 'sequence learning', 'skills']",NICHD,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2009,237807,-0.05643928357291187
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7599555,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2009,290671,-0.013327860403782433
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7790821,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Support', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'flexibility', 'innovation', 'interest', 'meetings', 'prevent', 'product development', 'public health relevance', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2009,4047,-0.010171716528154004
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7788875,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Ontology', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'empowered', 'genome wide association study', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'histone modification', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'public health relevance', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2009,142123,-0.04147967896696043
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,-0.022847837373705025
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7666186,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,256073,-0.038604794493349075
"An implantable device to predict and prevent seizures    DESCRIPTION (provided by applicant): After the first four years of our Bioengineering Research Partnership, implantable devices for epilepsy are now a reality. This is due, in part, to translation of technology developed by our group to industry. Data from multi-center clinical trials of first generation responsive antiepileptic devices indicate that this new technology is safe, and that there is promise of significant benefit to patients. They also demonstrate that 1st-generation devices rarely make patients seizure free. This is because we do not yet understand when, where and how to deliver electrical stimulation to pre-empt seizures, or the mechanisms underlying seizure generation in epileptic networks. These challenges, and translating them into more effective second-generation devices, are the focus of this proposal. Specifically, our aims are: (1) To understand mechanisms underlying seizure generation in two well characterized, spontaneously seizing animal models of epilepsy with documented similarities to refractory human epilepsy, (2) To map seizure generation in the epileptic network to determine where to place sensing electrodes and when to stimulate to maximize seizure suppression and minimize side effects. (3) To develop more effective closed loop stimulation algorithms for controlling seizures. These Aims will be accomplished through a series of projects led by established collaborators in neurology, neuroscience, bioengineering and industry, at Penn, CHOP, Georgia Tech, and BioQuantix, Inc. Teams will focus on improving upon results from first-generation human devices through detailed animal experiments on multiple temporal and spatial scales. These include: (1) the cellular level, through broad-band unit recording and biophysically accurate computational modeling; (2) the network level, with in vitro experiments on hippocampal slices using voltage sensitive dyes and multi-electrode arrays; and (3) the whole brain level, through simultaneous micro and macroelectrode field recordings and responsive brain stimulation in vivo. These experiments will build upon the substantial progress made during the first cycle of our Bioengineering Research Partnership grant. The unique composition of our group, its track record of successful technology transfer, and our ability to learn from and immediately convey our discoveries to existing programmable devices, provide an unprecedented opportunity to perform cutting-edge neuroscience and bioengineering research and immediately translate it into better treatment for patients.           n/a",An implantable device to predict and prevent seizures,7616844,R01NS041811,"['Adverse effects', 'Algorithms', 'Animal Experimentation', 'Animal Experiments', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Biological', 'Biomedical Engineering', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Devices', 'Drops', 'Dyes', 'Eels', 'Electric Stimulation', 'Electrodes', 'Elements', 'Engineering', 'Epilepsy', 'Event', 'Evolution', 'Feedback', 'Frequencies', 'Generations', 'Genetic', 'Grant', 'Hippocampus (Brain)', 'Human', 'In Vitro', 'Industry', 'Interneurons', 'Learning', 'Link', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Metabotropic Glutamate Receptors', 'Methods', 'Multi-Institutional Clinical Trial', 'Neurology', 'Neurosciences', 'Organism', 'Outcome', 'Patients', 'Pattern', 'Phase', 'Physiological', 'Prevention', 'Probability', 'Process', 'Protocols documentation', 'Refractory', 'Research', 'Research Personnel', 'Resolution', 'Rodent Model', 'Seizures', 'Series', 'Signal Transduction', 'Simulate', 'Site', 'Slice', 'Statistical Methods', 'Statistical Models', 'Subclinical Seizures', 'Synaptic plasticity', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Translating', 'Translations', 'base', 'computerized data processing', 'experience', 'implantable device', 'improved', 'in vitro Model', 'in vivo', 'insight', 'interest', 'markov model', 'neurosurgery', 'new technology', 'novel', 'prevent', 'programs', 'research study', 'sensor', 'voltage']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2009,1255061,-0.013005309326476857
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7804332,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2009,363247,-0.02095514567583906
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7625039,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Base Management', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data exchange', 'data format', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2009,536571,-0.020387331921485723
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,-0.01429938606532231
"Development of vector-specific, resistance-breaking insecticides to reduce malari    DESCRIPTION (provided by applicant): Malaria exacts a terrible toll in sub-Saharan Africa, killing an estimated 1-2 million persons each year, mostly children. Pyrethroid-based insecticide treated nets (pyrethroid ITNs) provide the first line of defense against disease transmission, but emerging resistant strains of the disease vector (Anopheles gambiae) threaten to render these ITNs ineffective. Our broad objective is to develop a new class of acetylcholinesterase (AChE)-targeting insecticide for deployment on ITNs, that is safe for use, effective against current pyrethroid- and AChE- resistant strains, and is less likely to foster emergence of new AChE-resistant strains. Thus our goal is consistent with the focus of the solicitation on novel interventions for the control of Malaria. FNIH-sponsored research from 2005-2008 enabled us to make significant progress towards our long-term goal. Further support from NIH will allow us to establish proof of concept that our novel AChE-based insecticide, deployed on an ITN, would constitute a superior intervention to manage the disease vector. Thus our goal is also consistent with the stated aim of the solicitation to fund translational research.       To achieve our goal we have assembled a team of chemists, structural biologists, entomologists, and toxicologists. Our specific aims are to 1)improve stability of An. gambiae AChE (AgAChE)-selective carbamates to oxidative detoxification; 2)acquire 3D structural information on AgAChE to optimize inhibition potency and selectivity; 3)develop bivalent carbamates for resilience to target-site mutation; 4)identify strategies to mitigate against carboxylesterase-mediated detoxification; and 5)make a preliminary assessment of mammalian toxicity of the most promising insecticides to emerge from these studies. To guide us through the proposed five years of research we have prepared a detailed timeline and decision tree that incorporate five integrated streams of insecticide discovery for optimizing field performance and human safety. Moreover the built-in complementarity of the chemical synthesis routes and the optimization approaches (e.g. resilience to both target-site and metabolic resistance mechanisms) means that unexpected difficulty in one stream need not slow progress in the other streams. These multiple approaches increase the probability of project success.       Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment, effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.              Malaria exacts a terrible toll in sub-Saharan African, and at present the first line of defense against the mosquito vector of the disease is provided by insecticide treated nets (ITNs). However, growing resistance to the class of insecticide used on the nets threatens to make this protection ineffective. We propose to develop a new class of insecticide that is safe for ITN deployment , effective against current insecticide-resistant mosquitoes, and less likely to promote emergence of new resistant strains.","Development of vector-specific, resistance-breaking insecticides to reduce malari",7657602,R01AI082581,"['Acetylcholinesterase', 'Acetylcholinesterase Inhibitors', 'Ache', 'Acute', 'Address', 'Africa South of the Sahara', 'African', 'Agriculture', 'Amines', 'Anopheles Genus', 'Anopheles gambiae', 'Antidotes', 'Binding', 'Binding Sites', 'Biological Assay', 'Carbamates', 'Carboxylic Ester Hydrolases', 'Catalytic Domain', 'Child', 'Cholinesterase Inhibitors', 'Computer Simulation', 'Crystallization', 'Culicidae', 'Decision Trees', 'Development', 'Disease Vectors', 'Drug Metabolic Detoxication', 'Enzymes', 'Fostering', 'Funding', 'Goals', 'Human', 'Insecticide Resistance', 'Insecticides', 'Intervention', 'Length', 'Life', 'Ligands', 'Malaria', 'Measures', 'Mediating', 'Metabolic', 'Methods', 'Mus', 'Mutation', 'Oral', 'Paper', 'Performance', 'Peripheral', 'Permethrin', 'Persons', 'Phenylcarbamates', 'Probability', 'Propoxur', 'Relative (related person)', 'Research', 'Research Design', 'Resistance', 'Risk', 'Roentgen Rays', 'Route', 'Safety', 'Screening procedure', 'Site', 'Stream', 'Structural Biologist', 'Structure', 'Testing', 'TimeLine', 'Toxic effect', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Variant', 'base', 'carboxylesterase', 'chemical synthesis', 'design', 'disease transmission', 'improved', 'in vitro Assay', 'inhibitor/antagonist', 'innovation', 'killings', 'mutant', 'novel', 'pharmacophore', 'pyrethroid', 'research study', 'resilience', 'resistance mechanism', 'resistant strain', 'success', 'transmission process', 'vector', 'vector control', 'vector mosquito', 'virtual']",NIAID,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2009,730811,-0.008161672171836745
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,-0.024283636176862026
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,7727890,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Stroke', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'computer based statistical methods', 'data modeling', 'graphical user interface', 'hemodynamics', 'improved', 'innovation', 'insight', 'mortality', 'nervous system disorder', 'network models', 'prognostic', 'programs', 'prospective', 'public health relevance', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2009,568500,-0.030462471411229108
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7470967,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Class', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Numbers', 'Outcome', 'Performance', 'Polymerase Chain Reaction', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Standards of Weights and Measures', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2008,74521,-0.002605419330863029
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7351765,R01NS051826,"['Affect', 'Age', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomy', 'Area', 'Back', 'Class', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diffuse Pattern', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Intuition', 'Knowledge', 'Learning', 'Localized', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Research', 'Methods', 'Modeling', 'Morphology', 'Neonatal', 'Neuroanatomy', 'Normal Range', 'Operative Surgical Procedures', 'Pathology', 'Patients', 'Population', 'Population Study', 'Process', 'Range', 'Research', 'Schizophrenia', 'Shapes', 'Standards of Weights and Measures', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Variant', 'base', 'computer studies', 'desire', 'disease classification', 'feeding', 'imaging Segmentation', 'improved', 'neonate', 'nervous system disorder', 'neurosurgery', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2008,281588,-0.019617763661010812
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7363692,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2008,135000,-0.028843787887365874
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,-0.011109811062455753
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7499147,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Depth', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Numbers', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'foot', 'insight', 'member', 'novel', 'quality assurance', 'scale up', 'size', 'symposium', 'theories', 'tool']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2008,1200000,-0.011013903556950932
"Computational Annotation of Orphan Metabolic Activities    DESCRIPTION (provided by applicant):  Even state-of-the-art homology methods cannot annotate metabolic genes with no or remote sequence identity to known enzymes. This presents a significant obstacle to network reconstruction, as about 30%- 40% (>1500) of known metabolic activities remain orphan, i.e. there are no known proteins catalyzing these activities in any organism. The scale of the orphan activities problem makes it arguably the single biggest challenge of modern biochemistry. We propose to develop, experimentally validate, and make available to the scientific community an efficient computational approach to fill the remaining gaps in metabolic networks. The main idea of the proposed method is to use genes assigned to the network neighbors of the remaining gaps as constraints in assigning genes for orphan activities. We demonstrate that this approach significantly outperforms simpler or existing methods. Our cross-validated results in model organisms demonstrate that the proposed method can predict the correct genes in more than 50% of the cases, without any sequence homology information. The calculations indicate that the prediction accuracy will also remain high in less studied organisms. Using the developed method we have already identified and validated a gene responsible for an E. coli metabolic activity which remained orphan for more than 25 years. There are four specific aims of the proposal: 1.) We will calculate the appropriate context-based descriptors of protein function for the majority of sequenced organisms. Many new functional descriptors will be developed and used for the predictions. 2.) We will investigate the ability of various machine learning approaches and fitness functions to integrate context-based descriptors. Based on the developed methodology we will make predictions for all orphan activities in sequenced organisms. 3.) The predictions will be available through a searchable and constantly updated Web server. We will also develop a method to detect functional misannotations and apply it to all public metabolic databases. 4.) In collaboration with the laboratories of Dr. Uwe Sauer (ETH Zurich) and Dr. George Church (Harvard) we will experimentally test at least 50 of the predicted genes without close sequence homologs in E. coli, B. subtilis, S. cerevisiae.           n/a",Computational Annotation of Orphan Metabolic Activities,7496031,R01GM079759,"['Animal Model', 'Arts', 'Base Sequence', 'Biochemical', 'Biochemical Genetics', 'Biochemical Pathway', 'Biochemistry', 'Biological Neural Networks', 'Church', 'Collaborations', 'Communities', 'Databases', 'Decision Trees', 'Descriptor', 'Enzymes', 'Escherichia coli', 'Evolution', 'Gene Fusion', 'Genes', 'Genetic', 'Internet', 'Laboratories', 'Link', 'Machine Learning', 'Metabolic', 'Methodology', 'Methods', 'Numbers', 'Operon', 'Organism', 'Orphan', 'Pathway interactions', 'Performance', 'Positioning Attribute', 'Proteins', 'Range', 'Research Personnel', 'Saccharomyces cerevisiae', 'Sequence Homologs', 'Sequence Homology', 'Specific qualifier value', 'Structure', 'Testing', 'Update', 'Validation', 'base', 'computer based statistical methods', 'fitness', 'gene correction', 'gene function', 'genome sequencing', 'metabolomics', 'protein function', 'reconstruction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,383732,-0.033006217663972985
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7504002,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2008,176002,-0.017473350950006156
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7431959,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Condition', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Numbers', 'Patients', 'Play', 'Population', 'Process', 'Public Health', 'Rate', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'particle', 'size', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2008,376423,-0.01233503867323192
"A Simulation Tool to Enable Identification of Critical Network Interactions Using    DESCRIPTION (provided by applicant): One of the main challenges in the discovery of intracellular biomarkers and identification of therapeutic targets is the lack of a mechanistic understanding of the complex underlying pathways. The tremendous increase in both the quantity and diversity of cellular data represents a significant challenge to researchers seeking to construct biologically relevant interaction maps, and objectively extract specific actionable information. Machine learning based clustering algorithms serve as a preliminary statistical data analysis metric, but they fail to capture the data in the proper biological context. While chemical kinetics based models have proved to be effective in elucidating the pathway mechanisms, accurate estimates for the model parameters are severely lacking and are often impossible to obtain owing to the inherent difficulties involved in making dynamic measurements of specific intracellular phenomena. Additionally, methods for rational prioritization and selection of critical intracellular interactions (in the absence of kinetic information) are sorely lacking. Therefore, there is a clear need for innovative software tools that enable quantitative analysis of available microarray data in a biological pathway context, ultimately leading to the objective identification of critical biological interactions, providing a direction for more focused future efforts. We propose to address this challenge by developing an automated software platform that utilizes microarray data to select and merge relevant canonical biological pathway models thereby placing significantly expressed genes in their biological context. The analysis software will utilize a microarray expression-weighted metric to objectively rank the most critical interactions within the network model using a novel chemical kinetics-free Boolean dynamics algorithm. In the Phase I effort, we will develop a software tool composed of an R library that enables the automated generation of a pathway model from a given microarray dataset. Additionally, a methodology, and associated R library will be developed to objectively rank critical interactions in the pathway model, using a microarray data expression-weighted metric. Demonstration and validation of proposed algorithm will be carried out using a well characterized lipopolysaccharide (LPS) stimulated RAW 264.7 macrophage system. In Phase II, we will extend the scope of the algorithmic framework to include proteomic and metabolomic weighting in the objective ranking of critical interactions, and add workflow improvements through the addition of a graphical user interface (GUI). Experimental verification and validation of critical interactions identified in Phase I will be carried out using gene-silencing techniques. We also intend to establish collaborative partnerships with commercial entities. The proposing team has extensive experience in the areas of systems biology and bioinformatics (CFDRC) and microarray data analysis (Shawn Levy, University of Vanderbilt). CFDRC has a strong track record in the commercialization of software and hardware. PUBLIC HEALTH RELEVANCE:  Recently, there has been a tremendous increase in both the amount and diversity of cellular data available to researchers, representing a clear need for the development of advanced computational analysis software to enable the discovery of biomarkers of disease states, and identification of new therapeutic targets. However, currently available analysis tools do not consider the data in a proper biological context. This research proposes to develop an automated software platform that utilizes available data to develop and analyze mathematical models of complex processes in an automated fashion, resulting in the identification of critical intracellular processes.             n/a",A Simulation Tool to Enable Identification of Critical Network Interactions Using,7482734,R43GM084890,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Bioinformatics', 'Biological', 'Biological Markers', 'Complex', 'Computer Analysis', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Future', 'Gene Silencing', 'Generations', 'Genes', 'Genomics', 'Kinetics', 'Lead', 'Libraries', 'Lipopolysaccharides', 'Machine Learning', 'Maps', 'Measurement', 'Methodology', 'Methods', 'Metric', 'Microarray Analysis', 'Modeling', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Software Tools', 'Statistical Data Interpretation', 'System', 'Systems Biology', 'Techniques', 'Title', 'Universities', 'Urination', 'Validation', 'Weight', 'base', 'chemical kinetics', 'commercialization', 'editorial', 'experience', 'graphical user interface', 'innovation', 'macrophage', 'mathematical model', 'metabolomics', 'network models', 'novel', 'novel therapeutics', 'simulation', 'therapeutic target', 'tool']",NIGMS,CFD RESEARCH CORPORATION,R43,2008,99571,-0.00955543214642325
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7596501,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Chromosome Pairing', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Numbers', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Purpose', 'Rate', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'concept', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,319129,-0.014834418641739596
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7354450,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Class', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Epidemiology, Other', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Infectious Disease Epidemiology', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'success', 'theories', 'tool', 'transmission process', 'transposon/insertion element', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2008,342967,-0.004505079728883449
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,-0.010096297488627929
"A Novel Computational Framework for Individualized Clinical Decision-Making    DESCRIPTION (provided by applicant):       Personalized therapy requires innovative genome-scale studies for identifying expression patterns in disease progression. Since each individual feature selection algorithm has different strengths, hybrid models, combining multiple algorithms, have become necessary for identifying clinically relevant biomarkers. Furthermore, it is important to reveal disease-mediated biomarker interactions, including feedback circuits, for more effective therapy. This proposal will develop a novel bioinformatics framework by combining genomics, proteomics, and clinical approaches for more informed clinical decision-making.       In Aim 1, we will develop a feature selection system by integrating multiple algorithms for biomarker identification. Combinations of several feature selection methods in different stages of gene filtering will be investigated. The optimal combination scheme for generating the highest prediction accuracy with the minimum number of biomarkers will be determined for several cancer types. The identified biomarkers will be validated by extensive public data sets. In Aim 2, we will develop a novel methodology for modeling biomarker interaction patterns for clinical classification. Based on the expression profiles of the biomarkers selected in Aim 1, Dempster-Shafer belief networks will be employed for predicting individual clinical outcome. The network structure will elucidate molecular interactions among the biomarker proteins in disease progression. Algorithms will be developed to optimize the performance of the Dempster-Shafer network formalism. Different combination rules of Dempster-Shafer theory will be implemented in the belief networks to handle various real- life clinical applications. In Aim 3, stringent criteria will be applied to compare Dempster-Shafer networks with Bayesian networks and other machine learning methods, using the same data sets. The best molecular classifiers will be identified and evaluated with respect to traditional prognostic factors. This strategy will allow patient stratification based on risk of tumor recurrence and the need for adjuvant chemotherapy. The biomarker interactions derived in Dempster-Shafer networks and Bayesian networks will be evaluated for providing useful biological insights. A web-based infrastructure for clinical decision-making will be developed and validated.       This project will focus on predicting metastasis and relapse in non-small cell lung cancer and colorectal cancer. This multidisciplinary research will involve collaborations among bioinformaticians, clinicians, and biomedical researchers for algorithm development and evaluation with respect to strategies for biomarker- based patient stratification and assessment of therapeutic outcomes in different prognostic groups. The project results will be evaluated in prospective clinical trials for colorectal cancer treatment intervention. Our long-term goals are to identify biomarkers that reveal important molecular mechanisms and/or therapeutic targets underlying disease and to make accurate clinical predictions for personalized therapy. This study will advance the computational modeling of human genome data in disease for clinical decision-making.           Local and distant recurrence is the major cause of cancer-related deaths. This research will develop an innovative computational system with implications for early detection of cancer as well as potential recurrence and metastasis, leading to increased survival rates. This study will advance biomedical informatics for applications in clinical decision- making.",A Novel Computational Framework for Individualized Clinical Decision-Making,7581252,R01LM009500,"['Address', 'Adjuvant Chemotherapy', 'Algorithms', 'Arts', 'Belief', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Caring', 'Classification', 'Clinical', 'Clinical Trials', 'Collaborations', 'Colorectal Cancer', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Decision Making', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Evaluation', 'Feedback', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Hybrids', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Life', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Molecular Profiling', 'Neoplasm Metastasis', 'Non-Small-Cell Lung Carcinoma', 'Numbers', 'Online Systems', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Prevention', 'Preventive Medicine', 'Prognostic Factor', 'Proteins', 'Proteomics', 'Recurrence', 'Relapse', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Scheme', 'Staging', 'Stratification', 'Structure', 'System', 'Testing', 'Therapeutic', 'base', 'cancer recurrence', 'cancer therapy', 'cancer type', 'clinical application', 'clinically relevant', 'combinatorial', 'computer based statistical methods', 'computer framework', 'data mining', 'design', 'forest', 'innovation', 'insight', 'network models', 'novel', 'outcome forecast', 'prognostic', 'prospective', 'statistics', 'theories', 'therapeutic target', 'tumor']",NLM,WEST VIRGINIA UNIVERSITY,R01,2008,342738,-0.04799414545476073
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,7495003,R01LM008713,"['Accounting', 'Algorithms', 'Biological', 'Chemicals', 'Class', 'Classification', 'Computational Technique', 'Consensus', 'Data Set', 'Dependency', 'Descriptor', 'Effectiveness', 'Elements', 'Facility Construction Funding Category', 'Figs - dietary', 'Frequencies', 'Generations', 'Goals', 'Graph', 'Hybrids', 'Lead', 'Learning', 'Libraries', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Numbers', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Structure', 'Techniques', 'Technology', 'base', 'design', 'desire', 'drug development', 'interest', 'predictive modeling', 'programs', 'vector']",NLM,UNIVERSITY OF MINNESOTA,R01,2008,270892,-0.02890833070419041
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7489320,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2008,52898,-0.01311839654305209
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7495734,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,353327,-0.013162946245757095
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7417618,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,263507,0.009444958478312306
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7611584,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,142424,0.00916604263781605
"Sequence Induction in Infancy: A systematic approach    DESCRIPTION (provided by applicant): Thirteen experiments investigate the early development of the ability to learn patterns in sequential input. Such evidence would seem likely to provide a substantial contribution to our knowledge of how infants identify both concrete and abstract patterns, a fundamental aspect of cognitive development. The proposed experiments are organized into three broad studies. In the first study, we will examine ""ancillary"" factors that may influence sequence learning in infancy. We emphasize the importance of two kinds of relations between elements in any multi-element group or string: itemwise relations, between specific instances (e.g., statistical learning), and variablewise relations, between algebraic placeholders (e.g., abstract pattern or rule learning). We argue that a first step toward any programmatic series of experiments on these kinds of learning-must involve an understanding of processing limitations that may constrain performance. In the second study, we will investigate the different kinds of patterns infants may be able to learn, using past and pilot data as a guide for our theorizing. In the final study, we will explore the intriguing possibility that infants may learn more than one pattern in any single set of inputs, a classic question that has received little empirical attention in the literature on cognitive development. The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the ability to detect abstract visual and auditory structure. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, how perceptual skills impact knowledge acquisition and structure, and how to best characterize early development. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.           n/a",Sequence Induction in Infancy: A systematic approach,7394931,R01HD048733,"['Address', 'Age', 'Attention', 'Auditory', 'Be++ element', 'Beds', 'Beryllium', 'Body of uterus', 'Child', 'Childhood', 'Classification', 'Cognition', 'Cognitive', 'Compatible', 'Conceptual Domain', 'Conflict (Psychology)', 'Count', 'Data', 'Dependency', 'Development', 'Diagnostic', 'Drug Formulations', 'Elements', 'Fostering', 'Future', 'Goals', 'Hand', 'Human', 'Indium', 'Infant', 'Investigation', 'Knowledge', 'Knowledge acquisition', 'Language Development', 'Learning', 'Life', 'Light', 'Literature', 'Machine Learning', 'Medial', 'Nature', 'Numbers', 'Pattern', 'Pattern Recognition', 'Performance', 'Process', 'Range', 'Research', 'Risk', 'Schools', 'Series', 'Shapes', 'Sorting - Cell Movement', 'Staging', 'Structure', 'Techniques', 'Testing', 'Time', 'Vegetables', 'Visual', 'Work', 'abstracting', 'age related', 'base', 'cognitive change', 'coping', 'developmental disease', 'infancy', 'neglect', 'object perception', 'postnatal', 'research study', 'sequence learning', 'size', 'skills']",NICHD,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2008,252858,-0.05643928357291187
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7407451,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2008,291451,-0.013327860403782433
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7535414,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Analysis', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Purpose', 'Records', 'Research', 'Research Methodology', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'Support of Research', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'desire', 'innovation', 'interest', 'prevent', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2008,99843,-0.010171716528154004
"Genotype/Phenotype Correlations in Lysosomal Storage Diseases    DESCRIPTION (provided by applicant): Our long-term goal is to understand the detailed molecular mechanisms that connect genotype, phenotype and response to therapy in lysosomal storage diseases (LSDs). LSDs are a family of genetic metabolic diseases caused by lysosomal enzyme deficiencies. In this project we use Fabry disease as a model system to develop a bioinformatics-based paradigm to address two fundamental issues: 1) The relationship between genotype and phenotype in LSDs. This task is challenging because in LSDs different mutations in the same enzyme often lead to different disease phenotypes. 2) The relationship between genotype and response to ""pharmacological chaperone"" therapy. Pharmacological chaperones are small-molecule ligands that are used to rescue mutants, resulting in increased enzymatic activity; several Fabry mutations have been shown to be rescueable in this way. The same therapy is likely to be useful for other LSDs, particularly those with neurological involvement, for which enzyme replacement therapy is not viable. The two aims of this application address, at different levels, both issues described above. The first aim, tests the hypothesis that knowing the change that occurs in the protein sequence, together with the structural environment in which it occurs, is sufficient to predict the resulting disease phenotype and response to pharmacological chaperone therapy. This is tested through the rigorous training of classification methods using sequence and structure-derived descriptors for a large set of Fabry mutants of known phenotype. The resulting classification provides a large- scale quantitative description of the correlation between genotype and phenotype. The accuracy of predictions based on this approach is a measure of how much information about the genotype the descriptors contain. The same approach will be used to establish a quantitative correlation between genotype and response to pharmacological chaperone therapy. Finally, applying the classification methods to mutations in other LSDs will test the generality of the approach. The second aim of this application addresses the issue of genotype/phenotype correlation from a biophysical point of view. We test the hypothesis that a combination of factors, mainly folding free energy, ligand binding affinity, and relative pH stability of the mutants determines the disease phenotype and response to pharmacological chaperone therapy. This is done analyzing selected mutants using molecular modeling and molecular dynamics simulations of the enzyme/ligand and enzyme/receptor interactions, as well as, pH stability, and other calculations. The methods used in the second aim are very detailed, but are not applicable at a large scale. Thus, both aims provide complementary views of genotype/phenotype correlation in LSDs. The successful completion of this project will, for the first time, provide a quantitative connection between genotype and phenotype in LSDs and a detailed biophysical description of the molecular mechanisms underlying genotype/phenotype correlations and response to pharmacological chaperone therapy in Fabry disease. Relevance of this research to public health. Lysosomal storage diseases (LSDs) are a group of more than 40 genetic metabolic disorders. Worldwide, the incidence of patients with LSDs is estimated to be ~ 1 in 8,000 live births. Understanding the correlation between genotype, phenotype, and response to treatment in these diseases will help in their diagnosis and treatment, particularly for LSDs that affect the brain, for which no effective treatment is available to date.          n/a",Genotype/Phenotype Correlations in Lysosomal Storage Diseases,7388981,R21DK078345,"['Address', 'Affect', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Binding', 'Bioinformatics', 'Biological Models', 'Brain', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Data', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Endoplasmic Reticulum', 'Environment', 'Enzymes', 'Fabry Disease', 'Feedback', 'Free Energy', 'Galactosidase', 'Genetic', 'Genotype', 'Goals', 'Incidence', 'Lead', 'Ligand Binding', 'Ligands', 'Live Birth', 'Lysosomal Storage Diseases', 'Lysosomes', 'Machine Learning', 'Measures', 'Metabolic Diseases', 'Methods', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Models', 'Mutate', 'Mutation', 'Neurologic', 'None or Not Applicable', 'Numbers', 'Output', 'Patients', 'Peptide Sequence Determination', 'Pharmacogenomics', 'Phenotype', 'Principal Investigator', 'Public Health', 'Relative (related person)', 'Research', 'Residual state', 'Structure', 'Testing', 'Time', 'Training', 'base', 'design', 'disease phenotype', 'enzyme deficiency', 'enzyme replacement therapy', 'enzyme structure', 'enzyme substrate', 'family genetics', 'improved', 'insight', 'molecular dynamics', 'molecular modeling', 'mutant', 'prevent', 'programs', 'protein degradation', 'receptor', 'receptor binding', 'response', 'simulation', 'small molecule', 'three dimensional structure']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R21,2008,124583,-0.016014611718963796
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,-0.022847837373705025
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7538527,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'GDF15 gene', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Histones', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Modification', 'Numbers', 'Ontology', 'PLAB Protein', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'RNA', 'Range', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2008,157474,-0.04147967896696043
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7386333,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'None or Not Applicable', 'Numbers', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Purpose', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'cancer microarray', 'cancer type', 'design', 'desire', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,255036,-0.038604794493349075
"An implantable device to predict and prevent seizures    DESCRIPTION (provided by applicant): After the first four years of our Bioengineering Research Partnership, implantable devices for epilepsy are now a reality. This is due, in part, to translation of technology developed by our group to industry. Data from multi-center clinical trials of first generation responsive antiepileptic devices indicate that this new technology is safe, and that there is promise of significant benefit to patients. They also demonstrate that 1st-generation devices rarely make patients seizure free. This is because we do not yet understand when, where and how to deliver electrical stimulation to pre-empt seizures, or the mechanisms underlying seizure generation in epileptic networks. These challenges, and translating them into more effective second-generation devices, are the focus of this proposal. Specifically, our aims are: (1) To understand mechanisms underlying seizure generation in two well characterized, spontaneously seizing animal models of epilepsy with documented similarities to refractory human epilepsy, (2) To map seizure generation in the epileptic network to determine where to place sensing electrodes and when to stimulate to maximize seizure suppression and minimize side effects. (3) To develop more effective closed loop stimulation algorithms for controlling seizures. These Aims will be accomplished through a series of projects led by established collaborators in neurology, neuroscience, bioengineering and industry, at Penn, CHOP, Georgia Tech, and BioQuantix, Inc. Teams will focus on improving upon results from first-generation human devices through detailed animal experiments on multiple temporal and spatial scales. These include: (1) the cellular level, through broad-band unit recording and biophysically accurate computational modeling; (2) the network level, with in vitro experiments on hippocampal slices using voltage sensitive dyes and multi-electrode arrays; and (3) the whole brain level, through simultaneous micro and macroelectrode field recordings and responsive brain stimulation in vivo. These experiments will build upon the substantial progress made during the first cycle of our Bioengineering Research Partnership grant. The unique composition of our group, its track record of successful technology transfer, and our ability to learn from and immediately convey our discoveries to existing programmable devices, provide an unprecedented opportunity to perform cutting-edge neuroscience and bioengineering research and immediately translate it into better treatment for patients.           n/a",An implantable device to predict and prevent seizures,7409622,R01NS041811,"['Adverse effects', 'Algorithms', 'Animal Experimentation', 'Animal Experiments', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Biological', 'Biomedical Engineering', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Devices', 'Drops', 'Dyes', 'Eels', 'Electric Stimulation', 'Electrodes', 'Elements', 'Engineering', 'Epilepsy', 'Event', 'Evolution', 'Feedback', 'Frequencies', 'Generations', 'Genetic Models', 'Grant', 'Hippocampus (Brain)', 'Human', 'In Vitro', 'Industry', 'Interneurons', 'Learning', 'Link', 'Localized', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Metabotropic Glutamate Receptors', 'Methods', 'Multi-Institutional Clinical Trial', 'Neurology', 'Neurosciences', 'Organism', 'Outcome', 'Patients', 'Pattern', 'Phase', 'Physiological', 'Prevention', 'Probability', 'Process', 'Protocols documentation', 'Range', 'Refractory', 'Research', 'Research Personnel', 'Resolution', 'Rodent Model', 'Seizures', 'Series', 'Signal Transduction', 'Simulate', 'Site', 'Slice', 'Statistical Methods', 'Statistical Models', 'Subclinical Seizures', 'Synaptic plasticity', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Translating', 'Translations', 'USA Georgia', 'Western Asia Georgia', 'base', 'computerized data processing', 'experience', 'implantable device', 'improved', 'in vitro Model', 'in vivo', 'insight', 'interest', 'markov model', 'neurosurgery', 'new technology', 'novel', 'prevent', 'programs', 'research study', 'sensor', 'voltage']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2008,1249160,-0.013005309326476857
"Dysbiosis in Inflammatory Bowel Disease    DESCRIPTION (provided by applicant): Inflammatory Bowel Diseases (IBDs), namely ulcerative colitis (DC) and Crohn's disease (CD), are chronic, lifelong, relapsing illnesses, affecting close to 1 million Americans. Despite the many clues that a dysbiosis may exist in IBD, the specific changes in the microflora of IBD patients are largely unknown. Amplicon length heterogeneity (ALH) is a sophisticated and well established PCR based technology that can be used as a screening tool to identify changes in the Bacterial microflora of IBD patients. We therefore have hypothesized that the ileocolonic microflora in IBD has an altered microbial composition compared to normal microflora. We have gathered preliminary data that shows statistical differences between controls and IBD as well as within IBD patients. Therefore, to test the above hypothesis, we are proposing the following 2 inter-related scientific aims: AIM 1. Identify bacterial fingerprint patterns associated with IBD using ALH. ALH patterns will be determined in a total of 160 IBD patients and health controls and will be analyzed using diversity indeces, multivariate reduction analysis and cluster analysis. ALH patterns associated with IBD will be determined using histograms, ecological software in conjunction with custom PERL scripts as well as supervised and unsupervised automated pattern recognition systems. AIM 2.Determine the bacterial contents of putatively IBD associated ALH fingerprint patterns using molecular cloning and sequencing. Cloning and sequencing of targeted samples will be linked to bacterial identities by employing multiple bioinformatics tools. Significance. This proposal involves the first time use of a sophisticated and highly reproducible molecular biology tool, ALH, in the study of microflora in the Gl tract. There is a growing recognition of the importance of microflora in health and disease, including IBD. Studies that characterized microflora in human using powerful techniques from environmental microbiology such as ALH can bring about significant advances in the understanding of Gl tract illnesses. ALH may enable a real-time survey of microfloral changes for the first time in medicine and may provide the first evidence linking IBD with specific microbial patterns.           n/a",Dysbiosis in Inflammatory Bowel Disease,7434510,R21DK071838,"['Affect', 'Age', 'Algorithms', 'American', 'Attention', 'Back', 'Bacteria', 'Bioinformatics', 'Biopsy Specimen', 'Bispecific Antibody 2B1', 'Celiac Disease', 'Chronic', 'Clinical', 'Cloning', 'Cluster Analysis', 'Colon', 'Colonoscopy', 'Colorectal', 'Complex', 'Computer software', 'Crohn&apos', 's disease', 'Custom', 'DNA', 'Data', 'Databases', 'Disease', 'Distal part of ileum', 'Effectiveness', 'Environment', 'Environmental Microbiology', 'Feces', 'Fingerprint', 'Flare', 'Flexible fiberoptic sigmoidoscopy', 'Future', 'Gender', 'Genus Cola', 'Hand', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Human', 'Human body', 'Immunosuppressive Agents', 'Incidence', 'Individual', 'Infection', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Intestines', 'Irritable Bowel Syndrome', 'Knowledge', 'Length', 'Link', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Molecular Biology', 'Molecular Cloning', 'Morbidity - disease rate', 'Mucous Membrane', 'Newly Diagnosed', 'Numbers', 'Operative Surgical Procedures', 'Organ', 'Organism', 'Patients', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Pharmaceutical Preparations', 'Polymerase Chain Reaction', 'Population', 'Probiotics', 'Procedures', 'Race', 'Reaction', 'Recruitment Activity', 'Relapse', 'Relative (related person)', 'Research Personnel', 'Ribosomal RNA', 'Sampling', 'Schedule', 'Screening procedure', 'Soil', 'Surveys', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Toxic effect', 'Ulcerative Colitis', 'Visual', 'base', 'carcinogenesis', 'clinical application', 'computerized', 'cost', 'data mining', 'disabling disease', 'ileum', 'indexing', 'microbial', 'microbial community', 'neglect', 'novel', 'novel diagnostics', 'pathogen', 'prebiotics', 'programs', 'satisfaction', 'success', 'time use', 'tool', 'treatment effect']",NIDDK,RUSH UNIVERSITY MEDICAL CENTER,R21,2008,181300,-0.01396978052881252
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): The proposed Clinical Cytometry Analysis Software Project described in this grant application is designed to create a new, more efficient and effective way of analyzing cells for the presence of cancer, HIV/AIDS and other disease, using a fully automated software system. Using modern data mining techniques (pattern recognition, feature recognition, image analysis) we will design software which will analyze data (the cell samples from patients) at a much faster rate and with fewer false positives and negatives than the manual method now in use. Objectives: Assemble and validate algorithms in software that can automatically classify regions of interest in flow cytometry data. We will demonstrate that the particular populations required by our use cases can be validly, rigorously and repeatably identified automatically. Develop and validate graphical and statistical results that satisfy FDA requirements for medical device software, simplify regulatory compliance by the clinical user, and automatically deliver analysis results to diagnostic expert systems and/or LIMS systems. Satisfy the  translational medicine  goals outlined in the NIH Roadmap. This software will bring the clinician streamlined testing currently only available in research labs. Methods: Four use cases have been selected, one employing synthetic data and three clinical data; Leukemia/Lymphoma test, Analysis of longitudinal Graft vs. Host Disease (GvHD) in bone marrow transplant specimens for predictive markers and HIV/AIDS - Gag-specific T cell cytokine response profile assay. For each we have access to a substantial body of existing data, analyzed by experts. Beginning with the autogating routines in our own FlowJo software, we will test and expand the application of Magnetic gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural and Support Vector Machines (SVM). Using a sampling of human operators to establish a control range, we will test each of these five techniques against the four use cases in cooperation with our collaborators. Events in the manually classified samples are given a weighted score based on the frequency with which they are included by all the operators. A single operator's score or the gating algorithm's score is compared with the cumulative score of the expert group and a match rating is computed. Additional validation techniques include combinatory validation on internal measures with respect to Pareto optimality, and Predictive Power/Stability self consistency checks using resampled or perturbed data measured with external indices such as the adjusted Rand index and the Variation of Information index.  PUBLIC HEALTH RELEVANCE: By eliminating the operator's time, we estimate that the cost of clinical flow cytometry analysis can be reduced to half the current figure while delivering the results much faster. By eliminating the subjectivity and human error of manually created regions and reducing the range of variability of the so created, there would result fewer false positives and false negatives, improving the clinical outcome for those patients needing therapy but undetected by current methods. An order of magnitude increase in speed means faster therapeutic intervention.  A less expensive test improves outcome by making the test accessible to more patients.          n/a",Clinical Cytometry Analysis Software with Automated Gating,7482923,R43RR024094,"['AIDS/HIV problem', 'Algorithms', 'Applications Grants', 'B-Lymphocytes', 'Biological Assay', 'Biological Neural Networks', 'Bone Marrow Transplantation', 'Cells', 'Characteristics', 'Class', 'Classification', 'Clinical', 'Clinical Data', 'Cluster Analysis', 'Complex', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Cytometry', 'Data', 'Data Analyses', 'Data Files', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Educational process of instructing', 'Environment', 'Evaluation', 'Event', 'Expert Systems', 'Facility Construction Funding Category', 'Flow Cytometry', 'Frequencies', 'Funding', 'Future', 'Gagging', 'Generations', 'Goals', 'Grant', 'Graph', 'Human', 'Image Analysis', 'Individual', 'Instruction', 'Knowledge', 'Legal patent', 'Life Cycle Stages', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Noise', 'Numbers', 'Outcome', 'Output', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Population', 'Probability', 'Process', 'Public Health', 'Publishing', 'Range', 'Rate', 'Regulation', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Scientist', 'Score', 'Software Design', 'Software Engineering', 'Software Tools', 'Solutions', 'Specific qualifier value', 'Specimen', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'T-Lymphocyte', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Tube', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Voting', 'Weight', 'base', 'clinical Diagnosis', 'commercialization', 'cost', 'cytokine', 'data mining', 'design', 'improved', 'indexing', 'innovation', 'interest', 'leukemia/lymphoma', 'novel', 'predictive modeling', 'relating to nervous system', 'research study', 'response', 'software systems', 'statistics', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R43,2008,100854,-0.03118635591242026
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7433931,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2008,322087,-0.013104273742450118
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7440169,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2008,535031,-0.020387331921485723
"Causal Discovery Algorithms for Translational Research with High-Throughput Data Project Summary Causal Discovery Algorithms for Translational Research with High-Throughput Data The long-term goal of this project is to provide to the biomedical community next-generation causal algorithms to facilitate discovery of disease molecular pathways and causative as well as predictive biomarkers and molecular signatures from high-throughput data. Such knowledge and methods are necessary toward earlier and more accurate diagnosis and prognosis, personalized medicine, and rational drug design. If successful, the proposed research will have significant and wide methodological and practical implications spanning several areas of biomedicine with a primary focus and immediate benefits in high-throughput diagnostics and personalized medicine. It will provide significantly improved computational methods and deeper theoretical understanding related to producing molecular signatures and understanding mechanisms of disease and concomitant leads for new drugs. It will provide evidence about applicability of novel causal methods in other types of data. It will generate insights in specific pathways of lung cancer in humans. It will deepen our understanding and solutions to the Rashomon effect in ¿omics¿ data. The proposed research will also shed light on the operational value of the stability heuristic. Finally the research will engage the international research community to address open computational causal discovery problems relevant to high-throughput and other biomedical data. ¿ Aim 1. Evaluate and characterize several novel causal algorithms for biomarker selection, molecular signature creation and reverse network engineering using real, simulated, resimulated, and experimental datasets. Study generality of the methods by means of applicability to non-¿omics¿ datasets. ¿ Aim 2. Evaluate and characterize, novel and state of the art causal algorithms against state-of-the-art non-causal and quasi-causal algorithms. ¿ Aim 3. Systematically investigate the Rashomon effect as it applies to biomarker and signature multiplicity. ¿ Aim 4. Systematically investigate the utility of applying the stability heuristic for causal discovery. ¿ Aim 5. Derive novel biomarkers, pathways and hypotheses for lung cancer. ¿ Aim 6. Induce novel solutions through an international causal discovery competition. ¿ Aim 7. Disseminate findings. n/a",Causal Discovery Algorithms for Translational Research with High-Throughput Data,7643514,R56LM007948,"['AKT1 gene', 'AKT2 gene', 'AKT3 gene', 'Address', 'Affect', 'Algorithms', 'Area', 'Arts', 'Benchmarking', 'Bioinformatics', 'Biologic Characteristic', 'Biological Markers', 'Biology', 'Biometry', 'Book Chapters', 'Books', 'Cancer cell line', 'Causations', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Communities', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Set', 'Depth', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discipline', 'Disease', 'Drug Design', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Ensure', 'Epidermal Growth Factor Receptor', 'European', 'Evaluation', 'Event', 'Excision', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Hereditary Disease', 'Home environment', 'Human', 'Human Cell Line', 'Inferior', 'Information Retrieval', 'Institution', 'International', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Malignant neoplasm of lung', 'Marker Discovery', 'Medicine', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Neighborhoods', 'Noise', 'Numbers', 'Online Systems', 'Outcome', 'Output', 'Paper', 'Pathway interactions', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteomics', 'Protocols documentation', 'Public Domains', 'Publishing', 'Quality Control', 'Random Allocation', 'Randomized', 'Rate', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Sample Size', 'Sampling', 'Schedule', 'Score', 'Services', 'Simulate', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Text', 'Thinking', 'Tissues', 'Translational Research', 'Variant', 'Work', 'base', 'c-erbB-1 Proto-Oncogenes', 'clinically relevant', 'computer based statistical methods', 'computer science', 'contextual factors', 'coping', 'data mining', 'design', 'drug development', 'heuristics', 'human data', 'human tissue', 'improved', 'innovation', 'insight', 'journal article', 'member', 'new technology', 'next generation', 'novel', 'novel diagnostics', 'outcome forecast', 'reconstruction', 'research study', 'software systems', 'symposium', 'theories', 'tool']",NLM,VANDERBILT UNIVERSITY,R56,2008,4434,-0.05314066217795248
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7458835,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,503603,-0.005712597921989931
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7688793,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,58299,-0.005712597921989931
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7222736,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2007,135000,-0.028843787887365874
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,-0.011109811062455753
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7186695,R01NS051826,"['Accounting', 'Adult', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomic structures', 'Anatomy', 'Area', 'Atlases', 'Back', 'Biomechanics', 'Boston', 'Brain', 'Caring', 'Class', 'Classification', 'Clinical assessments', 'Clutterings', 'Collaborations', 'Competence', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Corpus Callosum', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diffuse Pattern', 'Discipline of obstetrics', 'Disease', 'Disease Progression', 'Effectiveness', 'Effectiveness of Interventions', 'Electroencephalography', 'Elements', 'Ensure', 'Evaluation', 'Evolution', 'Fetal Growth Retardation', 'General Hospitals', 'Genetic Markers', 'Goals', 'Gold', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Hospitals', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Incidence', 'Individual', 'Infant', 'Intervention', 'Intuition', 'Invasive', 'Knowledge', 'Label', 'Learning', 'Learning Disabilities', 'Link', 'Localized', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Massachusetts', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Morphologic artifacts', 'Morphology', 'Motivation', 'Neonatal', 'Neuroanatomy', 'Neurosciences', 'Neurosurgeon', 'Noise', 'Normal Range', 'Operative Surgical Procedures', 'Outcome', 'Pathology', 'Patients', 'Pediatric Hospitals', 'Population', 'Population Characteristics', 'Population Study', 'Positioning Attribute', 'Premature Infant', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Property', 'Psyche structure', 'Range', 'Rate', 'Relative (related person)', 'Research', 'Research Personnel', 'Residual state', 'Resolution', 'Rest', 'Role', 'Scanning', 'Schizophrenia', 'Shapes', 'Site', 'Specificity', 'Staging', 'Standards of Weights and Measures', 'Statistical Distributions', 'Statistical Models', 'Statistical Study', 'Statistically Significant', 'Structure', 'Surface', 'Surgeon', 'Surgical Instruments', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Tweens', 'Universities', 'Validation', 'Variant', 'Washington', 'Woman', 'base', 'cohort', 'computer studies', 'computerized tools', 'desire', 'deviant', 'disease classification', 'expectation', 'feeding', 'healthy aging', 'imaging Segmentation', 'improved', 'instrument', 'interest', 'mortality', 'neonate', 'nervous system disorder', 'neuroimaging', 'neurosurgery', 'normal aging', 'novel', 'programs', 'radiologist', 'reconstruction', 'relating to nervous system', 'research clinical testing', 'response', 'shape analysis', 'statistics', 'tool', 'tumor']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2007,282619,-0.019617763661010812
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7226297,R01HL065462,"['Accounting', 'Algorithms', 'Arts', 'Biological', 'Class', 'Classification', 'Communities', 'Computer software', 'Condition', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease regression', 'Documentation', 'Effectiveness', 'Employee Strikes', 'Environment', 'Etiology', 'Gene Expression', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Heart failure', 'Knowledge', 'Least-Squares Analysis', 'Machine Learning', 'Mass Spectrum Analysis', 'Mechanics', 'Medical', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Profiling', 'Motivation', 'Pan Genus', 'Patients', 'Performance', 'Principal Investigator', 'Procedures', 'Property', 'Proteomics', 'Public Domains', 'Research Personnel', 'Sample Size', 'Sampling', 'Scheme', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Tissue-Specific Gene Expression', 'Weight', 'Work', 'base', 'forest', 'gene function', 'improved', 'novel', 'protein protein interaction', 'response', 'statistics', 'tool']",NHLBI,UNIVERSITY OF MINNESOTA,R01,2007,141753,-0.03255623444425706
"A RuleFit Product for Classification and Regression Prediction and data exploration are important aspects of modern commercial and scientific life. Regression methods predict dependent variables (e.g., tumor growth, severity of disease), while classification methods predict class membership (e.g., tumor or disease type). Both use a vector of independent variables to make the predictions. Because they are often superior predictors, can handle large numbers observations and large numbers of variables, can often yield insight into the data not provided by other methods, and because they can adapt to arbitrarily complex relationships, modern machine learning methods based on tree ensembles such as RANDOM FORESTS and MART have become leading modern analytical methods. Here we propose to commercially implement RULEFIT, a recent innovative method extending the RANDOM FORESTS and MART approaches, that shows strong evidence of being consistently more accurate than either ensemble. RULEFIT also includes groundbreaking new methods for variable selection in the face of huge numbers of predictors, and for identifying interactions, and ranking their importance. Optionally, RULEFIT extracts ""rules"" of special interest: succinct statements of conditions under which an outcome is especially likely or unlikely, or especially large or small. The primary output of RULEFIT is a numeric value reecting a prediction of the value of the dependent variable or the probability of a class membership. RULEFIT is likely to become a leading technique in the machine learning and statistics. It builds on RANDOM FORESTS and MART and includes all their useful benefits such as variable selection, data exploration, data reduction, outlier detection, and missing value imputation, while enhancing and extending these benefits.  COMMERCIAL POTENTIAL The market for advanced analytical tools has been growing strongly over the last decade and the growth shows no signs of diminishing. Modelers and data analysts in both university- based and commercial settings are increasingly aware of the power and value of new analytical tools derived from modern statistics and machine learning research. The increased accuracy of the new methods and the acceleration they provide to the analysis of complex data are fueling demand for this new technology. The advances embedded in the proposed product represent substantial improvements to existing technology and include methods to solve vexing problems in contemporary data analysis, and thus should find a welcoming market.  There are further reasons to forecast robust commercial potential for this product. The applicant organization has a strong track record in the industry and is widely recognized as a developer of high quality software. We have been working with consultant Friedman since 1990 and have gained exclusive rights to the proprietary sourcecode for a number of his innovations. These include CART, MARS, MART and PRIM. With the addition of RULEFIT and its associated sub-components, these products represent a unique collection of pedigreed tools. We have also forged a similar relationship with the (late) Leo Breiman and have the exclusive rights to commercialization of Breiman's Random Forests sourcecode. Our proposed package thus occupies a distinctive position in machine learning software which cannot be replicated by other vendors. Keywords: machine learning; classi?cation; prediction; supervised learning; variable importance; inter- action detection; Justi?cation Dr. Steinberg has extensive experience in software development for advanced statistical and machine learning methods, particularly in the area of classi?cation and regression trees, sur- vival analysis, adaptive modeling, RANDOM FORESTS and MART. He will oversee all aspects of the project. He will will work with Dr. Cardell, Professor Friedman, Mr. Colla, and with the Salford Systems software development engineer in creating and studying the software and methods used in this proposal. He will also be responsible for the architecture of the Phase I software. Professor Friedman and Dr. Cardell will provide technical support as follows: Dr. Fried- man is an expert on machine learning methods and is one of the developers of the RULEFIT technique. Regular consultation with him will be in this area. Dr. Cardell is an expert in asymptotic theory, and in the design of Monte Carlo and other tests for the evaluation of ma- chine learning algorithms. He also has extensive experience in machine learning, including adaptive modeling, neural networks, logistic regression, and classi?cation methods. He will review core algorithms of RULEFIT for possible improvement and extension and design the Monte Carlo tests. Mr. Colla has extensive experience in software development and with machine learning methods, including work on the commercial implementations of CART, MARS, RANDOM FORESTS, and MART. Working with Dr. Cardell, he will be responsible for much of the new software coding. 5 Project Description Page 7 Principal Investigator/Program Director (Last, first, middle): Steinberg, Dan Prediction models based upon classification and regression tree ensembles have become important in medical and other research. There are currently no commercial products available that implement the proposed RuleFit methodology. These methods have significant advantages over existing techniques, and will aid researchers in obtaining the best possible predictions.   n/a",A RuleFit Product for Classification and Regression,7268612,R43CA124294,"['Acceleration', 'Agreement', 'Algorithms', 'Architecture', 'Area', 'Beds', 'Build-it', 'Cations', 'Class', 'Classification', 'Code', 'Collection', 'Comparative Study', 'Complex', 'Computer software', 'Condition', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Detection', 'Disease', 'Disease regression', 'Engineering', 'Evaluation', 'Face', 'Generations', 'Growth', 'Industry', 'Information Systems', 'Investigation', 'Learning', 'Left', 'Life', 'Linear Models', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Neural Network Simulation', 'Numbers', 'Outcome', 'Output', 'Painless', 'Pattern', 'Performance', 'Phase', 'Plant Leaves', 'Play', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Rate', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Rights', 'Role', 'Sampling', 'Severity of illness', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Trees', 'Universities', 'Variant', 'Vendor', 'Work', 'analytical method', 'analytical tool', 'base', 'commercialization', 'data mining', 'data structure', 'design', 'evaluation/testing', 'experience', 'forest', 'forging', 'graphical user interface', 'innovation', 'insight', 'interest', 'loss of function', 'man', 'new technology', 'novel', 'professor', 'programs', 'prototype', 'relating to nervous system', 'research study', 'software development', 'statistics', 'theories', 'tool', 'tumor', 'tumor growth', 'vector']",NCI,SALFORD SYSTEMS,R43,2007,91700,-0.04829023414179748
"Computational Annotation of Orphan Metabolic Activities    DESCRIPTION (provided by applicant):  Even state-of-the-art homology methods cannot annotate metabolic genes with no or remote sequence identity to known enzymes. This presents a significant obstacle to network reconstruction, as about 30%- 40% (>1500) of known metabolic activities remain orphan, i.e. there are no known proteins catalyzing these activities in any organism. The scale of the orphan activities problem makes it arguably the single biggest challenge of modern biochemistry. We propose to develop, experimentally validate, and make available to the scientific community an efficient computational approach to fill the remaining gaps in metabolic networks. The main idea of the proposed method is to use genes assigned to the network neighbors of the remaining gaps as constraints in assigning genes for orphan activities. We demonstrate that this approach significantly outperforms simpler or existing methods. Our cross-validated results in model organisms demonstrate that the proposed method can predict the correct genes in more than 50% of the cases, without any sequence homology information. The calculations indicate that the prediction accuracy will also remain high in less studied organisms. Using the developed method we have already identified and validated a gene responsible for an E. coli metabolic activity which remained orphan for more than 25 years. There are four specific aims of the proposal: 1.) We will calculate the appropriate context-based descriptors of protein function for the majority of sequenced organisms. Many new functional descriptors will be developed and used for the predictions. 2.) We will investigate the ability of various machine learning approaches and fitness functions to integrate context-based descriptors. Based on the developed methodology we will make predictions for all orphan activities in sequenced organisms. 3.) The predictions will be available through a searchable and constantly updated Web server. We will also develop a method to detect functional misannotations and apply it to all public metabolic databases. 4.) In collaboration with the laboratories of Dr. Uwe Sauer (ETH Zurich) and Dr. George Church (Harvard) we will experimentally test at least 50 of the predicted genes without close sequence homologs in E. coli, B. subtilis, S. cerevisiae.           n/a",Computational Annotation of Orphan Metabolic Activities,7322388,R01GM079759,"['Animal Model', 'Arts', 'Base Sequence', 'Biochemical', 'Biochemical Genetics', 'Biochemical Pathway', 'Biochemistry', 'Biological Neural Networks', 'Church', 'Collaborations', 'Communities', 'Databases', 'Decision Trees', 'Descriptor', 'Enzymes', 'Escherichia coli', 'Evolution', 'Gene Fusion', 'Genes', 'Genetic', 'Internet', 'Laboratories', 'Link', 'Machine Learning', 'Metabolic', 'Methodology', 'Methods', 'Numbers', 'Operon', 'Organism', 'Orphan', 'Pathway interactions', 'Performance', 'Positioning Attribute', 'Proteins', 'Range', 'Research Personnel', 'Saccharomyces cerevisiae', 'Sequence Homologs', 'Sequence Homology', 'Specific qualifier value', 'Structure', 'Testing', 'Update', 'Validation', 'base', 'computer based statistical methods', 'fitness', 'gene correction', 'gene function', 'genome sequencing', 'metabolomics', 'protein function', 'reconstruction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,364130,-0.033006217663972985
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7301424,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2007,173307,-0.017473350950006156
"The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments    DESCRIPTION (provided by applicant): The goal of this research is to create software that uses mathematical algorithms to detect medical billing coding errors prior to payment. The well-publicized failure of current healthcare cost containment technologies to prevent improper payments in both the commercial healthcare market and the federal Medicare program highlights the urgent need for a new approach to the growing problem of out of control medical costs. A recent federal study by the GAO estimated that improper payments by Medicare alone were in excess of 21 billion dollars, a truly staggering 48.1 percent of all improper payments by federal programs. Like SPAM, whose dynamic nature makes static or post hoc remedies ineffective, effective cost containment in one area often merely leads to the creation of new areas of abuse. Clearly, the ideal solution is a system that can evaluate the fairness of payments before they are made, and that can respond to dynamic patterns of abuse. The first step in creating such a system is the creation of robust method for sorting bills for appropriate rule-based analysis on the basis of the type of bill. Currently neither Medicare nor major insurers are capable of making this classification reliably except through the use of inefficient, static rules and the use of manual sorting--a costly and inefficient approach to assuring timely payment to hospitals and medical providers. We propose a novel method for using mathematical algorithms that utilize machine-learning (ML) methods to address the problem of medical bill categorization, the first step in coding error detection. Specifically, we propose the evaluation of a variety of genetic algorithms that are well adapted to the problems of large, dynamic datasets and can be ""trained"" using real world correctly coded datasets in healthcare claims. This work is particularly timely due to recent Medicare contracting reform. Using more than 50 contractors and carriers, bill classification is largely determined by the carrier's contract. Centralizing this process to only four payment centers will require the classification system we propose. [This research is directed toward the development of software applications that will detect billing errors and perform proper edits to payment of medical bills. Current anticipated changes and reforms in the Medicare system will require these systems, which do not currently exist in the public or private sector.]             n/a",The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments,7316071,R43LM009190,"['Address', 'Age', 'Algorithms', 'Area', 'Arts', 'Classification', 'Code', 'Collaborations', 'Computer Simulation', 'Computer software', 'Contractor', 'Contracts', 'Cost Control', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Development', 'Elements', 'Environment', 'Evaluation', 'Failure', 'Genetic Programming', 'Goals', 'Health Care Costs', 'Health Care Fraud', 'Health Personnel', 'Healthcare', 'Healthcare Market', 'Healthcare Systems', 'Hospitals', 'Industry', 'Inpatients', 'Insurance Carriers', 'Learning', 'Machine Learning', 'Manuals', 'Mathematics', 'Medical', 'Medicare', 'Methods', 'Mining', 'Modeling', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outpatients', 'Pattern', 'Phase', 'Policies', 'Population', 'Prevention', 'Private Sector', 'Process', 'Provider', 'Rate', 'Reporting', 'Research', 'Running', 'Small Business Technology Transfer Research', 'Solutions', 'Sorting - Cell Movement', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Training', 'Work', 'base', 'college', 'computerized', 'cost', 'design', 'experience', 'improved', 'mathematical algorithm', 'novel', 'novel strategies', 'payment', 'prevent', 'programs', 'size', 'software development', 'stem', 'success']",NLM,"QMEDTRIX SYSTEMS, INC.",R43,2007,92482,-0.020548550199538353
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,-0.010096297488627929
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7275769,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2007,51278,-0.01311839654305209
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7318595,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA IRVINE,R01,2007,372000,-0.013162946245757095
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7262592,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2007,263847,0.009444958478312306
"Sequence Induction in Infancy: A systematic approach    DESCRIPTION (provided by applicant): Thirteen experiments investigate the early development of the ability to learn patterns in sequential input. Such evidence would seem likely to provide a substantial contribution to our knowledge of how infants identify both concrete and abstract patterns, a fundamental aspect of cognitive development. The proposed experiments are organized into three broad studies. In the first study, we will examine ""ancillary"" factors that may influence sequence learning in infancy. We emphasize the importance of two kinds of relations between elements in any multi-element group or string: itemwise relations, between specific instances (e.g., statistical learning), and variablewise relations, between algebraic placeholders (e.g., abstract pattern or rule learning). We argue that a first step toward any programmatic series of experiments on these kinds of learning-must involve an understanding of processing limitations that may constrain performance. In the second study, we will investigate the different kinds of patterns infants may be able to learn, using past and pilot data as a guide for our theorizing. In the final study, we will explore the intriguing possibility that infants may learn more than one pattern in any single set of inputs, a classic question that has received little empirical attention in the literature on cognitive development. The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the ability to detect abstract visual and auditory structure. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, how perceptual skills impact knowledge acquisition and structure, and how to best characterize early development. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.           n/a",Sequence Induction in Infancy: A systematic approach,7251991,R01HD048733,"['Address', 'Age', 'Attention', 'Auditory', 'Be++ element', 'Beds', 'Beryllium', 'Body of uterus', 'Child', 'Childhood', 'Classification', 'Cognition', 'Cognitive', 'Compatible', 'Conceptual Domain', 'Conflict (Psychology)', 'Count', 'Data', 'Dependency', 'Development', 'Diagnostic', 'Drug Formulations', 'Elements', 'Fostering', 'Future', 'Goals', 'Hand', 'Human', 'Indium', 'Infant', 'Investigation', 'Knowledge', 'Knowledge acquisition', 'Language Development', 'Learning', 'Life', 'Light', 'Literature', 'Machine Learning', 'Medial', 'Nature', 'Numbers', 'Pattern', 'Pattern Recognition', 'Performance', 'Process', 'Range', 'Research', 'Risk', 'Schools', 'Series', 'Shapes', 'Sorting - Cell Movement', 'Staging', 'Structure', 'Techniques', 'Testing', 'Time', 'Vegetables', 'Visual', 'Work', 'abstracting', 'age related', 'base', 'cognitive change', 'coping', 'developmental disease', 'infancy', 'neglect', 'object perception', 'postnatal', 'research study', 'sequence learning', 'size', 'skills']",NICHD,NEW YORK UNIVERSITY,R01,2007,304197,-0.05643928357291187
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7269383,R01RR014477,[' '],NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,307022,-0.034963967224684894
"Comparative Visualization and Analysis for GCxGC    DESCRIPTION (provided by applicant): Project Summary. This project will investigate and develop effective information technologies for comparative analysis and visualization of complex data generated by comprehensive two-dimensional gas chromatography (GCxGC). GCxGC is an emerging technology that provides an order-of-magnitude greater separation capacity, significantly better signal-to-noise ratio, and higher dimensional retention-structure relations than traditional GC. The principal challenge for utilization of GCxGC, in a wide range of public-health and other applications, is the difficulty of analyzing and interpreting the large, complex data it generates. The quantity and complexity of GCxGC data necessitates the investigation and development of new information technologies. This project will develop and demonstrate innovative methods and tools for comparative analysis of GCxGC datasets. The expected results of this research and development include a PCA-based method for chemical fingerprinting, decision trees with chemical constraints for sample classification, genetic programming for template and constraint-based matching and classification, and visualization methods for comparative GCxGC analyses. These methods will be implemented in commercial software that will support researchers and laboratory analysts in a wide range of commercial applications, including health care, environmental monitoring, and chemical processing. The power of GCxGC, supported by effective information technologies, will enable better understanding of chemical compositions and processes, a foundation for future scientific advances and discoveries. Relevance to Public Health. Today, a few advanced laboratories are pioneering GCxGC for a variety of applications such as environmental monitoring of exposure profiles in air, soil, food, and water; identification and quantification of toxic products in blood, urine, milk, and breath samples; and qualitative and quantitative metabolomics to provide a holistic view of the biochemical status or biochemical phenotype of an organism. Many analyses in these applications require detailed chemical comparisons of samples, e.g..monitoring changes, comparison to reference standards, chemical matching or ""fingerprinting"", and classification. GCxGC is a powerful new technology for such comparative analyses. This proposal will provide innovative information technologies to support users in these applications.           n/a",Comparative Visualization and Analysis for GCxGC,7270029,R44RR020256,"['Air', 'Archives', 'Biochemical', 'Blood', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Emerging Technologies', 'Environmental Monitoring', 'Fingerprint', 'Food', 'Foundations', 'Future', 'Gas Chromatography', 'Genetic Programming', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Information Technology', 'Investigation', 'Laboratories', 'Language', 'Machine Learning', 'Marketing', 'Methods', 'Milk', 'Monitor', 'Noise', 'Organism', 'Pattern', 'Phase', 'Phenotype', 'Principal Component Analysis', 'Process', 'Public Health', 'Range', 'Reference Standards', 'Reporting', 'Research Personnel', 'Sales', 'Sampling', 'Schedule', 'Scientific Advances and Accomplishments', 'Signal Transduction', 'Software Tools', 'Soil', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Today', 'Trademark', 'Urine', 'Water', 'base', 'chemical fingerprinting', 'commercial application', 'comparative', 'innovation', 'innovative technologies', 'instrument', 'metabolomics', 'new technology', 'research and development', 'tool', 'two-dimensional']",NCRR,"GC IMAGE, LLC",R44,2007,239373,-0.008460116016544578
"Developing computerized tools for cryosurgery planning    DESCRIPTION (provided by applicant):    Cryosurgery has been known as an invasive surgical technique since 1961, when Cooper and Lee invented the first cryoprobe. In the 1990s, new developments in Joule-Thomson cooling (the cooling effect associated with a sudden relief of a pressurized gas) led to a dramatic decrease in the size of cryoprobes and an increase in the number of cryoprobes that could be used simultaneously. A dozen or more cryoprobes operating simultaneously in a single prostate cryosurgery is already common practice. If localized effectively, one of the primary benefits of using a large number of miniaturized cryoprobes is superior control over the freezing process.   Currently, the process of selecting the correct placement of the cryoprobes for a specific procedure is an art held by the cryosurgeon, based on the surgeon's own experience and rules of thumb. Cryoprobes are typically operated in a trial-and-error fashion, until the entire target volume is thought to be frozen. Currently, there are no means to determine the optimal locations for the cryoprobes. Suboptimal cryoprobe localization may leave regions in the target volume unfrozen, may lead to cryoinjury of healthy surrounding tissues, may require an unnecessarily large number of cryoprobes, may increase the duration of the surgical procedure, and may increase the likelihood of post cryosurgery complications, all of which affect the quality and cost of the medical treatment. Computerized planning tools would help to alleviate these difficulties.   The ""cryoheater,"" a new device for cryosurgery control has recently been presented by the research team. The cryoheater is a temperature controlled electrical heater. In broad terms, cryoheaters can dramatically increase the ability to control the shape and size of the frozen region, however, to achieve the full benefits of cryoheaters, computerized planning tools for cryoheater localization are necessary.   Our goal is to develop computerized planning tools for cryosurgery that are suitable for all available cooling techniques. The proposed research includes: (1) Development of an efficient numerical scheme for bioheat transfer simulations of cyroprocedures, (2) Development of an efficient optimization technique based on a force-field analogy. (3) Development of knowledge-based optimization techniques. (4) Experimental verification of the planning tool.       Besides planning, another important application of the proposed tool is the training of cryosurgeons. The proposed tool will provide cryosurgeons with the ability to visualize the 3D volumetric nature of the freezing process.   Likewise, it will allow the surgeon to explore the performance of various configurations of cryoprobes and cryoheaters, and observe the defects that would result from each. Such visualization capabilities will provide surgeons with insights into the physics of cryosurgery that are difficult to obtain from physical experiments or surgical practice.         n/a",Developing computerized tools for cryosurgery planning,7210691,R01EB003563,"['Affect', 'Arts', 'Biological', 'Catheters', 'Computational Technique', 'Condition', 'Cool-X-A', 'Cryosurgery', 'Defect', 'Depth', 'Development', 'Devices', 'Europe', 'Feasibility Studies', 'Freezing', 'Frequencies', 'Furuncles', 'Gases', 'Goals', 'Heating', 'Imagery', 'Imaging Device', 'Invasive', 'Lasers', 'Lead', 'Learning', 'Left', 'Liquid substance', 'Localized', 'Location', 'Machine Learning', 'Medical', 'Methods', 'Modems', 'Nature', 'Nitrogen', 'Numbers', 'Operating Rooms', 'Operative Surgical Procedures', 'Patients', 'Performance', 'Physics', 'Placement', 'Procedures', 'Process', 'Prostate', 'Publishing', 'Purpose', 'Radio', 'Reporting', 'Research', 'Research Proposals', 'Scheme', 'Shapes', 'Simulate', 'Solutions', 'Source', 'Surgeon', 'Techniques', 'Temperature', 'Thermal Ablation Therapy', 'Thinking', 'Thumb structure', 'Time', 'Tissues', 'Training', 'Ultrasonography', 'Urethra', 'base', 'clinical application', 'computerized', 'computerized tools', 'cost', 'experience', 'insight', 'knowledge base', 'miniaturize', 'research study', 'simulation', 'size', 'thermal seeds', 'three-dimensional modeling', 'tool']",NIBIB,CARNEGIE-MELLON UNIVERSITY,R01,2007,87443,-0.0429824399129333
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7247404,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2007,292160,-0.013327860403782433
The RPI Exploratory Center for Cheminformatics (RMI) No abstract available n/a,The RPI Exploratory Center for Cheminformatics (RMI),7472067,P20HG003899,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2007,364010,0.0038775462069733794
Carolina Exploratory Center for Cheminformatics Research No abstract available n/a,Carolina Exploratory Center for Cheminformatics Research,7472715,P20HG003898,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,P20,2007,373960,0.0038775462069733794
MACE - Michigan Alliance for Cheminformatic Exploration No abstract available n/a,MACE - Michigan Alliance for Cheminformatic Exploration,7472717,P20HG003890,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Michigan', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIVERSITY OF MICHIGAN,P20,2007,271370,0.009254121419927116
Comparative and Web-Enabled Virtual Screening No abstract available n/a,Comparative and Web-Enabled Virtual Screening,7472716,P20HG003900,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'comparative', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project', 'virtual', 'web-enabled']",NHGRI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,P20,2007,363833,-0.00318427989339098
"Genotype/Phenotype Correlations in Lysosomal Storage Diseases    DESCRIPTION (provided by applicant): Our long-term goal is to understand the detailed molecular mechanisms that connect genotype, phenotype and response to therapy in lysosomal storage diseases (LSDs). LSDs are a family of genetic metabolic diseases caused by lysosomal enzyme deficiencies. In this project we use Fabry disease as a model system to develop a bioinformatics-based paradigm to address two fundamental issues: 1) The relationship between genotype and phenotype in LSDs. This task is challenging because in LSDs different mutations in the same enzyme often lead to different disease phenotypes. 2) The relationship between genotype and response to ""pharmacological chaperone"" therapy. Pharmacological chaperones are small-molecule ligands that are used to rescue mutants, resulting in increased enzymatic activity; several Fabry mutations have been shown to be rescueable in this way. The same therapy is likely to be useful for other LSDs, particularly those with neurological involvement, for which enzyme replacement therapy is not viable. The two aims of this application address, at different levels, both issues described above. The first aim, tests the hypothesis that knowing the change that occurs in the protein sequence, together with the structural environment in which it occurs, is sufficient to predict the resulting disease phenotype and response to pharmacological chaperone therapy. This is tested through the rigorous training of classification methods using sequence and structure-derived descriptors for a large set of Fabry mutants of known phenotype. The resulting classification provides a large- scale quantitative description of the correlation between genotype and phenotype. The accuracy of predictions based on this approach is a measure of how much information about the genotype the descriptors contain. The same approach will be used to establish a quantitative correlation between genotype and response to pharmacological chaperone therapy. Finally, applying the classification methods to mutations in other LSDs will test the generality of the approach. The second aim of this application addresses the issue of genotype/phenotype correlation from a biophysical point of view. We test the hypothesis that a combination of factors, mainly folding free energy, ligand binding affinity, and relative pH stability of the mutants determines the disease phenotype and response to pharmacological chaperone therapy. This is done analyzing selected mutants using molecular modeling and molecular dynamics simulations of the enzyme/ligand and enzyme/receptor interactions, as well as, pH stability, and other calculations. The methods used in the second aim are very detailed, but are not applicable at a large scale. Thus, both aims provide complementary views of genotype/phenotype correlation in LSDs. The successful completion of this project will, for the first time, provide a quantitative connection between genotype and phenotype in LSDs and a detailed biophysical description of the molecular mechanisms underlying genotype/phenotype correlations and response to pharmacological chaperone therapy in Fabry disease. Relevance of this research to public health. Lysosomal storage diseases (LSDs) are a group of more than 40 genetic metabolic disorders. Worldwide, the incidence of patients with LSDs is estimated to be ~ 1 in 8,000 live births. Understanding the correlation between genotype, phenotype, and response to treatment in these diseases will help in their diagnosis and treatment, particularly for LSDs that affect the brain, for which no effective treatment is available to date.          n/a",Genotype/Phenotype Correlations in Lysosomal Storage Diseases,7239204,R21DK078345,"['Address', 'Affect', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Binding', 'Bioinformatics', 'Biological Models', 'Brain', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Data', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Endoplasmic Reticulum', 'Environment', 'Enzymes', 'Fabry Disease', 'Feedback', 'Free Energy', 'Galactosidase', 'Genetic', 'Genotype', 'Goals', 'Incidence', 'Lead', 'Ligand Binding', 'Ligands', 'Live Birth', 'Lysosomal Storage Diseases', 'Lysosomes', 'Machine Learning', 'Measures', 'Metabolic Diseases', 'Methods', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Models', 'Mutate', 'Mutation', 'Neurologic', 'None or Not Applicable', 'Numbers', 'Output', 'Patients', 'Peptide Sequence Determination', 'Pharmacogenomics', 'Phenotype', 'Principal Investigator', 'Public Health', 'Relative (related person)', 'Research', 'Residual state', 'Structure', 'Testing', 'Time', 'Training', 'base', 'design', 'disease phenotype', 'enzyme deficiency', 'enzyme replacement therapy', 'enzyme structure', 'enzyme substrate', 'family genetics', 'improved', 'insight', 'molecular dynamics', 'molecular modeling', 'mutant', 'prevent', 'programs', 'protein degradation', 'receptor', 'receptor binding', 'response', 'simulation', 'small molecule', 'three dimensional structure']",NIDDK,MOUNT SINAI SCHOOL OF MEDICINE,R21,2007,127125,-0.016014611718963796
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,-0.022847837373705025
"Generation and Description of Dendritic Morphology    DESCRIPTION (provided by applicant): This continuing project is directed at describing dendrite structure in a compact yet sufficiently complete and detailed fashion to allow the computer generation of morphologically accurate neuronal models. Dendrite morphology plays a fundamental role in physiological and pathological brain function by subserving and shaping network connectivity and by integrating the complex pattern of synaptic inputs received by the neuron. A parsimonious and algorithmic description of dendritic shape is a crucial step towards the quantitative characterization of the structure-activity relationship in the nervous system and it constitutes an effective way to represent, compress, store, exchange, and amplify extremely complex neuroanatomical data. Neuroanatomical algorithms and models have been developed to simulate and quantitatively analyze the three-dimensional structure of dendritic trees in the same format used to represent experimentally reconstructed neurons.      The specific aims of this project are: (1) to expand and improve neuroanatomically plausible algorithms of dendritic structure and development by including determinants of three-dimensional branch orientation and dependence of growth upon local and global influences (e.g. diameter and neuronal size, respectively); (2) to enhance and distribute the analysis, modeling, and data basing software in order to provide experimental and computational neuroscientists with web-based tools to query, retrieve, measure, classify, and synthesize dendritic morphology data; (3) to continue the experimental reconstruction and analysis of hippocampal pyramidal cells and spinal motoneurons with different experimental protocols and in early postnatal periods; and to integrate these data with detailed biophysical models of neuronal electrophysiology. The informatics and neuroscience components of this research are deeply intertwined and span a variety of scientific approaches, including ""wet"" experiments, computational simulations, statistical analysis and data mining. This will require the design and implementation of novel neuroinformatics tools for data handling and integration, and their distribution to the wider neuroscience community.         n/a",Generation and Description of Dendritic Morphology,7233290,R01NS039600,"['Algorithms', 'Archives', 'Atlases', 'Brain', 'Caliber', 'Cells', 'Class', 'Classification', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Databases', 'Dendrites', 'Dependence', 'Development', 'Developmental Process', 'Electrophysiology (science)', 'Environment', 'Generations', 'Goals', 'Growth', 'Hippocampus (Brain)', 'Image', 'Imagery', 'Informatics', 'Internet', 'Java', 'Lead', 'Length', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Microscopic', 'Mining', 'Modeling', 'Morphology', 'Motor Neurons', 'Neonatal', 'Nervous system structure', 'Neurons', 'Neurosciences', 'One-Step dentin bonding system', 'Online Systems', 'Pattern', 'Physiological', 'Play', 'Protocols documentation', 'Pyramidal Cells', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Series', 'Shapes', 'Simulate', 'Software Tools', 'Spinal', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Synapses', 'System', 'Techniques', 'Trees', 'data mining', 'data modeling', 'density', 'design', 'digital', 'improved', 'insight', 'models and simulation', 'neuroinformatics', 'novel', 'postnatal', 'programs', 'reconstruction', 'research study', 'simulation', 'size', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'user-friendly', 'virtual']",NINDS,GEORGE MASON UNIVERSITY,R01,2007,61912,-0.021453943019926418
"Systems analysis of oxygen regulation in Halobacterium    DESCRIPTION (provided by applicant): To withstand environmental onslaught, biological systems mount global programs to coordinate the induction of protection and repair mechanisms. This proposal poses the hypothesis that the transcriptional networks underlying such responses to diverse stressors are interrelated. Halobacterium, a halophilic archaeon, has been chosen as a model for this study because it routinely negotiates an array of adverse conditions in its extreme environment, including anoxia, metal stress, and radiation damage. This proposal will investigate the inter-relationship of these responses using global approaches. Given that basal genetic information processing pathways in Halobacterium are mediated by eukaryotic-like proteins, findings from this study will have a direct impact on understanding how complex eukaryotic organisms elicit orthogonal responses in disease-perturbed or infection states. Specifically, I will (1) Characterize key transcriptional regulators responsible for mediating responses to fluctuating oxygen concentrations and identify regulons under their direct and indirect control; (2) Through statistical analysis of integrated datasets, evaluate the extent of cross-regulation of the anoxic response with other environmental perturbations; (3) Experimentally test new hypotheses generated by statistical analysis. These proposed experiments are expected to result in a transcriptional network model that addresses how organisms maintain homeostasis despite stress.           n/a",Systems analysis of oxygen regulation in Halobacterium,7261251,F32GM078980,"['Address', 'Aerobic', 'Algorithms', 'Anoxia', 'Archaea', 'Behavioral', 'Binding Sites', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Phenomena', 'Cells', 'Collection', 'Complex', 'Computer software', 'Condition', 'Couples', 'Data', 'Data Set', 'Defect', 'Disease', 'Electrophoretic Mobility Shift Assay', 'Environment', 'Equilibrium', 'Experimental Designs', 'Face', 'Facility Construction Funding Category', 'Fellowship', 'Gene Targeting', 'Genes', 'Genetic Information Processing Pathway', 'Genome', 'Goals', 'Growth', 'Halobacterium', 'Homeostasis', 'Hydrogen Peroxide', 'Individual', 'Infection', 'Information Systems', 'Knock-out', 'Laboratories', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Manuscripts', 'Maps', 'Mediating', 'Mediation', 'Metals', 'Modeling', 'Molecular Biology', 'Mutate', 'Names', 'Organism', 'Oxidation-Reduction', 'Oxidative Stress', 'Oxygen', 'Oxygen measurement, partial pressure, arterial', 'Play', 'Preparation', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Radiation', 'Regulation', 'Regulator Genes', 'Regulon', 'Relative (related person)', 'Role', 'Stress', 'Study models', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Testing', 'Time', 'TimeLine', 'Training', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Work', 'biological adaptation to stress', 'cell injury', 'chromatin immunoprecipitation', 'halobacteria', 'high throughput screening', 'in vivo', 'insight', 'metal poisoning', 'mutant', 'network models', 'novel', 'programs', 'repaired', 'research study', 'response', 'stressor', 'transcription factor']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,F32,2007,48796,-0.018135678840026207
"An implantable device to predict and prevent seizures    DESCRIPTION (provided by applicant): After the first four years of our Bioengineering Research Partnership, implantable devices for epilepsy are now a reality. This is due, in part, to translation of technology developed by our group to industry. Data from multi-center clinical trials of first generation responsive antiepileptic devices indicate that this new technology is safe, and that there is promise of significant benefit to patients. They also demonstrate that 1st-generation devices rarely make patients seizure free. This is because we do not yet understand when, where and how to deliver electrical stimulation to pre-empt seizures, or the mechanisms underlying seizure generation in epileptic networks. These challenges, and translating them into more effective second-generation devices, are the focus of this proposal. Specifically, our aims are: (1) To understand mechanisms underlying seizure generation in two well characterized, spontaneously seizing animal models of epilepsy with documented similarities to refractory human epilepsy, (2) To map seizure generation in the epileptic network to determine where to place sensing electrodes and when to stimulate to maximize seizure suppression and minimize side effects. (3) To develop more effective closed loop stimulation algorithms for controlling seizures. These Aims will be accomplished through a series of projects led by established collaborators in neurology, neuroscience, bioengineering and industry, at Penn, CHOP, Georgia Tech, and BioQuantix, Inc. Teams will focus on improving upon results from first-generation human devices through detailed animal experiments on multiple temporal and spatial scales. These include: (1) the cellular level, through broad-band unit recording and biophysically accurate computational modeling; (2) the network level, with in vitro experiments on hippocampal slices using voltage sensitive dyes and multi-electrode arrays; and (3) the whole brain level, through simultaneous micro and macroelectrode field recordings and responsive brain stimulation in vivo. These experiments will build upon the substantial progress made during the first cycle of our Bioengineering Research Partnership grant. The unique composition of our group, its track record of successful technology transfer, and our ability to learn from and immediately convey our discoveries to existing programmable devices, provide an unprecedented opportunity to perform cutting-edge neuroscience and bioengineering research and immediately translate it into better treatment for patients.           n/a",An implantable device to predict and prevent seizures,7125319,R01NS041811,"['Adverse effects', 'Algorithms', 'Animal Experimentation', 'Animal Experiments', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Biological', 'Biomedical Engineering', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Devices', 'Drops', 'Dyes', 'Eels', 'Electric Stimulation', 'Electrodes', 'Elements', 'Engineering', 'Epilepsy', 'Event', 'Evolution', 'Feedback', 'Frequencies', 'Generations', 'Genetic Models', 'Grant', 'Hippocampus (Brain)', 'Human', 'In Vitro', 'Industry', 'Interneurons', 'Learning', 'Link', 'Localized', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Metabotropic Glutamate Receptors', 'Methods', 'Multi-Institutional Clinical Trial', 'Neurology', 'Neurosciences', 'Organism', 'Outcome', 'Patients', 'Pattern', 'Phase', 'Physiological', 'Prevention', 'Probability', 'Process', 'Protocols documentation', 'Range', 'Refractory', 'Research', 'Research Personnel', 'Resolution', 'Rodent Model', 'Seizures', 'Series', 'Signal Transduction', 'Simulate', 'Site', 'Slice', 'Statistical Methods', 'Statistical Models', 'Subclinical Seizures', 'Synaptic plasticity', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Translating', 'Translations', 'USA Georgia', 'Western Asia Georgia', 'base', 'computerized data processing', 'experience', 'implantable device', 'improved', 'in vitro Model', 'in vivo', 'insight', 'interest', 'markov model', 'neurosurgery', 'new technology', 'novel', 'prevent', 'programs', 'research study', 'sensor', 'voltage']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2007,1322181,-0.013005309326476857
"Dysbiosis in Inflammatory Bowel Disease    DESCRIPTION (provided by applicant): Inflammatory Bowel Diseases (IBDs), namely ulcerative colitis (DC) and Crohn's disease (CD), are chronic, lifelong, relapsing illnesses, affecting close to 1 million Americans. Despite the many clues that a dysbiosis may exist in IBD, the specific changes in the microflora of IBD patients are largely unknown. Amplicon length heterogeneity (ALH) is a sophisticated and well established PCR based technology that can be used as a screening tool to identify changes in the Bacterial microflora of IBD patients. We therefore have hypothesized that the ileocolonic microflora in IBD has an altered microbial composition compared to normal microflora. We have gathered preliminary data that shows statistical differences between controls and IBD as well as within IBD patients. Therefore, to test the above hypothesis, we are proposing the following 2 inter-related scientific aims: AIM 1. Identify bacterial fingerprint patterns associated with IBD using ALH. ALH patterns will be determined in a total of 160 IBD patients and health controls and will be analyzed using diversity indeces, multivariate reduction analysis and cluster analysis. ALH patterns associated with IBD will be determined using histograms, ecological software in conjunction with custom PERL scripts as well as supervised and unsupervised automated pattern recognition systems. AIM 2.Determine the bacterial contents of putatively IBD associated ALH fingerprint patterns using molecular cloning and sequencing. Cloning and sequencing of targeted samples will be linked to bacterial identities by employing multiple bioinformatics tools. Significance. This proposal involves the first time use of a sophisticated and highly reproducible molecular biology tool, ALH, in the study of microflora in the Gl tract. There is a growing recognition of the importance of microflora in health and disease, including IBD. Studies that characterized microflora in human using powerful techniques from environmental microbiology such as ALH can bring about significant advances in the understanding of Gl tract illnesses. ALH may enable a real-time survey of microfloral changes for the first time in medicine and may provide the first evidence linking IBD with specific microbial patterns.           n/a",Dysbiosis in Inflammatory Bowel Disease,7197734,R21DK071838,"['Affect', 'Age', 'Algorithms', 'American', 'Attention', 'Back', 'Bacteria', 'Bioinformatics', 'Biopsy Specimen', 'Bispecific Antibody 2B1', 'Celiac Disease', 'Chronic', 'Clinical', 'Cloning', 'Cluster Analysis', 'Colon', 'Colonoscopy', 'Colorectal', 'Complex', 'Computer software', 'Crohn&apos', 's disease', 'Custom', 'DNA', 'Data', 'Databases', 'Disease', 'Distal part of ileum', 'Effectiveness', 'Environment', 'Environmental Microbiology', 'Feces', 'Fingerprint', 'Flare', 'Flexible fiberoptic sigmoidoscopy', 'Future', 'Gender', 'Genus Cola', 'Hand', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Human', 'Human body', 'Immunosuppressive Agents', 'Incidence', 'Individual', 'Infection', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Intestines', 'Irritable Bowel Syndrome', 'Knowledge', 'Length', 'Link', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Molecular Biology', 'Molecular Cloning', 'Morbidity - disease rate', 'Mucous Membrane', 'Newly Diagnosed', 'Numbers', 'Operative Surgical Procedures', 'Organ', 'Organism', 'Patients', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Pharmaceutical Preparations', 'Polymerase Chain Reaction', 'Population', 'Probiotics', 'Procedures', 'Race', 'Reaction', 'Recruitment Activity', 'Relapse', 'Relative (related person)', 'Research Personnel', 'Ribosomal RNA', 'Sampling', 'Schedule', 'Screening procedure', 'Soil', 'Surveys', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Toxic effect', 'Ulcerative Colitis', 'Visual', 'base', 'carcinogenesis', 'clinical application', 'computerized', 'cost', 'data mining', 'disabling disease', 'ileum', 'indexing', 'microbial', 'microbial community', 'neglect', 'novel', 'novel diagnostics', 'pathogen', 'prebiotics', 'programs', 'satisfaction', 'success', 'time use', 'tool', 'treatment effect']",NIDDK,RUSH UNIVERSITY MEDICAL CENTER,R21,2007,222000,-0.01396978052881252
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7244058,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2007,328325,-0.013104273742450118
"Bayesian Methods and Experimental Design for Molecular Biology Experiments    DESCRIPTION (provided by applicant): The goal of this proposal is to provide a suite of software tools for bioinformatics and systems biology researchers who are using molecular biology (Omics) data to identify the best experimental design and to analyze the resulting experimental data using Bayesian tools. A common problem for most bioinformatics experiments is low power due to low replication. This problem can be alleviated economically when an increase in adoption and use of a specific platform leads to a decrease in associated costs, thereby enabling an increase in samples allocated per treatment. Yet, many bioinformatics experiments remain underpowered as researchers use the offsets of decreased costs to explore more complex questions. When designing an experiment, the allocation of samples to treatment regimens, and the choice of treatments to test, are traditionally the only variables to manipulate. Bayesian experimental design provides a framework to find the optimal design out of n possible designs subject to a utility function that can include such items as time and material costs.      Bayesian statistical methods have been gaining substantial favor in bioinformatics and systems biology as they provide a highly flexible framework for fitting and exploring complex models. Bayesian models also provides to domain experts such as biologists and physicians easily interpretable models through posterior probabilities which are more naturally understood than the traditional p-value. While a number of open source tools based on Bayesian models are available, most are applied best in the context of a specific research data analysis problem or model and are not integrated into a single, complete system for data analysis.      We propose to research and develop a statistical analysis software package S+OBAYES (for S-PLUS and R) with generalized tools for Bayesian design of experiments, empirical and fully Bayesian analysis, and modeling and simulation using modern commercial software development practices. These tools will provide functionality for finding the optimal choice and layout of experimental treatments for molecular biology experiments and for fitting Bayesian linear and non-linear models to a variety of data types including time series. We propose to validate the software in molecular biology research problems such as the detection of differential gene, protein, and metabolite abundance. The benefits of this work will be a commercial-quality software package with validated statistical methodology and interactive visualization tools that will appeal to molecular biologists and systems biology investigators. The results of the proposed work will expedite discoveries in basic science, early disease detection, and drug discovery and development.          n/a",Bayesian Methods and Experimental Design for Molecular Biology Experiments,7325828,R43GM083023,"['Address', 'Adoption', 'Algorithms', 'Animal Genetics', 'Arizona', 'Basic Science', 'Bayesian Analysis', 'Bayesian Method', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biometry', 'Biotechnology', 'Cations', 'Chromosome Mapping', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Detection', 'Development', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Foundations', 'Funding', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Government', 'Government Agencies', 'Health', 'Imagery', 'Industry', 'Information Systems', 'Institution', 'Iowa', 'Libraries', 'Linear Models', 'Machine Learning', 'Manuals', 'Manuscripts', 'Maps', 'Marketing', 'Mass Spectrum Analysis', 'Measures', 'Medical Informatics', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Biology', 'Non-linear Models', 'Numbers', 'Pathway interactions', 'Phase', 'Physicians', 'Population Study', 'Principal Investigator', 'Probability', 'Property', 'Proteome', 'Proteomics', 'Proxy', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Rice', 'Risk Factors', 'SNP genotyping', 'Sampling', 'Science', 'Scientist', 'Series', 'Services', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Software Validation', 'Solutions', 'Speed', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Systems Biology', 'Techniques', 'Telecommunications', 'Testing', 'Time', 'Time Series Analysis', 'Training', 'Treatment Protocols', 'Universities', 'Validation', 'Washington', 'Wisconsin', 'Work', 'animal breeding', 'base', 'cost', 'design', 'drug discovery', 'experience', 'human subject', 'improved', 'interest', 'lecturer', 'models and simulation', 'open source', 'professor', 'programs', 'protein metabolite', 'research and development', 'research study', 'skills', 'software development', 'statistics', 'success', 'theories', 'tool', 'treatment effect']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,103995,-0.014067978949761223
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7240459,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2007,543226,-0.020387331921485723
"The BioSense Initiative to Improve Early Event Detection RTI International, in partnership with the University of North Carolina at Chapel Hill (UNC-CH), and in collaboration with the North Carolina Division of Public Health (NC-DPH), is submitting this application to work with the Centers for Disease Control and Prevention (CDC) to improve early detection of disease outbreaks of public health significance. Rapid detection of disease outbreaks rests on a foundation of accurate classification of patient symptoms early in the course of their illness. The overarching objective of this research is to define, evaluate, and standardize a methodology for creating useful case definitions designed for the early detection of intentional and naturally occurring disease outbreaks. The specific aim of this research proposal is to develop and test methods for increasing the sensitivity and specificity of syndrome definitions using timely emergency department data. Improved case definitions will enhance CDC's capacity to detect and investigate threats to the health of the population, which CDC undertakes as part of its mission. Emergency department data may serve as a rich source for early signals of health threats to the population, but case definitions have not been standardized, and new methods are needed to process and use the textual information found within the emergency record. To address these challenges, we propose an innovative and iterative research plan that leverages RTI's and UNC-CH's capabilities to best serveCDC and the public health community. We will use emergency department data captured through North Carolina's Bioterrorism and Emerging Infections PreventiveService, the operational syndromic surveillance system used by NC-DPH to monitor the state. After (1) developing a gold standard data set of ED visits for evaluating syndrometest characteristics, we will (2) evaluate natural language processing for preprocessing chief complaints; (3) explore use of semantic networking tools for developing definitions; (4) apply a reverse engineering process using ICD-9-CMcode groupings; and (5) assess the applicability of early event detection for creating situational awareness following detection of an event. These methods will make use of information within the emergency record and create syndrome definitions with acceptable sensitivity, specificity, and positisve predictive value. Valid syndromedefinitions will enable public health officials to operate a national monitoring system that can automatically detect signals that may represent disease outbreaks or other potential threats to health. Operation of this system will protect the public health and will strengthen the capacity of public health officials to investigate and respond to these threats rapidly. n/a",The BioSense Initiative to Improve Early Event Detection,7428896,R01PH000038,[' '],PHPPO,RESEARCH TRIANGLE INSTITUTE,R01,2007,415565,-0.03336196482777186
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7214118,R42ES013321,"['Accounting', 'Animals', 'Architecture', 'Biological Assay', 'Biological Neural Networks', 'Chemicals', 'Clinical', 'Clinical Trials', 'Computer Simulation', 'Computer software', 'Contracts', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Drug Formulations', 'Drug Industry', 'Drug toxicity', 'End Point', 'Expert Systems', 'Funding', 'Future', 'Fuzzy Logic', 'Gene Expression', 'Guidelines', 'Health Care Costs', 'Hepatotoxicity', 'Investments', 'Learning', 'Liver', 'Marketing', 'Methods', 'Network-based', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Proteomics', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Rate', 'Relative (related person)', 'Reliance', 'Research', 'Research Personnel', 'Screening procedure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Training', 'Validation', 'base', 'computational chemistry', 'cost', 'data acquisition', 'design', 'highly advanced system', 'improved', 'innovation', 'knowledge base', 'metabolomics', 'quantum', 'serial analysis of gene expression', 'subtraction hybridization', 'tool']",NIEHS,"YAHSGS, LLC",R42,2007,257269,-0.014254881448272774
"Accelerating metabolic discovery using characterization data    DESCRIPTION (provided by applicant): The long term goal of this project is to develop methods that will allow researchers to gain insight into the metabolic networks of organisms for which we have little or no high-throughput data. Such metabolic networks can reveal aspects of the organism's metabolism that might make it vulnerable to new or existing therapies. A core data set using genomic and other omic data from data-rich bacteria that are related to the organisms of interest will be assembled. The statistical tools needed to integrate these data and to infer metabolic networks using these core data plus characterization (phenotypic) data will then be built. Using the statistical inference algorithms, the characterization data can be leveraged to reveal the metabolic networks of data-poor bacteria for which we have only characterization data. This approach can eliminate the need for genome sequencing, gene expression experiments and the like for thousands of Gram-negative facultative rod bacteria (GNF). There are five tasks in the project: (1) assemble the data sets from data-rich organisms that will be used to inform the inference algorithm. These data include (a) the genomic sequences and annotation information, (b) extant pathway data and (c) gene expression data. All these data contain some level of information about the connectivity within the metabolic network; (2) process the genomic data to enhance its predictive value; (3) develop a data integration algorithm; (4) investigate modeling frameworks to be used for Bayesian data fusion and network inference; (5) validate the metabolic networks. Deliverables from this project should include: (1) a set of pathway genome databases for 35 GNF, This group includes 20 strains classified as category A or B biothreat agents, (2) a core dataset that integrates all the information we have relevant to the metabolic pathways in the 35 sequenced GNF, (3) a probabilistic graphical modeling framework capable of integrating disparate types of data and inferring networks from the integrated data, (4) a method for using characterization data, along with deliverables 2 and 3, to infer metabolic networks for bacterial strains for which we have only characterization data. The ability to rapidly construct models of metabolic networks means researchers will be able to respond to emerging infectious agents or biothreats more quickly. Relevance The methods developed as part of this proposal will allow us to quickly make metabolic maps for thousands of bacteria. Such maps can guide researchers to promising new targets for therapeutic or preventative measures against pathogenic bacteria. The fight against well-known pathogens and biothreat agents, as well as against new, emerging pathogens will be greatly aided by these tools.              n/a",Accelerating metabolic discovery using characterization data,7267998,R21AI067543,"['Adopted', 'Algorithms', 'American Type Culture Collection', 'Artificial Intelligence', 'Bacteria', 'Bacteriology', 'Biochemical', 'Biochemical Pathway', 'Biological Models', 'Biology', 'Bypass', 'Categories', 'Cholera', 'Code', 'Data', 'Data Collection', 'Data Set', 'Depth', 'Disease', 'Electronics', 'Facility Construction Funding Category', 'Gammaproteobacteria', 'Gene Expression', 'Genomics', 'Goals', 'Gram&apos', 's stain', 'Infectious Agent', 'Information Networks', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Nosocomial Infections', 'Numbers', 'Nutritional', 'Organism', 'Outcome', 'Oxygen', 'Pathway interactions', 'Plague', 'Predictive Value', 'Process', 'Prophylactic treatment', 'Proteomics', 'Research Personnel', 'Salmonella typhi', 'Shapes', 'Shigella', 'Shigella Infections', 'Signal Transduction', 'Source', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Typhoid Fever', 'Variant', 'Vibrio cholerae', 'Work', 'Writing', 'Yersinia pestis', 'biothreat', 'computerized data processing', 'data integration', 'design', 'falls', 'fight against', 'genome database', 'genome sequencing', 'gram negative facultative rods', 'innovation', 'insight', 'interest', 'network models', 'novel', 'pathogen', 'pathogenic bacteria', 'programs', 'research study', 'retinal rods', 'routine Bacterial stain', 'sound', 'success', 'therapeutic target', 'tool', 'transcriptomics']",NIAID,AMERICAN TYPE CULTURE COLLECTION,R21,2007,185677,-0.030196693731615543
"Dopaminergic Mechanisms of Reward in Schizophrenia    DESCRIPTION (provided by applicant): This revised proposal has been developed in response to PAR-02-062 (Building Translational Research in Behavioral Science). The goal of the proposed work is to demonstrate that an explicitly translational, multidisciplinary approach to defining the role of dopamine (DA) in schizophrenia (SC), is capable of illuminating some of the most poorly understood, most treatment-resistant cognitive and motivational aspects of the illness. Specifically, recent basic research has shown that DA cell firing plays a critical role in the encoding of reward prediction and reward based learning. The proposed work will provide a rigorous test of the applicability of this hypothesis as a framework for understanding the motivational and learning impairments of SC with important implications for schizophrenia therapeutics. To test this hypothesis, we have organized a group of basic and clinical investigators into three scientific modules:1) electrophysiology, 2) neuroimaging, and 3) behavior, with the overall program designed to address different aspects of same theory of DA function. These modules are designed to develop collaborative partnerships with the results of initial pilot experiments used to refine hypothesis for further experimental testing. Initial studies are proposed to document the specific behavioral, electrophysiological, and fMRI Bold signal abnormalities of SC patients during the performance of reward learning paradigms, the impact of different antipsychotics on reward processing in animals, and the physiological sequela associated with reward-driven changes in DA cell firing. Computational modeling will be used to determine whether the experimental results in animals and observed deficits in patients are consistent with the hypothesized role of DA in reward processing. The overall program is designed to develop resources, paradigms, and proof of principle studies that illuminate the nature of DA dysfunction in SC through the application of translational paradigms and models demonstrating the critical role of DA in reward and reinforcement learning. If successful, the proposed work is designed to establish the conceptual foundation and preliminary data needed to support individual RO1 applications and a Translational Center.         n/a",Dopaminergic Mechanisms of Reward in Schizophrenia,7278791,R24MH072647,"['Acute', 'Address', 'Algorithms', 'Animal Experiments', 'Animals', 'Anterior', 'Antipsychotic Agents', 'Appendix', 'Area', 'Artificial Intelligence', 'Basic Science', 'Behavior', 'Behavior Control', 'Behavioral', 'Behavioral Assay', 'Behavioral Paradigm', 'Behavioral Sciences', 'Brain', 'Cells', 'Clinical', 'Clinical Investigator', 'Code', 'Cognitive', 'Computer Simulation', 'Condition', 'Cues', 'Data', 'Depth', 'Dopamine', 'Dose', 'Electric Stimulation', 'Electrophysiology (science)', 'Environment', 'Event', 'Evoked Potentials', 'Feedback', 'Fire - disasters', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Generations', 'Goals', 'Gold', 'Grant', 'Habenula', 'Haloperidol', 'Human', 'Impairment', 'Individual', 'Lateral', 'Lead', 'Learning', 'Literature', 'Methodology', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Outcome Study', 'Patients', 'Pattern', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Pilot Projects', 'Play', 'Procedures', 'Process', 'Psychological reinforcement', 'Psychopathology', 'Rattus', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Rewards', 'Rodent', 'Role', 'Schizophrenia', 'Scientist', 'Series', 'Signal Transduction', 'Simulate', 'Stimulus', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Thinking', 'Translational Research', 'Work', 'base', 'behavioral pharmacology', 'blood oxygenation level dependent response', 'cingulate cortex', 'classical conditioning', 'clinical application', 'cognitive neuroscience', 'design', 'experience', 'hedonic', 'heuristics', 'human study', 'insight', 'interdisciplinary approach', 'neural circuit', 'neuroimaging', 'novel', 'olanzapine', 'programs', 'quetiapine', 'research study', 'response', 'reward circuitry', 'reward processing', 'theories', 'translational approach', 'volunteer']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2007,342842,-0.01841199132860265
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7284239,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2007,588968,-0.005712597921989931
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7111722,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,314029,-0.034963967224684894
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7125135,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2006,387181,-0.014254881448272774
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7106109,K22LM008389,"['clinical research', 'heart arrest', 'model']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2006,135000,-0.028843787887365874
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7015019,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2006,289590,-0.019617763661010812
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,7127208,R01LM008713,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'conformation', 'drug classification', 'mathematical model', 'mathematics']",NLM,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,276362,-0.02890833070419041
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,7015648,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2006,500182,-0.0021372660541380097
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,7107885,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2006,385718,-0.024475687035762823
"Mini Bone-Attached Robot for Joint Arthroplasty DESCRIPTION (provided by applicant):    Medical robotics has the potential to revolutionize how surgical procedures on bony anatomy are performed. It can assist surgeons by preparing bones much more accurately than mechanical guides or freehand cutting. It can help improve patient outcomes by decreasing surgical errors. In the orthopaedics community, though, medical robots have not been very successful due to a variety of issues ranging from robot size, to surgical time, and soft-tissue difficulties. We propose to overcome these difficulties by utilizing a miniature robotic milling device that attaches directly to the bone.      Realizing the potential of minimally invasive procedures, the implant industry is currently in the process of redesigning implants and, together with surgeons, reexamining surgical procedures. It can be expected that the next generation of implants will be smaller and more suitable for eventual development of less invasive procedures. One example of such procedures in knee arthroplasty is patellofemoral resurfacing of the knee. Without lose of generality we will examine the capability of the robot in improving the accuracy of the femoral-component preparation for patellofemoral arthroscopy. From engineering point of this procedure simulates the future generation of orthopaedic arthroplasty where there will be a need to machine bone surface into a more complex shape that is not planar or spherical but complex surfaces. Therefore, the technology demonstrated by this research, though, will not be specific to the patellofemoral procedure, and will be adaptable to many other areas.      We propose to develop a miniature robot that will be rigidly affixed directly to the bone. The robot itself will scan the shape of the femur directly, removing any need for preoperative imaging or intraoperative registration and tracking of the bone. With the additional input of the direction of patellar tracking, we will automatically optimize the planned position of the implant to ensure that it is properly aligned and congruent with the surrounding healthy cartilage. Congruency is a requirement for this procedure to make certain that the patellar component does not impinge on the edge of the femoral component and lead to early failure of the implant. The robot will then mill out the cavity to within 1mm of the planned location, guaranteeing complete coverage of the area with a defined surface uniformity.      By validating the results generated with the proposed robot, first on wax blocks and then plastic bone phantoms, porcine bones, and finally on cadaver knees, we will show that the robot can deliver the accuracy required to precisely place the implant. This precise placement of the femoral component should reduce the possibility of impingement, patellar maltracking, and component loosening, and improve patient outcomes. n/a",Mini Bone-Attached Robot for Joint Arthroplasty,7117290,R01AR052700,"['arthroplasty', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioimaging /biomedical imaging', 'biomedical equipment development', 'clinical biomedical equipment', 'computer assisted patient care', 'computer simulation', 'computer system design /evaluation', 'femur', 'hip prosthesis', 'image guided surgery /therapy', 'joint prosthesis', 'knee', 'medical implant science', 'miniature biomedical equipment', 'phantom model', 'postmortem', 'robotics']",NIAMS,WESTERN PENNSYLVANIA HOSPITAL,R01,2006,140714,-0.014812762916662557
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,7068069,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2006,19532,-0.014422110715328633
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,7243612,R33RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R33,2006,350636,-0.007653591368588947
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7056185,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,145987,-0.03255623444425706
"Generalization of the Client Matching Protocol    DESCRIPTION (provided by applicant):     This proposal tests the generalization of a standardized client-treatment matching interview and decision-tree algorithm (the Client Matching Protocol, or CMP) through an 18-month secondary analysis of the Drug Abuse Treatment Outcome Study (DATOS) database. The investigative team's previous study of Therapeutic Community (TC)-oriented agencies found that clients entering outpatient and residential treatment settings in which there was a concordance with the CMP algorithm (matched clients) showed significantly higher rates of treatment completion and long-term retention than clients entering settings that were discordant with the CMP algorithm (mismatched clients). The present study uses the DATOS variables to recreate the CMP algorithm. The study extends the previous research by 1) testing the generalization of the CMP to non-TC residential and outpatient programs, 2) determining the effect of matching on treatment process, and 3) testing the generalization of the matching effect to one- and five-year treatment outcomes. Additional research questions explore the extension of the CMP algorithm to short-term residential and methadone outpatient treatment, and to the interaction between the CMP match and organizational and client variables. Non-parametric statistics, ANOVA, logistic and multiple regression, and Structural Equation Modeling test the effects of matching and the interaction of matching and the program and client characteristics. The present study contains important research and clinical implications. This 18-month study contains significant implications for both treatment and research in that it will provide empirical clarification of whether and how matching contributes to treatment improvement. Specifically replicating a matching effect in the DATOS modalities will establish the empirical basis for a controlled study of matching and a refined version of the matching protocol for use in clinical practice.         n/a",Generalization of the Client Matching Protocol,7016275,R01DA015787,"['behavioral /social science research tag', 'clinical research', 'drug abuse therapy', 'health care model', 'health care service evaluation', 'health services research tag', 'human data', 'mathematical model', 'outcomes research', 'outpatient care', 'patient care', 'patient oriented research']",NIDA,NATIONAL DEVELOPMENT & RES INSTITUTES,R01,2006,103998,-0.010606837267062063
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,7072754,U01DE013331,"['biomarker', 'biopsy', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'enzyme linked immunosorbent assay', 'facial muscles', 'gas chromatography mass spectrometry', 'human subject', 'inflammation', 'interview', 'magnetic resonance imaging', 'mastication', 'musculoskeletal disorder diagnosis', 'oral facial pain', 'pain threshold', 'psychobiology', 'questionnaires', 'radioimmunoassay', 'sign /symptom', 'statistics /biometry', 'synovial fluid', 'temporomandibular joint syndrome', 'tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2006,371497,-0.004056098793072028
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6916483,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,321788,-0.034963967224684894
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7052491,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2005,180862,-0.014254881448272774
"The BioSense Initiative to Improve Early Event Detection RTI International, in partnership with the University of North Carolina at Chapel Hill (UNC-CH), and in collaboration with the North Carolina Division of Public Health (NC-DPH), is submitting this application to work with the Centers for Disease Control and Prevention (CDC) to improve early detection of disease outbreaks of public health significance. Rapid detection of disease outbreaks rests on a foundation of accurate classification of patient symptoms early in the course of their illness. The overarching objective of this research is to define, evaluate, and standardize a methodology for creating useful case definitions designed for the early detection of intentional and naturally occurring disease outbreaks. The specific aim of this research proposal is to develop and test methods for increasing the sensitivity and specificity of syndrome definitions using timely emergency department data. Improved case definitions will enhance CDC's capacity to detect and investigate threats to the health of the population, which CDC undertakes as part of its mission. Emergency department data may serve as a rich source for early signals of health threats to the population, but case definitions have not been standardized, and new methods are needed to process and use the textual information found within the emergency record. To address these challenges, we propose an innovative and iterative research plan that leverages RTI's and UNC-CH's capabilities to best serve CDC and the public health community. We will use emergency department data captured through North Carolina's Bioterrorism and Emerging Infections Preventive Service, the operational syndromic surveillance system used by NC-DPH to monitor the state. After (1) developing a gold standard data set of ED visits for evaluating syndrome test characteristics, we will (2) evaluate natural language processing for preprocessing chief complaints; (3) explore use of semantic networking tools for developing definitions; (4) apply a reverse engineering process using ICD-9-CM code groupings; and (5) assess the applicability of early event detection for creating situational awareness following detection of an event. These methods will make use of information within the emergency record and create syndrome definitions with acceptable sensitivity, specificity, and positisve predictive value. Valid syndrome definitions will enable public health officials to operate a national monitoring system that can automatically detect signals that may represent disease outbreaks or other potential threats to health. Operation of this system will protect the public health and will strengthen the capacity of public health officials to investigate and respond to these threats rapidly.  n/a",The BioSense Initiative to Improve Early Event Detection,7097771,R01PH000038,"['artificial intelligence', 'biohazard detection', 'bioterrorism /chemical warfare', 'communicable disease diagnosis', 'computer assisted diagnosis', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'disease /disorder classification', 'disease outbreaks', 'early diagnosis', 'emergency health services', 'emerging infectious disease', 'environmental health', 'health services research tag', 'human data', 'informatics', 'interdisciplinary collaboration', 'public health', 'rapid diagnosis', 'vocabulary development for information system']",PHPPO,RESEARCH TRIANGLE INSTITUTE,R01,2005,412947,-0.03085023675444568
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,6916728,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2005,292728,-0.019617763661010812
"BIOROBOTICS TOOLKIT FOR ENABLING RAPID EXPERIEMENTATION    DESCRIPTION (provided by applicant): We propose to create a biorobotic toolkit for rapid experimentation in the life sciences, medicine, and bioengineering. This toolkit will allow the rapid creation of biorobots derived from reference designs. These reference designs are contributed by the community of researchers. The anticipated outcome will be a vast improvement in methodology in this field. The specific aims of phase I are: (1) The design of 1 reference model (2) Demonstration of a modular plug and play sensor that will be part of a biorobot derived from the reference model (3)Demonstration of a modular plug and play Actuator that will be part of a biorobot derived from the reference model (4) Assemble a robot derived from the reference model, and using the plug and play sensors and actuators achieved in aims 2,3. (5) Quantify the closed loop performance of the sensor-actuator network. (6) Layout a preliminary specification of the architecture.         n/a",BIOROBOTICS TOOLKIT FOR ENABLING RAPID EXPERIEMENTATION,6975326,R43EB004827,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment', 'biomedical equipment development', 'computational biology', 'model design /development', 'neuropsychology', 'psychological models', 'robotics']",NIBIB,"IGUANA ROBOTICS, INC.",R43,2005,150419,-0.0018189828252193627
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,6965348,R01LM008713,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'conformation', 'drug classification', 'mathematical model', 'mathematics']",NLM,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,283196,-0.02890833070419041
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6924688,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2005,167063,-0.03931976913053811
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,6863029,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2005,498368,-0.0021372660541380097
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6949109,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2005,395000,-0.024475687035762823
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6850134,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2005,297104,-0.01900082420555531
"Mini Bone-Attached Robot for Joint Arthroplasty DESCRIPTION (provided by applicant):    Medical robotics has the potential to revolutionize how surgical procedures on bony anatomy are performed. It can assist surgeons by preparing bones much more accurately than mechanical guides or freehand cutting. It can help improve patient outcomes by decreasing surgical errors. In the orthopaedics community, though, medical robots have not been very successful due to a variety of issues ranging from robot size, to surgical time, and soft-tissue difficulties. We propose to overcome these difficulties by utilizing a miniature robotic milling device that attaches directly to the bone.      Realizing the potential of minimally invasive procedures, the implant industry is currently in the process of redesigning implants and, together with surgeons, reexamining surgical procedures. It can be expected that the next generation of implants will be smaller and more suitable for eventual development of less invasive procedures. One example of such procedures in knee arthroplasty is patellofemoral resurfacing of the knee. Without lose of generality we will examine the capability of the robot in improving the accuracy of the femoral-component preparation for patellofemoral arthroscopy. From engineering point of this procedure simulates the future generation of orthopaedic arthroplasty where there will be a need to machine bone surface into a more complex shape that is not planar or spherical but complex surfaces. Therefore, the technology demonstrated by this research, though, will not be specific to the patellofemoral procedure, and will be adaptable to many other areas.      We propose to develop a miniature robot that will be rigidly affixed directly to the bone. The robot itself will scan the shape of the femur directly, removing any need for preoperative imaging or intraoperative registration and tracking of the bone. With the additional input of the direction of patellar tracking, we will automatically optimize the planned position of the implant to ensure that it is properly aligned and congruent with the surrounding healthy cartilage. Congruency is a requirement for this procedure to make certain that the patellar component does not impinge on the edge of the femoral component and lead to early failure of the implant. The robot will then mill out the cavity to within 1mm of the planned location, guaranteeing complete coverage of the area with a defined surface uniformity.      By validating the results generated with the proposed robot, first on wax blocks and then plastic bone phantoms, porcine bones, and finally on cadaver knees, we will show that the robot can deliver the accuracy required to precisely place the implant. This precise placement of the femoral component should reduce the possibility of impingement, patellar maltracking, and component loosening, and improve patient outcomes. n/a",Mini Bone-Attached Robot for Joint Arthroplasty,6957258,R01AR052700,"['arthroplasty', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioimaging /biomedical imaging', 'biomedical equipment development', 'clinical biomedical equipment', 'computer assisted patient care', 'computer simulation', 'computer system design /evaluation', 'femur', 'hip prosthesis', 'image guided surgery /therapy', 'joint prosthesis', 'knee', 'medical implant science', 'miniature biomedical equipment', 'phantom model', 'postmortem', 'robotics']",NIAMS,WESTERN PENNSYLVANIA HOSPITAL,R01,2005,172920,-0.014812762916662557
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6937143,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2005,20000,-0.014422110715328633
"A Run-to-Run Algorithm for Glucose Regulation DESCRIPTION (provided by applicant):    The long term objective of this work is to develop new algorithmic approaches to optimize the delivery of insulin in an automated fashion to people with type 1 diabetes. Specifically, we aim to develop a strategy, inspired by run-to-run control theory established by the chemical process industries, that ""learns"" from the previous sequence of glucose responses to insulin dosing (over the course of days), and optimally predicts the appropriate strategy for the forthcoming day. The notion of a ""cycle"" in engineering will be extended to manage the 24 hour routine of repeated meals, activities, and sleep cycles and the corresponding dosing of insulin. The algorithm will be tested in both simulation and clinical trials for robustness to sensor noise, uncertainty in the patient characterization, variability in the timing of the postprandial glucose peak, and variability in the carbohydrate content in the meals. The Specific Aims of this project are to: i) construct predictive patient sensitivity models for calculation of optimal insulin dosing from elevated (or depressed) glucose levels, ii) develop run-to-run algorithm for insulin bolus dosing to provide corrections in subsequent days based on previous history of glucose levels and insulin dosage, and iii) evaluate the robustness of the algorithm through meal challenges of varying carbohydrate content. The aims will blend prototype algorithms that are drawn from systems engineering with validation in a series of clinical tests. The proposed collaboration between systems engineers and renowned diabetes researchers in an established clinical research setting will allow a novel fusion of methods that can be truly characterized as ""innovative"". The medical collaborators in the proposal are located at the prestigious Sansum Medical Research Institute, which is located less than 10 miles from the campus of the University of California, Santa Barbara. The exchange of personnel will be facilitated, allowing the student and post-doc supported on this project to work at both the institute and the university over the span of the project n/a",A Run-to-Run Algorithm for Glucose Regulation,6953163,R01DK068663,"['artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical automation', 'blood glucose', 'clinical research', 'clinical trials', 'computer assisted patient care', 'computer simulation', 'dietary carbohydrates', 'drug administration rate /duration', 'drug delivery systems', 'glucose metabolism', 'human subject', 'insulin dependent diabetes mellitus', 'insulin sensitivity /resistance', 'mathematical model', 'model design /development', 'patient oriented research']",NIDDK,SANSUM DIABETES RESEARCH INSTITUTE,R01,2005,202350,-0.009093316173446598
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6914863,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2005,167918,-0.007653591368588947
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,6922406,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,174500,-0.03255623444425706
"Generalization of the Client Matching Protocol    DESCRIPTION (provided by applicant):     This proposal tests the generalization of a standardized client-treatment matching interview and decision-tree algorithm (the Client Matching Protocol, or CMP) through an 18-month secondary analysis of the Drug Abuse Treatment Outcome Study (DATOS) database. The investigative team's previous study of Therapeutic Community (TC)-oriented agencies found that clients entering outpatient and residential treatment settings in which there was a concordance with the CMP algorithm (matched clients) showed significantly higher rates of treatment completion and long-term retention than clients entering settings that were discordant with the CMP algorithm (mismatched clients). The present study uses the DATOS variables to recreate the CMP algorithm. The study extends the previous research by 1) testing the generalization of the CMP to non-TC residential and outpatient programs, 2) determining the effect of matching on treatment process, and 3) testing the generalization of the matching effect to one- and five-year treatment outcomes. Additional research questions explore the extension of the CMP algorithm to short-term residential and methadone outpatient treatment, and to the interaction between the CMP match and organizational and client variables. Non-parametric statistics, ANOVA, logistic and multiple regression, and Structural Equation Modeling test the effects of matching and the interaction of matching and the program and client characteristics. The present study contains important research and clinical implications. This 18-month study contains significant implications for both treatment and research in that it will provide empirical clarification of whether and how matching contributes to treatment improvement. Specifically replicating a matching effect in the DATOS modalities will establish the empirical basis for a controlled study of matching and a refined version of the matching protocol for use in clinical practice.         n/a",Generalization of the Client Matching Protocol,6865332,R01DA015787,"['behavioral /social science research tag', 'clinical research', 'drug abuse therapy', 'health care model', 'health care service evaluation', 'health services research tag', 'human data', 'mathematical model', 'outcomes research', 'outpatient care', 'patient care', 'patient oriented research']",NIDA,NATIONAL DEVELOPMENT & RES INSTITUTES,R01,2005,248500,-0.010606837267062063
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6901098,U01DE013331,"['biomarker', 'biopsy', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'enzyme linked immunosorbent assay', 'facial muscles', 'gas chromatography mass spectrometry', 'human subject', 'inflammation', 'interview', 'magnetic resonance imaging', 'mastication', 'musculoskeletal disorder diagnosis', 'oral facial pain', 'pain threshold', 'psychobiology', 'questionnaires', 'radioimmunoassay', 'sign /symptom', 'statistics /biometry', 'synovial fluid', 'temporomandibular joint syndrome', 'tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2005,1520994,-0.004056098793072028
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6799187,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,321983,-0.034963967224684894
"Use of Microarray Test Data for Toxicogenomic Prediction    DESCRIPTION (provided by applicant):    This project bridges the understanding between physical and chemical principles and genomic/proteomic response by integrating three independent parallel toxicity prediction tools. Each uses computational neural networks (CNNs) and wavelets to rapidly and accurately make pharmaceutical/chemical toxicity predictions. A CNN-based Quantitative Structure-Activity Relationship (QSAR) module makes toxicological predictions based only on structure-activity analyses; a second CNN/wavelet module makes independent toxicogenomic predictions using microarray data; and a third CNN/wavelet module makes toxicogenomic predictions using Massively Parallel Signature Sequencing (MPSS) data. This multi-intelligent, three-module approach provides crosschecks to reduce false positives and false negatives while substantially increasing confidence in predictions relative to current computer-based toxicity prediction techniques. The resulting product could potentially become a primary tool used by (a) human health researchers, b) pharmaceutical companies for screening drugs early during development, c) companies designing/developing new chemicals and chemically treated materials, and (d) government organizations (e.g., military) for mission-related chemical deployments. Public benefits include reduced health and environmental risks (e.g., 4 out of 5 chemicals in use today have inadequate testing); reduced reliance on animal testing; and reduced time and cost required to bring new pharmaceuticals and chemicals into beneficial medical and commercial use.            n/a",Use of Microarray Test Data for Toxicogenomic Prediction,6743871,R41ES013321,"['computational neuroscience', 'computer data analysis', 'evaluation /testing', 'method development', 'microarray technology', 'polymerase chain reaction', 'toxicant screening', 'toxicology']",NIEHS,"YAHSGS, LLC",R41,2004,211770,-0.021792562405221382
"Intelligent Electric Stimulator with Sensor Capabilities    DESCRIPTION (provided by applicant):    The ultimate objective of this project is to develop intelligent electrical stimulation devices, which will automatically adjust the stimulator signal based on the conditions prevailing in the tissue, or in the media. This would allow for optimization of a long term treatment, as well as for an on demand treatment, if the conditions in the body suddenly change. Today such a treatment is used in, e.g., on-demand pacemakers. We are looking to extend it to physiological/biochemical processes in the body in addition to stimulation of functions.         n/a",Intelligent Electric Stimulator with Sensor Capabilities,6834042,R43RR021814,"['artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'biosensor device', 'biotechnology', 'electrodes', 'electronic stimulator', 'miniature biomedical equipment']",NCRR,"HERBST RESEARCH, INC.",R43,2004,449282,-0.02560048030667641
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6703756,R44CA093112,"['artificial intelligence', 'clinical research', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'mathematics', 'statistics /biometry']",NCI,"CYTEL, INC",R44,2004,411387,0.00042122639658516496
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6821032,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2004,153968,-0.03931976913053811
"Multiparametric Computational Echocardiography DESCRIPTION (provided by applicant):    In the United States, myocardial ischemia is the underlying etiology in nearly 70% of the 5 million patients suffering from mechanical failure of the cardiac left ventricle (LV). Echocardiography (echo) is well suited for the analysis of LV function, however, the problem of conveying this clinically important information in a quantitative, objective, and reproducible manner persists.      To address this problem and respond to the NIH PA-00-117 solicitation for innovations in biomedical information science and technology, the long-term goal of the proposed R21/R33 project is to develop methods for 1) quantitative and objective measurement of LV function, 2) reproducible diagnostic interpretation based on these measurements, and 3) standardized topologic mapping and parametric display of the results. We defined 3 parameters of LV function: 1) rate of local deformation (strain rate), 2) amplitude of deformation (strain), and 3) time interval to the transition (crossover) point of cyclic deformation.      The practical implementation of the long-term objectives will be achieved through the development of a Multiparametric Computational Echo (MPCE) system. The main hypothesis of this R21/R33 project is that the MPCE system will a) precisely and accurately quantitate (R21 phase) and b) reproducibly clinically interpret (R33 phase) segmental LV function. This hypothesis will be tested on digital echo data from animal models and clinical cases of myocardial ischemia representing a wide range of segmental LV dysfunction.      Specific Aims (R21 phase):   Aim 1 - Develop algorithms for parametric analysis of local ventricular function (Year 1).   Aim 2 - Test precision and accuracy of local LV functional analysis using MPCE (Year 2).   Satisfaction of the predefined milestones will initiate the commencement of the R33 phase.      Specific Aims (R33 phase):   Aim 1 - Develop a neural network MPCE system and test it initially in animals (Year 3).   Aim 2 - Refine parametric mapping and test reproducibility of MPCE data in humans (Year 4).   Successful completion of this project will result in noninvasive, quantitative, objective, and reproducible interpretation of LV function, thus facilitating optimal diagnostic and therapeutic decisions in cardiology. n/a",Multiparametric Computational Echocardiography,6783325,R21HL070363,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'echocardiography', 'evaluation /testing', 'heart function', 'human subject', 'method development', 'swine']",NHLBI,MAYO CLINIC,R21,2004,144693,-0.01762425858411974
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6797879,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2004,395000,-0.024475687035762823
"Wireless EEG/PSG System with Novel Artifact Removal DESCRIPTION (provided by applicant): EEG is a valuable non-invasive clinical tool in numerous applications, from the diagnosis and treatment of brain diseases to the clinical monitoring of neurological injuries, sleep disorders and depth of anesthesia.  However, EEG signals are very susceptible to various artifacts which seriously impede the EEG interpretation and compromise its therapeutic capabilities. Methods currently employed for removing artifacts from EEG recordings are not clinically effective or feasible for real-time and long-term neuro-monitoring. Hence, the overall goal of this project is to develop a novel, high-fidelity artifact identification and removal technique that will be specifically useful for ambulatory EEG recording and intervention.      The proposed novel artifact removal technique is based on the Wavelet-Based Artifact Removal (WBAR) method, which exploits the excellent time-frequency localization of artifacts provided by the wavelet decomposition. The WBAR method is computationally very efficient and allows for simultaneous, real-time removal of a variety of EEG artifacts. It has been recently developed by the PI and tested for a single EEG channel in an extensive clinical study as part of a novel depth-of-anesthesia monitor.       The WBAR method will be improved by combining it with the Wavelet Neural Networks for the precise artifact classification, and recursive EEG Parameterization methods for the reliable estimation of the corrupted EEG components. The combination of these methods will result in fully automated, real-timeartifact removal technique that maximally preserves valid EEG information.       The development and implementation of this novel method will greatly enhance the functionality and  utilization of Cleveland Medical Devices' entire line of ambulatory wireless EEG/PSG systems. n/a",Wireless EEG/PSG System with Novel Artifact Removal,6792393,R43NS046978,"['artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'brain electrical activity', 'clinical biomedical equipment', 'clinical research', 'computer program /software', 'electroencephalography', 'human subject', 'patient monitoring device', 'patient oriented research', 'polysomnography', 'portable biomedical equipment', 'sleep']",NINDS,"CLEVELAND MEDICAL DEVICES, INC.",R43,2004,204478,-0.025297776117500902
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6721300,R37GM030998,"['DNA', 'artificial intelligence', 'biochemical evolution', 'computational neuroscience', 'computer assisted sequence analysis', 'computer simulation', 'gene frequency', 'genetic models', 'mathematical model', 'method development', 'model design /development', 'natural selections', 'nucleic acid sequence', 'species difference', 'statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2004,161792,-0.020691277865846364
"Epileptic Seizures in the Neonatal EEG   DESCRIPTION: (provided by applicant) The identification of electrographic            seizures during long-term EEG monitoring in the neonate is currently based upon      visual interpretation of the graphic record, a process that is very                  time-consuming. While significant progress has been made in the automated            detection of seizures in the adult population, relatively little work has been       done in the neonatal area. Therefore, the major objective of this project is         the development of techniques for the reliable automated detection of                electrographic seizures in the neonatal EEG. We propose a multi-stage, hybrid        approach to detection that will employ a combination of signal processing,           pattern recognition, neural networks, and expert rules. Through the successive       stages of the detection process, multichannel neonatal EEG data containing all       types of background activity and artifacts will be analyzed to detect and            classify electrographic seizures. We postulate that the varied types and             morphologies of seizures in the neonatal EEG, as compared to seizures in the         adult EEG, can best be detected and classified using this hybrid approach. We        will test the methods we develop on data recorded from infants in the Clinical       Research Center for Neonatal Seizures, The Methodist Hospital, in Houston,           Texas. We expect that the information we gain from the research will lead to         the development of a practical seizure detection system and further our              long-term goals of reduced expense in the reading and interpretation of              neonatal EEGs, and also facilitate the efficient collection of seizure               parameters for possible use in future research studies.                                                                                                                   n/a",Epileptic Seizures in the Neonatal EEG,6780874,R01NS040577,"['adult human (21+)', 'age difference', 'artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'brain electrical activity', 'computer assisted diagnosis', 'electroencephalography', 'epilepsy', 'human subject', 'newborn human (0-6 weeks)', 'patient monitoring device', 'patient oriented research']",NINDS,UNIVERSITY OF HOUSTON,R01,2004,194888,-0.011828496275888127
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6701378,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,297104,-0.01900082420555531
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6849505,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,105415,-0.01900082420555531
"Validating and Translating Array Signatures    DESCRIPTION (provided by applicant)   In the area of cancer, microarray analysis is being used to identify specific differences between normal and disease tissues. Such efforts are leading to the identification of small gene sets, or signatures, that can provide key information for classifying cancer types. We propose here a mechanism for translating these discoveries into cost-effective, quantitative assays based on the use of highly multiplexed, universal-primer-driven rtPCR (UP-rtPCR). In this proposal we will demonstrate and validate the combined use of artificial neural network analysis for identification of small gene signatures and UP-rtPCR technology for translating these signatures into high throughput assays for use in research and clinical settings. Success will be measured by the ability of this approach to differentiate 4 types of small, round, blue-cell tumors. This work will be performed in collaboration with Drs. Javed Khan and Gary Fogel.            n/a",Validating and Translating Array Signatures,6836381,R43CA110542,"['RNA', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'gene expression', 'high throughput technology', 'human genetic material tag', 'microarray technology', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer genetics', 'polymerase chain reaction', 'technology /technique development']",NCI,"ALTHEA TECHNOLOGIES, INC.",R43,2004,99999,-0.04131843673440276
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6803809,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2004,20000,-0.014422110715328633
"A Run-to-Run Algorithm for Glucose Regulation DESCRIPTION (provided by applicant):    The long term objective of this work is to develop new algorithmic approaches to optimize the delivery of insulin in an automated fashion to people with type 1 diabetes. Specifically, we aim to develop a strategy, inspired by run-to-run control theory established by the chemical process industries, that ""learns"" from the previous sequence of glucose responses to insulin dosing (over the course of days), and optimally predicts the appropriate strategy for the forthcoming day. The notion of a ""cycle"" in engineering will be extended to manage the 24 hour routine of repeated meals, activities, and sleep cycles and the corresponding dosing of insulin. The algorithm will be tested in both simulation and clinical trials for robustness to sensor noise, uncertainty in the patient characterization, variability in the timing of the postprandial glucose peak, and variability in the carbohydrate content in the meals. The Specific Aims of this project are to: i) construct predictive patient sensitivity models for calculation of optimal insulin dosing from elevated (or depressed) glucose levels, ii) develop run-to-run algorithm for insulin bolus dosing to provide corrections in subsequent days based on previous history of glucose levels and insulin dosage, and iii) evaluate the robustness of the algorithm through meal challenges of varying carbohydrate content. The aims will blend prototype algorithms that are drawn from systems engineering with validation in a series of clinical tests. The proposed collaboration between systems engineers and renowned diabetes researchers in an established clinical research setting will allow a novel fusion of methods that can be truly characterized as ""innovative"". The medical collaborators in the proposal are located at the prestigious Sansum Medical Research Institute, which is located less than 10 miles from the campus of the University of California, Santa Barbara. The exchange of personnel will be facilitated, allowing the student and post-doc supported on this project to work at both the institute and the university over the span of the project n/a",A Run-to-Run Algorithm for Glucose Regulation,6827448,R01DK068663,"['artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical automation', 'blood glucose', 'clinical research', 'clinical trials', 'computer assisted patient care', 'computer simulation', 'dietary carbohydrates', 'drug administration rate /duration', 'drug delivery systems', 'glucose metabolism', 'human subject', 'insulin dependent diabetes mellitus', 'insulin sensitivity /resistance', 'mathematical model', 'model design /development', 'patient oriented research']",NIDDK,SANSUM DIABETES RESEARCH INSTITUTE,R01,2004,203050,-0.009093316173446598
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6810083,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2004,178840,-0.007653591368588947
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,-0.01758793159882331
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6773915,U01DE013331,"['biomarker', 'biopsy', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'enzyme linked immunosorbent assay', 'facial muscles', 'gas chromatography mass spectrometry', 'human subject', 'inflammation', 'interview', 'magnetic resonance imaging', 'mastication', 'musculoskeletal disorder diagnosis', 'oral facial pain', 'pain threshold', 'psychobiology', 'questionnaires', 'radioimmunoassay', 'sign /symptom', 'statistics /biometry', 'synovial fluid', 'temporomandibular joint syndrome', 'tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2004,1637046,-0.004056098793072028
"Tree Ensemble Regression and Classification Methods    DESCRIPTION (provided by applicant):    This SBIR aims to produce next generation classification and regression software based upon ensembles of decision trees: bagging, random forests, and boosting. The prediction accuracy of these methods has caused much excitement in the machine learning community, and both challenges and complements the data modeling culture prevalent among biostatisticians. Recent research extends the methodology to likelihood based methods used in biostatistics, leading to models for survival data and generalized forest models. Generalized forest models extend regression forests in the same way that generalized linear models extend linear models.      This software would apply broadly, including to medical diagnosis, prognostic modeling, and detecting cancer; and for modeling patient characteristics like blood pressure, discrete responses in clinical trials, and count data.      Phase I work will prototype software for survival data, and investigate the performance of ensemble methods on simulated and real data. For survival applications, we will assess out-of-bag estimates of performance, and investigate measures of variable importance and graphics that help clinicians understand the results. Experience writing prototypes and using them on data will lead to a preliminary software design that serves as the foundation of Phase II work.      Phase II will expand upon this work to create commercial software. We will research and implement algorithms for a wider range of applications including generalized forest models, classification, and least squares regression. We will also implement robust loss criteria that enable good performance on noisy data, and make adaptations to handle large data sets.      This proposed software will enable medical researchers to obtain high prediction accuracy, and complement traditional tools like discriminant analysis, linear and logistic regression models, and the Cox model.         n/a",Tree Ensemble Regression and Classification Methods,6832086,R43CA105724,"['clinical research', 'computer assisted medical decision making', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'method development', 'model design /development', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer remission /regression', 'prognosis', 'statistics /biometry']",NCI,INSIGHTFUL CORPORATION,R43,2004,99937,-0.0006567571368430849
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6682996,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' automated data processing', ' chemical structure', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' crystallization', ' data collection methodology /evaluation', ' image processing', ' mathematics', ' method development', ' protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,298672,-0.034963967224684894
"Development of a Multi-State Decoding Framework DESCRIPTION (provided by applicant): This proposal aims to increase the capability and decrease the cost of decoding the Illumina bead array platform by adding decode states. DNA probes attached to microbeads are randomly loaded onto fiber optic bundles. A decoding process of sequential hybridization stages is necessary to determine the locus correspondence of each bead. Decoders (sequences complementary to the DNA probes on the beads) that are either unlabeled, or labeled with a dye are hybridized to the array. Images are taken after each hybridization, and the experiment is designed so that the hybridization signature of each bead through the decode process, uniquely determines the identity of the bead. The cost and time of decoding is proportional to the number of decode stages. The number of stages is determined by the number of loci represented on the array and the number of distinguishable labels, or decode states, used in the decode process (e.g. ON in dye 1). The current availability of 3 states allows the decoding of 1,500 probes in 8 stages. The successful execution of this project would extend the number of states to at least 8. With 8 states, the number of stages for the 1,500-probe product would become 4. The number of probes that could be decoded in 8 stages would increase by 3 orders of magnitude. The main components of the project are wet lab chemistry and algorithm development. Wet lab chemistry will be used to determine the optimal mixture of dye labeled and unlabeled oligonucleotides that will lead to distinguishable intensity states. Beads will have signal levels in FAM and CY3 dye. Variability in the process will need to be sufficiently low to reliably distinguish different concentrations of dye labeled oligonucleo tides. Three levels of FAM and CY3 signal would lead to 9 states. It is likely that 8 of these will be reliably distinguishable. Pattern matching algorithms will be developed to decode the beads. Decision tree methods based on expected signal will be applied. Arrays will be decoded twice -- first with the current 3-state decoding, and then with the multi-state decoding -- to enable training and machine learning algorithms. Achieving 8 state decoding will decrease the cost of the array and dramatically increase the number of loci explored and the number of probes per locus. n/a",Development of a Multi-State Decoding Framework,6737095,R43HG003096,"['chemistry', ' cost effectiveness', ' dyes', ' mathematics', ' microarray technology', ' nucleic acid probes', ' technology /technique development']",NHGRI,"ILLUMINA, INC.",R43,2003,141830,-0.0017254202645278092
"Ab-Initio Geometry Optimization of Large Molecules    DESCRIPTION (provided by applicant):  While density-functional calculations of the energy are now feasible for biomolecules, the use of density-functional geometry optimizers is still confined to relatively small molecules containing no more than thirty atoms. The key limitation of conventional density-functional geometry optimizers is that the cost of the geometry optimization scales at least quadratically with the number of atoms in the molecule. In contrast the energy at a fixed geometry can be evaluated for a cost which scales linearly with molecule size, enabling very large molecules to be treated. This proposal is based on a radical change in the algorithm for density-functional geometry optimization, potentially reducing the total cost from quadratic to linear in molecule size and enabling a quantum leap in the size of molecules that can be optimized. The proposed algorithm resembles a conventional self-consistent calculation of the energy at a fixed geometry but at convergence the proposed algorithm yields not only the density but also the optimized geometry. This is achieved by simultaneous optimization of the wavefunction and the geometry via a modified self-consistent-field procedure. The proposed algorithm will be implemented in the QChem software package and, if successful, widely distributed through QChem Inc. and Spartan Inc.           n/a",Ab-Initio Geometry Optimization of Large Molecules,6583907,R43GM067335,"['artificial intelligence', ' chemical models', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' molecular dynamics', ' molecular size', ' quantum chemistry']",NIGMS,"Q-CHEM, INC.",R43,2003,99639,-0.026446224113988944
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6617906,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2003,170109,0.00943543213991779
"Vector Quantization for Image Pattern Recognition    DESCRIPTION (provided by applicant):    This Phase-I SBIR application addresses the increasingly significant challenges faced by pathologists and clinicians in manually inspecting microscope slides. Microscopic inspection suffers from being labor-intensive, subjective, expensive and limited by the need for physical access to the glass slide specimen of interest. The obstacle to automated microscopic inspection has been the inability to efficiently digitize entire microscope specimens at high resolutions. Aperio has developed the ScanScope (R), a novel microscope slide scanner that makes it practical - for the first time - to rapidly create virtual microscope slides at high resolutions. Virtual slides set the stage for automating microscopic inspection using automated pattern recognition. This research aims to adapt and optimize Aperio's existing and novel algorithms for vector quantization (VQ) to the problem of automatic pattern recognition in virtual slides. VQ is a general mathematical technique for encoding bitstreams using a vocabulary. The primary aim is to demonstrate the feasibility of using VQ for pattern recognition in a practical and well-characterized application: automatically finding virtually all micrometastasis clusters in cytology specimens. This proposed research represents a first attempt to automate pattern recognition in virtual slides using VQ.         n/a",Vector Quantization for Image Pattern Recognition,6695147,R43EB001617,"['artificial intelligence', ' automated data processing', ' bioimaging /biomedical imaging', ' cell line', ' computer system design /evaluation', ' cytology', ' digital imaging', ' high throughput technology', ' metastasis', ' microscopy', ' nomenclature']",NIBIB,"APERIO TECHNOLOGIES, INC.",R43,2003,97269,-0.02032036006848635
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6587476,R44CA093112,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' human data', ' mathematical model', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R44,2003,400084,0.00042122639658516496
"Multiparametric Computational Echocardiography DESCRIPTION (provided by applicant):    In the United States, myocardial ischemia is the underlying etiology in nearly 70% of the 5 million patients suffering from mechanical failure of the cardiac left ventricle (LV). Echocardiography (echo) is well suited for the analysis of LV function, however, the problem of conveying this clinically important information in a quantitative, objective, and reproducible manner persists.      To address this problem and respond to the NIH PA-00-117 solicitation for innovations in biomedical information science and technology, the long-term goal of the proposed R21/R33 project is to develop methods for 1) quantitative and objective measurement of LV function, 2) reproducible diagnostic interpretation based on these measurements, and 3) standardized topologic mapping and parametric display of the results. We defined 3 parameters of LV function: 1) rate of local deformation (strain rate), 2) amplitude of deformation (strain), and 3) time interval to the transition (crossover) point of cyclic deformation.      The practical implementation of the long-term objectives will be achieved through the development of a Multiparametric Computational Echo (MPCE) system. The main hypothesis of this R21/R33 project is that the MPCE system will a) precisely and accurately quantitate (R21 phase) and b) reproducibly clinically interpret (R33 phase) segmental LV function. This hypothesis will be tested on digital echo data from animal models and clinical cases of myocardial ischemia representing a wide range of segmental LV dysfunction.      Specific Aims (R21 phase):   Aim 1 - Develop algorithms for parametric analysis of local ventricular function (Year 1).   Aim 2 - Test precision and accuracy of local LV functional analysis using MPCE (Year 2).   Satisfaction of the predefined milestones will initiate the commencement of the R33 phase.      Specific Aims (R33 phase):   Aim 1 - Develop a neural network MPCE system and test it initially in animals (Year 3).   Aim 2 - Refine parametric mapping and test reproducibility of MPCE data in humans (Year 4).   Successful completion of this project will result in noninvasive, quantitative, objective, and reproducible interpretation of LV function, thus facilitating optimal diagnostic and therapeutic decisions in cardiology. n/a",Multiparametric Computational Echocardiography,6688878,R21HL070363,"['artificial intelligence', ' bioimaging /biomedical imaging', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' echocardiography', ' evaluation /testing', ' heart function', ' human subject', ' method development', ' swine']",NHLBI,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R21,2003,136861,-0.01762425858411974
"Software to Handle Missing Values in Large Data DESCRIPTION (provided by applicant):    This SBIR aims to produce commercial software for handling missing data in large data sets, where the goal is data mining and knowledge discovery. There may be a large number of subjects, variables, or both. Examples include microarray data, surveys, genomic data, and high throughput screening data.      Handling missing data is one important step of careful data preparation, which is key to the success of an entire project. Missing values often arise in medical data. This is an obstacle because many data mining tools either require complete data or are not robust to missing data.      Principled methods of handling missing data are computationally intensive. Therefore computational feasibility is a challenge to handling missing values in large data sets.      Phase I work will explore strategies such as sampling, constraining parameters, and monotone data algorithms for model based techniques. Factor analysis and multivariate linear mixed effects models will be used to reduce the number of parameters. A variable-by-variable approach using a popular data mining technique, recursive partitioning, will also be used to impute missing values.      For each of the methods, we will write prototype software and test performance on missing data patterns simulated on real data. Several ad hoc techniques will serve as a baseline for comparison.   Experience writing prototypes and using them in simulations will lead to preliminary software design that will serve as the foundation of Phase II work.       This proposed software will enable medical researchers to gain more from their data mining efforts: maximally extracting information and achieving unbiased predictions, despite missing data. n/a",Software to Handle Missing Values in Large Data,6690119,R43RR017862,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' human data', ' mathematical model', ' statistics /biometry']",NCRR,INSIGHTFUL CORPORATION,R43,2003,99847,-0.012058940313585675
"Osteoporosis in Hypercalciuric Kidney Stone Formers The aim of our research is to determine the risk of bone disease in hypercalciuric kidney stone formers and their families by establishing an algorithm that takes into account bone mineral density (BMD), demographic characteristics diet history, and urine and serum chemistries, including markers of bone turnover. The algorithm will allow us to determine which hypercalciurics need clinical evaluation for osteoporosis and will form the basis of a disease management product. In the process of creating this algorithm, we will be the first to determine the prevalence of bone disease in hypercalciuric stone formers and their families and over time make some estimates of incidence rates. A longer- term goal of our research is to study the genetic underpinnings of osteoporosis and nephrolithasis in these families. PROPOSED COMMERCIAL APPLICATIONS: Litholink provides disease management services for kidney stone patients. We hope to use the algorithm developed in this research to expand out services to include bone disease management in kidney stone forming patients and their families. n/a",Osteoporosis in Hypercalciuric Kidney Stone Formers,6622487,R44DK059086,"['artificial intelligence', ' bone density', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' disease /disorder proneness /risk', ' human population study', ' human subject', ' hypercalciuria', ' mathematical model', ' nephrocalcinosis', ' nephrolithiasis', ' osteoporosis', ' photon absorptiometry']",NIDDK,LITHOLINK CORPORATION,R44,2003,357180,-0.010917358361116469
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6663283,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2003,395000,-0.024475687035762823
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6635877,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2003,161792,-0.020691277865846364
"Epileptic Seizures in the Neonatal EEG   DESCRIPTION: (provided by applicant) The identification of electrographic            seizures during long-term EEG monitoring in the neonate is currently based upon      visual interpretation of the graphic record, a process that is very                  time-consuming. While significant progress has been made in the automated            detection of seizures in the adult population, relatively little work has been       done in the neonatal area. Therefore, the major objective of this project is         the development of techniques for the reliable automated detection of                electrographic seizures in the neonatal EEG. We propose a multi-stage, hybrid        approach to detection that will employ a combination of signal processing,           pattern recognition, neural networks, and expert rules. Through the successive       stages of the detection process, multichannel neonatal EEG data containing all       types of background activity and artifacts will be analyzed to detect and            classify electrographic seizures. We postulate that the varied types and             morphologies of seizures in the neonatal EEG, as compared to seizures in the         adult EEG, can best be detected and classified using this hybrid approach. We        will test the methods we develop on data recorded from infants in the Clinical       Research Center for Neonatal Seizures, The Methodist Hospital, in Houston,           Texas. We expect that the information we gain from the research will lead to         the development of a practical seizure detection system and further our              long-term goals of reduced expense in the reading and interpretation of              neonatal EEGs, and also facilitate the efficient collection of seizure               parameters for possible use in future research studies.                                                                                                                   n/a",Epileptic Seizures in the Neonatal EEG,6642804,R01NS040577,"['adult human (21+)', ' age difference', ' artificial intelligence', ' bioengineering /biomedical engineering', ' biomedical equipment development', ' brain electrical activity', ' computer assisted diagnosis', ' electroencephalography', ' epilepsy', ' human subject', ' newborn human (0-6 weeks)', ' patient monitoring device', ' patient oriented research']",NINDS,UNIVERSITY OF HOUSTON,R01,2003,193637,-0.011828496275888127
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6626641,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2003,482862,-0.03933845248238237
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6702676,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,205127,-0.01900082420555531
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6628097,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,297104,-0.01900082420555531
"Toxicological Evaluation Neuralnet Tools (TENT)  DESCRIPTION (provided by applicant):  YAHSGS' Toxicological Evaluation Neuralnet Tools (TEND is designed to advance the state-of-the-art in the prediction of toxicological end points for new or untested chemicals, drugs, and compounds. TENT deploys computational neural nets (CNN), innovative computational chemistry methods, and modem statistical regression methods into interactive modules that determine (a) a chemical's 3-D structure and physical chemistry properties, (b) Quantitative Structure Activity Relationships, (C) mechanistic modes leading to toxicological responses via microassay database analysis, and (d) a broad spectrum of toxicological properties via CNN 3-D structural similarity analyses. TENTs output includes physical chemistry properties, 3-D structure, predicted toxicological impacts, and confidence level associated with each. It is anticipated that TENT will become one of the primary tools used by (a) researchers in human health and toxicological fields, (b) pharmaceutical companies to screen out drugs early in the development process prior to expending hundreds of millions on clinical in vivo and in vitro testing, (C) by companies developing new chemicals, chemical compounds, and chemically treated materials to determine potential toxicological impacts including those caused by environmental changes during and after usage, (d) companies striving to show compliance with ISO 14000 for materials used in their products, and (e) federal and military organizations for chemicals and materials contemplated for use in their mission areas. Industry experts predict that the market for TENT-type tools and applications will reach $8 -$10 billion by 2006 and three times that amount by 2016. The benefits that the US should receive from TENT could include (a) a greatly enhanced understanding of potential toxicological impacts from pharmaceuticals, chemicals, and chemically treated materials (4 out of 5 chemicals in industrial use currently have not undergone adequate testing due to time and expense), (b) companies will avoid billions of dollars in clinical testing for chemicals and drugs that ultimately fail (the funds saved can be applied to the development of new and better materials that help mankind and the environment that might otherwise go unfunded), and (c) TENT can substantially reduce the number of laboratory animals used for clinical testing.   n/a",Toxicological Evaluation Neuralnet Tools (TENT),6550075,R43ES011918,"['alternatives to animals in research', ' chemical structure function', ' computational neuroscience', ' computer program /software', ' computer simulation', ' method development', ' microarray technology', ' molecular dynamics', ' neurotoxicology', ' statistics /biometry', ' three dimensional imaging /topography', ' toxicant screening']",NIEHS,"YAHSGS, LLC",R43,2003,84450,-0.018190636486442015
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,-0.01758793159882331
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6640921,U01DE013331,"['biomarker', ' biopsy', ' clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' enzyme linked immunosorbent assay', ' facial muscles', ' gas chromatography mass spectrometry', ' human subject', ' inflammation', ' interview', ' magnetic resonance imaging', ' mastication', ' musculoskeletal disorder diagnosis', ' oral facial pain', ' pain threshold', ' psychobiology', ' questionnaires', ' radioimmunoassay', ' sign /symptom', ' statistics /biometry', ' synovial fluid', ' temporomandibular joint syndrome', ' tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2003,1388868,-0.004056098793072028
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6707987,U01DE013331,"['biomarker', ' biopsy', ' clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' enzyme linked immunosorbent assay', ' facial muscles', ' gas chromatography mass spectrometry', ' human subject', ' inflammation', ' interview', ' magnetic resonance imaging', ' mastication', ' musculoskeletal disorder diagnosis', ' oral facial pain', ' pain threshold', ' psychobiology', ' questionnaires', ' radioimmunoassay', ' sign /symptom', ' statistics /biometry', ' synovial fluid', ' temporomandibular joint syndrome', ' tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2003,376755,-0.004056098793072028
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,-0.01820041908230733
FOUNDATION MODEL OF ANATOMY No abstract available n/a,FOUNDATION MODEL OF ANATOMY,6538208,R01LM006822,"['anatomy', ' artificial intelligence', ' biological models', ' computer simulation', ' human data', ' information systems', ' model design /development', ' physical model']",NLM,UNIVERSITY OF WASHINGTON,R01,2002,465957,0.009685311505204696
FOUNDATION MODEL OF ANATOMY No abstract available n/a,FOUNDATION MODEL OF ANATOMY,6555795,R01LM006822,"['anatomy', ' artificial intelligence', ' biological models', ' computer simulation', ' human data', ' information systems', ' model design /development', ' physical model']",NLM,UNIVERSITY OF WASHINGTON,R01,2002,20004,0.009685311505204696
FOUNDATION MODEL OF ANATOMY No abstract available n/a,FOUNDATION MODEL OF ANATOMY,6658862,R01LM006822,"['anatomy', ' artificial intelligence', ' biological models', ' computer simulation', ' human data', ' information systems', ' model design /development', ' physical model']",NLM,UNIVERSITY OF WASHINGTON,R01,2002,30000,0.009685311505204696
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6605420,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2002,174567,0.00943543213991779
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6513068,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2002,21871,0.00943543213991779
VOICE RESPONSE/INTERNET REGISTRATION & RANDOMIZATION No abstract available n/a,VOICE RESPONSE/INTERNET REGISTRATION & RANDOMIZATION,6540631,R44RR014168,"['Internet', ' artificial intelligence', ' clinical trials', ' computer program /software', ' computer system design /evaluation', ' interactive multimedia', ' patient /disease registry', ' statistics /biometry', ' telecommunications']",NCRR,"RHO FEDERAL SYSTEMS DIVISION, INC.",R44,2002,366772,0.0005641912490656281
"Cluster Comparison Methods & the NCI Expression Dataset There is a significant commercial and academic need for new tools that provide quantitative cluster comparison metrics. It is important for pharmaceutical and biotechnology companies to be able to critically evaluate the utility of using different clustering techniques on large high dimensional datasets, in order to make the most informed decisions based upon the clustering results. We propose to evaluate and build bluster comparison metrics, integrating them with high dimensional visualization techniques, so that not only an overall scope, but the cluster distributions can be compared in an intuitive visual fashion. In carrying out our analysis, we will focus on the NCI (approximately 1,400) compound, subset, 118 known mechanism of action compound gene expression dataset analyzed by Scherf, et.al (2000). IN A FOLLOW ON Phase II SBIR Proposal, we will create a robust software package for commercial release where cluster comparison metrics are integrated with the most valuable visualization tools we identify in the Phase I research. PROPOSED COMMERCIAL APPLICATIONS: The Specific Aims of this Phase I proposal will allow us to create new tools where cluster comparison metrics are integrated with high dimensional visualization techniques, so that not only an overall score, but the cluster distributions can be compared in an intuitive visual fashion. We will use the publicly available NCI DIS compound subset, gene expression dataset of Scherf, e.g. al. (2000) to carry out these aims, as ell as data mine this dataset for new discoveries. n/a",Cluster Comparison Methods & the NCI Expression Dataset,6484325,R43CA096179,"['artificial intelligence', ' cancer information system', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' informatics', ' information retrieval', ' mathematics']",NCI,"ANVIL INFORMATICS, INC.",R43,2002,98438,-0.03846142492633207
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6538226,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2002,66954,-0.02521351734727946
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6490198,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2002,331492,-0.03894187783875919
"Automated PCR Pathogen Detection and Quantification  DESCRIPTION (provided by applicant):  We will develop software for automated pathogen detection and quantification using data from PCR experiments. Automated pathogen detection using data from a PCR experiment requires software to determine whether DNA from the pathogen is present or absent in a sample. We will develop a pattern-matching algorithm to mathematically analyze PCR amplification data. We will optimize the algorithm against a data set of at least 5000 PCR reactions (including a significant set of data gathered during the anthrax attack) to determine its efficacy and limitations. We expect the pathogen detection algorithms to distinguish positives samples from negative samples in more than 98% of the samples, to find inconclusive results in less than 1% of the samples, and to incorrectly classify less than 1% of the samples. We will also develop software to perform automated melting curve analysis of samples that our detection algorithm has determined to be positive or inconclusive. The melting profile of the probes is a property of the assay, and it can be used for secondary confirmation of a pathogen by comparing the profile of the unknown samples to the profile of the assay's positive controls. We will develop algorithms to automatically determine whether the melting profile of the sample and controls match. With melting analysis confirmation, the failure rate of the final detection algorithm should be less than 0.5%.   Automated pathogen quantification requires software to determine the number of copies of a pathogen's DNA in a sample. We will develop discrete dynamical models of PCR for quantification. We will optimize these methods against a large data set of PCR reactions with dilution series. We will systematically determine the features of the models that provide information and the features that can be ignored. We will measure efficacy by comparing computed DNA copy numbers against the known concentrations (as specified by experimenters), and against each other. We will use the most effective model (or models) in the software we produce.   n/a",Automated PCR Pathogen Detection and Quantification,6555484,R43AI052944,"['artificial intelligence', ' bioterrorism /chemical warfare', ' communicable disease diagnosis', ' computer program /software', ' computer system design /evaluation', ' microorganism', ' nucleic acid denaturation', ' nucleic acid quantitation /detection', ' phase change', ' polymerase chain reaction']",NIAID,IDAHO TECHNOLOGY,R43,2002,100000,-0.00634442542491298
"Pharmacogenetic Prediction of Paroxetine Response  DESCRIPTION (provided by applicant):  This Small Business Innovative Research Phase I project proposes the development of a computational model, called GeneRx, to incorporate pharmacogenetics and nonlinear adaptive algorithms toward optimizing anti-schizophrenic therapy on a patient specific basis.   Preliminary studies on the anti-schizophrenic drug olanzapine show a 40% patient-by-patient error between predicted starting dose and optimal therapeutic dose, using a prototype trained only with patient chart information. This is a significant reduction from the range of starting doses for olanzapine currently used, which is from 1 to 80 mgs/day. Anti-schizophrenic drugs likewise have a large window of therapeutic options, including significant variation in dosages, medications, and combinations of therapies used. Using patient-specific genetic information in conjunction with patient medical chart information, obtained from schizophrenia studies performed by outside sources, we propose to develop computational models to predict efficacy of treatment (i.e. Response/Non-response) to the neuroleptic risperidone. Genetic data for each patient will be acquired by genotyping DNA from the blood samples, scored as single nucleotide polymorphisms (SNPs) present or absent in key schizophrenia-related genes. GeneRx will take a patient's individual genetic, demographic, and environmental variables and predict if the patient will respond initial risperidone drug therapy. Response will be measured by change in standard severity test scores. A more efficient method to prescribe effective anti-schizophrenic pharmaceuticals would expedite recovery, minimize side effects, and reduce medical costs.   n/a",Pharmacogenetic Prediction of Paroxetine Response,6550614,R43MH066671,"['antipsychotic agents', ' artificial intelligence', ' behavioral genetics', ' computer system design /evaluation', ' depression', ' genetic polymorphism', ' genotype', ' human genetic material tag', ' human subject', ' mathematical model', ' model design /development', ' paroxetine', ' pharmacogenetics', ' serotonin inhibitor']",NIMH,"PREDICTION SCIENCES, LLC",R43,2002,100000,-0.03345530316836374
"Osteoporosis in Hypercalciuric Kidney Stone Formers The aim of our research is to determine the risk of bone disease in hypercalciuric kidney stone formers and their families by establishing an algorithm that takes into account bone mineral density (BMD), demographic characteristics diet history, and urine and serum chemistries, including markers of bone turnover. The algorithm will allow us to determine which hypercalciurics need clinical evaluation for osteoporosis and will form the basis of a disease management product. In the process of creating this algorithm, we will be the first to determine the prevalence of bone disease in hypercalciuric stone formers and their families and over time make some estimates of incidence rates. A longer- term goal of our research is to study the genetic underpinnings of osteoporosis and nephrolithasis in these families. PROPOSED COMMERCIAL APPLICATIONS: Litholink provides disease management services for kidney stone patients. We hope to use the algorithm developed in this research to expand out services to include bone disease management in kidney stone forming patients and their families. n/a",Osteoporosis in Hypercalciuric Kidney Stone Formers,6447276,R44DK059086,"['artificial intelligence', ' bone density', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' disease /disorder proneness /risk', ' human population study', ' human subject', ' hypercalciuria', ' mathematical model', ' nephrocalcinosis', ' nephrolithiasis', ' osteoporosis', ' photon absorptiometry']",NIDDK,LITHOLINK CORPORATION,R44,2002,392623,-0.010917358361116469
"Advaced Cross-Correlator Development of a general purpose ultrasound cross-correlator module that is proposed for a) blood flow estimation in one, two and three dimensions b) blood flow estimation in an overlapped mode for use in high frequency small vessels c) coded excitation deconvolution d) A-Mode tissue characteristic correlation quantification The module would be capable of accepting Digitized RF data at rates up to 4o million 12 bit samples per second from a beamformer and returning the Sum of the Products (SOP) of multiple selectable ranges of up to 48 samples with a theoretical accuracy of 1/128th of a sample in the range dimension and a dynamic range of 36 bits in the intensity dimension at the rate of the input data. Multiple results based upon the SOP would also be output. The chosen algorithms would be loaded through a Firewire interface to a personal computer (PC) in a sub second rates. The correlator modules would output its results again through the Firewire interface into the PC for further image optimization and viewing. Initially the module would be tested with the company's Beamformer but efforts would be made to offer a universal interface so researchers could utilize the computing power of the correlator on other instruments. It is also intended to make available the parameters of the algorithms for researchers to use this tool for further developments. PROPOSED COMMERCIAL APPLICATIONS: This proposed tool would be applicable in the research then clinical evaluation of true three-dimensional real time blood flow in the major vessels of the body down to the capillary vessels and in tissue flow as in angiogenesis. The correlator potentially will be used in improving he dynamic range, and quality of ultrasonic imaging. A possible application to be investigated is the correlators potential for recognizing tissue characteristics in real time. n/a",Advaced Cross-Correlator,6479214,R43CA096018,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer human interaction', ' computer network', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' digital imaging', ' phantom model', ' radiowave radiation', ' ultrasound blood flow measurement']",NCI,WINPROBE CORPORATION,R43,2002,127797,-0.034114029105126585
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6519073,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2002,161792,-0.020691277865846364
"Epileptic Seizures in the Neonatal EEG   DESCRIPTION: (provided by applicant) The identification of electrographic            seizures during long-term EEG monitoring in the neonate is currently based upon      visual interpretation of the graphic record, a process that is very                  time-consuming. While significant progress has been made in the automated            detection of seizures in the adult population, relatively little work has been       done in the neonatal area. Therefore, the major objective of this project is         the development of techniques for the reliable automated detection of                electrographic seizures in the neonatal EEG. We propose a multi-stage, hybrid        approach to detection that will employ a combination of signal processing,           pattern recognition, neural networks, and expert rules. Through the successive       stages of the detection process, multichannel neonatal EEG data containing all       types of background activity and artifacts will be analyzed to detect and            classify electrographic seizures. We postulate that the varied types and             morphologies of seizures in the neonatal EEG, as compared to seizures in the         adult EEG, can best be detected and classified using this hybrid approach. We        will test the methods we develop on data recorded from infants in the Clinical       Research Center for Neonatal Seizures, The Methodist Hospital, in Houston,           Texas. We expect that the information we gain from the research will lead to         the development of a practical seizure detection system and further our              long-term goals of reduced expense in the reading and interpretation of              neonatal EEGs, and also facilitate the efficient collection of seizure               parameters for possible use in future research studies.                                                                                                                   n/a",Epileptic Seizures in the Neonatal EEG,6529026,R01NS040577,"['adult human (21+)', ' age difference', ' artificial intelligence', ' bioengineering /biomedical engineering', ' biomedical equipment development', ' brain electrical activity', ' computer assisted diagnosis', ' electroencephalography', ' epilepsy', ' human subject', ' newborn human (0-6 weeks)', ' patient monitoring device', ' patient oriented research']",NINDS,UNIVERSITY OF HOUSTON,R01,2002,193637,-0.011828496275888127
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6489213,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2002,474210,-0.03933845248238237
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6554738,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' method development', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2002,395000,-0.024475687035762823
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6497411,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,297104,-0.01900082420555531
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6558149,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,10000,-0.01900082420555531
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6520234,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2002,163400,0.008310586524729496
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,-0.01758793159882331
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6516567,U01DE013331,"['biomarker', ' biopsy', ' clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' enzyme linked immunosorbent assay', ' facial muscles', ' gas chromatography mass spectrometry', ' human subject', ' inflammation', ' interview', ' magnetic resonance imaging', ' mastication', ' musculoskeletal disorder diagnosis', ' oral facial pain', ' pain threshold', ' psychobiology', ' questionnaires', ' radioimmunoassay', ' sign /symptom', ' statistics /biometry', ' synovial fluid', ' temporomandibular joint syndrome', ' tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2002,1325497,-0.004056098793072028
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,-0.01820041908230733
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6525584,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2002,128860,-0.011307616364316049
ARTIFICIAL INTELLIGENCE METHODS FOR CRYSTALIZATION No abstract available n/a,ARTIFICIAL INTELLIGENCE METHODS FOR CRYSTALIZATION,6394754,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' chemical structure', ' computer system design /evaluation', ' crystallization', ' method development']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,232402,0.00586649820072831
"Development of Ultrasonic Apparatus for Dental Diagnosis   DESCRIPTION: Ultrasonic diagnostic apparatus has been proposed (Phases 1 and 2)      for Dental applications in determining tooth pathologies such as                     demineralization, caries, fractures, abscesses, and tooth wear. The equipment        adopts piezoelectric and optic hybrid transduction system for interrogation on       teeth. Ultrasonic responses of the tooth structure will then be analyzed by a        pattern recognition expert system (artificial intelligence) to determine the         diagnosis of the tooth inspected. The proposed research will eventually help to      reduce the use of harmful X-ray radiation in Dental clinic and also contribute       to artificial intelligence based diagnosis. In the Phase 1 research, tooth           specimens will be collected from local Dental clinics; demonstration                 instrumentation will be constructed; ultrasonic testing will be conducted on         the tooth specimens in vitro; and finally, the test data will be analyzed to         show the potential for Dental pathology identification. The feasibility of the       proposed research concept will be demonstrated, if: 1) meaningful ultrasonic         tests can be conducted using the simple piezo-/opto-ultrasonic system on the         tooth specimens collected; 2) various Dental pathologies in the tooth specimens      may be characterized by using wave pattern of the ultrasonic responses; and 3)       by identifying particular features of an ultrasonic wave pattern, the actual         tooth pathology may be recognized.                                                   PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                          n/a",Development of Ultrasonic Apparatus for Dental Diagnosis,6402448,R43DE014270,"['artificial intelligence', ' biomedical equipment development', ' dental disorder diagnosis', ' dental structure', ' dentistry', ' diagnosis design /evaluation', ' tooth', ' tooth surface']",NIDCR,AAC INTERNATIONAL,R43,2001,100000,-0.006265783186708154
FOUNDATION MODEL OF ANATOMY No abstract available n/a,FOUNDATION MODEL OF ANATOMY,6391279,R01LM006822,"['anatomy', ' artificial intelligence', ' biological models', ' computer simulation', ' human data', ' information systems', ' model design /development', ' physical model']",NLM,UNIVERSITY OF WASHINGTON,R01,2001,430609,0.009685311505204696
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,-0.004452537937012448
VOICE RESPONSE/INTERNET REGISTRATION & RANDOMIZATION No abstract available n/a,VOICE RESPONSE/INTERNET REGISTRATION & RANDOMIZATION,6310317,R44RR014168,"['Internet', ' artificial intelligence', ' clinical trials', ' computer program /software', ' computer system design /evaluation', ' interactive multimedia', ' patient /disease registry', ' statistics /biometry', ' telecommunications']",NCRR,"RHO FEDERAL SYSTEMS DIVISION, INC.",R44,2001,382311,0.0005641912490656281
"Simulation Algorithms for Spatial Pattern Recognition   DESCRIPTION (provided by applicant): A new generation of satellites is imaging       the earth's surface with unprecedented spatial and spectral resolution. With         the ability to identify local features related to environmental exposures, this      high-resolution imagery is gong to revolutionize health risk assessment. The         realization of this potential depends critically on our ability to recognize         spatial patterns on these large images. This project will develop fast spatial       null models for use in statistical pattern recognition, and will accomplish 4        aims.                                                                                                                                                                     (1) Implement fast simulation algorithms conditioned on properties of the data,      and on spatial functions;                                                            (2) Assess project feasibility by evaluating the performance of these                algorithms on existing high-resolution, hyperspectral imagery;                       (3) Implement the simulation algorithms in 2 commercial spatial analysis             software packages;                                                                   (4) Apply the software and methods to demonstrate the approach and unique            benefits for risk assessment.                                                                                                                                             The phase 1 research will address the first two aims; aims three and four will       be accomplished in phase 2 once feasibility is demonstrated. The technologic         and scientific innovations from this project are expected to greatly enhance         our ability to extract knowledge from high resolution imagery.                       PROPOSED COMMERCIAL APPLICATION:  The imminent launch of over a dozen satellites capable of high-resolution imagery is giving  health researchers powerful new data for relating environmental features to health   outcomes, but existing software packages cannot undertake spatial analysis of these  extraordinarly large data sets.   The fast simulation algorithms from this research will  be incorporated into 2 commercial software packages, providing advanced spatial  analysis for large imagery.                                                                                     n/a",Simulation Algorithms for Spatial Pattern Recognition,6401389,R43CA092807,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' image processing', ' imaging /visualization /scanning', ' statistics /biometry']",NCI,BIOMEDWARE,R43,2001,170490,-0.011995934884577229
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6343026,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2001,376147,-0.03894187783875919
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6340157,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2001,68753,-0.02521351734727946
"OSTEOPOROSIS IN HYPERCALCURIC KIDNEY STONE FORMERS   DESCRIPTION (Adapted from the Applicant's Abstract): The aim of our research is      to determine the risk of bone disease in hypercalcuric kidney stone formers and      their families by establishing an algorithm that takes into account bone             mineral density (BMD), demographic characteristics, diet history, and urine          chemistries, including markers of bone turnover. This algorithm will allow us        to determine which hypercalcurics need clinical evaluation for osteoporosis and      will form the basis of a disease management product. In the process of creating      this algorithm, we will be the first to determine the prevalence of bone             disease in hypercalcuric stone formers and their families and over time make         some estimates of incidence rates. A longer-term goal of our research is to          study the genetic underpinnings of osteoporosis and nephrolithiasis.                 PROPOSED COMMERCIAL APPLICATION:  Litholink provides disease managment services for kidney stone patients.  We hope to   use the algorithm developed in this research to expand our services to include bone  disease management in kidney stone forming patients and their families.                                                                                     n/a",OSTEOPOROSIS IN HYPERCALCURIC KIDNEY STONE FORMERS,6298905,R43DK059086,"['artificial intelligence', ' bone density', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' disease /disorder proneness /risk', ' human population study', ' human subject', ' hypercalciuria', ' nephrocalcinosis', ' nephrolithiasis', ' osteoporosis', ' photon absorptiometry']",NIDDK,LITHOLINK CORPORATION,R43,2001,100000,-0.01071125443167147
"Neural Network System for Detection of EEG Microsleeps   DESCRIPTION (Verbatim from the Applicant's Abstract): A software system based        on Artificial Neuro-fuzzy hybrid technology will be developed for automatic          detection of microsleep events from EEG data. The software system will be            designed for used as a model-free and rule-free classification tool that             achieves generalization power through learning from examples.   The development of the software system will require a Graphical User Interface       for data example selection, frequency-analytic preprocessing of EEG raw data,        feature extraction for microsleep characterization, design and training of           neural networks for single EEG channels, and a fuzzy system for contextual           combination of network response for multiple EEG channels to a single system         response.                                                                                                                       The training and testing of the neural networks will be based on a database of       visually scored examples of microsleep and non-microsleep events from                electrophysiological data, which will be randomly divided into training,             validation and test sets.                                                                                 The performnance of the software system will be evaluated based on the               false-positive and false-negative rate for the microsleep detection using data       examples unknown to the system. The agreement rate between the combined network      response and results from visual and conventional automatic scoring will be          used as additional evaluation parameter.        PROPOSED COMMERCIAL APPLICATION: The software system will be an attractive tool for researchers, medical and technical  personal, industrial engineers. It enables the user to quantify alertness/sleepiness  in studies on sleep disorders, shiftwork, drug effects and fatigue countermeasures.  It will help reduce time-consuming visual scoring by human experts. In addition, it  will widen our knowledge about the rapid transition events (microsleeps) between  wake and sleep and can contribute to the development of alertness monitor systems.                                                                                                                                                                                                                                                                                                      n/a",Neural Network System for Detection of EEG Microsleeps,6338195,R43NS039711,"['artificial intelligence', ' biomedical automation', ' computational neuroscience', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' electroencephalography', ' electrophysiology', ' human data', ' neural information processing', ' sleep']",NINDS,"CIRCADIAN TECHNOLOGIES, INC.",R43,2001,93457,-0.026987288797209137
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6326240,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2001,183883,0.00943543213991779
"Epileptic Seizures in the Neonatal EEG   DESCRIPTION: (provided by applicant) The identification of electrographic            seizures during long-term EEG monitoring in the neonate is currently based upon      visual interpretation of the graphic record, a process that is very                  time-consuming. While significant progress has been made in the automated            detection of seizures in the adult population, relatively little work has been       done in the neonatal area. Therefore, the major objective of this project is         the development of techniques for the reliable automated detection of                electrographic seizures in the neonatal EEG. We propose a multi-stage, hybrid        approach to detection that will employ a combination of signal processing,           pattern recognition, neural networks, and expert rules. Through the successive       stages of the detection process, multichannel neonatal EEG data containing all       types of background activity and artifacts will be analyzed to detect and            classify electrographic seizures. We postulate that the varied types and             morphologies of seizures in the neonatal EEG, as compared to seizures in the         adult EEG, can best be detected and classified using this hybrid approach. We        will test the methods we develop on data recorded from infants in the Clinical       Research Center for Neonatal Seizures, The Methodist Hospital, in Houston,           Texas. We expect that the information we gain from the research will lead to         the development of a practical seizure detection system and further our              long-term goals of reduced expense in the reading and interpretation of              neonatal EEGs, and also facilitate the efficient collection of seizure               parameters for possible use in future research studies.                                                                                                                   n/a",Epileptic Seizures in the Neonatal EEG,6383999,R01NS040577,"['adult human (21+)', ' age difference', ' artificial intelligence', ' bioengineering /biomedical engineering', ' biomedical equipment development', ' brain electrical activity', ' computer assisted diagnosis', ' electroencephalography', ' epilepsy', ' human subject', ' newborn human (0-6 weeks)', ' patient monitoring device', ' patient oriented research']",NINDS,UNIVERSITY OF HOUSTON,R01,2001,200999,-0.011828496275888127
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6385455,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2001,161792,-0.020691277865846364
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6286183,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2001,465813,-0.03933845248238237
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6333620,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,297104,-0.01900082420555531
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6487190,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,10000,-0.01900082420555531
"MULTICHANNEL EEG DATA COMPRESSION Recently, recording high-resolution Electroencephalograms (EEGs) from            a large number of electrodes has become a clear trend in both brain              research and clinical diagnosis.  However, the current EEG data                  acquisition systems store the collected data in a form that has never            changed since digital EEG emerged about 30 years ago.  As a result, the          size of the output data file increases enormously as the number of               recording channels increases, causing various problems including high            costs in data analysis, database management, archiving, and transmission         through the internet.                                                                                                                                             This proposal seeks to solve this problem through fundamental research           on data compression specifically for EEG data, but applicable to other           physiological data as well.  Our key approach is based on the                    application of advanced mathematical and signal processing technologies          to this critical problem.  We will develop and optimize a variable               sampling technique which eliminates redundant data samples using spline          interpolation and wavelet transformation.  We will also investigate              lossless data compression algorithms that possess two important                  features: 1) any part of the data within the compressed file can be read         without having to decompress the entire file, and 2) the compressed data         can be transmitted and presented in coarse or fine resolutions as                needed.  We expect that, using both variable sampling and lossless               compression, the EEG file size can be reduced by approximately 70                percent.                                                                          n/a",MULTICHANNEL EEG DATA COMPRESSION,6363936,R01NS038494,"['Internet', ' artificial intelligence', ' bioengineering /biomedical engineering', ' bioimaging /biomedical imaging', ' clinical biomedical equipment', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' digital imaging', ' electroencephalography', ' human data', ' human subject', ' informatics', ' technology /technique development']",NINDS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,177776,-0.0217943677410912
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6351629,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2001,106893,-0.011688389769928115
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6392266,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2001,232139,-0.013609278447872708
"RULE DISCOVERY IN BODY CAVITY EFFUSIONS  DESCRIPTION (adapted from the Abstract):                                             Machine learning methods are innovative tools used to find patterns in medical       data.  Laboratory data is suited to computerized Interpretation because of its       objective, quantitative nature.  Body fluid analysis is a good model for             evaluating machine learning in the laboratory.  Pathologists spend a                 substantial amount of time analyzing and classifying body fluids, or                 effusions, which are abnormal accumulations of fluid within body cavities of         human beings and animals, caused by diseases such as congestive heart failure.       Fluid classification provides clinicians with important diagnostic information       about the underlying disease process.  Automation of body fluid analysis by a        machine learning system would substantially increase the efficiency and              profitability of a medical laboratory.  In a pilot study, RIPPER (Repeated           Incremental Pruning to Produce Error Reduction), a rule discovery tool,              accurately classified effusions from animals into five standard categories,          based on the physical, chemical, and cellular characteristics of the fluid.          The purposes of this study are: 1) to determine the accuracy of RIPPER on a          larger data set, to expand and strengthen the results of the pilot; 2) to test       the accuracy of RIPPER's fluid classifications prospectively in a large              veterinary teaching hospital laboratory, 3) to determine the acceptance rate         or reason for rejection of RIPPER's classification by clinical pathologists;         and (4) to use RIPPER to discover novel rules for classifying effusions by           underlying disease process.                                                                                                                                               The results of this study will validate and test the acceptance of a machine         learning system applicable to fluid analysis in both human and veterinary            clinical laboratories.  By discovering new patterns in quantitative data that        identify the specific underlying disease, RIPPER can greatly enhance the             diagnostic value of laboratory analysis.                                                                                                                                  n/a",RULE DISCOVERY IN BODY CAVITY EFFUSIONS,6467346,F32LM000095,"['body fluids', ' classification', ' computer assisted instruction', ' programmed instruction', ' veterinary science']",NLM,UNIVERSITY OF CALIFORNIA DAVIS,F32,2001,52501,-0.019515949634823528
"ACTUARIAL STRATEGIES IN CHILD DIAGNOSTIC ASSESSMENT  The proposed project involves a preliminary investigation of                                                                                   potentially significant methodological advance in diagnostic assessment. The         current state-of-the-art in diagnostic assessment involves the use of a              structured interview. Typically, structured interviews involve a static skip         structure, i.e., some portions of the interview are administered conditional on                                 particular responses to prior questions. For example, if there is a negative         response to a question about depression and anhedonia, most structured               interviews require the clinical to skip the remaining questions about                associated symptoms (e.g., sleep disturbance, impaired concentration, etc.).         Although structured interviews represent an enormous advantage over earlier          diagnostic procedures, their inflexible structure is often incompatible with         the heterogeneity of most child and adolescent populations, and can result in        superfluous questioning about uncommon disorders and insufficient follow-up          about more common ones. Many interviews do not make exceptions for individual        characteristics. For example, 1) a 17 year old boy might need to answer ""no"" to      5 or 6 questions about separation anxiety before the interviewer may move on to      another set of questions; or 2) an underweight 16 year old girl might not be         asked important follow-up questions when replying ""no"" to the initial question       about eating disorders. One might conclude that introducing more clinician           flexibility would be the solution; however, the literature on clinical judgment      suggests that increasing clinician involvement in determination of interview         structure would likely degrade classification accuracy and introduce unwanted        sources of error and bias. To address this issue in another manner, the              principal investigator has developed a data-driven, actuarial expert system to       guide a flexible interview structure. Thus, interview structure is dynamically       responsive to individual characteristics, without introducing error associated       with qualitative clinical judgments. Pilot modeling revealed that his system         offers advantages in classification accuracy over state-of-the-art diagnostic        approaches, with the additional benefit of reducing administration time for          particular disorders. The current project is planned to generate requisite data      to develop a formal expert system and to forecast its relative accuracy and          efficiency in a child and adolescent population. It is predicted that this           system will demonstrate improvements in classification accuracy over a static        structured interview approach, with reduced administration time. If the data         are supportive, these developments have the potential to significantly advance       the manner in which future diagnostic interviews are conducted with mental           health populations.                                                                                                                                                       n/a",ACTUARIAL STRATEGIES IN CHILD DIAGNOSTIC ASSESSMENT,6392530,R03MH060134,"['adolescence (12-20)', ' anxiety', ' artificial intelligence', ' behavior test', ' behavioral /social science research tag', ' child (0-11)', ' child behavior disorders', ' child psychology', ' clinical research', ' computer assisted diagnosis', ' data collection methodology /evaluation', ' depression', ' diagnosis design /evaluation', ' human subject', ' interview', ' mathematical model', ' mental disorder diagnosis', ' model design /development', ' mood disorders', ' psychometrics', ' questionnaires']",NIMH,UNIVERSITY OF HAWAII AT MANOA,R03,2001,60756,-0.0019447441039341284
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6387141,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2001,163400,0.008310586524729496
"RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY DESCRIPTION: (adapted from the applicant?s abstract): Temporomandibular disorders (TMD) are common problems in the general population. The Research Diagnostic Criteria (RDC) for TMD were developed to establish specific operationalized examination procedures and diagnostic criteria for 8 Axis I biomedical diagnoses of TMD, as well as Axis II biobehavioral assessment procedures. Procedural reliability testing has been established for the RDC Axis I examination items, but full acceptance of the RDC requires confirmation of the temporal stability, validity, generalizability and clinical utility of the examination items, biomedical diagnoses, biobehavioral assessments, and general protocol. In order to accomplish these overall goals, a multi-center project is proposed that will first reassess at the University of Minnesota the reliability of 6 examiners (2 examiners each from 3 participating universities) to collect clinical Axis I examination data using the RDC operational definitions. In a second step, reliable blinded examiners will collect clinical examination data and formulate RDC diagnoses, which will be compared to criterion diagnoses made by an expert imaging. New clinical exam items will also be assessed. Subject to expert consensus, they will be added to yield a final exam protocol that will include all current RDC examination items as well as the new items. With this new protocol, the 6 examiners will continue examining normal and TMD subjects at their respective centers in order to test the temporal stability and diagnostic validity of the new and existing examination items, patient history items, and TMJ imaging by plain film and MRI. Discriminant analyses and decision tree analyses will be used to determine the best diagnostic algorithms for rendering RDC diagnoses. Masticatory muscle biopsies and TMJ synovial fluid will be collected from subjects at the University of Minnesota to evaluate biological markers for their role as potential mediators underlying the biomedical diagnoses. Simultaneously, Axis II biobehavioral assessment procedures will be tested for temporal reliability, compared with other accepted self-report instruments for concurrent validity, and, at two centers, evaluated for criterion validity against highly structured psychiatric interviews. The clinical and health services utility of adding Axis II biobehavioral assessments, imaging and biopsy results to Axis I clinical examination findings will be appraised. Advancement in our understanding of the prevalence, etiologies, natural progression, and treatment of TMD is dependent on having reliable and valid diagnostic criteria for these disorders. n/a",RESEARCH DIAGNOSTIC CRITERIA-- RELIABILITY AND VALIDITY,6286594,U01DE013331,"['biomarker', ' biopsy', ' clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' enzyme linked immunosorbent assay', ' facial muscles', ' gas chromatography mass spectrometry', ' human subject', ' inflammation', ' interview', ' magnetic resonance imaging', ' mastication', ' musculoskeletal disorder diagnosis', ' oral facial pain', ' pain threshold', ' psychobiology', ' questionnaires', ' radioimmunoassay', ' sign /symptom', ' statistics /biometry', ' synovial fluid', ' temporomandibular joint syndrome', ' tissue /cell culture']",NIDCR,UNIVERSITY OF MINNESOTA TWIN CITIES,U01,2001,1222618,-0.004056098793072028
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,-0.01820041908230733
"3-D SPATIAL MAPPING AND RECKONING FOR SURGICAL DEVICES   DESCRIPTION (Adapted from the applicant's abstract):  Innovative 3-D computer                                                                              vision algorithms that can enable a new generation of ""spatially aware""              instruments by providing real-time absolute positioning capability non-                                                             invasively, similar to the Global Positioning Satellite System for terrestrial       applications, are proposed.  They rely on a spatial reference map that is            constructed during the diagnostic exploration.  The position information can         be used for numerous purposes, including surgical navigation, guidance,              planning, on-line treatment monitoring, error detection, alarms and safety           shutoffs is when the tool strays from the target, change analysis, and even          surgical simulation.  This is a much better paradigm for instrument design           than (the largely unsuccessful) tracking algorithms that measure relative            displacements, and are thus prone to drift and tracking loss.  The proposed          algorithms will operate robustly and accurately at frame rates for extended          periods in poor and variable imaging conditions.  They could be incorporated         into existing clinical instruments without the need for precise calibration,         which is often not possible anyway, because the patient's anatomy (e.g., the         eye) is part of the imaging system.                                                                                                                                       The algorithms will be validated in the context of laser retinal surgery -           compelling as the only long-term proven treatment for the leading blindness-         causing conditions affecting over 20 million people in the US.  Yet, the             current success rate of this procedure is less than 50%, largely due to the          lack of spatial mapping and navigation aids in current clinical instruments.                                                                                              Beyond laser retinal surgery, the algorithms may be applied whenever:  (1)           precise locations on the retina are important (e.g., perimetry); (2) the             retinal periphery is of interest (AIDS/CMV, diabetes); (3) motion compensation       is needed; (4) a tool such as a laser or endoscope is to be monitored or             guided precisely at a chosen location; or 5) even when stable measurements of        the vasculature (retinopathy of prematurity) and retinal changes are of              interest (e.g., angiogenesis research).  Overall, the core methods are               expected to be broadly useful in a number of minimally-invasive surgical             techniques, including emerging alternatives to laser.                                                                                                                     n/a",3-D SPATIAL MAPPING AND RECKONING FOR SURGICAL DEVICES,6394720,R21RR014038,"['bioimaging /biomedical imaging', ' biomedical equipment development', ' charge coupled device camera', ' clinical research', ' computer assisted diagnosis', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' eye fundus photography', ' eye laser surgery', ' human subject', ' retina disorder', ' surgery material /equipment']",NCRR,RENSSELAER POLYTECHNIC INSTITUTE,R21,2001,97900,-0.01740811512912555
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6385653,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2001,127154,-0.011307616364316049
ARTIFICIAL INTELLIGENCE METHODS FOR CRYSTALIZATION No abstract available n/a,ARTIFICIAL INTELLIGENCE METHODS FOR CRYSTALIZATION,6188557,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' chemical structure', ' computer system design /evaluation', ' crystallization', ' method development']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,228436,0.00586649820072831
ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS No abstract available n/a,ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS,6361843,01LM003528,"['anatomy', ' artificial intelligence', ' vocabulary development for information system']",NLM,UNIVERSITY OF WASHINGTON,N01,2000,228331,-0.016664619330473574
"SELECTING AMONG MATHEMATICAL MODELS OF COGNITION DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling       of cognition, it is important to have well-justified criteria for choosing       among differing explanations (i.e., models) of observed data.  This project      investigates those criteria as well as their instantiation in five model         selection methods.                                                                                                                                                Two lines of research will be undertaken.  In the first, a thorough              investigation of model complexity will be conducted.  Comprehensive              simulations re intended to determine complexity's contribution to model fit      and to model selection.  An analytical solution will also be sought with the     hope of quantifying model complexity.                                                                                                                             The second line of work examines the utility of each of the five selection       methods in choosing among models in three topic areas in cognitive               psychology (information integration, categorization, connectionist               modeling), the end goal being to identify their merits and shortcomings.                                                                                          Findings should provide a better understanding of model selection than           currently available and serve as a useful guide for researchers comparing        the suitability of quantitative models of cognition.                                                                                                               n/a",SELECTING AMONG MATHEMATICAL MODELS OF COGNITION,6185788,R01MH057472,"['artificial intelligence', ' choice', ' cognition', ' computer simulation', ' information dissemination', ' mathematical model', ' psychometrics']",NIMH,OHIO STATE UNIVERSITY,R01,2000,77332,0.003937169775372321
"NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating             Characteristic (ROC) analysis is recognized widely as the best way of            measuring and specifying the accuracies of diagnostic procedures, because it     is able to distinguish between actual differences in discrimination              capacity, on one hand, and apparent differences that are due only to             decision-threshold effects, on the other.  Key methodological needs remain       to be satisfied before ROC analysis can address all of the practically           important situations that arise in diagnostic applications, however.  This       project employs signal detection theory and computer simulation to address       several of those needs, by:  (1) refining and continuing distribution of         software developed previously by the applicants for fitting ROC curves and       for testing the statistical significance of differences between ROC curve        estimates; (2) developing and evaluating new algorithms for ROC                  curve-Fitting and statistical testing, based on their recently-developed         ""proper"" binormal model, that should provide more meaningful results in          experimental situations that involve small samples of cases; (3)                 investigating the usefulness of a form of ROC methodology that is based on       mixture distributions in order to rduce the need for diagnostic truth in ROC     experiments; (4) investigating the effect of case-saple difficulty on the        statistical power tests for differences between ROC curves, in order to          determine the optimal difficulty of cases that shouldbe studied on rank          diagnostic systems; and (5) developing methods for training artificial           neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC           analysis and signal detection theory.                                             n/a",NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY,6181168,R01GM057622,"['artificial intelligence', ' computer assisted diagnosis', ' computer system design /evaluation', ' method development', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R01,2000,218176,-0.003311125615759711
FOUNDATION MODEL OF ANATOMY No abstract available n/a,FOUNDATION MODEL OF ANATOMY,6185236,R01LM006822,"['anatomy', ' artificial intelligence', ' biological models', ' computer simulation', ' human data', ' information systems', ' model design /development', ' physical model']",NLM,UNIVERSITY OF WASHINGTON,R01,2000,415930,0.009685311505204696
"IMPROVED COLLIMATION FOR PIXELATED RADIATION DETECTORS   DESCRIPTION: The investigators propose to test the feasibility of developing         improved collimators for use with higher performance pixelated detectors under       development for use in gamma cameras now under development, as further               described by their abstract:                                                                                                                                              ""In nuclear medicine, the collimator plays a critical role in the formation of       a projection image of the radiopharmaceutical distribution within a patient.         The current state-of-the-art of collimator design for Nuclear Medicine has           matured, under the assumption that gamma-ray detectors have an intrinsic             position dependant Gaussian response function. A fundamental rethinking of           collimator design is necessary to optimize collimation for solid state               detectors that have a fixed intrinsic rect function response. We will construct      design tools by first developing a mathematical model of collimation for             detectors with intrinsic pixels and then implement it by computer algorithms.        We will conduct experiments to measure performance and validate the simulation       tools. Using the validated simulator we will then explore novel collimator           designs and hole patterns. We will examine all proposed designs for                  sensitivity, resolution, cost and manufacture. To advance clinical                   applications, collimator design will need to keep pace with the anticipated          improvements in detector technology. Phase II brings a production prototype of       the new collimator design to laboratory and clinical testing.""  PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                                          n/a",IMPROVED COLLIMATION FOR PIXELATED RADIATION DETECTORS,6071510,R41RR013519,"['artificial intelligence', ' biomedical equipment development', ' mathematical model', ' model design /development', ' nuclear medicine', ' radiation detector', ' scintillation cameras']",NCRR,MOSAIC IMAGING TECHNOLOGY,R41,2000,137074,-0.011554945320209419
"PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  n/a",PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY,6182183,R21GM059398,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer simulation', ' computer system design /evaluation', ' electron crystallography', ' electron density', ' molecular biology information system', ' physical model', ' protein structure', ' structural biology']",NIGMS,TEXAS ENGINEERING EXPERIMENT STATION,R21,2000,101500,-0.04455371762370753
"KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA Abstractions of time-stamped clinical data are useful for planning               therapy, for monitoring therapy, and for creating high-level summaries of        time-oriented clinical databases.  Temporal abstractions also support            explanations by an intelligent patient-record system and can be used for         representation of the goals and intentions of clinical guidelines and            protocols.                                                                                                                                                        We propose to reengineer and expand the scope of the RESUME system, a            prototype computer program that implements the knowledge-based temporal-         abstraction method, a conceptual and computational framework that we have        developed for abstraction of time-stamped clinical data into clinically          meaningful interval-based concepts. RESUME has been evaluated with highly        encouraging results in several clinical areas. We will address the               practical and theoretical issues of representation, acquisition,                 maintenance, and reuse of temporal-abstraction knowledge. Our specific           aims are defined by a four-step research plan:                                                                                                                    1. We will define formally the knowledge requirements for five                   computational modules (mechanisms) we employ, thus facilitating the              acquisition, maintenance, reuse, and sharing of the required knowledge.                                                                                           2. We will enhance, expand, and redesign five computational temporal-            abstraction mechanisms:                                                          (a) Automatic formation of meaningful contexts for interpretation of             clinical data.                                                                   (b) Classification of clinical data that have equivalent time stamps into        higher-level concepts.                                                           (c) Temporal inference (e.g., the join of certain interval-based clinical        abstractions into longer ones).                                                  (d) Interpolation between temporally disjoint clinical abstractions,             including a development of a probabilistic representation and semantics.         (e) Matching of predefined and runtime temporal patterns, given time-            stamped data and conclusions.                                                                                                                                     3. We will develop a tool for automated acquisition, from expert                 physicians, of temporal-abstraction knowledge, using techniques from the         PROTEGE-II project for designing knowledge-based systems.                                                                                                         4. We will validate and evaluate our methodology and its implementation.         (a) We will assess the value of the knowledge-acquisition tool in several        experiments.                                                                     (b) We will validate the performance of the computational mechanisms in          the domain of therapy of patients who have insulin-dependent diabetes by         collaboration with expert endocrinologists.                                      (c) We will evaluate the overall framework within EON, a project in which        researchers are implementing an integrated architecture for protocol-based       care.                                                                             n/a",KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA,6185217,R29LM006245,"['abstracting', ' artificial intelligence', ' computer assisted medical decision making', ' computer program /software', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' time resolved data']",NLM,STANFORD UNIVERSITY,R29,2000,121082,-0.018680625183322714
"WAVELET-BASED AUTOMATED CHROMOSOME IDENTIFICATION Commercial automated karyotyping instruments have improved to the point where the major factor limiting throughput is the time required for operator correction of chromosome classification errors. An improvement in chromosome classification accuracy would significantly increase the value of these instruments in cytogenetics labs. The goal of this project is to develop and commercialize significantly improved chromosome measurement and classification techniques for automated karyotyping. Currently the best-performing chromosome classification approach uses Weighted Density Distribution (WDD) features [11] to quantify the banding pattern of the chromosomes. These are computed as inner products between the banding profile and a set of WDD basis functions. The particular set of 1unctions originally proposed by Granum [11,38] has come into widespread use. In Phase I we showed that better function sets exist and that our new approach can find better WDD features than the best currently used. We have an innovative wavelet-based method for generating WDD functions and a chromosome classification testbed which supports large scale classification experiments. We propose to conduct a thorough, methodical search for better performing basis functions in Phase II. Phase III will incorporate the technology into PSI's PowerGene automated karyotyping instruments. PROPOSED COMMERCIAL APPLICATIONS: When the new chromosome classification technology is qualified for routine application, it will be incorporated into PSI's Powergene products, both in new systems sold and as an upgrade to existing systems.  n/a",WAVELET-BASED AUTOMATED CHROMOSOME IDENTIFICATION,6173267,R44CA076896,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosomes', ' computer program /software', ' cytogenetics', ' density', ' genetic mapping', ' genetic techniques', ' human genetic material tag', ' human tissue', ' image processing']",NCI,"ADVANCED DIGITAL IMAGING RESEARCH, LLC",R44,2000,366807,-0.004727333686575071
"PERMUTATION TEST SOFTWARE FOR RANDOMIZED CLINICAL TRIALS The randomized clinical trial (RCT) is arguably the linchpin of the drug development process, and the results of a randomized clinical trial are almost always analyzed using some form of statistical hypothesis test. Most hypothesis tests used for analyzing clinical trials assume a population model for statistical inference, when in fact a randomization model is more consistent with the way randomized clinical trials are actually conducted. Failure to consider the randomization model when analyzing clinical trials can lead to effective drugs being declared ineffective, and ineffective drugs being declared effective. In order to analyze clinical trials in accordance with the randomization model, sophisticated software for conducting permutation tests is needed. The overall goal of this research is to develop flexible and robust software, usable by statisticians or other medical data analysts, for conducting permutation tests for single- or multi-clinic randomized clinical trials. The ongoing advances in computing technology have created a favorable climate for development of software for conducting permutation tests. This project includes a collaboration with Dr. Rosenberger of the University of Maryland, Baltimore County who is a recognized expert on randomization based inference and adaptive designs. PROPOSED COMMERCIAL APPLICATIONS: Software that can use general permutation tests to analyze clinical trial data would have clear commercial value to clinical research organizations in academia, Government, and the pharmaceutical, biotechnology, and medical device industries. Key applications are the analysis of clinical trials with unusual randomization schemes, trials with unusual patterns of treatment response, and trials where standard distributional assumptions are invalid.  n/a",PERMUTATION TEST SOFTWARE FOR RANDOMIZED CLINICAL TRIALS,6141347,R43CA086556,"['artificial intelligence', ' clinical trials', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' experimental designs', ' human data', ' mathematics', ' statistics /biometry']",NCI,"LINCOLN TECHNOLOGIES, INC.",R43,2000,98172,-0.009939297458551972
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6071498,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2000,364001,-0.03894187783875919
"ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS Predictive models that generate estimate probabilities for medical outcomes have become widely used in health services research, in health policy, and increasingly, for the assessment of health care and for real-time decision support.  Logistic regression models for medical events are central to most probabilistic predictive clinical decision aids and are fundamental to comparative analyses of medical care based on risk-adjusted events.  In such applications, inaccurate assessment of patient risk can have significant health care and health policy implications. New computer-based modeling techniques including generalized additive models, classification trees, and neural networks may potentially capture information that regression methods may miss or misrepresent.  However, these methods use very local information in model construction and may be overfit to the sample data and thus not transport well to new settings.  In years 1-3, we investigated the relative accuracy of predictions made by these modeling methods under a variety of data structures, including the presence of outliers and missing data. For many of these data structures we found that the more ""local"" procedures frequently did not generalize to new test data as well as traditional regression methods.  However, our results suggest that as sample size and data complexity increases the performance of these procedures may substantially improved. Thus, to test these findings under more general conditions, we now propose two additional years of research to 1) rigorously assess the relative predictive performance and transportability of other new innovative modeling methods and of original hybrid model construction methods; 2) systematically investigate the relative predictive performance and model transportability of modeling methods applied to large and complex data structures; and 3) explore and assess procedures for handling outliers and missing data for classification trees and neural networks. The completion of the proposed work will result in the first systematic exploration of the factors affecting the predictive performance of the major modeling methods used to predict medical outcomes, and the comparative performance of models constructed by these methods on the extremely large data sets of the type that are becoming increasing available to researchers.  n/a",ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS,6185210,R01LM005607,"['artificial intelligence', ' computational neuroscience', ' computer assisted medical decision making', ' computer simulation', ' health care facility information system', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' outcomes research', ' prognosis', ' statistics /biometry']",NLM,TUFTS MEDICAL CENTER,R01,2000,270618,-0.01617522302648312
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6131906,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2000,152928,-0.020691277865846364
INTELLIGENT CRITIQUING OF CLINICAL-GUIDELINE APPLICATION No abstract available n/a,INTELLIGENT CRITIQUING OF CLINICAL-GUIDELINE APPLICATION,6045000,R01LM006806,"['artificial intelligence', ' behavioral /social science research tag', ' clinical research', ' computer assisted medical decision making', ' computer system design /evaluation', ' experimental designs', ' health care quality', ' health services research tag', ' human data', ' medical records', ' vocabulary development for information system']",NLM,STANFORD UNIVERSITY,R01,2000,294258,0.007393096804164002
"KNOWLEDGE BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY DESCRIPTION (Adapted from Applicant's Abstract):  Knowledge-guided, fully        automated image analytic procedures will be applied, and further developed       for the extraction of diagnostic and prognostic information from                 histopathologic sections.  It is proposed to develop knowledge files for the     grading of solar lesions, for the analysis of prostatic intraepithelial          neoplastic lesions (PIN), for benign proliferative epithelial lesions of the     breast, and for kidney tumors.  Quantitative progression indices will be         derived from histometric measurements.  These may serve to identify patients     at high risk to develop infiltrating disease, to measure rate of lesion          progression, and to allow a numeric assessment of the efficacy of                chemopreventive intervention.                                                                                                                                     Knowledge files are under development for a quantitative measurement of the      vascularization around PIN lesions.                                                                                                                               For nuclei, lesions and patients, novel methodology is proposed to               characterize these entities by identification, rather than by mere               classification.  This will allow a significantly more precise                    characterization of the nuceli in a lesion and of the state of lesion            progression.  The identification methods will be integrated into the current     diagnostic decision support system, and be given capabilities to handle          missing data, contradictory evidence, atypical diagnostic clue expression.       This capability relies on automated reasoning will be developed, and the         methodology will be adapted for used in histopathologic diagnosis.                n/a",KNOWLEDGE BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6137486,R01CA053877,"['artificial intelligence', ' bioimaging /biomedical imaging', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' image processing', ' information system analysis', ' kidney neoplasms', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2000,442719,-0.03115697393916609
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6151393,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2000,103781,-0.008099147271089208
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6186179,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2000,234591,-0.013609278447872708
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,6185220,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,2000,117821,-0.014498539490112884
"RULE DISCOVERY IN BODY CAVITY EFFUSIONS  DESCRIPTION (adapted from the Abstract):                                             Machine learning methods are innovative tools used to find patterns in medical       data.  Laboratory data is suited to computerized Interpretation because of its       objective, quantitative nature.  Body fluid analysis is a good model for             evaluating machine learning in the laboratory.  Pathologists spend a                 substantial amount of time analyzing and classifying body fluids, or                 effusions, which are abnormal accumulations of fluid within body cavities of         human beings and animals, caused by diseases such as congestive heart failure.       Fluid classification provides clinicians with important diagnostic information       about the underlying disease process.  Automation of body fluid analysis by a        machine learning system would substantially increase the efficiency and              profitability of a medical laboratory.  In a pilot study, RIPPER (Repeated           Incremental Pruning to Produce Error Reduction), a rule discovery tool,              accurately classified effusions from animals into five standard categories,          based on the physical, chemical, and cellular characteristics of the fluid.          The purposes of this study are: 1) to determine the accuracy of RIPPER on a          larger data set, to expand and strengthen the results of the pilot; 2) to test       the accuracy of RIPPER's fluid classifications prospectively in a large              veterinary teaching hospital laboratory, 3) to determine the acceptance rate         or reason for rejection of RIPPER's classification by clinical pathologists;         and (4) to use RIPPER to discover novel rules for classifying effusions by           underlying disease process.                                                                                                                                               The results of this study will validate and test the acceptance of a machine         learning system applicable to fluid analysis in both human and veterinary            clinical laboratories.  By discovering new patterns in quantitative data that        identify the specific underlying disease, RIPPER can greatly enhance the             diagnostic value of laboratory analysis.                                                                                                                                  n/a",RULE DISCOVERY IN BODY CAVITY EFFUSIONS,6144004,F32LM000095,"['body fluids', ' classification', ' computer assisted instruction', ' programmed instruction', ' veterinary science']",NLM,UNIVERSITY OF CALIFORNIA DAVIS,F32,2000,52420,-0.019515949634823528
"MULTICHANNEL EEG DATA COMPRESSION Recently, recording high-resolution Electroencephalograms (EEGs) from            a large number of electrodes has become a clear trend in both brain              research and clinical diagnosis.  However, the current EEG data                  acquisition systems store the collected data in a form that has never            changed since digital EEG emerged about 30 years ago.  As a result, the          size of the output data file increases enormously as the number of               recording channels increases, causing various problems including high            costs in data analysis, database management, archiving, and transmission         through the internet.                                                                                                                                             This proposal seeks to solve this problem through fundamental research           on data compression specifically for EEG data, but applicable to other           physiological data as well.  Our key approach is based on the                    application of advanced mathematical and signal processing technologies          to this critical problem.  We will develop and optimize a variable               sampling technique which eliminates redundant data samples using spline          interpolation and wavelet transformation.  We will also investigate              lossless data compression algorithms that possess two important                  features: 1) any part of the data within the compressed file can be read         without having to decompress the entire file, and 2) the compressed data         can be transmitted and presented in coarse or fine resolutions as                needed.  We expect that, using both variable sampling and lossless               compression, the EEG file size can be reduced by approximately 70                percent.                                                                          n/a",MULTICHANNEL EEG DATA COMPRESSION,6165278,R01NS038494,"['Internet', ' artificial intelligence', ' bioengineering /biomedical engineering', ' bioimaging /biomedical imaging', ' clinical biomedical equipment', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' digital imaging', ' electroencephalography', ' human data', ' human subject', ' informatics', ' technology /technique development']",NINDS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,172553,-0.0217943677410912
"ACTUARIAL STRATEGIES IN CHILD DIAGNOSTIC ASSESSMENT  The proposed project involves a preliminary investigation of                                                                                   potentially significant methodological advance in diagnostic assessment. The         current state-of-the-art in diagnostic assessment involves the use of a              structured interview. Typically, structured interviews involve a static skip         structure, i.e., some portions of the interview are administered conditional on                                 particular responses to prior questions. For example, if there is a negative         response to a question about depression and anhedonia, most structured               interviews require the clinical to skip the remaining questions about                associated symptoms (e.g., sleep disturbance, impaired concentration, etc.).         Although structured interviews represent an enormous advantage over earlier          diagnostic procedures, their inflexible structure is often incompatible with         the heterogeneity of most child and adolescent populations, and can result in        superfluous questioning about uncommon disorders and insufficient follow-up          about more common ones. Many interviews do not make exceptions for individual        characteristics. For example, 1) a 17 year old boy might need to answer ""no"" to      5 or 6 questions about separation anxiety before the interviewer may move on to      another set of questions; or 2) an underweight 16 year old girl might not be         asked important follow-up questions when replying ""no"" to the initial question       about eating disorders. One might conclude that introducing more clinician           flexibility would be the solution; however, the literature on clinical judgment      suggests that increasing clinician involvement in determination of interview         structure would likely degrade classification accuracy and introduce unwanted        sources of error and bias. To address this issue in another manner, the              principal investigator has developed a data-driven, actuarial expert system to       guide a flexible interview structure. Thus, interview structure is dynamically       responsive to individual characteristics, without introducing error associated       with qualitative clinical judgments. Pilot modeling revealed that his system         offers advantages in classification accuracy over state-of-the-art diagnostic        approaches, with the additional benefit of reducing administration time for          particular disorders. The current project is planned to generate requisite data      to develop a formal expert system and to forecast its relative accuracy and          efficiency in a child and adolescent population. It is predicted that this           system will demonstrate improvements in classification accuracy over a static        structured interview approach, with reduced administration time. If the data         are supportive, these developments have the potential to significantly advance       the manner in which future diagnostic interviews are conducted with mental           health populations.                                                                                                                                                       n/a",ACTUARIAL STRATEGIES IN CHILD DIAGNOSTIC ASSESSMENT,6096946,R03MH060134,"['adolescence (12-20)', ' anxiety', ' artificial intelligence', ' behavior test', ' behavioral /social science research tag', ' child (0-11)', ' child behavior disorders', ' child psychology', ' clinical research', ' computer assisted diagnosis', ' data collection methodology /evaluation', ' depression', ' diagnosis design /evaluation', ' human subject', ' interview', ' mathematical model', ' mental disorder diagnosis', ' model design /development', ' mood disorders', ' psychometrics', ' questionnaires']",NIMH,UNIVERSITY OF HAWAII AT MANOA,R03,2000,63589,-0.0019447441039341284
"CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be       taken to advance our understanding of the genetic and associated co-             variates affecting the inheritance of complex diseases. To that end, this        proposal will develop improved quantitative methods to detect genetic            factors contributing to increased susceptibility to complex disorders and        implement these methods in software for distribution to the research             community.                                                                                                                                                        The methods will concentrate on the use of classification techniques             applied to allele sharing data and other risk factors which affect the           trait. Allele sharing methods for mapping genes will be extended to              include the classification methods known as latent class models, cluster         analysis, and artificial neural networks, as well as a novel use of              logistic regression Co-variates such as gender, parental diagnosis, or           other concomitant factors will be systematically studied through                 applications to both stimulated and existing data sets. An additional goal       is to determine the optimal distribution of relative pairs (e.g. siblings,       first cousins) for these methods. Of great importance to this proposal is        the development of well-documented, user-friendly software and                   documentation which will be distributed to the scientific community via          the Internet. Existing software developed by the PI will be extensively          expanded for latent class models. Existing cluster analysis software will        be modified and combined for ease of use.                                                                                                                         This proposal consists of theoretical exploration, computer simulation,          data analysis, and software development. First, solutions of theoretical         questions relating to classification techniques will be pursued; second,         adaptation of computer programs to implement the analytic methods, and           investigation into alternative research strategies will be accomplished.         The new strategies will be applied to stimulated data, and finally, to           existing data sets of pedigrees in which a complex trait has been                diagnosed. Findings from this research may contribute to the ability to          locate susceptibility loci in complex traits and to the clarification of         those etiological mechanisms responsible for susceptibility.                      n/a",CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI,6168495,R01AA012239,"['alleles', ' analytical method', ' artificial intelligence', ' biomedical resource', ' computer program /software', ' computer simulation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' family genetics', ' gene environment interaction', ' gene expression', ' genetic disorder', ' genetic disorder diagnosis', ' genetic mapping', ' genetic markers', ' genetic susceptibility', ' human data', ' mathematical model', ' model design /development', ' quantitative trait loci', ' statistics /biometry']",NIAAA,WASHINGTON UNIVERSITY,R01,2000,180260,-0.03500659486282433
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6090912,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2000,214602,0.008310586524729496
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,-0.01820041908230733
"3-D SPATIAL MAPPING AND RECKONING FOR SURGICAL DEVICES   DESCRIPTION (Adapted from the applicant's abstract):  Innovative 3-D computer                                                                              vision algorithms that can enable a new generation of ""spatially aware""              instruments by providing real-time absolute positioning capability non-                                                             invasively, similar to the Global Positioning Satellite System for terrestrial       applications, are proposed.  They rely on a spatial reference map that is            constructed during the diagnostic exploration.  The position information can         be used for numerous purposes, including surgical navigation, guidance,              planning, on-line treatment monitoring, error detection, alarms and safety           shutoffs is when the tool strays from the target, change analysis, and even          surgical simulation.  This is a much better paradigm for instrument design           than (the largely unsuccessful) tracking algorithms that measure relative            displacements, and are thus prone to drift and tracking loss.  The proposed          algorithms will operate robustly and accurately at frame rates for extended          periods in poor and variable imaging conditions.  They could be incorporated         into existing clinical instruments without the need for precise calibration,         which is often not possible anyway, because the patient's anatomy (e.g., the         eye) is part of the imaging system.                                                                                                                                       The algorithms will be validated in the context of laser retinal surgery -           compelling as the only long-term proven treatment for the leading blindness-         causing conditions affecting over 20 million people in the US.  Yet, the             current success rate of this procedure is less than 50%, largely due to the          lack of spatial mapping and navigation aids in current clinical instruments.                                                                                              Beyond laser retinal surgery, the algorithms may be applied whenever:  (1)           precise locations on the retina are important (e.g., perimetry); (2) the             retinal periphery is of interest (AIDS/CMV, diabetes); (3) motion compensation       is needed; (4) a tool such as a laser or endoscope is to be monitored or             guided precisely at a chosen location; or 5) even when stable measurements of        the vasculature (retinopathy of prematurity) and retinal changes are of              interest (e.g., angiogenesis research).  Overall, the core methods are               expected to be broadly useful in a number of minimally-invasive surgical             techniques, including emerging alternatives to laser.                                                                                                                     n/a",3-D SPATIAL MAPPING AND RECKONING FOR SURGICAL DEVICES,6085479,R21RR014038,"['bioimaging /biomedical imaging', ' biomedical equipment development', ' charge coupled device camera', ' clinical research', ' computer assisted diagnosis', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' eye fundus photography', ' eye laser surgery', ' human subject', ' retina disorder', ' surgery material /equipment']",NCRR,RENSSELAER POLYTECHNIC INSTITUTE,R21,2000,98763,-0.01740811512912555
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6180399,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2000,150497,-0.011307616364316049
"Deep learning for population genetics Project Summary The revolution in genome sequencing technologies over the past 15 years has created an explosion of population genomic data but has left in its wake a gap in our ability to make sense of data at this scale. In particular, whereas population genetics as a field has been traditionally data-limited, the massive volume of current sequencing means that previously unanswerable questions may now be within reach. To capitalize on this flood of information we need new methods and modes of analysis.  In the past 5 years the world of machine learning has been revolutionized by the rise of deep neural networks. These so-called deep learning methods offer incredible flexibility as well as astounding improvements in performance for a wide array of machine learning tasks, including computer vision, speech recognition, and natural language processing. This proposal aims to harness the great potential of deep learning for population genetic inference.  In recent years our group has made great strides in using supervised machine learning for population genomic analysis (reviewed in Schrider and Kern 2018). However, this work has focused primarily on using more traditional machine learning methods such as random forests. As we argue in this proposal, DNA sequence data are particularly well suited for modern deep learning techniques, and we demonstrate that the application of these methods can rapidly lead to state-of-the-art performance in very difficult population genetic tasks such as estimating rates of recombination. The power of these methods for handling genetic data stems in part from their ability to automatically learn to extract as much useful information as possible from an alignment of DNA sequences in order to solve the task at hand, rather than relying on one or more predefined summary statistics which are generally problem-specific and may omit information present in the raw data.  In this proposal we lay out a systematic approach for both empowering the field with these tools and understanding their shortcomings. In particular, we propose to design deep neural networks for solving population genetic problems, and incorporate successful networks into user-friendly software tools that will be shared with the community. We will also investigate a variety of methods for estimating the uncertainty of predictions produced by deep learning methods; this area is understudied in machine learning but of great importance to biological researchers who require an accurate measure of the degree of uncertainty surrounding an estimate. Finally, we will explore the impact of training data misspecification—wherein the data used to train a machine learning method differ systematically from the data to which it will be applied in practice. We will devise techniques to mitigate the impact of such misspecification in order to ensure that our tools will be robust to the complications inherent in analyzing real genomic data sets. Together, these advances have the potential to transform the methodological landscape of population genetic inference. Project Narrative Deep learning has revolutionized such disparate fields as computer vision, natural language processing, and speech recognition. In this proposal we aim to harness the great potential of deep learning for population genetic inference. We will design, implement, and apply novel deep learning methods and provide open source software for others to both use and build upon, thereby producing valuable tools for the genetics researchers at large.",Deep learning for population genetics,9976348,R01HG010774,"['Algorithms', 'Area', 'Biological', 'Biology', 'Classification', 'Code', 'Communities', 'Computer Vision Systems', 'Computer software', 'DNA Sequence', 'Data', 'Development', 'Ensure', 'Floods', 'Genetic', 'Genetic Recombination', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Image', 'Lead', 'Learning', 'Left', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Natural Language Processing', 'Natural Selections', 'Nature', 'Performance', 'Population', 'Population Explosions', 'Population Genetics', 'Process', 'Program Development', 'Publishing', 'Research Personnel', 'Sequence Alignment', 'Software Tools', 'Techniques', 'Technology', 'Training', 'Trees', 'Uncertainty', 'Ursidae Family', 'Work', 'base', 'computational chemistry', 'convolutional neural network', 'deep learning', 'deep neural network', 'design', 'empowered', 'flexibility', 'genetic information', 'genome sequencing', 'genomic data', 'infancy', 'innovation', 'learning classifier', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'network architecture', 'neural network', 'neural network architecture', 'next generation', 'novel', 'open source', 'random forest', 'recurrent neural network', 'research and development', 'speech recognition', 'statistics', 'stem', 'success', 'supervised learning', 'tool', 'tool development', 'user friendly software']",NHGRI,UNIVERSITY OF OREGON,R01,2020,529154,-0.021649897316433915
"MegaTox for analyzing and visualizing data across different screening systems Project Summary Computational toxicology aims to use rules, models and algorithms based on prior data for specific endpoints, to enable the prediction of whether a new molecule will possess similar liabilities or not. Our recent efforts have used sources like PubChem and ChEMBL to build predictive models for different toxicity-related and drug discovery endpoints. Our Phase I SBIR proposal called MegaTox will provide toxicity machine learning models developed with different algorithms for 40-50 in vitro and in vivo toxicity datasets. We propose using this technology to generate machine learning models for predicting potential compounds against either TGF- a target for countering chlorine induced lung inflammation as well as the adenosine A1 receptor to identify agonists as potential anticonvulsants. In addition, we can also compile molecules that can reactivate acetylcholinesterase which would enable the potential to discover medical countermeasures to address nerve agent and pesticide poisoning. We will access multiple machine learning approaches and validate these Bayesian or other machine learning models (including Linear Logistic Regression, AdaBoost Decision Tree, Random Forest, Support Vector Machine and deep neural networks (DNN) of varying depth) with our own in-house technology for these selected targets. We will aim for ROC values greater than 0.75 and MCC and F1 scores that are acceptable (>0.3). These models will be used to virtually screen FDA approved drugs, clinical candidates, commercially available drugs or other molecules. We will select up to 50 molecules to be tested using in vitro assays alongside controls for each target. These combined efforts should in the first instance provide commercially viable treatments which will be used to experimentally validate our computational models that can be shared with the medical countermeasures scientific community. In summary, we are proposing to build and validate models for targets based on public databases, select compounds for testing, create proprietary data and use this as a starting point for further optimization of compounds if needed. Our goal is to identify at least one promising compound for each target that we then pursue and protect our IP. We will pursue additional grant funding to take these medical countermeasures through additional in vitro and in vivo preclinical studies. Ultimately, we will license our products to larger companies for development prior to clinical trials. Project Narrative There is an urgent need to develop medical countermeasures (MCM) to address pulmonary agents, nerve agents and organophosphorus pesticides. Our approach leverages public and private data to build machine learning models for different targets involved in the physiological effects of the aforementioned agents. We then use these computational models to select new molecules to test in vitro. Our approach builds on our MegaTox approach focused on modeling toxicology targets to specifically focus on identifying compounds for TGF-β and Adenosine A1 as well as potential AChE reactivators. This computational approach will be validated using in vitro testing and offers several advantages to identify potential novel or repurposed molecules as MCM including speed and cost-effectiveness.",MegaTox for analyzing and visualizing data across different screening systems,10094026,R43ES031038,"['Acetylcholinesterase', 'Ache', 'Address', 'Adenosine', 'Adenosine A1 Receptor', 'Agonist', 'Algorithms', 'Anticonvulsants', 'Chlorine', 'Clinical Trials', 'Communities', 'Computer Models', 'Data', 'Data Set', 'Databases', 'Decision Trees', 'Development', 'FDA approved', 'Funding', 'Goals', 'Grant', 'In Vitro', 'Licensing', 'Logistic Regressions', 'Lung Inflammation', 'Machine Learning', 'Modeling', 'Pesticides', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Privatization', 'PubChem', 'Small Business Innovation Research Grant', 'Source', 'Speed', 'System', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Transforming Growth Factor alpha', 'Transforming Growth Factor beta', 'base', 'clinical candidate', 'computational toxicology', 'cost effectiveness', 'deep neural network', 'drug discovery', 'in vitro Assay', 'in vitro testing', 'in vivo', 'medical countermeasure', 'nerve agent', 'novel', 'pesticide poisoning', 'preclinical study', 'predictive modeling', 'pulmonary agents', 'random forest', 'screening', 'support vector machine', 'virtual']",NIEHS,"COLLABORATIONS PHARMACEUTICALS, INC.",R43,2020,124915,-0.03496920852580644
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10224492,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,233900,-0.01964416397762782
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10017950,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,513041,-0.01964416397762782
"Stakeholder Guidance to Anticipate and Address Ethical Challenges in Applications of Machine Learning and Artificial Intelligence in Algorithmic Medicine: a Novel Empirical Approach PROJECT ABSTRACT The potential for artificial intelligence applications, specifically machine learning, to prevent, predict, and help manage disease sparks immense hope not only for the individuals affected, but also for the overall health of populations. Particularly exciting examples of these novel computing strategies are increasingly found in the development of deep learning algorithms for medical use. Already embedded in our daily lives, algorithms have begun to impact human-decision making, from recruitment and hiring of employees to criminal sentencing. Outside of medicine, recognition of the ways algorithms may reflect, reproduce, and perpetuate bias has led to an explosion of theoretical and empirical research on the subject. There is an increasing awareness of potential algorithmic weaknesses, including some that raise concerns about fundamental issues of fairness, justice, and bias. The need to anticipate and address emerging ethical issues in algorithmic medicine is time- sensitive. As health care systems increasingly utilize algorithms for patient identification, diagnosis, and treatment direction, the consequences of algorithmic bias yield real and significant costs. Numerous stakeholders are responsible for the development, application and interpretation of algorithms in medicine, and yet there has been very little engagement of stakeholders most affected by these learning systems and tools. The overarching goal of this empirical and hypothesis driven project is to articulate the landscape of ethical concerns and the issues emerging in the context of the development, refinement, and application of machine learning in algorithmic medicine. First, we determine the distinct ethical issues and problems encountered in the development, refinement, and application of machine learning, by querying the perspectives of a diverse array of stakeholders involved—machine learning researchers, clinicians, ethicists, and patients. Using the new insights generated from the first half, we will conduct an evidence-based, information-sharing vignette survey to understand the impact of the contexts of algorithms on the ethically salient perspectives of physicians—those poised to implement such innovation in their own decision-making for the care of patients. Maximizing our established record of expertise in empirical ethics investigations, this sequence of projects leverages access to the exceptional machine learning research conducted at Stanford University, including work by NIH-funded investigators, and provides extensive, systematically collected data on ethical issues encountered and anticipated throughout the development and implementation of algorithms. Finally, the project develops and refines an evidence-informed information-sharing survey for use in better understanding how physicians react to intelligent systems. PROJECT NARRATIVE  Machine learning-driven algorithmic medicine now faces an urgent need to anticipate and address emerging ethical issues. For machine learning applications in algorithmic medicine, the failure to examine ethical issues from the perspective of stakeholders will inevitably limit the ecological validity and utility of the algorithms and threaten society's future embrace of these innovations. A hypothesis-driven, empirical study is needed to anticipate and address ethical concerns, and provide clinicians, machine learning researchers, policymakers, and the public with evidence to better enable ethical application and translation of algorithms in medicine.",Stakeholder Guidance to Anticipate and Address Ethical Challenges in Applications of Machine Learning and Artificial Intelligence in Algorithmic Medicine: a Novel Empirical Approach,10099785,R01TR003505,"['Address', 'Adoption', 'Affect', 'Agreement', 'Algorithm Design', 'Algorithms', 'Artificial Intelligence', 'Attention', 'Attitude', 'Awareness', 'Clinical', 'Clinical Investigator', 'Complex', 'Data', 'Decision Making', 'Development', 'Diagnosis', 'Dimensions', 'Disclosure', 'Disease Management', 'Effectiveness', 'Empirical Research', 'Employee', 'Ensure', 'Ethical Issues', 'Ethicists', 'Ethics', 'Evaluation', 'Expert Systems', 'Explosion', 'Face', 'Failure', 'Familiarity', 'Funding', 'Future', 'Goals', 'Health', 'Healthcare Systems', 'Human', 'Human Resources', 'Individual', 'Interview', 'Investigation', 'Judgment', 'Justice', 'Knowledge', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Perception', 'Persons', 'Physicians', 'Play', 'Randomized', 'Research', 'Research Personnel', 'Role', 'Science', 'Shapes', 'Societies', 'Structure', 'Surveys', 'System', 'Time', 'Training', 'Translations', 'Trust', 'United States National Institutes of Health', 'Universities', 'Work', 'clinical decision-making', 'clinical risk', 'cost', 'court', 'deep learning algorithm', 'evidence base', 'experience', 'improved', 'innovation', 'insight', 'meetings', 'multidisciplinary', 'novel', 'patient population', 'population health', 'precision medicine', 'prevent', 'recruit', 'response', 'tool']",NCATS,STANFORD UNIVERSITY,R01,2020,429327,-0.015360565044048495
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,-0.01676380275612213
"Improved Glaucoma Monitoring Using Artificial-Intelligence Enabled Dashboard Detecting functional and structural loss due to glaucoma is critical to making treatment decisions with the goal of preserving vision and maintaining quality of life. However, most of the approaches for glaucoma assessment through visual fields (VFs) or optical coherence tomography (OCT) measurements have several limitations that poses critical challenge to their clinical utility. Identifying glaucoma-induced changes from a sequence of VF or OCT data is challenging either if the patients is in the early stages of the disease with subtle manifested structural and functional signs or if the patients are in the later stages of the disease with significant VF variability and OCT flooring effect. A major limitation of the current glaucoma monitoring techniques is that they generate a binary outcome of whether the glaucoma is worsening or not while current high-throughput data (e.g., OCT) has more information than a binary outcome. Another major drawback of some of these approaches is that they rely on traditional paradigms for progression detection such as linear regression. However, rates of glaucomatous progression may be non-linear and rapid, particularly during the later stages of the disease. Another limitation is that ad-hoc rules are adopted to define glaucoma progression while objective criteria are required to define thresholds for progression. Finally, a major deficiency of most of these methods is that they lack advanced visualization and interpretation. We propose to address these limitations by developing artificial intelligence (AI)-enabled visualization tools for effectively monitoring the functional and structural loss in patients with glaucoma. This approach provides qualitative and quantitative means to monitor 1) global visual functional and structural worsening, 2) extent of loss in hemifields, and 3) local patterns of functional and structural loss on advanced 2-D visualization tools. To achieve these objectives, we have assembled a team of interdisciplinary experts with access to large clinically annotated glaucoma data. The central hypothesis of this proposal is that advanced interpretable machine learning applied to a complete profile of VFs in all test locations (e.g., 54 in 24-2 system) and OCT-derived measurements of retinal nerve fiber layer (RNFL) (e.g., 768 A-scans around the optic disc and 7 global sectoral regions) can objectively and automatically learn and quantify the most important features, yielding a more specific and sensitive means for monitoring of glaucoma worsening than current subjectively-specified or statistically-identified approaches. We also hypothesize that machine learning can provide interpretable models with several layers of glaucoma knowledge that may provide a promising complement to current glaucoma assessment tests. Our proposed studies may offer substantial improvements in prognosis and management of glaucoma through effective use of analysis and visualization to improve glaucoma management and making more informed treatment options. Current glaucoma assessment is hampered by several limitations including lack of visualization and interpretation, providing binary rather than more-informed results, utilizing traditional approaches for data analysis, and adopting ad-hoc assessment criteria. Glaucoma is best managed and treated if both functional and structural data is utilized and mined using advanced computational tools to generate more-informative quantitative results. We propose developing artificial intelligence (AI)-enabled visualization dashboards for qualitative and quantitative monitoring of global visual functional and structural worsening, extent of loss in hemifields, and local patterns of functional and structural loss on advanced interpretable 2-D and 3-D visualization maps.",Improved Glaucoma Monitoring Using Artificial-Intelligence Enabled Dashboard,10043768,R21EY031725,"['3-Dimensional', 'Address', 'Adopted', 'Affect', 'Algorithms', 'Artificial Intelligence', 'Axon', 'Big Data to Knowledge', 'Clinical', 'Clinical Trials', 'Communities', 'Complement', 'Computer Systems', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Eye', 'Floor', 'Glaucoma', 'Goals', 'Health', 'High Performance Computing', 'Image', 'Incidence', 'Knowledge', 'Learning', 'Linear Regressions', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Pattern', 'Principal Component Analysis', 'Quality of life', 'Reproducibility', 'Research', 'Retinal Ganglion Cells', 'Savings', 'Scanning', 'Severities', 'Specific qualifier value', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Treatment Cost', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual Fields', 'Visualization', 'Visualization software', 'base', 'computerized tools', 'dashboard', 'field study', 'glaucoma test', 'hands-on learning', 'improved', 'large datasets', 'longitudinal dataset', 'multidimensional data', 'open source', 'outcome forecast', 'preservation', 'retinal nerve fiber layer', 'structured data', 'three-dimensional visualization', 'tool', 'treatment strategy', 'unsupervised learning']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R21,2020,253379,-0.021568437561994867
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9941090,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2020,605875,-0.00379696634290627
"Deep learning to quantify glaucomatous damage on fundus photographs for teleophthalmology PROJECT SUMMARY/ABSTRACT Candidate: Atalie Carina Thompson, MD, MPH is a current glaucoma fellow and Heed fellow with a long-term career goal of becoming an independent clinician-scientist and leader in the field of glaucoma and public health. She has a long-standing interest in addressing healthcare disparities in medicine, and in improving the diagnosis of glaucoma and other ophthalmic diseases through imaging technology. While obtaining a medical degree at Stanford, she received a fellowship to complete a master’s degree in public health with additional higher-level coursework in biostatistics and epidemiology. Her immediate goal in this proposal is to refine and validate a deep learning (DL) algorithm capable of quantifying neuroretinal damage on optic disc photographs and then to apply it in a pilot teleophthalmology program. With a K23 Mentored Patient-Oriented Research Career Development Award, she will acquire additional didactic training and mentored research experience in glaucoma imaging, machine learning, biostatistics, clinical research, and the responsible conduct of research. Environment: The mentorship and expertise of the advisory committee, the extensive resources at the Duke Eye Center and Departments of Biostatistics and Biomedical Engineering, and the significant institutional commitment will provide her with the support needed to transition successfully into an independent clinician-scientist. Research: This proposal will test the hypothesis that a DL algorithm trained with SDOCT detects glaucoma on optic disc photographs with greater accuracy than human graders. In Specific Aim 1, a DL algorithm that quantifies neuroretinal damage on optic disc photographs will be refined. The main hypothesis is that the quantitative output provided by the DL algorithm will allow accurate discrimination of eyes at different stages of the disease according to standard automated perimetry, and will generate cut-offs suitable for use in a screening setting. In Specific Aim 2, the short-term repeatability and reproducibility of the DL algorithm in optic disc photographs acquired over a time period of several weeks will be determined. The hypothesis is that the test-retest variability of the predictions from the DL algorithm will be similar to the original measurements acquired by SDOCT. In Specific Aim 3, the DL algorithm will be applied to optic disc photographs obtained during a pilot screening teleophthalmology program in primary care clinics and assisted living facilities. The hypothesis is that the DL algorithm will be more accurate than human graders when a full ophthalmic examination is used as the gold standard. This work will constitute the basis of an R01 grant and will advance our understanding of the application of deep learning algorithms in glaucoma and teleophthalmology. PROJECT NARRATIVE Glaucoma is the leading cause of irreversible blindness in the world. However, since the disease can be asymptomatic until later stages, many patients with glaucoma will not know they have glaucoma until they suffer substantial and irreversible visual field loss. This study seeks to refine and validate a deep learning algorithm for early diagnosis of glaucoma on optic disc photographs and subsequently test it in a pilot teleophthalmology program.",Deep learning to quantify glaucomatous damage on fundus photographs for teleophthalmology,9868507,K23EY030897,"['Address', 'Adult', 'Advisory Committees', 'Agreement', 'Algorithms', 'Artificial Intelligence', 'Assisted Living Facilities', 'Biomedical Engineering', 'Biometry', 'Blindness', 'Clinic', 'Clinical', 'Clinical Research', 'Consumption', 'Data', 'Dependence', 'Detection', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Early Diagnosis', 'Effectiveness', 'Environment', 'Epidemiology', 'Evaluation', 'Eye', 'Eye diseases', 'Fellowship', 'Frequencies', 'Fundus', 'Fundus photography', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Human', 'Image', 'Imaging technology', 'Improve Access', 'Individual', 'Label', 'Machine Learning', 'Manuals', 'Masks', 'Master&apos', 's Degree', 'Measurement', 'Medical', 'Medicine', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Nature', 'Optic Disk', 'Optical Coherence Tomography', 'Output', 'Patients', 'Perimetry', 'Primary Health Care', 'Public Health', 'Reference Standards', 'Reproducibility', 'Research', 'Research Priority', 'Research Proposals', 'Resources', 'Scientist', 'Screening procedure', 'Sensitivity and Specificity', 'Severity of illness', 'Specialist', 'Suspect Glaucomas', 'Technology', 'Testing', 'Thick', 'Time', 'Training', 'Validation', 'Visual Fields', 'Visual impairment', 'Width', 'Work', 'algorithm training', 'career', 'carina', 'cohort', 'cost', 'cost effective', 'deep learning', 'deep learning algorithm', 'deep neural network', 'demographics', 'experience', 'eye center', 'health care disparity', 'high risk', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'learning network', 'neural network', 'novel', 'novel diagnostics', 'population based', 'programs', 'prospective', 'public health intervention', 'responsible research conduct', 'retinal nerve fiber layer', 'screening', 'tool']",NEI,DUKE UNIVERSITY,K23,2020,195131,-0.026303569589718044
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,10020995,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2020,287504,-0.038927827842897895
"Center for Machine Learning in Urology PROJECT SUMMARY We propose to establish an Exploratory Center for Interdisciplinary Research in Benign Urology at the Children’s Hospital of Philadelphia (CHOP) and the University of Pennsylvania (Penn), the central mission of which is to apply machine learning to improve the understanding of the pathophysiology, diagnosis, risk stratification, and prediction of treatment responses of benign urological disease among children and adults. The proposed CHOP/Penn Center for Machine Learning in Urology (CMLU) addresses critical structural and scientific barriers that impede the development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. Structurally, urologic research occurs in silos, with little interaction among investigators that study different diseases or different populations (e.g. pediatric and adult). Scientifically, analysis of imaging and other types complex data is limited by inter-observer variability, and incomplete utilization of available information. This proposal overcomes these barriers by applying cutting-edge approaches in machine learning to analyze CT images that are routinely obtained for evaluation of individuals with kidney stone disease. Central to the CHOP/Penn CMLU is the partnership of urologists and experts in machine learning, which will bring a new approach to generating knowledge that advances research and clinical care. In addition, the CMLU will expand the urologic research community by providing a research platform and standalone machine learning executables that could be applied to other datasets. The Center’s mission will be achieved through the following Aims, with progress assessed through systematic evaluation: Aim 1. To expand the research base investigating benign urological disease. We will establish a community with the research base, particularly with the KURe, UroEpi programs, other P20 Centers, and O’Brien Centers. We will build this community by providing mini-coaching clinics to facilitate application of machine learning to individual projects, developing an educational hub for synchronous and asynchronous engagement with the research base, and making freely available all source codes and standalone executables for all machine learning tools. Aim 2. To improve prediction of ureteral stone passage using machine learning of CT images. The CMLU has developed deep learning methods that segment and automate measurement of urinary stones and adjacent renal anatomy. In the Research Project, we will compare these methods to existing segmentation methods and the current gold standard of manual measurement. We will then extract informative features from thousands of CT scans to predict the probability of spontaneous passage of ureteral stones for children and adults evaluated in the CHOP and Penn healthcare systems. Aim 3. To foster collaboration in benign urological disease research across levels of training and centers through an Educational Enrichment Program. We will amplify interactions across institutions and engage investigators locally and nationally by providing summer research internships, and interinstitutional exchange program, and an annual research symposium. PROJECT NARRATIVE The proposed CHOP/Penn O’Brien Center for Machine Learning in Urology addresses critical structural and scientific barriers that impede development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. This application overcomes these barriers by applying cutting- edge approaches in machine learning to analyze complex imaging data for individuals with kidney stone disease.The Center’s strategic vision of using machine learning to generate knowledge that improves diagnosis, risk stratification strategies, and prediction of outcomes among children and adults will be achieved through the implementation of a Educational Enrichment Program and a Research Project.",Center for Machine Learning in Urology,10133362,P20DK127488,"['Address', 'Adult', 'Algorithms', 'Anatomy', 'Area', 'Benign', 'Characteristics', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Investigator', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Educational Status', 'Evaluation', 'Fostering', 'Functional disorder', 'Funding', 'Future', 'Gold', 'Healthcare Systems', 'Image', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Internships', 'Interobserver Variability', 'Investigation', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Lead', 'Longevity', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Mission', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pediatric Hospitals', 'Pennsylvania', 'Philadelphia', 'Population', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Probability', 'Publishing', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk stratification', 'Site', 'Source Code', 'Structure', 'Students', 'Techniques', 'United States National Institutes of Health', 'Universities', 'Urinary Calculi', 'Urologic Diseases', 'Urologist', 'Urology', 'Vision', 'Visit', 'X-Ray Computed Tomography', 'base', 'clinical care', 'complex data ', 'deep learning', 'deep neural network', 'design', 'experience', 'feature selection', 'human error', 'improved', 'interdisciplinary collaboration', 'interest', 'learning strategy', 'novel strategies', 'outcome prediction', 'peer', 'programs', 'routine imaging', 'senior faculty', 'skills', 'summer research', 'symposium', 'tool', 'urologic', 'web page']",NIDDK,CHILDREN'S HOSP OF PHILADELPHIA,P20,2020,358890,-0.02983645258765066
"A Deep Learning Model to Improve Pathologist Interpretation of Donor Kidney Biopsies ABSTRACT More people die every year from kidney disease than breast or prostate cancer. Kidney transplantation is life-saving, yet the donor organ shortage and high organ discard rate contributes to 13 deaths daily among patients awaiting transplant. The decision to use or discard a donor kidney relies heavily on microscopic quantitation of chronic damage by pathologists. The current standard of care relies on a manual process that is subject to significant human variability and inefficiency, resulting in potentially healthy kidneys being discarded and potentially damaged kidneys being transplanted inappropriately. Our team developed the first Deep Learning model to quantify percent global glomerulosclerosis in donor kidney frozen section biopsy whole slide images. We developed a cloud-based platform to apply the Deep Learning model to analyze kidney biopsy whole slide images in under 6 minutes with accuracy and precision equal to or greater than current standard of care pathologists. We have also developed a Deep Learning model to quantify interstitial fibrosis on donor kidney biopsy whole slide images. This innovative approach has the potential to transform donor kidney biopsy evaluation by improving pathologist efficiency, accuracy, and precision ultimately resulting in optimized donor organ utilization, improved patient outcomes, and diminished health care costs. The goal of this project is to establish our Deep Learning automated techniques as the standard for evaluating donor kidneys prior to transplantation. This will be achieved by assembling a team of expert pathologists and computer scientists specializing in machine learning. The proposal will evaluate the accuracy and precision of the interstitial fibrosis Deep Learning model, use the automated quantitation of key microscopic findings to develop an outcome-based chronic damage score that predicts graft outcome, and test the ability of the Deep Learning models to withstand variations encountered using different scanners and processing in different laboratories. The functionality of the Trusted Kidney software platform will be improved beyond the current usable product into a commercially viable solution for multiple laboratories. PUBLIC HEALTH RELEVANCE STATEMENT Before kidneys can be transplanted, they must be examined using a microscope to ensure the kidney is healthy enough for transplant. A limitation of microscopic examination by pathologists is the inherent human variability in quantifying the amount of scar tissue, or chronic damage, present. The result is potentially healthy organs being discarded or damaged kidneys being used inappropriately. This funding will support developing artificial intelligence tools to assist pathologists with quantifying scar tissue in donor kidneys prior to transplantation, resulting in more consistent and objective biopsy evaluations, minimizing discard of potentially healthy kidneys, and optimizing placement of kidneys for transplant.",A Deep Learning Model to Improve Pathologist Interpretation of Donor Kidney Biopsies,10138826,R42DK120253,"['Adoption', 'Americas', 'Artificial Intelligence', 'Biopsy', 'Canada', 'Cessation of life', 'Chronic', 'Cicatrix', 'Clinical', 'Computer software', 'Computers', 'Contracts', 'Data', 'Databases', 'Development', 'Ensure', 'Evaluation', 'Fast Healthcare Interoperability Resources', 'Fibrosis', 'Frozen Sections', 'Funding', 'Goals', 'Gold', 'Graft Survival', 'Health Care Costs', 'Human', 'Interobserver Variability', 'Kidney', 'Kidney Diseases', 'Kidney Transplantation', 'Knowledge', 'Laboratories', 'Letters', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Manuals', 'Measurement', 'Microscope', 'Microscopic', 'Midwestern United States', 'Modeling', 'Multivariate Analysis', 'Online Systems', 'Organ', 'Organ Donor', 'Organ Procurements', 'Outcome', 'Pathologist', 'Pathology', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Personal Satisfaction', 'Phase', 'Process', 'Reproducibility of Results', 'Research Personnel', 'Savings', 'Scanning', 'Scientist', 'Secure', 'Services', 'Slide', 'Small Business Technology Transfer Research', 'Specialist', 'Speed', 'System', 'Techniques', 'Testing', 'Tissues', 'Transplantation', 'Trichrome stain', 'Trichrome stain method', 'Trust', 'United Network for Organ Sharing', 'Universities', 'Variant', 'Washington', 'Work', 'analytical tool', 'base', 'clinical biomarkers', 'cloud based', 'cloud platform', 'commercial application', 'cost', 'deep learning', 'functional improvement', 'glomerulosclerosis', 'image processing', 'imaging biomarker', 'improved', 'innovation', 'interstitial', 'kidney biopsy', 'learning strategy', 'malignant breast neoplasm', 'pathology imaging', 'phase 1 study', 'predictive modeling', 'public health relevance', 'renal damage', 'shared database', 'standard of care', 'technological innovation', 'tool', 'whole slide imaging']",NIDDK,"NEWVENTUREIQ, LLC",R42,2020,811695,-0.01163535381620128
"Development of a joint machine learning/de novo assembly system for resolving viral quasispecies PROJECT SUMMARY Viral hepatitis from hepatitis B (HBV) establishes chronic infections in >250M people worldwide; chronicity is on the rise, and approximately one-third of the world’s population (2 billion) has serologic evidence of exposure. HBV coinfection with HCV and HIV is a hidden consequence of the substance use disorder epidemic. Viral populations have extremely high sequence diversity and rapidly evolve, which explains the vaccine failure rates and viral resistance to existing therapies and makes discovering lasting therapies extremely challenging. Next Generation Sequencing (NGS) is the method of choice to assess the intra-host virus population, termed a “quasispecies”. While a large set of short DNA sequencing reads are acquired that represent the virions in the quasispecies, computational technologies are limited in their analysis capabilities, resulting in particularly low resolution of complex HBV genomic structures. Another challenge is assembling NGS reads representing short fragment of the host genome into full strains (haplotypes) without knowledge of their true occurrence in the samples. To meet these challenges, GATACA is developing pathogen-specific bioinformatics software, GAT-ML (GATACA Assembly Tool – machine learning [ML]) to support treatment discovery and improve infection control. Its specifically designed algorithm utilizes novel ML methodologies adapted and modified for assisting genome assembly that will allow GAT-ML to reconstruct complete viral haplotypes and populations by learning the ‘language’ of the sequences. Tailored initially for HBV samples, GAT and its new ML system will be integrated for feasibility testing in this Phase I with the following Specific Aims: 1. Specific Aim 1. Build a joint learning system. Train and test natural language processing (NLP) methods on HBV genetic variation. 2. Specific Aim 2. Implement and test the machine learning methods in GAT (GAT-ML). We anticipate a working tool for characterizing HBV haplotypes, validated with multi-sourced datasets, and extensive testing and benchmarking of offline and integrated methods. The proposed project will develop and increase the capabilities of our novel computational tool, GAT, to help researchers identify the full spectrum of genetic features of a viral population—such as emergence and persistence of resistance or baseline polymorphisms regardless of their frequencies—and translate these findings to the development of new or improved antiviral drugs and other applications requiring high analytic sensitivity. GAT will particularly benefit researchers working in preclinical stages of drug development who require rapid, sensitive, and reliable results to inform decisions about which targets to advance to clinical trial testing.",Development of a joint machine learning/de novo assembly system for resolving viral quasispecies,10011686,R43AI152894,"['Adoption', 'Algorithm Design', 'Algorithms', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Chronic', 'Chronic Hepatitis', 'Classification', 'Clinical Trials', 'Complex', 'Computer software', 'DNA Structure', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Epidemic', 'Failure', 'Frequencies', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'HIV', 'HIV/HCV', 'Haplotypes', 'Healthcare', 'Hepatitis B', 'Hepatitis B Virus', 'Infection Control', 'Joints', 'Knowledge', 'Language', 'Language Development', 'Learning', 'Link', 'Liver diseases', 'Machine Learning', 'Metagenomics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Natural Language Processing', 'Outcome', 'Pattern', 'Performance', 'Phase', 'Population', 'Population Analysis', 'Privatization', 'Research Personnel', 'Resistance', 'Resolution', 'Sampling', 'Semantics', 'Serological', 'Serotyping', 'Source', 'Speed', 'Substance Use Disorder', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translating', 'Trust', 'Vaccines', 'Validation', 'Variant', 'Viral', 'Viral hepatitis', 'Virion', 'Virus', 'base', 'chronic infection', 'co-infection', 'commercialization', 'computerized tools', 'contig', 'design', 'drug development', 'improved', 'insertion/deletion mutation', 'machine learning algorithm', 'machine learning method', 'multiple data sources', 'neural network', 'next generation sequencing', 'novel', 'pathogen', 'pre-clinical', 'structural genomics', 'syntax', 'tool', 'viral resistance']",NIAID,"GATACA, LLC",R43,2020,267225,-0.021799950924550324
"Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine Project Summary  The objective of this proposal is to provide a robust course of training for Gilmer Valdes, PhD, DABR, a candidate with an excellent foundation in clinical and machine learning research, to enable him to become an independent investigator. The proposed research aims to address a tradeoff between interpretability and accuracy of modern machine learning algorithms which limits their use in clinical practice. The candidate’s central hypothesis is that the current tradeoff is not a law of nature but rather a limitation of current interpretable machine learning algorithms. Towards proving this hypothesis, the candidate, leading a multidisciplinary team, have developed unique mathematical frameworks (MediBoost and the Conditional Interpretable Super Learner) to build interpretable and accurate models. The proposed research will I) implement and extensively benchmark these frameworks and II) use the algorithms develop to solve three clinical problems where potentially suboptimal models are currently used to make clinical decisions: 1) predicting mortality in the Intensive Care Unit, 2) predicting risk of Hospital Acquired Venous Thromboembolism, 3) predicting which prostate cancer patients benefit the most from adjuvant radiotherapy. The candidate’s training and research plan, multidisciplinary by nature, takes advantage of the proximity of UC San Francisco, Stanford and UC Berkeley and proposes a training plan that cannot be easily replicated elsewhere. Recognizing the multidisciplinary nature of the work proposed, the author will be mentored and work closely with a stellar committee from three institutions and different scientific areas (Machine Learning, Biostatistics, Statistics, Hospital Medicine, Cancer Research and Quality Assurance in Medicine): Jerome H. Friedman PhD (Stanford Statistics Department), Mark Van der Laan PhD (Berkeley Biostatistics and Statistics Department), Mark Segal (UCSF Epidimiology and Biostatistics Deparments), Andrew Auerbach MD (UCSF Medicine Department), Felix Y. Feng MD (UCSF Radiation Oncology),and Timothy D. Solberg PhD (UCSF Radiation Oncology). This committee will be coordinated by Dr Solberg. The candidate also counts with a strong a multidisciplinary team of collaborators. Successful completion of the proposed research will develop the next generation of accurate and interpretable Machine Learning algorithms and solve three important clinical problems where linear models are currently used in clinical settings. This proposal has wide-ranging implications across the healthcare spectrum. The intermediate-term goal is for the candidate to acquire the knowledge, technical skills and expertise necessary to submit a successful R01 proposal. PROJECT NARRATIVE Current state of the art machine learning algorithms have a marked tradeoff between accuracy and interpretability. In medicine, where errors can have a dire consequence and knowledge representation and validation is as relevant as accuracy, the development of accurate and interpretable algorithms is of paramount importance. My research project will address a critical public health need by developing machine learning algorithms that are both accurate and interpretable, and apply them to solve specific clinical problems.",Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine,9989861,K08EB026500,"['Address', 'Adjuvant Radiotherapy', 'Algorithms', 'Area', 'Benchmarking', 'Biometry', 'Cancer Patient', 'Classification', 'Clinical', 'Collection', 'Communities', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Doctor of Philosophy', 'Foundations', 'Goals', 'Healthcare', 'Hospitals', 'Institution', 'Intensive Care Units', 'Knowledge', 'Label', 'Laws', 'Libraries', 'Limb structure', 'Linear Models', 'Machine Learning', 'Malignant neoplasm of prostate', 'Mathematics', 'Mediating', 'Medical', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Patient Triage', 'Patients', 'Performance', 'Physicians', 'Pneumonia', 'Polynomial Models', 'Public Health', 'Radiation Oncology', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'San Francisco', 'Survival Analysis', 'Technical Expertise', 'Testing', 'Training', 'Trees', 'Validation', 'Work', 'anticancer research', 'artificial neural network', 'asthmatic patient', 'classification trees', 'clinical decision-making', 'clinical practice', 'design', 'improved', 'information organization', 'machine learning algorithm', 'medical specialties', 'mortality', 'multidisciplinary', 'neural network', 'next generation', 'novel', 'quality assurance', 'random forest', 'regression trees', 'standard care', 'statistics', 'structured data', 'task analysis', 'theories', 'venous thromboembolism']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2020,182232,-0.03672838184996598
"Deep Learning Approaches for Personalized Modeling and Forecasting of Glaucomatous Changes Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at managing the optimal care for individual cases due to difficulties of accurately assessing the potential progression and its speed and magnitude. These difficulties are due to a variety of causes that change over the course of the disease, including large inter-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we propose novel agnostic data-driven deep learning approaches to detect glaucoma and accurately forecast its progression that are optimized to each individual case. We will use state- of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. Instead of relying on the conventional knowledge-based approaches (e.g. quantifying tissues known to be significantly associated with glaucoma such as retinal nerve fiber layer), the proposed cutting-edge agnostic deep learning approaches determine the features responsible for future structural and functional changes out of thousands of features autonomously by learning from the provided large longitudinal dataset. This program will advance the use of structural and functional information obtained in the clinics with a substantial impact on the clinical management of subjects with glaucoma. Furthermore, the developed methods have potentials to be applied to various clinical applications beyond glaucoma and ophthalmology. Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies using agnostic deep learning approaches that will substantially improve detection of glaucoma and its progression forecasting and monitoring in order to prevent blindness.",Deep Learning Approaches for Personalized Modeling and Forecasting of Glaucomatous Changes,9864905,R01EY030929,"['3-Dimensional', 'Area', 'Atlases', 'Blindness', 'Brain', 'Caring', 'Clinic', 'Clinic Visits', 'Clinical', 'Clinical Management', 'Collaborations', 'Color', 'Complex', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Set', 'Decision Making', 'Detection', 'Development', 'Disease', 'Disease Progression', 'Disease model', 'Early Diagnosis', 'Eye', 'Future', 'Glaucoma', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Intervention', 'Investments', 'Knowledge', 'Learning', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurable', 'Measurement', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Ophthalmology', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Performance', 'Research', 'Research Proposals', 'Retina', 'Sampling', 'Series', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thinness', 'Time', 'Tissues', 'Training', 'Vision', 'Visit', 'Visual Fields', 'analytical method', 'base', 'case-by-case basis', 'clinical application', 'clinical practice', 'cohort', 'computerized', 'cost', 'deep learning', 'falls', 'feature selection', 'follow-up', 'image processing', 'imaging modality', 'improved', 'in vivo', 'individual patient', 'innovation', 'insight', 'knowledge base', 'longitudinal analysis', 'longitudinal dataset', 'machine learning method', 'novel', 'ocular imaging', 'personalized approach', 'personalized medicine', 'personalized predictions', 'predictive modeling', 'preservation', 'prevent', 'programs', 'retinal nerve fiber layer', 'theories', 'tool', 'treatment planning', 'trend']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,400056,-0.01307315114513537
"Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement PROJECT SUMMARY/ABSTRACT Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing. Project Narrative Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing",Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement,10149058,R24MH117179,"['Address', 'Administrative Supplement', 'Archives', 'Artificial Intelligence', 'Award', 'BRAIN initiative', 'Benefits and Risks', 'Consent Forms', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Data Set', 'Ensure', 'Ethics', 'Foundations', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Subject Research', 'International', 'Investments', 'Laws', 'Legal', 'Light', 'Machine Learning', 'Magnetic Resonance Imaging', 'Neurosciences', 'Parents', 'Policies', 'Privacy', 'Process', 'Regulation', 'Research', 'Research Subjects', 'Risk', 'Security Measures', 'Series', 'Software Tools', 'Solid', 'Surveys', 'Techniques', 'United States', 'United States National Institutes of Health', 'data archive', 'data privacy', 'data sharing', 'design', 'human subject', 'human subject protection', 'machine learning algorithm', 'neuroethics', 'neuroimaging', 'novel', 'prevent', 'privacy protection', 'research study', 'sharing platform', 'sound', 'stem']",NIMH,STANFORD UNIVERSITY,R24,2020,126592,-0.0205927648232642
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,10020414,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'feature selection', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2020,388750,-0.02116943623472907
"Personalized Deep Learning Models of Rapid Changes in Major Depressive Disorder Symptoms using Passive Sensor Data from Smartphones and Wearable Devices ABSTRACT Major depressive disorder (MDD) is highly prevalent and the leading cause of global disease burden. Associated with over 1,000 different symptom profiles, MDD is highly heterogeneous. The majority of MDD symptom change occurs across hours. Consequently, there is a need to increasingly focus MDD research on personalized assessment of these rapid symptom fluctuations. To date, personalized models of MDD have shown promise, but relied solely on self-report measures. There is thus a critical need to develop personalized models of MDD that incorporate objective signals. Passively collected information from smartphones and wearable sensors can continuously and unobtrusively track behavioral and physiological signals related to core disturbances associated with MDD, including psychomotor retardation, sleep disturbances, social contact, behavioral activation, heart rate variability, and screen time. Preliminary data suggest that personalized artificial intelligence (i.e., personally weighted deep learning models) are well suited for creating novel personalized digital biomarkers of these passive indicators, and that these biomarkers can predict rapid changes in MDD symptoms. This proposal will investigate the ability to develop personalized deep learning models of rapid changes in MDD symptoms among a nationally representative sample of 120 treatment seeking adults with MDD across 90 days using passively collected data from smartphones and wearable sensors. This proposal aims to test the accuracy of personalized, subtyped, and cohort-based modeling techniques and uncover personalized digital biomarkers of moment-to-moment changes in MDD symptoms. The project proposes the following innovations: it will (1) conduct the first passive-sensing study of MDD in a nationally-representative cohort; (2) utilize deep learning models to aid in the discovery of novel maintenance factors of MDD symptom changes; and (3) use personalized multimodal assessments of MDD to address the heterogeneity in MDD. In line with the aims of the NIMH Research Domain Criteria (RDoC), this project will study MDD symptom changes across multiple units of analysis and integrate multiple systems. This study will provide a critical step towards uncovering novel personalized maintenance patterns of MDD symptom changes in daily life. Further, it will allow for scalable personalized treatments to be developed using technology to deliver behavioral interventions in the moments immediately preceding rapid MDD symptom changes. PROJECT NARRATIVE This project aims to utilize personalized artificial intelligence techniques and objective data (collected from smartphones and wearable devices) to create individualized digital biomarkers of rapid changes in major depressive disorder symptoms. This is important because, if we were to uncover personalized patterns between objectively measured physiology and behavioral changes and understand their resulting impact on rapid fluctuations in major depressive disorder symptoms, we would be able to define new, person-specific maintenance patterns that underlie the wide-ranging heterogeneity that is currently seen in patients suffering from major depressive disorder. Moreover, these advancements will provide a crucial step forward towards developing personalized, scalable, technology-based interventions that will be able to be delivered immediately (and, ideally, before rapid symptom changes) among those persons with major depressive disorder.",Personalized Deep Learning Models of Rapid Changes in Major Depressive Disorder Symptoms using Passive Sensor Data from Smartphones and Wearable Devices,10029386,R01MH123482,"['Address', 'Adult', 'Affect', 'Arousal', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological Markers', 'Cellular Phone', 'Cessation of life', 'Collection', 'Data', 'Depressed mood', 'Deterioration', 'Devices', 'Ecological momentary assessment', 'Enrollment', 'Exposure to', 'Fostering', 'Heterogeneity', 'Hour', 'Individual', 'Intervention', 'Life', 'Light', 'Location', 'Maintenance', 'Major Depressive Disorder', 'Measures', 'Modeling', 'Moods', 'Motor', 'National Institute of Mental Health', 'Negative Valence', 'Participant', 'Patient Self-Report', 'Patients', 'Pattern', 'Performance', 'Persons', 'Photoplethysmography', 'Physiological', 'Physiology', 'Population', 'Positive Valence', 'Process', 'Research', 'Research Domain Criteria', 'Sampling', 'Signal Transduction', 'Sleep', 'Sleep disturbances', 'Subgroup', 'Surveys', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Variant', 'Wrist', 'actigraphy', 'analog', 'base', 'biomarker-driven', 'burden of illness', 'cohort', 'deep learning', 'depressive symptoms', 'digital', 'disability', 'heart rate variability', 'innovation', 'learning strategy', 'meetings', 'microphone', 'multimodality', 'novel', 'personalized medicine', 'phenomenological models', 'premature', 'prevent', 'sensor', 'social', 'treatment planning', 'treatment response', 'tv watching', 'wearable device', 'wearable sensor technology']",NIMH,DARTMOUTH COLLEGE,R01,2020,250435,-0.003945442416418501
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy Project Summary/Abstract Artificial intelligence on genomic/healthcare data that is performed jointly between multiple collaborating institutions relies on a trust model but can accelerate genomic medicine research and facilitate quality improvement. To conduct such machine learning while protecting patient privacy and reducing security risks, we are developing blockchain-based privacy-preserving learning methods in a K99/R00 study supported by the National Human Genome Research Institute (NHGRI). However, our previous design of privacy-preserving learning on private blockchain assumed “semi-honesty” as the underlying adversary assumption. That is, we assume that each participating site is curious yet very careful and honest, such that it would only submit correct predictive models. Nevertheless, in real world this assumption may be too optimistic; the models submitted could be an old one due to network latency or malicious users may try to create fake models, which can in turn lead to bioethical concerns and reduce the incentives for genomic/clinical institutions to participate in the collaborative predictive modeling. Therefore, the capability to detect, assess and prevent “model misconducts” is critical to increase the integrity/reliability of machine learning. To address this issue, we consider the following 3 types of model misconducts: (1) model plagiarism, of which a site becomes a free-rider and just submits a copy of a model from the other sites, trying to hide their own information and inspect models from other sites; (2) model fabrication, of which a site mocks up a model, trying to hide information and disturb the machine learning process; and (3) model falsification, of which a site tweaks its model a bit, trying to just disturb the learning process. For each type of the model misconducts, we are interest in how to detect these misconducts of another site, how to assess the losses of machine learning results due to misconducts, and how to prevent these model misconducts. Our aims include (a) detecting model misconducts using model properties, (b) assessing model misconducts losses via model simulation, and (c) preventing model misconducts based on whole model history. The innovative components to our proposed project include (i) summarizing various types of model misconduct, (ii) developing a complete strategy to handle the model misconduct, and (iii) providing a generalizable approach to mitigate bioethical concerns for collaborative machine learning. Project Narrative Artificial intelligence performed jointly between multiple collaborating institutions can accelerate genomic medicine research and facilitate quality improvement, but relies on a trust model which may be too optimistic in real-world setting. In this project, we plan to develop a comprehensive detection, assessment and prevention mechanism to address the potential bioethical risks brought by misconducts of model plagiarism, fabrication, and falsification. The proposed study can supplement the considerations of model misconducts for our original project of privacy-preserving learning on blockchain.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10130868,R00HG009680,"['Address', 'Artificial Intelligence', 'Bioethics', 'Budgets', 'Calibration', 'Clinical', 'Data', 'Data Set', 'Detection', 'Digit structure', 'Discrimination', 'Event', 'Genomic medicine', 'Genomics', 'Healthcare', 'Incentives', 'Institution', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pattern', 'Plagiarism', 'Prevention', 'Privacy', 'Privatization', 'Process', 'Property', 'Randomized', 'Recording of previous events', 'Research', 'Risk', 'Security', 'Site', 'Sum', 'Testing', 'Time', 'Trust', 'base', 'blockchain', 'design', 'distributed ledger', 'innovation', 'interest', 'learning strategy', 'models and simulation', 'patient privacy', 'predictive modeling', 'prevent', 'privacy preservation', 'statistics']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,102049,-0.003741460009509286
"Deep-learning-based prediction of AMD and its progression with GWAS and fundus image data Age-related macular degeneration (AMD) is a leading cause of irreversible blindness worldwide. Successful genome-wide association studies (GWAS) of AMD have identified many disease-susceptibility genes. Through great efforts from international GWAS consortium and large-scale collaborative projects, massive datasets including high-quality GWAS data and well-characterized clinical phenotypes are now available in public repositories such as dbGaP and UK Biobank. Clinically, color fundus images have been extensively used by ophthalmologists to diagnose AMD and its severity level. The combination of wealthy GWAS data and fundus image data provides an unprecedented opportunity for researchers to test new hypotheses that are beyond the objectives of original projects. Among them, predictive models for AMD development and its progression based on both GWAS and fundus image data have not been explored. Most existing prediction models only focus on classic statistical approaches, often regression models with a limited number of predictors (e.g., SNPs). Moreover, most predictions only give static risks rather than dynamic risk trajectories over time, of which the latter is more informative for a progressive disease like AMD. Recent advances of machine learning techniques, particularly deep learning, have been proven to significantly improve prediction accuracy by incorporating multiple layers of hidden non-linear effects when large-scale training datasets with well-defined phenotypes are available. Despite its success in many areas, deep learning has not been fully explored in AMD and other eye diseases. Motivated by multiple large-scale studies of AMD development or progression, where GWAS and/or longitudinal fundus image data have been collected, we propose novel deep learning methods for predicting AMD status and its progression, and to identify subgroups with significant different risk profiles. Specially, in Aim 1, we will construct a novel local convolutional neural network to predict disease occurrence (AMD or not) and severity (e.g., mild AMD, intermediate AMD, late AMD) based on (1a): a large cohort of 35,000+ individuals with GWAS data and (1b): a smaller cohort of 4,000+ individuals with both GWAS and fundus image data. In Aim 2, we will develop a novel deep neural network survival model for predicting individual disease progression trajectory (e.g., time to late-AMD). In both aims, we will use the local linear approximation technique to identify important predictors that contribute to individual risk profile prediction and to identify subgroups with different risk profiles. In Aim 3, we will validate and calibrate our methods using independent cohorts and implement proposed methods into user-friendly software and easy-to-access web interface. With the very recent FDA approval for Beovu, a novel injection treatment for wet AMD (one type of late AMD) by inhibiting VEGF and thus suppressing the growth of abnormal blood vessels, it makes our study more significant, as it will provide most cutting-edge and comprehensive prediction models for AMD which have great potential to facilitate early diagnosis and tailored treatment and clinical management of the disease. PROJECT NARRATIVE The objective of this proposal is to develop new analytic methods and software tools to facilitate novel prediction of AMD development and its progression. The successful completion of the project will generate the first comprehensive set of deep-learning-based prediction models and web-based interfaces, which jointly analyzes large-scale GWAS and fundus image data and has the great potential to enhance the early diagnosis and current clinical management of AMD. The analytic approach can be applied to other eye diseases where large-scale genetics and/or image data are collected.",Deep-learning-based prediction of AMD and its progression with GWAS and fundus image data,10056062,R21EY030488,"['Achievement', 'Age related macular degeneration', 'Applications Grants', 'Area', 'Biological', 'Blindness', 'Blood Vessels', 'Categories', 'Characteristics', 'Clinical', 'Clinical Management', 'Cohort Studies', 'Collection', 'Color', 'Communities', 'Computer software', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease Progression', 'Disease susceptibility', 'Early Diagnosis', 'Elderly', 'Exposure to', 'Eye diseases', 'Genes', 'Genetic', 'Genotype', 'Growth', 'Image', 'Individual', 'Injections', 'International', 'Knowledge', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'National Eye Institute', 'Network-based', 'Online Systems', 'Ophthalmologist', 'Phenotype', 'Positioning Attribute', 'Progressive Disease', 'Research', 'Research Personnel', 'Risk', 'Sampling', 'Severities', 'Software Tools', 'Statistical Methods', 'Subgroup', 'Susceptibility Gene', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Vascular Endothelial Growth Factors', 'Work', 'analytical method', 'base', 'biobank', 'clinical phenotype', 'cohort', 'computerized tools', 'convolutional neural network', 'data warehouse', 'database of Genotypes and Phenotypes', 'deep learning', 'deep neural network', 'fundus imaging', 'genome wide association study', 'genome-wide', 'genome-wide analysis', 'graphical user interface', 'improved', 'individualized medicine', 'innovation', 'interest', 'learning strategy', 'neural network', 'novel', 'personalized predictions', 'personalized risk prediction', 'predictive modeling', 'public repository', 'secondary analysis', 'success', 'synergism', 'user friendly software', 'user-friendly', 'web based interface', 'web interface']",NEI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2020,188198,-0.04497387756514087
"Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models Project Summary Amyotrophic lateral sclerosis (ALS) and Frontotemporal Dementia FTD are devastating neurodegenerative disorders that lie on a genetic and mechanistic continuum. ALS is a disease of motor neurons that that is almost uniformly lethal within only 3-5 years of diagnosis. FTD is a heterogeneous, rapidly progressing syndrome that is among the top three causes of presenile dementia. About 10% of ALS cases are caused by dominantly transmitted gene defects. SOD1 and FUS mutations cause aggressive motor neuron pathology while TDP43 mutations cause ALS-FTD. Further, wild type FUS and TDP43 are components of abnormal inclusions in many FTD cases, suggesting a mechanistic link between these disorders. Early phenotypes are of particular interest because these could lead to targeted interventions aimed at the root cause of the disorder that could stem the currently inexorable disease progression. Elucidating such early, potentially shared characteristics of these disorders should be greatly aided by: 1) knock-in animal models expressing familial ALS-FTD genes; 2) sensitive, rigorous and objective behavioral phenotyping methods to analyze and compare models generated in different laboratories. In published work the co-PIs applied their first-generation, machine vision-based automated phenotyping method, ACBM ‘1.0’ (automated continuous behavioral monitoring) to detect and quantify the earliest-observed phenotypes in Tdp43Q331K knock-in mice. This method entails continuous video recording for 5 days to generate >14 million frames/mouse. These videos are then scored by a trained computer vision system. In addition to its sensitivity, objectivity and reproducibility, a major advantage of this method is the ability to acquire and archive video recordings and to analyze the data at sites, including the Cloud, remote from those of acquisition. We will use Google Cloud TPUs supercomputers that have been designed from the ground up to accelerate cutting-edge machine learning workloads, with a special focus on deep learning. We will analyze this data using Bayesian hierarchical spline models that describe the different mouse behaviors along the circadian rhythm. The current proposal has two main goals: 1) Use deep learning to refine and apply a Next Generation ACBM - ‘2.0’ - that will allow for more sensitive, expansive and robust automated behavioral phenotyping of four novel knock-in models along with the well characterized SOD1G93A transgenic mouse. 2) To establish and validate procedures to enable remote acquisition of video recording data with cloud-based analysis. Our vision is to establish sensitive, robust, objective, and open-source machine vision-based behavioral analysis tools that will be widely available to researchers in the field. Since all the computer-annotated video data is standardized in ACBM 2.0 and will be archived, we envision a searchable ‘behavioral database’, that can be freely mined and analyzed. Such tools are critical to accelerate the development of novel and effective therapeutics for ALS-FTD. Narrative ALS and Frontotemporal Dementia (FTD) are devastating, rapidly progressing diseases and current treatments are of limited value. In this proposal a neuroscientist and a computer scientist have teamed up to develop a new machine vision-based method for behavioral analysis novel mouse models of ALS-FTD. The ultimate goal is to reveal early phenotypes in ALS-FTD models that can be used in understanding disease pathology and in the development of new therapeutic targets.",Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models,9979408,R21NS112743,"['Amyotrophic Lateral Sclerosis', 'Animal Model', 'Archives', 'Behavior', 'Behavior monitoring', 'Behavioral', 'Characteristics', 'Circadian Rhythms', 'Computer Vision Systems', 'Computers', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Expression Profiling', 'Familial Amyotrophic Lateral Sclerosis', 'Frontotemporal Dementia', 'Gene Expression', 'Generations', 'Genes', 'Genetic', 'Goals', 'Hour', 'Human', 'Intervention', 'Knock-in', 'Knock-in Mouse', 'Laboratories', 'Lead', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Motor Neuron Disease', 'Motor Neurons', 'Mus', 'Mutation', 'Neurodegenerative Disorders', 'Paralysed', 'Pathology', 'Phenotype', 'Plant Roots', 'Presenile Dementia', 'Procedures', 'Publishing', 'Reproducibility', 'Research', 'Research Personnel', 'Respiratory Paralysis', 'Scientist', 'Site', 'Standardization', 'Syndrome', 'TensorFlow', 'Time', 'Training', 'Transgenic Mice', 'Transgenic Organisms', 'Treatment Efficacy', 'Video Recording', 'Vision', 'Work', 'Workload', 'base', 'behavioral phenotyping', 'cloud based', 'data archive', 'deep learning', 'design', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'interest', 'knockin animal', 'machine vision', 'mouse model', 'new therapeutic target', 'next generation', 'novel', 'open source', 'programs', 'protein TDP-43', 'stem', 'supercomputer', 'superoxide dismutase 1', 'tool']",NINDS,BROWN UNIVERSITY,R21,2020,446875,-0.015116408936719483
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9921505,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'data fusion', 'deep learning', 'design', 'feature extraction', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neural network classifier', 'neurophysiology', 'novel', 'post-trauma', 'predictive modeling', 'prevent', 'random forest', 'support vector machine', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,245552,-0.008323577787773952
"Lupus Nephritis Neural Network, LuNN Up to 60% of adults and 80% of children with systemic lupus erythematosus (SLE) develop nephritis (LN), with 10–30% progressing to end-stage renal disease (ESRD). The gold standard for diagnosis of LN is a renal biopsy. Histological parameters remain the best predictors of ESRD. Despite being the gold standard, histological diagnosis of LN has several shortcomings. In multiple inter-observer renal pathology assessment studies reported thus far, the inter- pathologist correlation coefficients, or concordance, in assessing most histological parameters have been sub-optimal. This has provided the impetus for the current proposal. We propose to leverage the power of computer vision and deep learning to build a classifier that rivals the best-trained renal pathologists in making a histological diagnosis of LN using current diagnostic criteria. We propose to train a deep convolutional neural network to distinguish the different LN classes, and to identify a full spectrum of histological attributes useful for diagnosis. We will compare the performance of the newly generated neural network in scoring glomerular/tubulo-interstitial features and LN classes, against a panel of human renal pathologists. Finally, we propose to build a neural network that can predict clinical outcome based on baseline renal pathology. Reliable and reproducible classification of LN could dramatically improve patient management and long-term renal and patient survival. Despite being the gold standard, histological diagnosis of lupus nephritis is imprecise, and marked by significant inter-pathologist discordance in readings. We propose to leverage the power of computer vision and deep learning to build a classifier that rivals the best-trained renal pathologists in making a histological diagnosis of lupus nephritis. Reliable and reproducible classification of LN could dramatically improve patient management and long-term renal and patient survival.","Lupus Nephritis Neural Network, LuNN",10246669,R56DK122036,"['Adult', 'Algorithms', 'Automobile Driving', 'Cellular Structures', 'Child', 'Chronic', 'Classification', 'Computer Vision Systems', 'Diagnosis', 'Diagnostic', 'End stage renal failure', 'Feedback', 'Gold', 'Histologic', 'Human', 'Image', 'Kidney', 'Lupus', 'Lupus Nephritis', 'Machine Learning', 'Mus', 'Nephritis', 'Outcome', 'Outcome Study', 'Pathologist', 'Pathology', 'Patients', 'Performance', 'Phenotype', 'Prediction of Response to Therapy', 'Reading', 'Reporting', 'Reproducibility', 'Retrieval', 'Supervision', 'Systemic Lupus Erythematosus', 'Testing', 'Tissues', 'Training', 'Uncertainty', 'accurate diagnosis', 'base', 'convolutional neural network', 'deep learning', 'diagnosis standard', 'falls', 'improved', 'indexing', 'innovation', 'kidney biopsy', 'neural network', 'novel', 'predict clinical outcome', 'time interval', 'tool', 'treatment response', 'user-friendly', 'web portal']",NIDDK,UNIVERSITY OF HOUSTON,R56,2020,100750,-0.013613259326824497
"Characterizing and targeting subphenotypes of schizophrenia and bipolar disorder via individually imputed tissue and cell-type specific transcriptomes PROJECT SUMMARY  Schizophrenia (SCZ) and bipolar disorder (BD) are highly heritable, severe and complex brain disorders characterized by substantial clinical and biological heterogeneity. Despite this, case-control studies often ignore such heterogeneity through their focus on the average patient, which may be the core reason for a lack of robust biomarkers indicative of an individual’s treatment response and outcome. Although they are classified as independent diagnostic entities, SCZ and BD are highly genetically correlated, exhibit high relative risks among relatives of both BD & SCZ patients, and have partially overlapping symptomatology and treatment. In this project we will use tissue and cell-type specific imputed transcriptomes for individuals with SCZ or BD in our VA discovery cohort comprising the Million Veteran Program (MVP) and Cooperative Studies Program 572 (CSP #572, “The Genetics of Functional Disability in Schizophrenia and Bipolar Illness”), as an intermediate molecular phenotype, to identify, characterize and target subphenotypes of these disorders. Findings from the VA discovery cohort will be validated in the PsycheMERGE and BioMe cohorts.  First, we will impute tissue and cell-type specific transcriptomes for all individuals with schizophrenia (SCZ) or bipolar disorder (BD) in the VA discovery cohort. To achieve this, we will train tissue (brain and peripheral tissues) and cell-type (glutamatergic & GABAergic neurons, astrocytes, oligodendrocytes, and microglia from DLPFC) specific EpiXcan transcriptomic imputation models at the gene and isoform level. Secondly, we will use the imputed transcriptomes as an intermediate molecular phenotype to identify genetically-regulated gene expression (GReX) based subpopulations and within them the key molecular drivers using deep neural networks (DNNs). Lastly, we will identify key non-genetic biomarkers and effective treatments for each validated subphenotype. Non-genetic biomarkers will be based on pre-mined features available from the electronic health records (EHR) and features extracted from the EHR via natural language processing (NLP). The subphenotypes will be validated in the civilian cohorts PsycheMERGE and BioMe.  This project will take place at the Icahn School of Medicine, one of the leading centers of data science, genomics and precision medicine. The mentoring committee comprises experts in the fields of computational and functional genomics, integrative analysis, machine learning (including DNNs and NLP), and EHR mining. Dr. Voloudakis will develop the skills necessary to launch an independent academic career in genetically based EHR-informed precision psychiatry. PROJECT NARRATIVE  Schizophrenia (SCZ) and bipolar disorder (BD) are genetically correlated, highly heritable, severe and complex brain disorders characterized by substantial clinical and biological heterogeneity with partially overlap- ping symptomatology and treatment. This project will use tissue and cell-type specific imputed transcriptomes for individuals with SCZ or BD to identify, characterize and target subphenotypes of those disorders. We will use the Million Veteran Program and Cooperative Studies Program 572 (“The Genetics of Functional Disability in Schizophrenia and Bipolar Illness”) as the discovery cohorts and will validate our findings in the PsycheMERGE and BioMe cohorts.",Characterizing and targeting subphenotypes of schizophrenia and bipolar disorder via individually imputed tissue and cell-type specific transcriptomes,10055546,K08MH122911,"['Astrocytes', 'Biological', 'Biological Markers', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Case-Control Studies', 'Classification', 'Complex', 'Data Science', 'Development', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Epigenetic Process', 'Exhibits', 'Exposure to', 'Functional disorder', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Genotype', 'Glutamates', 'Goals', 'Heritability', 'Heterogeneity', 'Individual', 'Intervention', 'Machine Learning', 'Mentors', 'Methods', 'Microglia', 'Mining', 'Modeling', 'Molecular', 'Natural Language Processing', 'Neurons', 'Neurosciences', 'Oligodendroglia', 'Outcome', 'Patients', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacology', 'Phenotype', 'Population Genetics', 'Positioning Attribute', 'Precision therapeutics', 'Prefrontal Cortex', 'Productivity', 'Protein Isoforms', 'Psychiatry', 'Relative Risks', 'Research', 'Risk', 'Sample Size', 'Schizophrenia', 'Selection for Treatments', 'Severity of illness', 'Symptoms', 'Tissues', 'Training', 'Treatment outcome', 'Variant', 'Veterans', 'base', 'biological heterogeneity', 'career', 'cell type', 'clinical heterogeneity', 'cohort', 'comorbidity', 'computational basis', 'cooperative study', 'deep learning', 'deep neural network', 'effective therapy', 'experience', 'functional disability', 'functional genomics', 'improved', 'medical schools', 'molecular phenotype', 'neuropsychiatric disorder', 'next generation', 'non-genetic', 'novel', 'novel therapeutic intervention', 'patient subsets', 'polygenic risk score', 'precision medicine', 'programs', 'psychopharmacologic', 'skills', 'symptomatology', 'trait', 'transcriptome', 'transcriptomics', 'treatment response']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,K08,2020,191944,-0.03470982595651477
"Robust AI to develop risk models in retinopathy of prematurity using deep learning ROP is a retinal neovascular disease affecting preterm infants, and is a leading cause of childhood blindness worldwide. Known clinical risk factors include preterm birth, low birthweight and use of supplemental oxygen but improved risk models are needed to identify infants that progress to treatment requiring disease and blindness. Deep learning techniques have been used to successfully identify “plus” disease in multi- institutional cohorts and to provide a continuous measure of disease severity. A major limitation of deep learning, however, is the need for large amounts of well curated datasets. Other limitations include overfitting and “brittleness” that can cause model performance to drop on external data. There are, however, numerous barriers to building and hosting these large central repositories with multi-institutional data required for robust deep learning including concerns about data sharing, regulations costs, patient privacy and intellectual property. In this project, we aim to demonstrate the utility of distributed/federated deep learning approaches where the data are located within institutions, but model parameters are shared with a central server. A major challenge thwarting this research, however, is the requirement for large quantities of labeled image data to train deep learning models. Efforts to create large public centralized collections of image data are hindered by barriers to data sharing, costs of image de-identification, patient privacy concerns, and control over how data are used. Current deep learning models that are being built using data from one or a few institutions are limited by potential overfitting and poor generalizability. Instead of centralizing or sharing patient images, we aim to distribute the training of deep learning models across institutions with computations performed on their local image data. Specifically, we seek to build robust risk models for predicting treatment requiring disease. Two large cohorts will be used to validate the hypothesis that the performance of the risk models using distributed learning approaches that of centrally hosted and is more robust than models built on single institutional datasets.  Grants Admin Updated 04.01.2019 JBou Retinopathy of prematurity is a retinal neovascular disease affecting preterm infants and a leading cause of preventable blindness worldwide. We are developing machine-learning based techniques to collaboratively build risk models for treatment requiring disease using multi-institutional data repositories. Distributed deep learning will be used to build robust models to improve clinical decision making in ROP.",Robust AI to develop risk models in retinopathy of prematurity using deep learning,10048436,R21EY031883,"['Affect', 'Architecture', 'Blindness', 'Blood Vessels', 'Childhood', 'Clinical', 'Collection', 'Communities', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Disease', 'Drops', 'Ecosystem', 'Eye diseases', 'Future', 'Gestational Age', 'Grant', 'Heterogeneity', 'Image', 'Infant', 'Institution', 'Intellectual Property', 'Label', 'Lead', 'Learning', 'Left', 'Logistic Regressions', 'Low Birth Weight Infant', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Oxygen', 'Patient imaging', 'Patients', 'Performance', 'Premature Birth', 'Premature Infant', 'Protocols documentation', 'Publishing', 'Rare Diseases', 'Regulation', 'Research', 'Research Personnel', 'Retina', 'Retinal Detachment', 'Retinopathy of Prematurity', 'Risk', 'Risk Factors', 'Sensitivity and Specificity', 'Severities', 'Severity of illness', 'Site', 'Techniques', 'Testing', 'Time', 'Training', 'Update', 'Vascular Diseases', 'Vascular Proliferation', 'Weight', 'Work', 'base', 'clinical decision-making', 'clinical risk', 'cohort', 'convolutional neural network', 'cost', 'data de-identification', 'data sharing', 'data warehouse', 'deep learning', 'deep learning algorithm', 'experience', 'improved', 'individual patient', 'large datasets', 'learning strategy', 'multiple data sources', 'neovascular', 'open source', 'patient population', 'patient privacy', 'patient subsets', 'predictive modeling', 'repository', 'risk prediction model', 'screening guidelines', 'secondary analysis', 'tool']",NEI,MASSACHUSETTS GENERAL HOSPITAL,R21,2020,274883,-0.01798734811451363
"Data-driven models of symptom heterogeneity to empower transdiagnostic multimodal biomarker discovery in mood disorders PROJECT SUMMARY/ABSTRACT My goal is to pursue an independent career in computational psychiatry by leveraging cutting-edge neuroimaging and data-driven analysis approaches to advance precision medicine in mental health. To build on my strong neuroimaging and computational background, the training component of this award emphasizes coursework and mentorship in the clinical and behavioral aspects of psychopathology. I will also receive mentorship to advance my theoretical and applied understanding of deep learning in this burgeoning field. The overarching research goal in this proposal is to develop computational strategies that account for the heterogeneity of mood disorders to improve the identification of treatment-response biomarkers. Response to pharmaceutical and behavioral antidepressant treatments is low, likely due to the symptomatic and etiological heterogeneity of depression whereby certain treatments may confer differential benefits for patients having particular symptom constellations. In the K99 phase, I will seek to improve prediction of individual antidepressant response using electroconvulsive therapy (ECT), which elicits robust and rapid antidepressant effects, as the treatment model. I will use MRI and clinical data from patients undergoing ECT collected for the large the Global ECT-MRI Research Collaboration (GEMRIC). In Aim 1, I will use exploratory factor analysis to characterize latent symptom dimensions of the GEMRIC cohort before, during, and after ECT. The accuracy of predicting clinical outcomes along the recovered symptom dimensions will be compared to traditional means of evaluating response using the total score of the Hamilton Depression Rating Scale (HDRS). Pursuit of this aim will expand my understanding of clinical psychiatry and lay foundational knowledge for the independent aims. Aim 2 will expand my deep learning and multimodal neuroimaging skillsets as I develop novel deep learning architectures to fuse multimodal imaging features of GEMRIC participants to further improve predictions of treatment response and cognitive impairment following ECT. Rather than simply concatenating multimodal features together, deep network architectures will discover latent feature representations. The R00 phase will be a logical progression of the skill sets I develop in the mentored phase and expand on these lines of research. Aim 3 will draw from a collection of large-scale MRI datasets from patients with more broadly defined mood disorders to identify multimodal imaging markers associated with transdiagnostic symptom domains. Aim 4 uses treatment groups from aim 3, including patients undergoing ketamine, sleep deprivation, cognitive behavioral therapy, and pharmaceuticals, to explore the extent to which biomarkers of therapeutic response, defined along the transdiagnostic symptom dimensions identified in Aim 3, are shared across treatment groups. I anticipate that discrete categorizations of mood disorders artificially obscures discovery of treatment-response biomarkers. Fulfillment of these aims will simultaneously propel me to independence and yield important insight into the treatment of heterogeneous mood disorders. PROJECT NARRATIVE Mood disorders including depression, bipolar, and post-traumatic stress disorder constitute the world's leading cause of disability and their burden is increasing. This proposal seeks to mitigate the burden of these related mood disorders by identifying patterns of brain structure and function indicative of a patient's likelihood of benefiting from various related interventions. This research has the potential to inform more personalized treatment strategies than are currently available and will likely further inform development of precision interventions targeting mood disorders.",Data-driven models of symptom heterogeneity to empower transdiagnostic multimodal biomarker discovery in mood disorders,10011838,K99MH119314,"['Anhedonia', 'Antidepressive Agents', 'Anxiety Disorders', 'Architecture', 'Award', 'Behavioral', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Categories', 'Clinical', 'Clinical Data', 'Cognitive', 'Cognitive Therapy', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease remission', 'Electroconvulsive Therapy', 'Etiology', 'Factor Analysis', 'Foundations', 'Functional Imaging', 'Goals', 'Hamilton Rating Scale for Depression', 'Heterogeneity', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Individual', 'Infusion procedures', 'Intervention', 'Ketamine', 'Knowledge', 'Left', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mentors', 'Mentorship', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Multimodal Imaging', 'National Institute of Mental Health', 'Negative Valence', 'Neurobiology', 'Participant', 'Patients', 'Pattern', 'Pharmacologic Substance', 'Pharmacotherapy', 'Phase', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Psychiatry', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Rest', 'Severity of illness', 'Sleep Deprivation', 'Sleeplessness', 'Structure', 'Symptoms', 'System', 'Therapeutic Intervention', 'Training', 'Transcend', 'Treatment Side Effects', 'antidepressant effect', 'anxious', 'biological systems', 'biomarker discovery', 'biomarker identification', 'career', 'classification algorithm', 'cohort', 'convolutional neural network', 'data archive', 'deep learning', 'disability', 'effective therapy', 'electric field', 'hippocampal morphometry', 'imaging biomarker', 'improved', 'insight', 'learning strategy', 'machine learning method', 'multimodality', 'network architecture', 'neurobiological mechanism', 'neuroimaging', 'novel', 'personalized intervention', 'personalized medicine', 'precision medicine', 'predict clinical outcome', 'predicting response', 'random forest', 'relating to nervous system', 'response', 'response biomarker', 'side effect', 'skills', 'statistical and machine learning', 'support vector machine', 'therapy outcome', 'treatment group', 'treatment response', 'treatment strategy']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,K99,2020,102857,-0.017484312737832967
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9878070,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'machine learning method', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,591130,-0.02281490004533545
"A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER) Many promising peripheral neuromodulation techniques have been proposed to treat lower urinary tract (LUT) dysfunction, but our lack of predictive models has forced the community (including the PI’s lab) to explore the vast parameter space of nerve targets, stimulation parameterizations, and electrode designs empirically in animal experiments by trial and error. This type of exploratory experimentation is the only current method of optimizing, personalizing, or discovering novel LUT neuromodulation techniques. Motivated by this clinical need, our long-term goal for this work is to predict the effects of neuromodulation on the LUT. To move toward this goal, we propose to develop a new modeling framework that integrates disparate biophysics models through machine learning, thereby emulating an entire organ system through a process we call Biomechanistic Learning Augmentation of Deep Differential Equation Representations (BLADDER). We will develop and use the general BLADDER framework to create an organ-level model of the normal healthy LUT throughout its filling and voiding cycles, including non-volitional neural reflex control over the bladder and urethra. Our focus on neural reflex control and organ-level scales ensures that, if successful, the BLADDER LUT model will be poised to predict effects of neuromodulation using computational studies, which so far has been impossible due to the complexity of the LUT. The BLADDER framework unites multiple individual mechanistic models (each accounting for a component function of an organ system) by using deep recurrent neural networks (RNN) to learn the appropriate coupling dynamics linking each component model. The combination of mechanistic and machine learning models under a single framework allows us to harness the advantages of both: mechanistic models excel at interpretability but suffer from a lack of scalability (becoming intractable at the level of organ systems), while machine learning models are excellent at scale but lack generalizability and insights for hypothesis generation. The BLADDER framework will scale up mechanistic models to the level of systems physiology by linking tractable model components together using a supervisory RNN, allowing the BLADDER framework to deliver both interpretability and scale. We will draw on existing SPARC datasets in the cat (e.g., Bruns and Gaunt), existing publicly available data in rat, and generate new data in the rat to construct a training dataset for the supervisory RNN. We will further draw from already published small-scale mechanistic models, validated on human and animal data, for the mechanistic components of the BLADDER LUT model. The formal process of identifying these models and datasets, and checking their validity and robustness, will clearly reveal the deficits and strengths in our theoretical and experimental understanding of the LUT in a straightforward and rational way. We will use the 10 Simple Rules to vet mechanistic models for inclusion in the BLADDER LUT model and compile a public inventory for the neurourology community. Major task 1 (Q1-2): Identify available datasets and candidate mechanistic models from published literature. Major deliverables are a public database and a whitepaper detailing the state of the field and prospects for modeling and experimental work. Major Task 2 (Q1-3): Demonstrate proof of concept of BLADDER framework. Major deliverables are a publicly available code linking two LUT component models via supervisory RNN and a report on suitable RNN architectures based on fully described dynamical systems. Major Task 3 (Q3-6): Create a multi-component BLADDER model. Major deliverables are code used to link separate mechanistic LUT models via the supervisory RNN, and an in vivo rat dataset to fill in critical measurables for the machine learning training set. Major Task 4 (Q6-8): Deploy the fully operational BLADDER model of the LUT, including autonomously predicted neural reflex control. Major deliverables are publicly available codes and datasets, and a hypothesis-driven computational experiment to predict simple interventions. n/a",A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER),10206953,OT2OD030524,"['Accounting', 'Animal Experiments', 'Bladder', 'Clinical', 'Code', 'Communities', 'Coupling', 'Data', 'Data Set', 'Databases', 'Differential Equation', 'Electrodes', 'Ensure', 'Equipment and supply inventories', 'Felis catus', 'Functional disorder', 'Generations', 'Goals', 'Individual', 'Intervention', 'Learning', 'Link', 'Literature', 'Lower urinary tract', 'Machine Learning', 'Measurable', 'Methods', 'Modeling', 'Nerve', 'Organ', 'Peripheral', 'Physiology', 'Process', 'Publishing', 'Rattus', 'Reflex control', 'Reporting', 'System', 'Techniques', 'Training', 'Urethra', 'Work', 'animal data', 'base', 'biophysical model', 'body system', 'computer studies', 'design', 'dynamic system', 'experimental study', 'human data', 'in vivo', 'insight', 'neural network architecture', 'neuroregulation', 'novel', 'predictive modeling', 'recurrent neural network', 'relating to nervous system', 'scale up']",OD,FLORIDA INTERNATIONAL UNIVERSITY,OT2,2020,1025141,-0.019013383587153175
"Developing a virtual placenta biobank Project Summary / Abstract The placenta is the first organ to develop and functions as the fetal lung, kidney, gut, skin, immune and endocrine systems. It is the cause of, and reflects changes from, most diseases in pregnancy, yet remains understudied. This career development proposal will train me in the tools and practice of digital pathology, while I apply them to the placenta with the hypothesis that there are reproducible, quantitative changes in the placenta that can be modeled and used to identify abnormalities via artificial intelligence (AI). I will create a publicly available atlas of microscopically normal placentas from throughout the 2nd and 3rd trimesters. Whole slide imaging will be performed on microscopic slides of placentas from the beginning of the 2nd trimester (13 weeks) through post-term (42 weeks). I will lead a team to annotate tissue type, structures, and cells. Algorithms will be trained to replicate the manual annotations. To study the changes in the placenta over time, automated measurements will be performed to identify changes in shape, size, and cellularity of placental structures that correlate with gestational age. This research can be used to develop a model of placental development and study prematurity. I will demonstrate detection of diseases of pregnancy, using preeclampsia (PreE) as an example. Placentas with microscopic changes classically seen in PreE will be scanned and annotated and algorithms trained and tested to identify them. Like many diseases of pregnancy, placental changes in PreE are variable and sometimes absent. Slides from PreE cases with no microscopic abnormalities will be scanned and examined using the quantitative parameters developed for normal placentas, testing the hypothesis that one or more of them will significantly differ between PreE cases and gestational age- matched controls. I am an Assistant Professor of Pathology at Northwestern University with an emerging focus in informatics and machine learning for diseases of pregnancy. The mentor for this project is Lee D.A. Cooper, PhD, an expert in digital pathology and machine learning. The co-mentor is David M. Aronoff, MD, an expert in maternal-child health. Mentor and co-mentor both have a history of NIH funding and graduating mentees to independence. The advisory committee consists of a digital pathology expert (Gutman), a pediatrician (Mestan) and a pathologist physician scientist (Yang). They have proposed an aggressive schedule of one-on-one meetings, coursework, seminars, and scientific meetings to supplement learning by doing the science. Completion of these studies will build my expertise in the application of machine learning to placental pathology while creating a new, publicly- accessible tool for the rapid assessment and understanding of organ structure and function with great potential to improve maternal-child health. Project Narrative The placenta grows over the course of gestation from a single layer of cells to a complex organ that acts as the fetal skin, lung, gut, kidney, immune system, and endocrine system. This project will develop an online repository of placenta microscopic images over the course of gestation from normal placentas and one disease of pregnancy, preeclampsia. Using artificial intelligence to quantitatively describe the changes over time in normal placentas and those with disease could help understand preterm birth and diseases of pregnancy.",Developing a virtual placenta biobank,10040733,K08EB030120,"['Advisory Committees', 'Algorithms', 'Architecture', 'Area', 'Artificial Intelligence', 'Atlases', 'Biological', 'Cells', 'Cellularity', 'Child', 'Chorion', 'Complex', 'Data', 'Decidual Cell', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Elements', 'Endocrine system', 'Endothelium', 'Event', 'Feeds', 'Fetal Lung', 'Fibrinoid necrosis', 'Funding', 'Gestational Age', 'Glass', 'Goals', 'Hematoma', 'Hemosiderosis', 'Histology', 'Histopathology', 'Human', 'Immune system', 'Infarction', 'Informatics', 'Kidney', 'Lead', 'Learning', 'Length', 'Liver', 'Lung', 'Machine Learning', 'Manuals', 'Maternal and Child Health', 'Measurement', 'Membrane', 'Mentors', 'Microscopic', 'Modeling', 'Morphology', 'Organ', 'Pathogenicity', 'Pathologic', 'Pathologist', 'Pathology', 'Physicians', 'Physiological', 'Physiology', 'Placenta', 'Placentation', 'Pre-Eclampsia', 'Pregnancy', 'Premature Birth', 'Radar', 'Recording of previous events', 'Reporting', 'Reproducibility', 'Research', 'Resources', 'Scanning', 'Schedule', 'Science', 'Scientist', 'Second Pregnancy Trimester', 'Shapes', 'Skin', 'Slide', 'Specimen', 'Spiral Artery of the Endometrium', 'Structure', 'Techniques', 'Testing', 'Thinness', 'Third Pregnancy Trimester', 'Time', 'Tissues', 'Training', 'Umbilical cord structure', 'United States National Institutes of Health', 'Universities', 'Variant', 'Villous', 'Villus', 'Yang', 'algorithm training', 'biobank', 'career development', 'cell type', 'chorionic plate', 'digital', 'digital pathology', 'fetal', 'health of the mother', 'improved', 'interest', 'intrahepatic cholestasis of pregnancy', 'machine learning algorithm', 'macrophage', 'meetings', 'microscopic imaging', 'novel', 'online repository', 'pediatrician', 'premature', 'professor', 'supplemental instruction', 'tool', 'trophoblast', 'virtual', 'whole slide imaging']",NIBIB,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2020,185630,-0.022574477807629372
"The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy Project Summary: The long-term goal of this research program is to develop a rigorously experimentally validated all-atom computational model of the cardiac thin filament (CTF) bound to myosin S1 which provides a unique and accessible platform to identify novel, high resolution disease mechanisms linked to Hypertrophic Cardiomyopathy (HCM). In the prior funding period, we refined and extended our existing CTF computational model and successfully employed it to identify unique and clinically relevant allosteric disease mechanisms including HCM mutation-induced changes in myofilament Ca2+ kinetics, mutation-specific molecular causes of differential cardiac remodeling and disease progression. This included an in vivo validation via the development of a novel transgenic mouse model of cTnT-linked dilated cardiomyopathy and a predictive algorithm to determine the pathogenicity of cTnT mutations that out-performed existing computational approaches in a preliminary test. The key to these advances has been the ability of the current model to precisely identify and locate allosteric changes caused by mutations throughout all components of the CTF followed by closely coupled experimental validation and eventual in vivo model correlation. We now propose to significantly expand the biological complexity of the model to include myosin S1, the molecular motor that drives contraction and the second most common genetic cause of HCM. This important and challenging advance will facilitate a deeper understanding of disease pathogenesis by, for the first time, incorporating the role of molecular allosteric mechanisms between myosin S1 and thin filament. This new computational – experimental platform will be used for both mechanistic insight (for example used for the identification of novel myofilament disease targets,) and the development of a comprehensive deep-learning predictive algorithm to assign pathogenicity to both myosin and thin filament HCM mutations. The latter represents the first use of high-resolution structure, dynamics and function to predict HCM disease allele pathogenicity, a central challenge in the clinical management of these complex patients. Both the training and testing components of the deep learning development will utilize data from the highly annotated and curated SHaRe HCM registry thus greatly improving translational power. Two Specific Aims will be pursued: Aim 1 will utilize state of the art rare event simulation methods developed in one of our groups and refinement of existing unstructured domains of the CTF via FRET to establish the new model. Aim 2 will employ an extensive program of computational analysis and subsequent in vitro validation using pathogenic, variants of unknown significance and non- pathogenic HCM alleles derived from SHaRe to provide inputs to the machine learning environment for algorithm development. Novel disease mechanisms for myosin and thin filament HCM that include crosstalk between the two components will also be explored. Elucidation of these mechanisms can be the basis for robust molecular approaches to disease. Precision medicine and “molecular” medicine are concepts that aim to employ a patient’s genetic structure to discern the best medical treatments for disease. Hypertrophic cardiomyopathy is a genetic disease that afflicts 1/500 people. This application translates our knowledge of the molecular level effects of cardiac tissue mutation to disease and will aim to lead to eventual treatment.",The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy,10071638,R01HL107046,"['Address', 'Alleles', 'Anisotropy', 'Artificial Intelligence', 'Biological', 'Biological Assay', 'Biology', 'Biophysics', 'Breath Tests', 'C-terminal', 'Cardiac', 'Chemistry', 'Clinical Management', 'Complex', 'Computer Analysis', 'Computer Models', 'Contracts', 'Coupled', 'Data', 'Data Set', 'Descriptor', 'Development', 'Differential Scanning Calorimetry', 'Dilated Cardiomyopathy', 'Disease', 'Disease Progression', 'Distant', 'Engineering', 'Enzymes', 'Event', 'Fluorescence Anisotropy', 'Fluorescence Resonance Energy Transfer', 'Functional disorder', 'Funding', 'Generations', 'Genetic', 'Genetic Diseases', 'Genetic Structures', 'Goals', 'Grant', 'Hand', 'Human', 'Hypertrophic Cardiomyopathy', 'In Vitro', 'Individual', 'Induced Mutation', 'Kinetics', 'Knowledge', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Microfilaments', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Medicine', 'Molecular Motors', 'Motor', 'Mutation', 'Myosin ATPase', 'Pathogenesis', 'Pathogenicity', 'Patients', 'Perception', 'Physiological', 'Play', 'Protein Conformation', 'Protein Dynamics', 'Proteins', 'Registries', 'Research', 'Resolution', 'Resources', 'Role', 'Sampling', 'Sarcomeres', 'Site', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thick Filament', 'Thin Filament', 'Thinness', 'Time', 'Tissues', 'Training', 'Transgenic Mice', 'Translating', 'Validation', 'Variant', 'Work', 'algorithm development', 'automated analysis', 'base', 'cell motility', 'clinically relevant', 'deep learning', 'educational atmosphere', 'experimental study', 'improved', 'in vivo', 'in vivo Model', 'inherited cardiomyopathy', 'insight', 'machine learning algorithm', 'mouse model', 'neural network', 'next generation', 'novel', 'phosphorescence', 'precision medicine', 'prediction algorithm', 'programs', 'quantum chemistry', 'response', 'simulation', 'stopped-flow fluorescence', 'success', 'variant of unknown significance']",NHLBI,UNIVERSITY OF ARIZONA,R01,2020,585711,-0.0248417466742576
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9948018,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'airway obstruction', 'base', 'career development', 'cigarette smoke', 'comorbidity', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'imaging biomarker', 'improved', 'machine learning algorithm', 'machine learning method', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2020,189000,-0.07061272998642412
"Novel machine learning approaches for improving structural discrimination in cryo-electron tomography Project Summary Cellular cryo-electron tomography (Cryo-ET) has made possible the observation of cellular organelles and macromolecular complexes at nanometer resolution with native conformations. The rapid increasing amount of Cryo-ET data available however brings along some major challenges for analysis which we will timely ad- dress in this proposal. We will design novel data-driven machine learning algorithms for improving structural discrimination and resolution. In particular, we have the following speciﬁc aims: (1) We will develop a novel Autoencoder and Iterative region Matching (AIM) algorithm for marker-free alignment of image tilt-series to re- construct tomograms with improved resolution; (2) We will develop a saliency-based auto-picking algorithm for better detecting macromolecular complexes, and combine it with an innovative 2D-to-3D framework to further improve structure detection accuracy; (3) We will design an end-to-end convolutional model for pose-invariant clustering of subtomograms. This model will produce an initial clustering which will be reﬁned by a new subto- mogram averaging algorithm that automatically down-weights subtomograms of noise and little contribution; (4) We will perform experimental evaluations by using previously reported bacterial secretion systems and mito- chondrial ultrastructures datasets to improve the ﬁnal resolution. Implementing algorithms in Aims 1-3, we will develop a user-friendly open-source graphical user interface -tom to directly beneﬁt the scientiﬁc community.  -tom will be systematically compared with existing software including IMOD, EMAN2, and Relion on simulated and benchmark datasets. To facilitate distribution, -tom will be integrated into existing software platforms Sci- pion and TomoMiner. Our data-driven algorithms and software not only will facilitate and accelerate the future use of Cryo-ET, but also can be readily used on analyzing the existing large amounts of Cryo-ET data to im- prove our understanding of the structure, function, and spatial organization of macromolecular complexes in situ. Project Narrative This project will create a system of machine learning algorithms to accelerate and facilitate the use and re-use of the rapidly accumulating Cryo-ET datasets. For easy use, we will develop an open-source GUI -Tom (to be disseminated into the Scipion and TomoMiner software platforms) that streamlines the new approaches from the initial tomogram reconstruction step to the ﬁnal subtomogram averaging step. We will validate the performance of our system by applying it on published Cryo-ET datasets and monitor the improvement of the ﬁnal results.",Novel machine learning approaches for improving structural discrimination in cryo-electron tomography,9973462,R01GM134020,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Back', 'Benchmarking', 'Biological Process', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Cryo-electron tomography', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Discrimination', 'Evaluation', 'Future', 'Gaussian model', 'Group Structure', 'Hour', 'Image', 'In Situ', 'Knowledge', 'Laplacian', 'Literature', 'Machine Learning', 'Macromolecular Complexes', 'Manuals', 'Methods', 'Mitochondria', 'Modeling', 'Molecular Conformation', 'Monitor', 'Neurophysiology - biologic function', 'Noise', 'Organelles', 'Performance', 'Process', 'Publishing', 'Reporting', 'Resolution', 'Series', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tomogram', 'Weight', 'Work', 'autoencoder', 'automated algorithm', 'base', 'deep learning', 'design', 'falls', 'feature detection', 'graphical user interface', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'nano', 'nanometer resolution', 'novel', 'novel strategies', 'open source', 'particle', 'pi-Mesons', 'programs', 'reconstruction', 'success', 'user-friendly']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,342970,-0.018532867096584724
"Development and Deployment of Artificial Intelligence (AI) Driven Methods to Enable Chest X-ray Radiography as an Alternative Diagnostic Method for COVID-19 Pneumonia ABSTRACT In this competitive revision, within the same scope of developing and deploying algorithms to make a quantum leap in clinical diagnosis as that in our current U01EB021183, we would like to revise the original aims to add a new Aim to leverage our expertise in the areas of algorithm development and clinical translation to make immediate contributions to combat the COVID-19 pandemic. Specifically, we propose to develop and deploy artificial intelligence (AI) methods to enable chest x-ray radiography (CXR) as an alternative diagnostic tool to diagnose COVID-19 pneumonia, to rapidly triage patients for appropriate treatment, to monitor the treatment response in a contained environment, and to optimize the distribution of the limited medical resources during the current COVID-19 crisis. PROJECT NARRATIVE In this project, our overarching objective is to develop automated artificial intelligence (AI)-based algorithms to help radiologists to differentiate COVID-19 related pneumonia from other non-COVID-19 related pneumonia using CXR images. The advantages of the proposed AI equipped CXR technique include: i) widely available, ii) inexpensive, iii) excellent coronavirus exposure profile for patient, technologist, and equipment, and iv) rapid and automated DL interpretation, which is effectively instantaneous.",Development and Deployment of Artificial Intelligence (AI) Driven Methods to Enable Chest X-ray Radiography as an Alternative Diagnostic Method for COVID-19 Pneumonia,10156179,U01EB021183,"['Accident and Emergency department', 'Air', 'Algorithms', 'American College of Radiology', 'Anosmia', 'Appearance', 'Area', 'Artificial Intelligence', 'Bilateral', 'COVID-19', 'COVID-19 pandemic', 'Case Study', 'Cessation of life', 'China', 'Clinic', 'Clinical', 'Communities', 'Containment', 'Coronavirus', 'Coughing', 'Country', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Diagnostic Sensitivity', 'Diagnostic radiologic examination', 'Diarrhea', 'Disease', 'Disease Outbreaks', 'Dyspnea', 'Environment', 'Equipment', 'European', 'Exposure to', 'Fatigue', 'Fever', 'Glass', 'Gold', 'Health Personnel', 'Health care facility', 'Hospitals', 'Human', 'Image', 'Individual', 'Investigation', 'Lung', 'Lung diseases', 'Medical', 'Medical Imaging', 'Methods', 'Monitor', 'North America', 'Parents', 'Pathway interactions', 'Patient Triage', 'Patients', 'Performance', 'Persons', 'Pleural effusion disorder', 'Pneumonia', 'Process', 'Radiology Specialty', 'Reading', 'Reporting', 'Resources', 'Reverse Transcriptase Polymerase Chain Reaction', 'Rural', 'Sensitivity and Specificity', 'Societies', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Time', 'Triage', 'United States', 'Viral Pneumonia', 'War', 'World Health Organization', 'X-Ray Computed Tomography', 'accurate diagnosis', 'algorithm development', 'base', 'chest computed tomography', 'clinical Diagnosis', 'clinical translation', 'combat', 'deep learning', 'high risk', 'high risk population', 'imaging facilities', 'imaging modality', 'improved', 'intelligent algorithm', 'neural network architecture', 'pandemic disease', 'prevent', 'profiles in patients', 'quantum', 'radiologist', 'screening', 'success', 'tool', 'treatment response', 'urgent care']",NIBIB,UNIVERSITY OF WISCONSIN-MADISON,U01,2020,605070,-0.006748372386347824
"Deep Learning Algorithms for FreeSurfer Abstract FreeSurfer is a tool for the analysis of Magnetic Resonance Imaging (MRI) that has proven to be a flexible and powerful technology for quantifying the effects of many conditions, including numerous neurological disorders, on human brain anatomy, connectivity, vasculature, chemical composition, physiology and function. In the past 20 years, these open source tools have been developed to accurately and automatically segment an array of brain structures and have become the core analysis infrastructure for the Alzheimer’s Disease NeuroImaging Initiative (ADNI). In this project, we seek the resources to radically increase the speed, accuracy and flexibility of these tools, taking advantage of exciting new results in Deep Learning. This will enable us to more accurately quantify neuroanatomical changes that are critical to diagnosing, staging and assessing the efficacy of potential therapeutic interventions in diseases such as Alzheimer’s. This includes the generation of documentation, tutorials, unit tests, regression tests and system tests to harden the tools and make them usable by clinicians and neuroscientists, and finally the distribution and support of the data, manual labelings and tools to the more than 40,000 researchers that use FreeSurfer through our existing open source mechanism. In addition, we will analyze the entire Alzheimer’s Disease NeuroImaging Initiative dataset and return it for public release, including a set of manually labeled data that can be used to optimize Deep Learning tools for Alzheimer’s Disease over the next decade. Relevance Successful completion of the proposed project will increase the usability and accuracy of our publicly available segmentation tools, and open up new possibilities, such as integrating them into the MRI scanner and rapidly detecting Alzheimer’s-related changes. These new capabilities well enable other studies to significantly increase their ability to detect AD and other disease effects in research settings as well as phase II and phase III clinical trials due to the radical increase in speed of the new tools, enabling them to be applied to a diverse set of MRI contrasts and much larger datasets, rapidly and accurately. Further, they will allow rapid application of cutting-edge analyses to the ongoing Alzheimer’s Disease NeuroImaging Initiative dataset, improving the ability to extract early biomarkers of this devastating disease.",Deep Learning Algorithms for FreeSurfer,9970009,R01AG064027,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Chemicals', 'Code', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Engineering', 'Ensure', 'Excision', 'Functional Magnetic Resonance Imaging', 'Future', 'Generations', 'Hour', 'Human', 'Image', 'Infrastructure', 'Label', 'Licensing', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Memory', 'Modeling', 'Neurobiology', 'Pattern', 'Phase II Clinical Trials', 'Phase III Clinical Trials', 'Physiology', 'Population', 'Procedures', 'Publishing', 'Recording of previous events', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Rest', 'Sensitivity and Specificity', 'Speed', 'Staging', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Validation', 'Variant', 'Work', 'base', 'contrast imaging', 'convolutional neural network', 'cranium', 'deep learning', 'deep learning algorithm', 'early detection biomarkers', 'flexibility', 'high resolution imaging', 'human disease', 'improved', 'large datasets', 'morphometry', 'nervous system disorder', 'neuroimaging', 'novel', 'open source', 'prevent', 'prototype', 'skills', 'spatial relationship', 'support tools', 'tool', 'usability', 'web site', 'wiki']",NIA,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,649026,-0.08286511947313582
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9963080,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Alzheimer&apos', 's disease pathology', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Models', 'Computer software', 'Data', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genetic testing', 'genome-wide', 'genomic data', 'genomic locus', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'large scale data', 'machine learning algorithm', 'machine learning method', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2020,410000,-0.07806203877178262
"Investigation of Stereotyped High-Frequency Oscillations with Computational Intelligence for the Prediction of Seizure Onset Zone in Epilepsy PROJECT SUMMARY Neurosurgical therapy of refractory epilepsy requires accurate localization of seizure onset zone (SOZ). In clinical practice, intracranial EEG (iEEG) is recorded in the epilepsy monitoring unit (EMU) over many days where multiple seizures are recorded to provide information to localize the SOZ. The prolonged monitoring in the EMU adds to the risk of complications and can include intracranial bleeding and potentially death. Recently, high frequency oscillations (HFO) of iEEG between 80 to 500 Hz are highly valued as a promising clinical biomarker for epilepsy. HFOs are believed to be clinically significant, and thus could be used for SOZ localization. However, HFOs can also be recorded from normal and non-epileptic cerebral structures. When defined only by rate or frequency, pathological HFOs are indistinguishable from physiological ones, which limit their application in epilepsy pre-surgical planning. In this proposal, to the best of our knowledge, we show of a recurrent waveform pattern that distinguishes pathological HFOs from physiological ones. In particular, we observed that the SOZ generates repeatedly a set of stereotyped HFO waveforms whereas the HFOs from nonepileptic regions were irregular in their waveform morphology. Based on these observations, using computational tools built on recent advances in sparse coding and unsupervised machine learning techniques, we propose to detect these stereotyped recurrent HFO waveform patterns directly from the continuous iEEG data of adult and pediatric patients and test their prognostic value by correlating the spatial distribution of detected events to clinical findings such as SOZ, resection zone and seizure freedom. We hypothesize that accurate detection of pathologic HFOs in brief iEEG recordings can identify the SOZ and eliminate the necessity of prolonged EMU monitoring and reduce the associated risks. With these motivations, in this project an interdisciplinary team composed of biomedical engineers, epileptologists and neurosurgeons will work together to develop and test novel computational tools to detect stereotyped HFOs and its subtypes in large iEEG datasets recorded with clinical electrodes. Developed algorithms and iEEG data will be shared with the research community to contribute to the reproducible research and help other research groups to develop novel methods. The results of this study will be essential for achieving our group's long term goal of developing an online neural signal processing system for the rapid and accurate identification of SOZ with brief invasive recording. PROJECT NARRATIVE Prolonged iEEG monitoring for SOZ localization does add to the risk of complications and may include serious issues, such as intracranial bleeding, meningoencephalitis, and eventually death. The intellectual merit of this project is to develop computational intelligence tools based on recent advances in sparse coding and unsupervised machine learning techniques to investigate stereotyped high frequency oscillations (HFOs) in long-term iEEG and test the hypothesis whether the automated detection of HFOs will yield accurate and fast identification of SOZ.",Investigation of Stereotyped High-Frequency Oscillations with Computational Intelligence for the Prediction of Seizure Onset Zone in Epilepsy,9974350,R01NS112497,"['Adult', 'Algorithms', 'Area', 'Biomedical Engineering', 'Brain', 'Cerebrum', 'Cessation of life', 'Characteristics', 'Child', 'Clinical', 'Code', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Electrodes', 'Electroencephalography', 'Environment', 'Epilepsy', 'Event', 'Excision', 'Freedom', 'Frequencies', 'Goals', 'Hemorrhage', 'High Frequency Oscillation', 'Hospitals', 'Hour', 'Intractable Epilepsy', 'Investigation', 'Laboratories', 'Language', 'Lesion', 'Meningoencephalitis', 'Methods', 'Modernization', 'Monitor', 'Morphology', 'Motivation', 'Motor', 'Motor Cortex', 'Multicenter Studies', 'Neocortex', 'Neurosurgeon', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Pattern', 'Physiological', 'Recurrence', 'Reproducibility', 'Research', 'Risk', 'Scheme', 'Seizures', 'Site', 'Spatial Distribution', 'Stereotyping', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Translations', 'Visual', 'Work', 'awake', 'base', 'clinical biomarkers', 'clinical practice', 'clinically significant', 'computational intelligence', 'computerized tools', 'cost', 'neurotransmission', 'novel', 'pediatric patients', 'prognostic value', 'prospective', 'signal processing', 'tool', 'unsupervised learning']",NINDS,UNIVERSITY OF HOUSTON,R01,2020,459252,-0.011772686216260663
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,-0.016489256061672786
"Improving Population Representativeness of the Inference from Non-Probability Sample Analysis SUMMARY The critical role of population-representativeness for estimating disease incidence and prevalence has been widely accepted in epidemiologic studies. Improving population representativeness of nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, however, has received little attention by biostatisticians or epidemiologists. In this project, we propose two innovative “pseudoweight” construction methods: 1) two-step matching, and 2) calibration, under an adapted exchangeability assumption, for unbiased estimation of disease incidence and prevalence in the target population. The proposed methods, combined with machine learning methods for propensity score estimation, will achieve significant bias reduction, especially when selection into nonprobability samples is driven by complex relationships between the covariates. We will quantify the bias reduced by the proposed “pseudoweights”, numerically and empirically, on the estimation of disease incidence and prevalence in the target population. Monte Carlo simulation studies are designed under varying degrees of departure from the adapted exchangeability assumption to evaluate the bias of the proposed estimates. The robustness of the proposed estimators against varying sample sizes, number of clusters in survey, and complexities of the true propensity score modeling will be investigated in scenarios that differ by levels of non-linearity, non-additivity and correlations between covariates in the true propensity model. Using data from National Institutes of Health and the American Association of Retired Persons (NIH-AARP, a nonprobability cohort sample) data and the US National Health Interview Survey (NHIS, a probability survey sample), the proposed methods will be applied to estimate the prevalence of self-reported diseases and all-cause or all-cancer mortality rates for people aged 50-71 in the US. To test our methods, we will purposely select outcome variables that are available in both the NIH-AARP and the NHIS. Thus, the amount of bias in NIH-AARP estimates corrected by the proposed pseudoweights can be quantified in practice, assuming the weighted NHIS estimate is true. The proposed methods, although motivated by the volunteer-based epidemiological studies, have wide applications outside of epidemiology, such as electronic health records or web surveys. The results from this project can be used by epidemiologists and health policy makers to improve the understanding of the health-related characteristics in the general population. Computer software that implements the proposed methods will be made available for public use. PROJECT NARRATIVE The project proposes innovative “pseudoweights” construction methods for nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, to improve their population representativeness. The project will quantify the amount of bias reduced by the proposed “pseudoweights,” numerically and empirically, on the estimation of population parameters such as disease incidence and prevalence. The result can be used by epidemiologists and health policy makers to improve the understanding of the health related characteristics in the general population.",Improving Population Representativeness of the Inference from Non-Probability Sample Analysis,10046869,R03CA252782,"['American', 'Attention', 'Calibration', 'Characteristics', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Disease', 'Effectiveness', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Equilibrium', 'General Population', 'Health', 'Health Policy', 'Incidence', 'Internet', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monte Carlo Method', 'National Health Interview Survey', 'Outcome', 'Patient Self-Report', 'Policy Maker', 'Population', 'Prevalence', 'Probability', 'Probability Samples', 'Research', 'Role', 'Sample Size', 'Sampling', 'Source', 'Surveys', 'Target Populations', 'Testing', 'Trees', 'United States National Institutes of Health', 'Weight', 'aged', 'base', 'cohort', 'complex data ', 'design', 'epidemiology study', 'flexibility', 'improved', 'innovation', 'machine learning method', 'mortality', 'random forest', 'retiree', 'software development', 'volunteer']",NCI,"UNIV OF MARYLAND, COLLEGE PARK",R03,2020,154500,-0.008348403870259436
"The Environmental and Human Factors that Determine Ixodes scapularis-borne Diseases Incidence Project Summary Vector-borne diseases (VBDs) are the most common types of emerging and re-emerging infectious diseases in the world. VBD epidemics have been increasing over recent decades, with tickborne diseases having doubled in the last decade in the United States. Despite the increase in public health burden, over 80% of vector-control organizations lack preventative capabilities. Understanding the interplay between the environment, vectors, pathogens, and humans that expedite disease spread remains a challenge. The overarching goal of this project is to identify the key environmental and human drivers that have led to the emergence of VBDs. Current models that predict tickborne disease risk have oversimplified the process by focusing only on the vector, i.e. risk of tick exposure. A human’s risk of infection is not only a function of entomological risk but also of factors inherent to the individual including behavior or characteristics that increase susceptibility to disease. This project proposes a novel approach to tickborne disease prediction by developing a comprehensive model that incorporate pathogen population dynamics and human factors to predict disease risk. This study will investigate several pathogens vectored by the black-legged tick (Ixodes scapularis): Borrelia burgdorferi (Lyme disease), Anaplasma phagocytophilum (human granulocytic anaplasmosis), and Babesia microti (babesiosis). The central hypothesis is that the prediction of tickborne disease risk can be improved by using sophisticated statistical methods to identify environmental drivers that impact pathogen population dynamics while incorporating human demographic characteristics. The hypothesis will be addressed in the following aims: (1) Determine the current and historical population dynamic patterns of pathogens vectored by I. scapularis to predict pathogen distribution; (2) Determine the association between human characteristics and tick-borne disease risk in order to develop an improved spatial disease risk model. This model will allow the identification and quantification of factors that are associated with the emergence of tickborne diseases in New York State, which is geographically advantageous because it is representative of much of the natural environment that ticks encounter in the northeastern US including rapid and recent changes in climate and landscapes. The results of this project will be used to develop a public disease warning system that will use contemporary and future climate forecasts to monitor tick populations and predict potential disease outbreaks for areas with vulnerable populations. With climate forecasts predicting an increase in 2-3°C in temperature by 2100, there is uncertainty in how diseases will shift and a warning system will allow preparation accordingly. At the completion of the proposed research project, the applicant will have acquired the following skillsets through intensive, interdisciplinary mentorship: big data analysis, advanced statistics including Bayesian and machine learning methods, spatial analyses, and risk analysis. This will enable the applicant to succeed as an independent investigator to address the challenges posed by emerging infectious diseases. Project Narrative Vector-borne diseases are the most common types of emerging infectious diseases in the world and constitute major threats to public health. My project uses advanced statistical methods to uncover how interactions between environmental factors affecting pathogen population dynamics and human demographic characteristics determine human risk for three tick-borne diseases: Lyme disease, human granulocytic anaplasmosis, and babesiosis. The results of my project will be used to create a risk map to predict the spatial and temporal occurrence of tickborne diseases, which can inform public health strategies in order to mitigate disease burden.",The Environmental and Human Factors that Determine Ixodes scapularis-borne Diseases Incidence,10018461,F31AI133871,"['Accounting', 'Address', 'Adopted', 'Affect', 'Anaplasma phagocytophilum', 'Anaplasmosis', 'Area', 'Award', 'Babesia microti', 'Babesiosis', 'Bayesian Analysis', 'Bayesian learning', 'Behavior', 'Big Data', 'Big Data Methods', 'Black-legged Tick', 'Borrelia burgdorferi', 'Characteristics', 'Climate', 'Communicable Diseases', 'Communities', 'Contracts', 'Dangerousness', 'Data', 'Data Analyses', 'Decision Trees', 'Disease', 'Disease Outbreaks', 'Ecological Change', 'Ecology', 'Emerging Communicable Diseases', 'Entomology', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiological trend', 'Future', 'Geography', 'Goals', 'Health', 'Human', 'Human Characteristics', 'Incidence', 'Individual', 'Infection', 'Linear Regressions', 'Link', 'Lyme Disease', 'Machine Learning', 'Maps', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'New York', 'Pattern', 'Population', 'Population Dynamics', 'Predisposition', 'Preparation', 'Prevalence', 'Process', 'Public Health', 'Research Personnel', 'Research Project Grants', 'Risk', 'Risk Assessment', 'Risk Factors', 'Role', 'Sampling', 'Statistical Methods', 'System', 'Techniques', 'Temperature', 'Tick-Borne Diseases', 'Tick-Borne Infections', 'Ticks', 'Time', 'Training', 'Uncertainty', 'United States', 'Variant', 'Vector-transmitted infectious disease', 'Vulnerable Populations', 'anthropogenesis', 'base', 'burden of illness', 'cost effective', 'decision tree learning', 'demographics', 'disorder risk', 'environmental change', 'geographic population', 'granulocyte', 'human disease', 'human pathogen', 'improved', 'infection risk', 'machine learning method', 'model design', 'novel strategies', 'pathogen', 'pathogen exposure', 'predictive modeling', 'regression trees', 'spatial temporal variation', 'statistics', 'tick bite', 'tick-borne pathogen', 'trend', 'vector', 'vector control', 'vector transmission']",NIAID,UNIVERSITY OF PENNSYLVANIA,F31,2020,38185,-0.043264132974270404
"Using Causal Inference and Machine Learning Methods to Predict Cognitive Behavioral Treatment Response PROJECT SUMMARY/ABSTRACT Many patients with anxiety and fear disorders (AFDs) report minimal benefits when treated with an evidence- based psychotherapy (e.g., cognitive-behavioral therapy [CBT]) or pharmacotherapy (e.g., antidepressants; benzodiazepines). Conversely, some AFD patients are likely to benefit from virtually any treatment (e.g., supportive therapy). Using data collected in randomized controlled trials (RCTs), there has been limited progress determining how to use pre-treatment characteristics to match AFD patients to the treatment that is most likely to provide benefit. As a result, NIMH has forwarded Strategic Objectives focused on identifying treatment moderators and developing tools that predict differential treatment response. The broad goal of this proposed secondary data analysis is to apply causal inference and machine learning methods to prospective observational data to predict differential treatment response among patients with AFDs. The sample (n = 1,528) is from a longstanding NIMH-funded study of AFD patients who received: (a) CBT with concurrent pharmacotherapy, (b) CBT without pharmacotherapy, or (c) treatment as usual (TAU). Targeted maximum likelihood estimation (a causal inference method) and super learning (an ensemble machine learning method) will be used to accomplish the proposed Aims. Aim 1 will estimate the (overall) average effects of the three treatment types. Aim 2 will estimate “optimal treatment rules” to determine if differential treatment response can be meaningfully predicted based on a patient's multidimensional profile of pre-treatment symptoms. Aim 3 will estimate “optimal treatment rules” to determine if differential treatment response can be meaningfully predicted using all available pre-treatment covariates. The proposed study is highly innovative and could significantly impact the growing literature focused on predicting differential treatment effects and personalizing treatment for patients with AFDs. Although treatment effects estimates obtained using observational data and causal inference methods (i.e., adjusted for nonrandom treatment selection) are similar to those estimated in RCTs, this would be the first study to apply such methods to AFD patient data. This study will also be the first to use ensemble machine learning to develop composite moderators for AFDs (i.e., optimal treatment rules). In comparison, prior attempts to develop composite moderators have relied on less flexible model-building procedures prone to overfitting and unable to capture complex predictor-outcome associations (e.g., interactions among predictors; nonlinear associations). In achieving the proposed Aims, the current study would be a catalyst for future research using causal inference and machine learning to study predictors of differential treatment response among AFD patients. Results will be used to justify future research aimed at expanding and validating the models in larger observational samples and pragmatic RCTs (e.g., optimal treatment rules for specific medications/doses; timing of pharmacotherapy relative to CBT; second-wave CBT versus acceptance-based CBT). PROJECT NARRATIVE Some patients with anxiety and fear disorders may benefit only from certain evidence-based treatments, while others may benefit from virtually any treatment (e.g., talk therapy). The current study will use casual inference and machine learning methods to develop predictive models that identify the treatment(s) most likely to benefit a particular patient. The models developed in this study will serve as an important initial step toward efficient and personalized treatment for patients with anxiety and fear disorders.",Using Causal Inference and Machine Learning Methods to Predict Cognitive Behavioral Treatment Response,9912204,R21MH119492,"['Antidepressive Agents', 'Anxiety', 'Assessment tool', 'Benzodiazepines', 'Characteristics', 'Chronic', 'Clinic', 'Cognitive Therapy', 'Communities', 'Complex', 'Counseling', 'Data', 'Data Analyses', 'Diagnosis', 'Dimensions', 'Disease', 'Dose', 'Evidence based treatment', 'Fright', 'Funding', 'Goals', 'Heterogeneity', 'Intervention', 'Learning', 'Life Stress', 'Literature', 'Machine Learning', 'Mental Health', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'National Institute of Mental Health', 'Onset of illness', 'Outpatients', 'Patient observation', 'Patients', 'Personality', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Procedures', 'Psychotherapy', 'Randomized Controlled Trials', 'Recommendation', 'Reporting', 'Sampling', 'Selection for Treatments', 'Severities', 'Supportive care', 'Symptoms', 'Treatment Effectiveness', 'Treatment outcome', 'Validation', 'anxiety treatment', 'base', 'catalyst', 'comorbidity', 'cost', 'depressive symptoms', 'effective therapy', 'evidence base', 'excessive anxiety', 'experience', 'flexibility', 'functional disability', 'improved', 'improved outcome', 'innovation', 'machine learning method', 'model building', 'optimal treatments', 'outcome prediction', 'personalized medicine', 'predictive modeling', 'predictive tools', 'prospective', 'response', 'sociodemographics', 'stress reactivity', 'symptom treatment', 'treatment as usual', 'treatment disparity', 'treatment effect', 'treatment response', 'virtual']",NIMH,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2020,207977,-0.023225560805781732
"Predicting Parkinson's Disease Progression Rate Using Causal Measures of Functional MRI with Deep Learning Predictive Models Project abstract Parkinson's disease (PD) is the second most common neurodegenerative disease. A critical gap in the treatment of PD patients is that there is no clinically adopted method to predict an individual's progression rate. A predictor would enable the enrichment of disease modifying drug trials with fast progressors likely to show changes in the short duration of a clinical trial and enable a more informed discussion with patients about their prognosis. This proposal develops a composite biomarker of progression rate using the connectivity information provided by resting-state functional Magnetic Resonance Imaging (rs-fMRI) and deep learning. Deep learning (DL) is well suited to form predictive models because it learns both an optimal hierarchy of features and how to combine them for accurate prediction. In rs-fMRI the blood-oxygen level dependent signal can be analyzed to infer connectivity throughout the brain. Traditionally, connectivity has been computed as the correlation between average regional activation time courses. However correlation based connectivity is prone to inferring spurious connections due to its inability to distinguish indirect from direct connectivity and inability to distinguish bidirectional from unidirectional connectivity. A causal connectivity approach can discern these differences and thereby provide a more faithful characterization of the true neurobiological connectivity. The existing literature suggests connectivity, particularly causal connectivity, from rs-fMRI can inform the estimation of PD progression, but the attempt to predict progression rate with causal connectivity in a DL model is unique to this project.  This research develops several distinct approaches for building a progression rate predictor and apply them to three datasets including: the Parkinson's Progression Markers Initiative dataset, the NINDS Parkinson's Disease Biomarkers Program (PDBP) dataset, and the University of Texas Southwestern Medical Center's prospective imaging extension to the NINDS PBDP. In these studies, individual progression rates have been tracked over multiple years using multiple clinical measures. First, causal and correlative measures will be generated regionally and used with a DL model to create a baseline predictor of progression rate. Second, voxel- level causal measures will be generated as the increased granularity is expected to improve prediction accuracy. Third, since purely data-driven DL methods can be sensitive to dataset limitations, such as insufficient subjects and noise, these limitations will be addressed by developing a new structural connectivity regularization approach that constrains causal connectivity by the subject's own diffusion MRI. This regularization method will be general and likely applicable for building predictors for other neurological disorders such as stroke and Alzheimer's disease. This proposal will yield both DL models for predicting progression rate and a novel method to calculate constrained causal connectivity. All predictive models, composite neuroimaging biomarkers of progression rate and software will be publicly disseminated for ready incorporation by the scientific and clinical communities. Project narrative Parkinson's disease is the second most common neurodegenerative disease and this debilitating and incurable disease has no known cure. A cure for PD remains elusive due to the lack of clinically adopted predictors of progression rate, which if constructed would 1) hasten the discovery of disease modifying drugs by enriching clinical trials with fast progressors who likely will show changes over the trial, 2) allow for stratification of patients by progression rate in those trials, and 3) enable an informed discussion with patients about their prognosis. This proposal develops and validates distinct approaches to predict PD progression rate, identifies new biomarkers of progression rate, and yields a generalizable framework for constraining causal measures from fMRI with the subject's own structural connectivity that is readily repurposable for other neurological disorders with connectivity changes such as stroke and Alzheimer's.",Predicting Parkinson's Disease Progression Rate Using Causal Measures of Functional MRI with Deep Learning Predictive Models,10019347,F31NS115348,"['Address', 'Adopted', 'Adoption', 'Alzheimer&apos', 's Disease', 'Biological Markers', 'Brain', 'Characteristics', 'Clinical', 'Clinical Trials', 'Communities', 'Computer software', 'Data', 'Data Set', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Disease Progression', 'Etiology', 'Fiber', 'Florida', 'Functional Magnetic Resonance Imaging', 'Image', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Medical center', 'Methods', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Nerve Degeneration', 'Neurobiology', 'Neurodegenerative Disorders', 'Neurologic', 'Noise', 'Outcome', 'Parkinson Disease', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Research', 'Rest', 'Signal Transduction', 'Site', 'Stroke', 'Structure', 'Texas', 'Time', 'Training', 'Universities', 'Work', 'base', 'blood oxygen level dependent', 'cognitive testing', 'deep learning', 'improved', 'interest', 'learning community', 'learning strategy', 'nervous system disorder', 'neural network', 'neuroimaging marker', 'novel', 'outcome forecast', 'patient stratification', 'predictive modeling', 'prognostic value', 'programs', 'progression marker', 'prospective', 'tractography']",NINDS,UT SOUTHWESTERN MEDICAL CENTER,F31,2020,34768,-0.06946049150430786
"Developing and validating prognostic metabolomic signatures of diabetic kidney disease PROJECT SUMMARY/ABSTRACT Rationale. Diabetes is a leading cause of renal disease, accounting for 40% of the estimated 20 million US adult cases of chronic kidney disease. There is, however, substantial heterogeneity across diabetic patients with regards to development of kidney disease. Hence, there is an urgent need to identify prognostic biomarkers that can provide early and reliable evidence of future kidney disease, so that high-risk patients can receive optimal medical care. Existing clinical, proteomic and genomic markers do not consistently nor accurately predict kidney function decline. Metabolomics, a systematic evaluation of the end-products of cellular function in fluids, has the potential to inform physiological and pathological effects of chronic diseases. Metabolomic analysis combined with advanced quantitative methods could play a key role in building clinically useful prognostic signatures of diabetic kidney disease. Yet, development of computational methods with adequate rigor has lagged behind the technical capacity to perform large scale quantitative metabolomics. In this proposal we aim to address this computational gap in diabetic kidney disease research. Aims. We will implement rigorous computational methods to identify robust prognostic metabolite + clinical + genetic signatures of diabetic kidney disease progression. Specifically, we aim to (i) test the accuracy of previous signatures, and apply state-of-the-art analytic techniques and novel statistical methods to identify new multivariate metabolite sets for predicting kidney disease progression; (ii) quantify patterns of co-regulation of metabolites in diabetic kidney disease, and develop new tools in network biology to discover novel enzymes, proteins, metabolites, and molecular pathways which are implicated in diabetic kidney disease progression; (iii) test if these models can accurately predict kidney disease progression in independent prospective cohorts. Methods. Using clinical, genetic and metabolomic data from large prospective cohorts of > 1200 diverse, well- characterized patients with Type 2 diabetes, we will apply statistical methods for variable selection (e.g., penalized regression), and machine learning methods (e.g., random forest), which are known to perform well in the high-dimensional setting, to identify robust and parsimonious signatures of kidney disease progression. We will quantify inter-metabolite co-regulation patterns and infer biological pathways implicated in diabetic kidney disease. Throughout the modeling process, a rigorous training-validation paradigm will be adopted in order to improve reproducibility of models and reduce chance findings. Impact. A major product of this work will be the development of a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline. Our findings will also provide insight into markers of renal dysfunction, and elucidate possible therapeutic targets for treating diabetic kidney disease, thus potentially informing the design of future clinical trials. PROJECT NARRATIVE Kidney disease, a major and common complication of diabetes, can lead to repeated hospitalizations and premature death. There is an urgent need to develop clinical tools that can provide early evidence that a given diabetic patient is likely to progress to kidney disease in the future. In this proposal, we will identify new urinary biomarkers and use novel statistical modeling methods to create a clinically useful algorithm for identifying diabetic patients at high-risk for kidney function decline, with the ultimate goal of improving disease management and reducing mortality rates for these patients.",Developing and validating prognostic metabolomic signatures of diabetic kidney disease,9923450,R01DK110541,"['Accounting', 'Address', 'Adopted', 'Adult', 'Albuminuria', 'Algorithms', 'American', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biology', 'Caring', 'Cell physiology', 'Cessation of life', 'Chronic Disease', 'Chronic Kidney Failure', 'Chronic Kidney Insufficiency', 'Clinical', 'Clinical Trials', 'Collaborations', 'Complications of Diabetes Mellitus', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Diagnosis', 'Disease', 'Disease Management', 'Disease Progression', 'Enzymes', 'Evaluation', 'Funding', 'Future', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Heterogeneity', 'Hospitalization', 'Kidney', 'Kidney Diseases', 'Laboratories', 'Lead', 'Link', 'Liquid substance', 'Longitudinal cohort', 'Medical', 'Medical Genetics', 'Methods', 'Modeling', 'Molecular', 'Non-Insulin-Dependent Diabetes Mellitus', 'Pathologic', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Pima Indian', 'Play', 'Process', 'Prognostic Marker', 'Prospective cohort', 'Proteomics', 'Publishing', 'Recommendation', 'Regulation', 'Renal function', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Sampling', 'Sampling Studies', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Testing', 'Training', 'Type 2 diabetic', 'Urine', 'Validation', 'Work', 'bioinformatics tool', 'biological heterogeneity', 'chemical association', 'cohort', 'comorbidity', 'design', 'diabetic', 'diabetic patient', 'genetic signature', 'genomic biomarker', 'high dimensionality', 'high risk', 'improved', 'innovation', 'insight', 'kidney dysfunction', 'machine learning method', 'metabolome', 'metabolomics', 'model development', 'mortality', 'multidimensional data', 'nephrogenesis', 'network models', 'novel', 'open source', 'personalized medicine', 'predictive modeling', 'predictive signature', 'predictive test', 'premature', 'prognostic', 'prognostic signature', 'prospective', 'protein metabolite', 'random forest', 'targeted treatment', 'therapeutic target', 'tool', 'urinary']",NIDDK,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,334508,-0.00552808174181489
"Using machine learning techniques to characterize the Metabolomics Workbench Dataset PROJECT SUMMARY/ABSTRACT  Mass spectrometry in combination with chromatography provides a powerful approach to characterize small molecules produced in cells, tissues and other biological systems. In essence, measured metabolites provide a functional readout of cellular state, allowing novel biological studies that advance our understanding of health and disease. Currently, the main bottleneck in metabolomics is determining the chemical identities associated with the spectral signatures of measured masses. Despite the growth of spectral databases and advances in annotation tools that recommend the chemical structure that best explains each signature, the large majority of measured masses cannot be assigned a chemical identity. There is now consensus that gleaning partial information regarding the measured spectra in terms of chemical substructure or chemical classification can inform biological studies. This consensus is reflected in the newly updated reporting standards for metabolite annotation as proposed by the Metabolite Identification Task Group of the Metabolomics Society. As we show in our Preliminary Results, spectral characterization results in “features” that can enhance performance in machine-learning tasks such as annotation.  This work aims to enhance the use and value of the metabolomics dataset in Metabolomics Workbench by: (1) developing machine-learning tools trained on this dataset to characterize unknown spectra, and (2) adding characterization information to the Metabolomics Workbench dataset. In Aim 1, we identify spectral patterns (motifs) that can represent chemically meaningful groupings of peaks within the spectra (e.g., peaks associated with aromatic substructures, loss of a substructure fragment, etc.). We utilize neural topic models that use variational inference to identify such motifs. We expect such models to offer computational speedups and to identify more chemically coherent motifs when compared to earlier implementations of topic modeling. We generate motifs across all spectra in the Metabolomics Workbench and provide annotations for each spectrum.  In Aim 2, we map spectral signatures to chemical ontology classes. As ontologies are hierarchical and as a molecule can be associated with multiple classes at different hierarchical levels of an ontology, we cast this mapping problem as a hierarchical multi-label classification problem and use neural networks to implement such a classifier. The classifier will be trained using the Metabolomics Workbench dataset. Learned motifs from Aim 1 will be used as additional input features to improve classification. We expect that the developed classifier can be used by others to elucidate measurements of unidentified molecules with chemical ontology classes, or to generate ontology terms that can be used as features in downstream machine-learning tasks. Relevance to Public Health The project proposes to investigate machine learning techniques to enhance the utility of a Common Fund data set hosted through the Metabolomics Workbench. This data set consists of biologically relevant molecules and information about their structural composition and their mass spectrometry signatures. We anticipate that our techniques will result in annotating and adding information to the data set, which in turn will advance discoveries in biomedical research and have direct benefits to human health.",Using machine learning techniques to characterize the Metabolomics Workbench Dataset,10111982,R03OD030601,"['Biochemical', 'Biological', 'Biomedical Research', 'Catalogs', 'Cells', 'Chemical Structure', 'Chemicals', 'Chromatography', 'Classification', 'Complement', 'Computational Technique', 'Computing Methodologies', 'Consensus', 'Consumption', 'Coupled', 'Data', 'Data Set', 'Databases', 'Disease', 'Funding', 'Gas Chromatography', 'Gene Expression', 'Glean', 'Goals', 'Grouping', 'Growth', 'Health', 'Histidine', 'Human', 'Ions', 'Label', 'Liquid Chromatography', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Modeling', 'Molecular', 'Molecular Structure', 'Nature', 'Ontology', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Public Health', 'Reporting', 'Research Personnel', 'Sampling', 'Societies', 'Structure', 'Taxonomy', 'Techniques', 'Time', 'Tissues', 'Training', 'Update', 'Validation', 'Variant', 'Vocabulary', 'Work', 'annotation  system', 'biological systems', 'biomarker discovery', 'cost', 'functional outcomes', 'improved', 'metabolomics', 'neural network', 'novel', 'protein expression', 'relating to nervous system', 'response', 'small molecule', 'tool']",OD,TUFTS UNIVERSITY MEDFORD,R03,2020,263120,-0.030131935537140775
"Validating of Machine Learning-Based EEG Treatment Biomarkers in Depression SUMMARY/ABSTRACT The overarching aim of Alto Neuroscience is to advance brain-based biomarkers for psychiatric disorders in order to both optimize treatment pathways and drive the development of novel pharmacological and non- pharmacological interventions. Alto does this by developing and applying sophisticated machine learning computational models to electroencephalography (EEG) data collected at scale in real-world clinical treatment contexts. Specifically, in this direct-to-phase II SBIR proposal we will refine, and then independently validate, two EEG-based candidate biomarkers we have identified for stratifying patients with depression in a manner that both factors biological heterogeneity and informs treatment response. One of our biomarkers was derived in a “top-down” (i.e. supervised) manner by trying to directly predict treatment outcome, while the other biomarker presents a complimentary “bottom-up” (i.e. unsupervised) approach that begins by first identifying the most biologically homogeneous subset of patients and then testing the treatment relevance of the subtyping. Together, these findings represent very robust individual patient-level treatment-relevant EEG biomarkers, and in both cases, help define a critically-important objective approach to prospectively identifying and treating treatment- resistant depressed patients. A successful outcome of the proposed work would yield the first FDA-cleared biomarkers for stratifying psychiatric conditions. It would also provide a basis for targeted development of pharmacological and non-pharmacological interventions based on the EEG biomarkers. Both outcomes hold substantial commercial value and exciting potential for transforming psychiatry. PROJECT NARRATIVE The overarching aim of Alto Neuroscience is to advance brain-based biomarkers for psychiatric disorders in order to both optimize treatment pathways and drive the development of novel pharmacological and non- pharmacological interventions. Here we propose to refine, and then independently validate, two EEG-based candidate biomarkers we have identified for stratifying patients with depression in a manner that both factors biological heterogeneity and informs treatment response. A successful outcome of the proposed work would yield the first FDA-cleared biomarkers for stratifying psychiatric conditions. It would also provide a basis for targeted development of pharmacological and non-pharmacological interventions based on the EEG biomarkers.",Validating of Machine Learning-Based EEG Treatment Biomarkers in Depression,10009501,R44MH123373,"['Address', 'Antidepressive Agents', 'Award', 'Base of the Brain', 'Biological', 'Biological Factors', 'Biological Markers', 'Caring', 'Clinic', 'Clinical', 'Clinical Treatment', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Electroencephalography', 'Enrollment', 'Extravasation', 'Feedback', 'Funding', 'Intervention', 'Laboratories', 'Lead', 'Machine Learning', 'Maps', 'Medical Device', 'Mental Depression', 'Mental disorders', 'Methods', 'Neurosciences', 'Outcome', 'Pathway interactions', 'Patient Triage', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacology', 'Phase', 'Placebos', 'Procedures', 'Psychiatry', 'Regulation', 'Research', 'Resistance', 'Resistance profile', 'Scientist', 'Seeds', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Supervision', 'System', 'Testing', 'Training', 'Training Programs', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'base', 'biological heterogeneity', 'candidate marker', 'clinical care', 'cohort', 'commercialization', 'comorbidity', 'computerized data processing', 'cost', 'data acquisition', 'depressed patient', 'individual patient', 'meetings', 'novel', 'patient stratification', 'patient subsets', 'programs', 'prospective', 'repetitive transcranial magnetic stimulation', 'response', 'software development', 'supervised learning', 'therapy resistant', 'tool', 'treatment optimization', 'treatment response', 'treatment-resistant depression', 'unsupervised learning']",NIMH,"ALTO NEUROSCIENCE, INC.",R44,2020,988093,-0.017681839493483658
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.020373515544318366
"Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution Project Summary/Abstract It is hard to overstate the importance of monoclonal antibodies in the life sciences. Antibodies are critical tools in biomedical research and diagnostics (e.g. western blotting, immunoprecipitation, cytometry, biomarker discovery, and histology), are one of the most rapidly growing class of therapeutics, and are the basis for myriad new strategies in cancer therapy, such as checkpoint inhibitors that are revolutionizing treatment. Unfortunately, current methods for the generation of custom antibodies, including animal immunization and phage display, are slow, costly, inaccessible to most researchers, and often unsuccessful. We propose Autonomously EvolvinG Yeast-displayed antibodieS (AEGYS), a system for the continuous and rapid evolution of high-quality antibodies against custom antigens that requires only the simple culturing of yeast cells. We believe this can be achieved by combining cutting-edge generative machine learning algorithms for antibody library design with a new technology for in vivo continuous evolution and a yeast antigen-presenting cell that we will engineer. If successful, AEGYS should have a transformative impact across the whole of biomedicine by turning monoclonal antibody generation into a rapid, scalable, and accessible process where any lab with standard molecular biology capabilities can generate custom antibodies on demand simply by “immunizing” a test tube of yeast cells with an antigen. We anticipate that this democratization of antibody generation will also result in an explosion of crowdsourced antibody sequence data that will train our machine learning algorithms to design better antibody libraries for AEGYS, starting a virtuous cycle. We ourselves will use AEGYS to generate a panel of subtype- and conformation-specific nanobodies against biogenic amine receptors including those that respond to acetylcholine, adrenaline, dopamine, and other neurotransmitters, so that we can understand their role in neurobiology and addiction.! Project Narrative This proposal will provide a system for the scalable continuous evolution and computational design of antibodies against user-selected antigens. Antibodies are critical tools in medical research and are the basis for numerous therapies, but the generation of custom antibodies against new targets is a difficult and specialized task. The system proposed will turn antibody generation into a routine and widely accessible process for researchers in almost any field.","Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution",10021311,R01CA260415,"['Acetylcholine', 'Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antibody Formation', 'Antigen Targeting', 'Antigen-Presenting Cells', 'Antigens', 'Architecture', 'Area', 'Back', 'Biogenic Amine Receptors', 'Biological Sciences', 'Biomedical Research', 'Cell Surface Receptors', 'Cells', 'Chemistry', 'Clinic', 'Collection', 'Communities', 'Cultured Cells', 'Custom', 'Cytometry', 'Data', 'Data Set', 'Detergents', 'Diagnostic', 'Directed Molecular Evolution', 'Docking', 'Dopamine', 'Elements', 'Engineering', 'Epidemic', 'Epinephrine', 'Evolution', 'Explosion', 'G-Protein-Coupled Receptors', 'Generations', 'Genes', 'Genetic', 'Histology', 'Human', 'Hybridomas', 'Image', 'Immune checkpoint inhibitor', 'Immune system', 'Immunization', 'Immunize', 'Immunoglobulin Fragments', 'Immunoprecipitation', 'Libraries', 'Machine Learning', 'Medical Research', 'Medicine', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monoclonal Antibodies', 'Neuraxis', 'Neurobiology', 'Neurosciences', 'Neurotransmitters', 'Nobel Prize', 'Outcome', 'Pathogen detection', 'Phage Display', 'Pharmaceutical Preparations', 'Pheromone', 'Play', 'Problem Solving', 'Process', 'Production', 'Protein Engineering', 'Proteins', 'Proteome', 'Public Health', 'Reagent', 'Research', 'Research Personnel', 'Role', 'Signal Transduction', 'Specificity', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Training', 'Tube', 'Update', 'V(D)J Recombination', 'Western Blotting', 'Yeasts', 'addiction', 'antibody engineering', 'antibody libraries', 'antigen binding', 'base', 'biomarker discovery', 'cancer therapy', 'cost', 'crowdsourcing', 'decision research', 'design', 'empowered', 'experimental study', 'follow-up', 'improved', 'in vivo', 'innovation', 'insight', 'interest', 'machine learning algorithm', 'nanobodies', 'new technology', 'novel', 'receptor', 'response', 'scaffold', 'structural biology', 'tool']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,1690552,-0.023014089974312552
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9952373,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biomedical Engineering', 'Blindness', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Consumption', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal macula', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neural network', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'retina circulation', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,564228,-0.01844619117644635
"A Transfer Learning Framework for Creating Subject-Specific Musculoskeletal Models of the Hand PROJECT SUMMARY Restoring hand function remains an elusive goal for many clinical conditions, including stroke, osteoarthritis, tetraplegia, amputation, and traumatic injury. The hand’s anatomical complexity makes restoring hand function particularly challenging because altering any one parameter in the hand can have cascading effects that are difficult to predict, but essential to control. In this proposal, as a critical step toward informing personalized treatments for the hand, we will study how subject-specific differences influence hand function. Completion of this proposal will rely on collection of three datasets that are designed to provide varying levels of biomechanical detail and require varying levels of effort to collect. Briefly, these datasets include (1) a simulation dataset containing 500,000 simulations fully describing all musculoskeletal parameters involved in hand force production, (2) a dense, biomechanical datasets that describes the kinematics, kinetics, and muscle activity required for hand force production in 30 adults, and (3) a sparse, clinically-inspired dataset that describes demographics, anthropometrics, and clinical metrics of hand function in 1000 adults. In Aim 1, we will leverage the first two datasets to design a data-driven analysis framework that identifies the most important biomechanical parameter(s) and maps how those parameters influence hand force production. Completion of this aim will elucidate the biomechanical mechanisms that modulate hand force production and evaluate the ability to use simulation data, instead of experimental data, to identify these mechanisms. In Aim 2, we will leverage all three datasets to create a transfer learning framework capable of efficiently and accurately predicting subject-specific muscle force-generating parameters from easy to collect clinical data. We specifically focus on muscle force- generating parameters because these parameters remain challenging to quickly and accurately estimate, are known to vary across the population, and are highly related to functional metrics like strength. Completion of this aim will provide a new approach for rapidly estimating subject-specific musculoskeletal parameters, thereby enabling efficient creation of subject-specific models and potentially catalyzing use of such models in a clinical setting. Overall, the results from this study could enhance our ability to provide personalized diagnoses and prognoses for individuals suffering from hand impairments. PROJECT NARRATIVE The proposed project aims to understand the biomechanical mechanisms underlying force production in the hand. Specifically, we utilize machine learning methods to examine how subject-specific differences influence hand force production and create subject-specific computer models from easy to obtain clinical data. The results, which integrate modeling with an individual’s clinical data, could enhance our ability to provide personalized diagnoses and prognoses for individuals suffering from hand impairments.",A Transfer Learning Framework for Creating Subject-Specific Musculoskeletal Models of the Hand,10040078,R21EB030068,"['Address', 'Adult', 'Amputation', 'Anatomy', 'Biomechanics', 'Clinical', 'Clinical Data', 'Code', 'Collection', 'Complex', 'Computer Models', 'Computer Simulation', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Diagnostic', 'Floor', 'Future', 'Goals', 'Hand', 'Hand Strength', 'Hand functions', 'Individual', 'Joints', 'Kinetics', 'Learning', 'Maps', 'Methods', 'Modeling', 'Muscle', 'Musculoskeletal', 'Musculoskeletal System', 'Outcome', 'Patients', 'Perception', 'Physics', 'Population', 'Production', 'Psychological Transfer', 'Quadriplegia', 'Research', 'Sensory', 'Stroke', 'Study Subject', 'System', 'Testing', 'Traumatic injury', 'Work', 'Wrist', 'base', 'bone', 'computational platform', 'computerized tools', 'deep neural network', 'demographics', 'design', 'experimental study', 'grasp', 'hand dysfunction', 'hand rehabilitation', 'individual patient', 'kinematics', 'machine learning method', 'motor control', 'neural network', 'neuromuscular', 'novel strategies', 'open-access repositories', 'personalized diagnostics', 'personalized medicine', 'prognostic', 'random forest', 'simulation', 'tool']",NIBIB,UNIVERSITY OF FLORIDA,R21,2020,560939,-0.01257679240342487
"THE SEARCH FOR COVID-19 PREVENTION AND CURE: ADDRESSING THE CRITICAL ROLE OF INNATE/ADAPTIVE IMMUNITY BY INTEGRATING NOVEL INFORMATICS, TRANSLATIONAL TECHNOLOGIES, AND ONGOING CLINICAL TRIAL RESEARCH Individual CTSA hubs are leading the national clinical and translational research efforts in developing new approaches to address the COVID-19 pandemic. This crucial role was natural. Long before the current crisis, CTSA hubs were committed to translation, building multidisciplinary teams of investigators and community partners, overcoming regulatory burdens, ensuring quality in clinical and human research, developing transformative informatics, and disruptive technologies for diagnostics and therapeutics. In this proposal, we build on our center’s active participation in meaningful clinical trials (e.g., the NIH Remdesivir RCT), the early creation of a biospecimen repository from COVID-19 patients, institutional commitment and fundraising that led to a $3.5 million pilot fund distribution, a robust and accessible clinical database repository, and the ongoing work of an NCATS-supported CTSA Collaboration Innovation Award (a coalition of the J. Craig Venter Institute, UCSD, UCI, and Stanford) focused on artificial intelligence approaches for the analysis of flow cytometry data. Using the emerging informatics framework of supervised generalized canonical correlation for integrative data analysis, we will link clinical data from COVID-19 patients enrolled in a variety of trials and at various stages of disease with innovative in vitro evaluation of innate and adaptive immunity, an area still poorly understood in SARS-CoV-2 pathology, obtained from patient biospecimens to obtain mechanistic insights of COVID-19 pathogenesis at a systems level. Innate and adaptive immunity are particularly relevant to COVID-19 disease pathogenesis because they play key, but distinct, roles at all phases of the illness (initial tissue-virus interaction; systemic responses; the cell-mediated cytokine storm leading to multi- organ failure and death, likely long after levels of viremia have fallen; and, ultimately, protective immunity). The current CCIA novel flow cytometry informatics research permits elucidation of dynamic cellular immune responses related to the COVID-19 pandemic that were heretofore unobservable. Using Hi-DAFi for mass cytometry analysis, validated informatics pipelines for single cell transcriptomics analysis, and cutting-edge statistical data integration and machine learning strategies tied back to the available clinical data we will be able to discover novel associations between cellular biomarkers and disease state, a particular therapy, and disease mediating factors such as age, health disparities, and the presence of other diseases or conditions like obesity. This information will aid in critical efforts to target new therapies and possibly identify idiosyncratic individual physiologic variables that render certain patients who seem to have no known comorbidities more vulnerable to severe COVID-19 disease. Finally, the robust connection between the UCI hub and both regional and national networks (e.g., BRAID, the coalition of the 5 UC CTSAs, and NCATS Trial Innovation Network) will provide an unprecedented opportunity to rapidly disseminate clinically relevant discoveries and engage the talent and insight of the many clinicians and scientists working tirelessly to end this pandemic. BLANK PER PA-18-591","THE SEARCH FOR COVID-19 PREVENTION AND CURE: ADDRESSING THE CRITICAL ROLE OF INNATE/ADAPTIVE IMMUNITY BY INTEGRATING NOVEL INFORMATICS, TRANSLATIONAL TECHNOLOGIES, AND ONGOING CLINICAL TRIAL RESEARCH",10158982,UL1TR001414,"['2019-nCoV', 'Acceleration', 'Address', 'Age', 'Antibody titer measurement', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'B-Lymphocytes', 'Back', 'Biological Assay', 'Biological Markers', 'Biological Specimen Banks', 'Biomedical Research', 'Blood Circulation', 'Blood specimen', 'COVID-19', 'COVID-19 pandemic', 'Cardiovascular system', 'Cells', 'Cessation of life', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Controlled Clinical Trials', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Enrollment', 'Ensure', 'Evaluation', 'Failure', 'Flow Cytometry', 'Foundations', 'Funding', 'Future', 'Genetic Determinism', 'Genetic Transcription', 'Goals', 'Health', 'Human', 'IL6 gene', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunologics', 'Immunology', 'Immunology procedure', 'In Vitro', 'Incidence', 'Individual', 'Industry', 'Infection', 'Inflammatory', 'Informatics', 'Institutes', 'Institution', 'Interleukin-10', 'Leadership', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Molecular', 'Monitor', 'Multiomic Data', 'Natural Immunity', 'Nucleic Acids', 'Obesity', 'Paper', 'Participant', 'Pathogenesis', 'Pathology', 'Patients', 'Phase', 'Physiological', 'Placebos', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Research', 'Research Personnel', 'Resources', 'Respiratory distress', 'Role', 'Sampling', 'Schedule', 'Scientist', 'Serologic tests', 'Severity of illness', 'Signal Transduction', 'Supervision', 'Symptoms', 'System', 'T-Lymphocyte', 'TNF gene', 'Talents', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Trials', 'Therapy Clinical Trials', 'Tissues', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Vaccines', 'Validation', 'Viral Load result', 'Viremia', 'Virus', 'Work', 'adaptive immunity', 'clinical database', 'clinically relevant', 'comorbidity', 'computer framework', 'computing resources', 'cytokine', 'cytokine release syndrome', 'data integration', 'data warehouse', 'dissemination research', 'early detection biomarkers', 'enzyme linked immunospot assay', 'experience', 'feature selection', 'health disparity', 'innovation', 'insight', 'interest', 'learning strategy', 'meetings', 'multidisciplinary', 'multiple omics', 'new therapeutic target', 'novel', 'novel strategies', 'pandemic disease', 'predictive marker', 'remdesivir', 'repository', 'respiratory', 'response', 'statistical and machine learning', 'transcriptome sequencing', 'transcriptomics', 'treatment response', 'virus genetics']",NCATS,UNIVERSITY OF CALIFORNIA-IRVINE,UL1,2020,1088735,-0.02262910741417481
"Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1 Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at certain stages of the disease severity spectrum, specifically in the early stage and in advanced disease. These difficulties are due to a variety of causes that change over the course of the disease, including large between-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we build on our long-standing contribution to ocular imaging and propose novel and sensitive means to detect glaucoma and its progression that are optimized to the various stages of disease severity. We will use information gathered from visual fields (functional information) and a leading ocular imaging technology – optical coherence tomography (OCT; structural information) to map the capability of detecting changes across the entire disease severity spectrum to identify optimal parameters for each stage of the disease. Both commonly used parameters provided by the technologies and newly developed parameters with good diagnostic potential will be analyzed. We will use state-of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. We will also utilize a new imaging technology, the visible light OCT, to generate retinal images with outstanding resolution to extract information about the oxygen saturation of the tissue. This will provide in-vivo, real time, and noninvasive insight into tissue functionality. Taken together, this program will advance the use of structural and functional information with a substantial impact on the clinical management of subjects with glaucoma Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies that will substantially improve detection of glaucoma and its progression monitoring in order to prevent blindness.",Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1,10019553,R01EY013178,"['3-Dimensional', 'Blindness', 'Characteristics', 'Clinical', 'Clinical Management', 'Clinical Research', 'Complex', 'Data', 'Detection', 'Development', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Evaluation', 'Eye', 'Floor', 'Future', 'Glaucoma', 'Health', 'Human', 'Image', 'Imaging technology', 'Inner Plexiform Layer', 'Knowledge', 'Laboratories', 'Lead', 'Light', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Oxygen Consumption', 'Oxygen saturation measurement', 'Pathology', 'Research Proposals', 'Resolution', 'Retina', 'Retinal Diseases', 'Scanning', 'Severities', 'Severity of illness', 'Signal Transduction', 'Source', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Technology', 'Thick', 'Time', 'Tissue Extracts', 'Tissues', 'Translating', 'Visible Radiation', 'Vision', 'Visual Fields', 'Width', 'advanced disease', 'analytical method', 'base', 'clinical practice', 'cohort', 'computerized', 'deep learning', 'density', 'ganglion cell', 'improved', 'in vivo', 'innovation', 'innovative technologies', 'insight', 'instrument', 'invention', 'knowledge base', 'longitudinal dataset', 'machine learning method', 'macula', 'mathematical methods', 'new technology', 'novel', 'novel strategies', 'ocular imaging', 'preservation', 'prevent', 'programs', 'research study', 'retinal imaging', 'retinal nerve fiber layer', 'tissue oxygenation', 'tool']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,687519,-0.022542957649557997
"Learning to learn in structural biology with deep neural networks Project Summary/Abstract  Deep learning is gaining traction across many elds as a powerful tool. In medicine, there have been recent successes in drug design, predicting protein structure, and in functional genomics. These successes have thus far been in areas where there are hundreds of thousands of data points and deep learning in medicine is still limited by lack of large homongeous datasets.  This proposal focuses on applying a new kind of deep learning called meta-learning that mimics the human-like ability to learn from few examples. The PI will establish a sustainable research program on meta-learning by developing benchmark problems and datasets. The PI will further explore meta-learning speci cally on peptide-protein structure and NMR spectra prediction. Due to the imperative need for interpretability when using deep learning in medicine, a strong component will be connecting biophysical modeling with the deep learning models.  The outcome of this work will be a demonstrated new approach to deep learning that can work with little data. The PI will bring these research ideas together to design peptides that can bind to intrinsically disordred proteins, a challenging but important task for curing neurodegenerative diseases. This will be accomplished through meta-learning, molecular simulation, and iterative peptide design. Project Narrative  Deep learning is a technique from arti cial intelligence that has driven many high-pro le break- throughs in recognizing objects in images, translating human languages, and playing games like Chess and Go. Its use is medicine is currently limited by deep learning's need for large amounts of data and its lack of interpretability. This researh plan works towards solving these challenges and applies interpretable deep learning to designing new therapeutics.",Learning to learn in structural biology with deep neural networks,10027477,R35GM137966,"['Area', 'Benchmarking', 'Binding', 'Data', 'Data Set', 'Drug Design', 'Human', 'Image', 'Intelligence', 'Language', 'Learning', 'Medicine', 'Modeling', 'Molecular', 'Neurodegenerative Disorders', 'Outcome', 'Peptides', 'Play', 'Proteins', 'Research', 'Techniques', 'Traction', 'Translating', 'Work', 'biophysical model', 'deep learning', 'deep neural network', 'design', 'functional genomics', 'novel strategies', 'novel therapeutics', 'programs', 'protein structure', 'simulation', 'structural biology', 'success', 'tool']",NIGMS,UNIVERSITY OF ROCHESTER,R35,2020,360287,-0.035540549756338094
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,10022125,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'MeSH Thesaurus', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'large datasets', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2020,378983,-0.03932133366598671
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,10145183,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'MeSH Thesaurus', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'large datasets', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2020,10920,-0.03932133366598671
"Objective Quantification of Neural Damage for Screening, Diagnosis and Monitoring of Glaucoma with Fundus Photographs PROJECT SUMMARY Glaucoma is a progressive optic neuropathy and the leading cause of irreversible blindness in the world. As the disease remains largely asymptomatic until late stages, there is a pressing need to develop affordable approaches for screening before visual impairment occurs. Although sophisticated imaging technologies such as Spectral domain-optical coherence tomography (SDOCT) can provide highly reproducible and accurate quantitative assessment of glaucomatous damage, their application in widespread screening or non-specialized settings is unfeasible, given the high cost and operator requirements. Fundus photography is a low-cost alternative that has been used successfully in teleophthalmology programs. However, subjective human grading of fundus photos for glaucoma is poorly reproducible and highly inaccurate, as gradings tend to largely over- or underestimate damage. We propose a new paradigm for assessing glaucomatous damage by training a deep learning (DL) convolutional neural network to provide quantitative estimates of the amount of neural damage from fundus photographs. In our Machine-to-Machine (M2M) approach, we trained a DL network to analyze fundus photos and predict quantitative measurements of glaucomatous damage provided by SDOCT, such as retinal nerve fiber layer (RNFL) thickness and neuroretinal rim measurements. Our preliminary results showed that the M2M predictions have very high correlation and agreement with the original SDOCT observations. This provides an objective method to quantify neural damage in fundus photos without requiring human graders, which could potentially be used for screening, diagnoses and monitoring in teleophthalmology and non- specialized point-of-care settings. In this proposal, we aim at refining and validating the M2M model in suitable, large datasets from population-based studies, electronic medical records, and clinical trial data. Our central hypothesis is that the M2M approach will be more accurate than subjective human gradings in screening, diagnosing, predicting and detecting longitudinal damage over time. In Aim 1, we will investigate the performance of the M2M model to screen for glaucomatous damage using large datasets from 6 population-based studies: Blue Mountains Eye Study, Los Angeles Latino Eye Study, Tema Eye Survey, Beijing Eye Study, Central India Eye and Medical Study and the Ural Eye and Medical Study, which will provide data on over 25,000 subjects of diverse racial groups. In Aim 2, we will investigate the ability of the M2M model to predict future development of glaucoma in eyes of suspects using the data from the Ocular Hypertension Treatment Study (OHTS). In Aim 3, we will investigate the ability of the M2M model in detecting glaucomatous progression over time using data from the Duke Glaucoma Registry, a large database of longitudinal structure and function data in glaucoma with over 25,000 patients followed over time. If successful, this proposal will lead to a validated, inexpensive, and widely applicable tool for screening, early diagnosis and monitoring of glaucoma, that could be applied under population-based settings and also at non-specialized point-of-care settings. Project Narrative Glaucoma is a leading cause of irreversible visual impairment in the world. This proposal will employ a novel artificial intelligence paradigm for quantifying neural damage on ocular fundus photographs for the purpose of screening, diagnosing and monitoring glaucoma damage. The approach will be validated on large datasets from population-based studies, electronic medical records and clinical trial data.","Objective Quantification of Neural Damage for Screening, Diagnosis and Monitoring of Glaucoma with Fundus Photographs",10047364,R21EY031898,"['Agreement', 'Artificial Intelligence', 'Blindness', 'Clinical Trials', 'Computerized Medical Record', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Exhibits', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Future', 'Glaucoma', 'Human', 'Imaging technology', 'India', 'Investigation', 'Label', 'Latino', 'Los Angeles', 'Manuals', 'Measurement', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Names', 'Nature', 'Ocular Hypertension', 'Ophthalmology', 'Optical Coherence Tomography', 'Output', 'Patients', 'Performance', 'Population Study', 'Race', 'Reference Standards', 'Registries', 'Reproducibility', 'Risk', 'Science', 'Screening procedure', 'Structure', 'Surveys', 'Testing', 'Thick', 'Time', 'Training', 'Validation', 'Visual impairment', 'algorithm training', 'clinical care', 'convolutional neural network', 'cost', 'cost effective', 'deep learning', 'deep learning algorithm', 'deep neural network', 'flexibility', 'hypertension treatment', 'intelligent algorithm', 'interest', 'large datasets', 'learning network', 'longitudinal database', 'novel', 'optic nerve disorder', 'point of care', 'population based', 'predictive modeling', 'programs', 'racial diversity', 'relating to nervous system', 'retinal nerve fiber layer', 'screening', 'time use', 'tool']",NEI,DUKE UNIVERSITY,R21,2020,241500,-0.01971293470346447
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",9946212,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Communities', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2020,611043,-0.026823187199537135
"Deep Learning and Fluid Dynamics Based Phenotyping of Expiratory Central Airway Collapse Project Summary/ Abstract Expiratory central airway collapse (ECAC), defined by >50% collapse of large airways during expiration, resulting from either cartilaginous weakening or redundancy of the posterior membranous wall of the trachea, is an increasingly recognized disorder associated with cigarette smoking and chronic obstructive pulmonary disease (COPD). Airflow obstruction in smokers primarily arises from increased resistance to airflow in the small distal conducting airways <2 mm in diameter. It is plausible that in a subset of smokers with and without COPD, central airway collapse results in additional resistance to airflow, resulting in substantial respiratory morbidity. Ninety-two million adults in the Unites States are active or past smokers, and ECAC is present in approximately 5% of current and former smokers. The presence of ECAC is associated with greater dyspnea, worse respiratory-quality of life and greater frequency of exacerbations after adjustment for underlying lung disease. Whether these patients will benefit from interventional therapies such as stenting or tracheopexy depends on whether the airflow resistance caused by ECAC contributes to symptoms, and this in turn depends on the relative contribution of central and small airways to overall airflow resistance. If the overall airflow resistance is primarily due to distal small airways obstruction in a given patient with ECAC, treating central airway collapse is unlikely to benefit such a patient. Our central hypothesis is that ECAC results in additional airflow obstruction beyond that incurred in the small airways, and that in a subset of patients the central airways are the major site of airflow obstruction and hence are amenable to therapy. The complex interplay of proximal and distal airway resistances and transpulmonary pressures does not lend itself to direct measurements in human subjects across a range of physiological pressure and flow changes. We propose a combination of CT-derived imaging and patient-personalized benchtop model and deep learning to answer these questions with the following specific aims. Aim 1 of this application will be to derive personalized patient- specific information on airway geometry and resistance using airway segmentation from computed tomography (CT) scans. We will calculate airway resistances in central and small airways using standard formulae. The goal of Aim 2 is to create bench-top simulations to understand the complex interplay between the resistance of small and large airways. In Aim 3, we will use deep learning to derive probability scores for clinically substantial ECAC from segmented airway images on computed tomography. The results of our study will enable patient-specific personalized therapies for ECAC. The mechanistic insights gained from this study will help identify patients with clinically significant ECAC and hence most likely to benefit from therapeutic interventions. PROJECT NARRATIVE Expiratory central airway collapse (ECAC), greater than 50% collapse of the large airways during expiration, is present in 5% of chronic smokers, and is associated with substantial respiratory morbidity disproportionate to underlying lung disease. Resistance to airflow in smokers is thought to primarily occur in the small conducting airways. By using benchtop models and deep learning to determine the effect of central airway collapse on overall airway resistance and its contribution to airflow obstruction relative to small airway resistance, the proposed project will identify patients with ECAC who will benefit from intervention, and cause a paradigm shift in the therapy of these patients.",Deep Learning and Fluid Dynamics Based Phenotyping of Expiratory Central Airway Collapse,10013198,R21EB027891,"['3-Dimensional', '3D Print', 'Adult', 'Affect', 'Age', 'Air Movements', 'Airway Resistance', 'Area', 'Body mass index', 'Breathing', 'Caliber', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complex', 'Data', 'Data Set', 'Databases', 'Disease', 'Distal', 'Dyspnea', 'Exhalation', 'Forced expiratory volume function', 'Frequencies', 'Generations', 'Geometry', 'Goals', 'Image', 'Individual', 'Intervention', 'Length', 'Liquid substance', 'Lung diseases', 'Maps', 'Measurement', 'Methods', 'Modeling', 'Neural Network Simulation', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physiological', 'Probability', 'Pulmonary Emphysema', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Quality of life', 'Race', 'Resistance', 'Scanning', 'Site', 'Smoker', 'Smoking', 'Spirometry', 'Stents', 'Symptoms', 'Testing', 'Therapeutic Intervention', 'Trachea', 'Training', 'Translations', 'Tube', 'United States', 'Visualization', 'X-Ray Computed Tomography', 'airway obstruction', 'base', 'cartilaginous', 'cigarette smoking', 'clinical application', 'clinical predictors', 'clinically significant', 'convolutional neural network', 'deep learning', 'expiration', 'human subject', 'individual patient', 'insight', 'patient subsets', 'personalized medicine', 'pressure', 'respiratory', 'respiratory morbidity', 'response', 'sex', 'simulation', 'three-dimensional modeling']",NIBIB,UNIVERSITY OF ALABAMA AT BIRMINGHAM,R21,2020,187049,-0.024870057000120818
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.0178412472074193
"Optical Coherence Elastography of the Cornea PROJECT SUMMARY The fundamental physical properties of the outer tunic of the eye determine the structural characteristics of the ocular globe and may be altered in several devastating disease states including axial elongation in myopia, pathological deformation in keratoconus, and iatrogenic keratoectasia following corneal refractive surgery. These biomechanical tissue characteristics not only influence our clinical interpretation of diagnostic tests, e.g. measurement of intraocular pressure, but have been implicated as important factors in the development of glaucoma. Currently, there is no available reliable method to perform quantitative measurement of corneal elasticity in vivo. Here we will develop novel method for the assessment of corneal elastic properties that could potentially be used for routine clinical diagnostic and treatment. This method will take advantages of highly localized air pressure stimulation and ultra-sensitive detection and analysis of the pressure waves propagation on corneal posterior and anterior surfaces with a line-field Optical Coherence Tomography to reconstruct volumetric biomechanical properties of the cornea. Our previous work has made fundamental advances in the understanding of corneal biomechanics through a novel approach with potentially impactful applications in other disciplines (e.g. cataract surgery, LAISK, corneal cross-linking, and tissue transplants with personalize treatments). The proposed studies will accelerate transition of this technology into clinics, influence our selection and application of corneal surgical treatments and will help us to understand the structural consequences of corneal disease and wound healing: Aim 1. Develop a line-field OCE (LF-OCE) system for ultrafast 3D clinical imaging. Aim 2. In vivo studies with rabbits. Aim 3. Preliminary clinical studies in humans. Aim 4. Refine numerical (FEM) and Artificial Intelligence (AI) models of the depth-dependent nonlinear viscoelastic properties of the cornea. PROJECT NARRATIVE This proposal will focus on the development of novel technology and methods for noninvasive assessment of biomechanical properties of the cornea. Development of such a technique would significantly advance our understanding of the corneal disorders, allow developing novel clinical therapies and interventions, and improve outcome of current surgical ant therapeutic interventions.",Optical Coherence Elastography of the Cornea,10063805,R01EY022362,"['3-Dimensional', 'Achievement', 'Agreement', 'Air Pressure', 'Animals', 'Anisotropy', 'Anterior', 'Ants', 'Artificial Intelligence', 'Beds', 'Biological', 'Biomechanics', 'Cataract Extraction', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Treatment', 'Connective Tissue', 'Cornea', 'Corneal Diseases', 'Custom', 'Dependence', 'Detection', 'Development', 'Diagnostic', 'Diagnostic tests', 'Discipline', 'Disease', 'Elasticity', 'Eye', 'Glaucoma', 'Goals', 'Heterogeneity', 'Human', 'Iatrogenesis', 'Image', 'Individual', 'Intervention', 'Keratoconus', 'Knowledge', 'Laser In Situ Keratomileusis', 'Link', 'Location', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Morphologic artifacts', 'Motion', 'Myopia', 'Nature', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Optics', 'Oryctolagus cuniculus', 'Outcome', 'Pathologic', 'Patients', 'Physiologic Intraocular Pressure', 'Physiological', 'Property', 'Protocols documentation', 'Reaction', 'Reporting', 'Research', 'Routine Diagnostic Tests', 'Shapes', 'Signal Transduction', 'Speed', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Intervention', 'Time', 'Tissue Transplantation', 'Tissues', 'Training', 'Tunic', 'Validation', 'Variant', 'Work', 'base', 'biomechanical model', 'clinical diagnostics', 'clinical imaging', 'clinical translation', 'clinically significant', 'convolutional neural network', 'corneal epithelial wound healing', 'crosslink', 'deep learning', 'denoising', 'design', 'elastography', 'improved', 'improved outcome', 'in vivo', 'insight', 'mechanical properties', 'models and simulation', 'new technology', 'novel', 'novel strategies', 'personalized medicine', 'physical property', 'pressure', 'response', 'success', 'viscoelasticity', 'visual tracking']",NEI,UNIVERSITY OF HOUSTON,R01,2020,410000,-0.006118298849428086
"Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment Summary: The public health burden of major depressive disorder (MDD) is immense and current approaches for selecting antidepressant treatment have had limited success. By some estimates, fewer than one in three MDD patients will respond to their prescribed antidepressant and the quest for a treatment that will work is typically characterized by a lengthy course of trial-and-error. The need to identify patient characteristics (biomarkers) that can be used to objectively select personalized antidepressant treatment is clear. Accordingly, large clinical studies like the NIMH-funded Establishing Moderators and Biosignatures of Antidepressant Response for Clinical Care (EMBARC) study have collected massive amounts of baseline measures including those from various neuroimaging sources in the hope that some can be used to guide antidepressant treatment selection. These data bring with them many statistical challenges that have yet to be effectively addressed. These challenges include (1) dealing with high-dimensionality, (2) handling data missingness, and (3) determining how best to simultaneously model relationships between measures from multiple imaging modalities and the response of interest. The goal of this project is to acquire the essential training and experience to make significant progress in this area by addressing each of these challenges. Aim 1 of this project will employ state-of-the-art ensemble machine learning algorithms and targeted estimation to identify moderators of antidepressant treatment effect using scalar clinical, demographic, and summary neuroimaging data from clinical trials of antidepressant treatments, including EMBARC. Strategies for handling missing data in this context will also be investigated and guidelines on best practices will be proposed. Aim 2 will extend the methods used in Aim 1 and develop user-friendly software to directly incorporate high- dimensional multimodal neuroimaging data into treatment decision rules. Included in this aim will be an investigation into best practices for handling missing high-dimensional imaging data in the context of estimating treatment decision rules. Aim 3 will employ the novel methods developed in Aim 2 and the estimated treatment decision rules will be evaluated and compared with those developed in Aim 1. I have put together a training program that directly supports the completion of these research aims. It includes instruction, mentoring, and hands-on-experience (1) in psychopathology and the neural basis for psychiatric disorders and treatment for those disorders; (2) in the use of neuroimaging data to understand depression and response to antidepressant treatment; (3) in the use of modern algorithms to store, process, manipulate, and analyze big biomedical data like those arising in multimodal neuroimaging studies. This K01 Mentored Research Scientist Development Award will provide the training, time, and resources to be able to make substantial progress in addressing this important problem and will provide the skills and experience that will be crucial in my transition to an independent investigator. Public Health Relevance Statement: This proposal seeks to advance precision medicine through the development of new statistical methods that integrate clinical, demographic, and high-dimensional multimodal neuroimaging data to estimate treatment decision rules. The proposed research and training are laid out in the context of depression but the statistical tools to be developed will be general enough for constructing treatment decision rules for a wide array of diseases using a variety of data types. These statistical tools have the potential to reduce the burden of diseases like depression by providing personalized treatment that has the best chance for success.",Statistical and Machine Learning Methods for Integrating Clinical and Multimodal Imaging Data to Select Optimal Antidepressant Treatment,9994391,K01MH113850,"['Address', 'Algorithms', 'Antidepressive Agents', 'Area', 'Automobile Driving', 'Award', 'Biological Markers', 'Brain', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease remission', 'Funding', 'Goals', 'Guidelines', 'Heterogeneity', 'Image', 'Instruction', 'Investigation', 'Knowledge', 'Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Multimodal Imaging', 'National Institute of Mental Health', 'Neurosciences', 'Outcome', 'Patients', 'Performance', 'Placebos', 'Play', 'Positron-Emission Tomography', 'Process', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Public Health', 'Quality of Care', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Risk', 'Role', 'Selection for Treatments', 'Sertraline', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Training', 'Training Programs', 'Work', 'Writing', 'big biomedical data', 'biosignature', 'burden of illness', 'clinical care', 'clinical imaging', 'data reduction', 'diverse data', 'experience', 'flexibility', 'functional magnetic resonance imaging/electroencephalography', 'health data', 'high dimensionality', 'imaging modality', 'individual patient', 'individualized medicine', 'innovation', 'insight', 'interest', 'large scale data', 'machine learning algorithm', 'machine learning method', 'multimodality', 'neuroimaging', 'neuroimaging marker', 'novel', 'personalized medicine', 'precision medicine', 'public health relevance', 'relating to nervous system', 'response', 'skills', 'statistical and machine learning', 'success', 'therapy development', 'tool', 'treatment effect', 'treatment response', 'user friendly software']",NIMH,GEORGE WASHINGTON UNIVERSITY,K01,2020,162343,-0.0018429905443947856
"Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure Project Summary Glaucoma is the second leading cause of blindness globally, and is characterized by optic nerve damage that leads to the death of retinal ganglion cells with accompanying visual field (VF) loss. The optic nerve head (ONH) is the site of injury to the optic nerve fibers and plays a central role in glaucoma pathogenesis and diagnosis. Traditionally, glaucoma is diagnosed based on fundus inspection of the ONH, which provides information about the surface contour of the ONH. However, the optic nerve damage occurs in the deeper layers. With the development of optical coherence tomography (OCT) techniques for three-dimensional (3D) retinal imaging, parameters derived from the 3D ONH related structure (e.g., Bruch's membrane opening minimum rim width, peripapillary retinal nerve fiber layer thickness, disc tilt etc.) have been studied to better understand glaucoma pathogenesis, and are used to supplement clinical diagnosis. In addition, studies of the ONH biomechanics have also shown that the strain level at the ONH at any given intraocular pressure level depends on the 3D geometry of the ONH related structure. A high strain level is hypothesized to contribute to retinal ganglion cell injury. Previous research has suggested that the 3D ONH related structure is correlated to glaucoma pathogenesis and critically important to glaucoma diagnosis. However, to date, a systematic study using clinical data to determine the impact of the 3D ONH related structure on glaucoma has not been conducted.  We propose to study the relationship between the 3D ONH related structure and glaucoma with a diverse set of combined techniques including image processing, computational mechanics and machine learning. The specific aims of this project are to: (1) Derive features from the 3D ONH related structure and study their implications on VF loss patterns (K99 Phase). (2) Investigate the impact of the strain field patterns at the ONH on glaucoma (K99 Phase). (3) Study the effect of the 3D ONH related features on OCT diagnostic parameters (R00 Phase). (4) Model central vision loss from the 3D ONH related structural features (R00 Phase). Collectively, these studies will provide new insights and perspectives into the structure-function relationships in glaucoma and establish ocular anatomy specific norms of retinal nerve fiber layer profiles, which will advance our current understanding of glaucoma pathogenesis and improve glaucoma diagnosis. Our research is of high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.  Through the proposed research and training plans, the applicant will build a solid knowledge base in ophthalmology and further improve his expertise in mathematical modeling and data science. This project will provide critical training opportunities to further enhance the applicant's capabilities to become an independent computational vision scientist in ophthalmology. Project Narrative The proposed research will study the relationship between the three-dimensional (3D) optic nerve head (ONH) related structure and glaucoma. Our study will advance the current understanding of glaucoma pathogenesis and improve glaucoma diagnosis by gaining new insights into the structure-function relationships in glaucoma and establishing ocular anatomy specific norms of retinal nerve fiber layer profiles. Our research has high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.",Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure,9857605,K99EY028631,"['3-Dimensional', 'Age', 'Anatomy', 'Biomechanics', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Cessation of life', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Cross-Sectional Studies', 'Data', 'Data Science', 'Development', 'Diagnosis', 'Diagnostic', 'Ear', 'Elements', 'Eye', 'Fundus', 'Gaussian model', 'Geometry', 'Glaucoma', 'Hour', 'Image', 'Individual', 'Injury', 'Lead', 'Linear Models', 'Linear Regressions', 'Location', 'Machine Learning', 'Measurement', 'Mechanics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Multivariate Analysis', 'Nerve Fibers', 'Observational Study', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Pathogenesis', 'Patients', 'Pattern', 'Phase', 'Physiologic Intraocular Pressure', 'Play', 'Population Study', 'Process', 'Quality of life', 'Research', 'Research Training', 'Resolution', 'Retinal Ganglion Cells', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Severities', 'Site', 'Solid', 'Source', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Techniques', 'Testing', 'Thick', 'Translating', 'Variant', 'Vision', 'Visual Fields', 'Width', 'archetypal analysis', 'base', 'cell injury', 'clinical Diagnosis', 'clinical practice', 'clinically relevant', 'deep neural network', 'demographics', 'fundus imaging', 'image processing', 'improved', 'independent component analysis', 'insight', 'knowledge base', 'machine learning method', 'mathematical model', 'nonlinear regression', 'optic cup', 'retina blood vessel structure', 'retinal imaging', 'retinal nerve fiber layer', 'study population', 'training opportunity', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,K99,2020,145891,-0.021459179362040944
"Automatic analysis of 3D skin images for chronic graft-versus-host disease (cGVHD) severity assessment Project Summary More than 20,000 hematopoietic stem cell transplants (including bone marrow transplants) are performed in the U.S. each year to cure a range of diseases ranging from leukemias to sickle cell anemia to autoimmune deficiencies in children. Unfortunately, most long-term non-relapse survivors will die of chronic graft-versus-host disease (cGVHD), which remains a disease of steadily increasing incidence and profound unmet need. A fundamental barrier in cGVHD management and research is a lack of sensitive and objective assessment tools that permit objective and reproducible measures of disease severity and progression. Skin is the most commonly affected organ in cGVHD and automated techniques capable of measuring precisely the surface area of involved skin in photographs may provide the tools necessary for effectively evaluating patient progress. We propose to (1) create the data set necessary to develop machine learning-based methods for the automatic analysis of cGVHD images, and (2) implement and evaluate these methods. Project Narrative  Chronic graft-versus-host disease (cGVHD) is a lethal disease that affects most long-term hematopoietic stem cell transplant (including bone marrow transplants) recipients. Skin images are used to assess disease severity and progression but the technology required to quantitatively and reproducibly analyze these images is lacking. This project aims at developing and evaluating this technology.",Automatic analysis of 3D skin images for chronic graft-versus-host disease (cGVHD) severity assessment,9864040,R21AR074589,"['3-Dimensional', 'Achievement', 'Affect', 'Agreement', 'Allogenic', 'Area', 'Assessment tool', 'Autoimmune Process', 'Body Surface', 'Bone Marrow Transplantation', 'Characteristics', 'Child', 'Circumscribed Lesion', 'Clinic', 'Clinical', 'Clinical Research', 'Cutaneous', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dermatologic', 'Dermatologist', 'Disease', 'Disease Management', 'Disease Progression', 'Documentation', 'Erythema', 'Exanthema', 'Future', 'Goals', 'Hematologic Neoplasms', 'Hematopoietic Stem Cell Transplantation', 'Hematopoietic System', 'Human', 'Image', 'Incidence', 'Industry', 'Institution', 'Label', 'Lead', 'Lesion', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Methods', 'Morbidity - disease rate', 'Organ', 'Outcome', 'Outcome Measure', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Population', 'Protocols documentation', 'Psoriasis', 'Reaction', 'Reproducibility', 'Research', 'Resources', 'Role', 'Scanning', 'Severities', 'Severity of illness', 'Sickle Cell Anemia', 'Site', 'Skin', 'Skin Cancer', 'Standardization', 'Surface', 'Survivors', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Three-dimensional analysis', 'Time', 'Transplant Recipients', 'Visit', 'Vitiligo', 'automated analysis', 'base', 'cancer imaging', 'chronic graft versus host disease', 'data warehouse', 'deep learning', 'deep neural network', 'digital', 'graft vs host disease', 'high risk', 'image processing', 'improved', 'interdisciplinary approach', 'learning strategy', 'leukemia', 'machine vision', 'mortality', 'network architecture', 'neural network architecture', 'novel therapeutics', 'patient subsets', 'prototype', 'repository', 'response', 'skin disorder', 'skin lesion', 'stereoscopic', 'success', 'tool']",NIAMS,VANDERBILT UNIVERSITY,R21,2020,201692,-0.023708141006619988
"Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1 PROJECT SUMMARY Previous studies showed discrepancies of health and behavior prevalence between American Indians (AI) population and other racial or ethnic groups. Most health surveys have certain limitations when studying AI population due to the small sample sizes for AI population. Data collected by AI Tribal Epidemiology Centers (TECs) provides an excellent opportunity to conduct research for AI population due to sufficient sample size and extensive information. However, most surveys conducted by TECs used non-probability sampling design (e.g. convenient sample) due to its lower cost and increased time efficiency. Non-probability sample may suffer from sampling, coverage and nonresponse errors without further proper adjustments. Such difficulties greatly hampers the analysis of AI population in health and behavior research. Our general hypothesis is that data integration by combining information from non-probability and probability samples can reduce sampling, coverage and nonresponse errors in original non-probability sample. The Goal of this project is to develop an accurate and robust data integration methodology for AI population analysis specifically tailored to health and behavior research. During the past years, we have 1) studied data integration using calibration and parametric modeling approaches; 2) investigated machine learning and propensity score modeling methods in survey sampling and other fields; and 3) assembled an experienced team of multi-disciplinary team of experts. In this project, we propose to capitalize on our expertise and fulfill the following Specific Aims: Aim 1. Develop a data integration approach using machine learning and propensity score modeling We will develop machine learning and propensity score based data integration approaches to combine information from non-probability and probability samples. Compared to existing methods (i.e., Calibration, Parametric approach), our proposed approaches are more robust against the failure of underlying model assumptions. The inference is more general and multi-purpose (e.g. one can estimate most parameters such as means, totals and percentiles). Simulation studies will be performed to compare our proposed methods with other existing methods. A computing package will be built to implement the method in other settings. Aim 2. Evaluate the accuracy and robustness of the proposed method in AI health and behavior research We will use real data to validate the proposed methods in terms of accuracy and robustness to the various data types. The performance will also be assessed by comparing with results from existing data integration methods such as calibration and parametric modeling approaches. The planned study takes advantage of a unique data source and expands the impact of the Indian Health Service (IHS)-funded research. We expect this novel integration method will vertically advance the field by facilitating the analysis based on non-probability sample, which can provide in-depth understanding regarding the AI population health and behavior studies. Project Narrative The overall goal of this R21 project is to develop an accurate, robust and multi-purpose data integration methodology for AI population (non-probability sample) analysis specifically tailored to health and behavior research such as diabetes and smoking. The code implementing the proposed method will be released and is general enough to be applied to AI population studies of other fileds. The success of this study will vertically advance the field by facilitating the AI population analysis, which can provide a better guidance and new insights on the future precision personalized prevention and treatment of certain diseases.",Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1,10063407,R21MD014658,"['Adult', 'Age', 'American', 'American Indians', 'Behavioral', 'Behavioral Risk Factor Surveillance System', 'Calibration', 'Censuses', 'Code', 'Communities', 'Community Surveys', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Disease', 'Epidemiology', 'Ethnic group', 'Event', 'Failure', 'Funding', 'Future', 'General Population', 'Geographic state', 'Goals', 'Health', 'Health Fairs', 'Health Surveys', 'Health behavior', 'High Prevalence', 'Kansas', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Not Hispanic or Latino', 'Oklahoma', 'Performance', 'Population', 'Population Analysis', 'Population Study', 'Prevalence', 'Probability', 'Probability Samples', 'Publishing', 'Race', 'Research', 'Research Personnel', 'Respondent', 'Risk Factors', 'Sample Size', 'Sampling', 'Smoking', 'Surveys', 'Target Populations', 'Testing', 'Texas', 'Time', 'Tobacco', 'Training', 'United States Indian Health Service', 'Weight', 'Work', 'Youth', 'base', 'behavioral study', 'cigarette smoking', 'cluster computing', 'cost', 'data integration', 'data quality', 'design', 'experience', 'improved', 'individualized prevention', 'innovation', 'insight', 'multidisciplinary', 'novel', 'personalized medicine', 'population health', 'simulation', 'smoking prevalence', 'success', 'therapy development', 'tribal health']",NIMHD,UNIVERSITY OF OKLAHOMA HLTH SCIENCES CTR,R21,2020,115176,-0.0061859872950899755
"Effects of Chronic Kidney Disease on Cardiovascular Disease and Dementia Among People with Diabetes: Causal Modeling with Machine Learning Approach PROJECT SUMMARY/ABSTRACT Diabetes has been the major public health issues imposing substantial health and economic burden on individuals and society. Given the Sustainable Development Goals (SDGs) in which United Nations has resolved to reduce morbidity and mortality from non-communicable diseases by one-third by year 2030, understanding the major risk factors of long-term adverse health outcomes such as cardiovascular disease (CVD) among patients with diabetes are imperative. While chronic kidney disease (CKD) and depression are closely interrelated with both diabetes and CVD, the causal link between these non-communicable diseases have not been sufficiently established. This is possibly due to (1) ill-defined temporality (i.e. unclear time- ordering of disease occurrence) and (2) their complex multifactorial and high-dimensional interaction with potential confounders such as demographic characteristics, socio-economic status, and comorbidities. The overall objective of this application is to investigate the causal relationship between diabetes and its complications including CKD. My specific aims are as follows: Aim 1 (F99 phase) assesses the causal relationship between depression and CVD among people with diabetes. After summarizing the previous literature, I will utilize longitudinal data to examine the joint effect of diabetes and depression on CVD sufficiently considering time-dependent exposure and confounders. Aim 2 (K00 phase) examines the causal pathway from diabetes to CKD, and to CVD mortality. I will develop the machine learning-based prediction model of CKD among people with diabetes, and then estimate the effect of CKD on CVD mortality using the obtained prediction model within causal inference structure. I will also investigate the extent to which CKD mediates the pathway from diabetes to CVD mortality. This study presents a timely opportunity to contribute to growing literature on how these non-communicable diseases (i.e. diabetes, depression, CKD, and CVD) interact with each other. Moreover, applications of machine learning in causal inference structure will contribute to the “precision health” concept by targeting high-risk populations and design effective interventions to prevent future non-communicable diseases and their complications. PROJECT NARRATIVE Diabetes, depression, chronic kidney disease (CKD), and Cardiovascular disease (CVD) are major health concerns imposing substantial health and economic burden on individuals and society. This study proposes to estimate the effects of depression on CVD events among people with diabetes (F99 phase) and investigate the causal relationship between diabetes, CKD, and CVD mortality using machine learning within the causal inference structure (K00 phase). This project will provide research and training, contributing a much-needed analysis of diabetes and CKD on long-term adverse health outcomes as well as helping me develop into an independent researcher with expertise in diabetic kidney disease epidemiology.",Effects of Chronic Kidney Disease on Cardiovascular Disease and Dementia Among People with Diabetes: Causal Modeling with Machine Learning Approach,10059131,F99DK126119,"['Address', 'Adult', 'Affect', 'Aging', 'Area', 'Award', 'Big Data', 'Biological Markers', 'Cardiovascular Diseases', 'Cessation of life', 'Characteristics', 'Chronic Kidney Failure', 'Complex', 'Data', 'Dementia', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Dimensions', 'Disease', 'Disease Outcome', 'Economic Burden', 'Epidemiologic Methods', 'Epidemiology', 'Event', 'Fostering', 'Future', 'General Population', 'Genetic', 'Goals', 'Health', 'Incidence', 'Individual', 'Intervention', 'Joints', 'Laboratories', 'Latino', 'Link', 'Literature', 'Longitudinal cohort', 'Machine Learning', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Mental Depression', 'Mentors', 'Metabolic Diseases', 'Methodology', 'Mexican Americans', 'Morbidity - disease rate', 'National Health and Nutrition Examination Survey', 'Observational Study', 'Outcome', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Precision Health', 'Prevalence', 'Probability', 'Public Health', 'Research', 'Research Personnel', 'Research Training', 'Risk Factors', 'Scoring Method', 'Societies', 'Socioeconomic Status', 'Statistical Methods', 'Structure', 'Sustainable Development', 'Testing', 'Time', 'United Nations', 'United States', 'Update', 'Weight', 'base', 'cardiovascular disorder risk', 'cardiovascular risk factor', 'causal model', 'clinical practice', 'cohort', 'comorbidity', 'demographics', 'depressive symptoms', 'design', 'diabetic patient', 'diet and exercise', 'effective intervention', 'epidemiology study', 'follow-up', 'health economics', 'high dimensionality', 'high risk', 'high risk population', 'improved', 'individual patient', 'innovation', 'lifestyle intervention', 'mortality', 'predictive modeling', 'prevent', 'successful intervention', 'therapy development']",NIDDK,UNIVERSITY OF CALIFORNIA LOS ANGELES,F99,2020,34360,-0.01511925835326
"Systems Immunology in Aging and Chronic Diseases of Aging PROJECT SUMMARY / ABSTRACT The funds requested in this R13 application are for partial support of “Systems Immunology in Aging and Chronic Diseases of Aging” annual meetings to be offered each September from 2020 through 2022 at The Jackson Laboratory for Genomic Medicine (JAX-GM) in Farmington, Connecticut. This meeting builds on its very successful first instance in 2019 and will bring together up to 150 interdisciplinary scientists including molecular biologists, immunologists, computational biologists, and geriatricians, who share a common interest in understanding aging and aging-associated disease at the systems level. Many aging-associated diseases, such as cancer and cardiovascular disease, are influenced by dysfunctions in the immune system. Recent advances in genomic profiling techniques (e.g., single cell transcriptomics) provide an opportunity to uncover aging-related changes in human cells/tissues and to link these changes to health and lifespan. The wealth and complexity of data produced using these technologies is ever increasing, as is the need to develop advanced computational methods to mine and integrate these data. Despite this need, there are currently no formal venues at which scientists, specifically those in the aging field, can be trained in the basics and application of data mining techniques (i.e., machine learning algorithms). Furthermore, current conferences on aging are not aimed at specifically bringing together computational biologists, immunologists and basic and clinical aging researchers. Therefore, the objectives of this meeting are: (1) to recognize and emphasize the highly interdisciplinary nature of the aging field and to promote and accelerate collaborations and cross-pollination of ideas across the three disciplines: aging, immunology, and computational biology; (2) to provide trainees (students and postdoctoral fellows) an opportunity to closely interact with, and gain feedback from, more senior investigators to advance their projects and establish connections to help build their careers; and (3) to provide an opportunity for researchers in the field of aging to learn the basics of machine learning techniques, which they will be able to immediately apply to their own research upon return to their home institutions. We will reach these objectives through carrying out the following Aims. In Aim 1, we will organize an interdisciplinary meeting and hands-on workshop focused on aging and aging-related diseases. The meeting will include a 2-day seminar session featuring talks by leading scientists, followed by a 1-day hands-on workshop on the basics of machine learning. In Aim 2, we will promote interactions to foster collaborative research and career advancement, including through a poster session. In Aim 3, we will recruit diverse attendees. Our proposed speaker list features several female scientists, and we will use our partnership networks to specifically recruit attendees from nationally underrepresented racial and ethnic groups. The ultimate goal of the meeting is to advance the aging research field through expediting collaborations and the understanding of aging-related genomic data via application of advanced data mining approaches. PROJECT NARRATIVE / RELEVANCE TO PUBLIC HEALTH Aging and aging-associated diseases, such as Alzheimer's, cancer and cardiovascular disease, represent a significant and growing health and economic burden, with the elderly population of the US projected to double by 2030. Herein, we propose to organize an interdisciplinary conference with a hands-on computational training component that will bring together scientists from the fields of aging, immunology, and computational biology, which will enable creative collaborations and train early career scientists in the aging research field on the basics of advanced computational techniques to mine aging-related genomic data. This is ultimately expected to lead to a better molecular understanding of the aging process and to novel approaches for the improvement of human healthspan and/or lifespan.",Systems Immunology in Aging and Chronic Diseases of Aging,10070754,R13AG069519,"['Academia', 'Address', 'Affect', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Cardiovascular Diseases', 'Career Mobility', 'Cell physiology', 'Cells', 'Chronic Disease', 'Cities', 'Clinical', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Technique', 'Computing Methodologies', 'Connecticut', 'Data', 'Data Analyses', 'Data Science', 'Development', 'Discipline', 'Disease', 'Economic Burden', 'Educational workshop', 'Elderly', 'Ethnic group', 'Etiology', 'Feedback', 'Female', 'Fostering', 'Functional disorder', 'Funding', 'Genomic medicine', 'Genomics', 'Geroscience', 'Goals', 'Health', 'Home environment', 'Human', 'Immune', 'Immune system', 'Immunologist', 'Immunology', 'Industry', 'Inflammation', 'Institution', 'Lead', 'Learning', 'Link', 'Location', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Organism', 'Outcome', 'Participant', 'Phenotype', 'Play', 'Population', 'Postdoctoral Fellow', 'Process', 'Public Health', 'Pythons', 'Race', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scholarship', 'Science', 'Scientist', 'Series', 'Shock', 'Societies', 'Students', 'Support System', 'System', 'Techniques', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissues', 'Training', 'Underrepresented Minority', 'Universities', 'Work', 'aging population', 'cancer type', 'career', 'clinical biomarkers', 'clinically significant', 'data mining', 'deep learning algorithm', 'epigenomics', 'frailty', 'genomic biomarker', 'genomic data', 'genomic profiles', 'graduate student', 'health economics', 'healthspan', 'innovation', 'interdisciplinary approach', 'interest', 'machine learning algorithm', 'meetings', 'next generation', 'novel strategies', 'posters', 'programs', 'recruit', 'response', 'senescence', 'single cell technology', 'skills', 'symposium', 'technology development', 'transcriptomics', 'translational approach']",NIA,JACKSON LABORATORY,R13,2020,34380,-0.039916162064638855
"Identification of Biomarkers and Novel Pathways of Alcoholic Liver Disease by Leveraging Metabolomics, Tissue Imaging Mass Spectrometry, and Integrative Machine Learning ABSTRACT Alcoholic liver disease (ALD) is a serious global health problem. It encompasses a spectrum of pathological conditions, ranging from simple hepatic steatosis, steatohepatitis, fibrosis, alcoholic hepatitis, to liver cirrhosis. Unfortunately, no definitive diagnostic markers exist for ALD (or its different phases), and diagnosis requires a liver biopsy which itself carries significant risk. As a result, management of ALD is frequently empiric. In recent years, some progress has been made using metabolomics to identify potential biomarkers of ALD in animal models and human cohorts. However, global metabolomic profiling of ALD in humans has proceeded slowly and as of today, no studies have been performed that relate metabolomic profiles with pathological changes occurring during the development of ALD. Our working hypothesis predicts that biomarkers specific to ALD may be more effectively identified by applying integrative machine learning to the analysis of data from two state-of-the-art analytical approaches, i.e., metabolomics and imaging mass spectrometry (IMS). As such, we propose to use plasma metabolomics (Specific Aim 1), and histological analysis and liver tissue IMS (Specific Aim 2) in three mouse models of ALD (alcohol-induced steatosis, hepatitis or mild fibrosis) to gain unique insights into the feasibility of using these approaches to identify pathogenic markers of ALD. Ethanol-induced damage to the liver results in alterations in cellular function that can be documented as changes in the metabolome of biological fluids (plasma) and hepatic cells. Metabolomics, the analysis of low molecular metabolites (e.g., lipids and small molecules) in a sample, can be used to directly investigate changes in biochemical pathways induced by alcohol in the liver, such as occurs during ALD. Tissue IMS maps molecules in a tissue section, thereby allowing the quantitation of lipids, proteins and metabolites within a tissue in unprecedented detail. When interfaced with histological analysis of a paired adjacent tissue section, the cellular source of the mapped molecules may be identified. We strongly believe that the integration of metabolomics, IMS and histology (Specific Aim 3) using integrative machine learning will greatly enhance our understanding of the biochemical basis of ALD pathophysiology, and in so doing, allow the development of diagnostic tools that can be used to detect biomarkers in other forms of ALD, thereby improving early diagnosis and treatment of ALD. The management and interpretation of large metabolomics and proteomic data generated as part of the project (10-100GB of raw IMS data per single tissue section) require advanced data-analytics solutions. We will capitalize on our recently published bespoke machine learning solution (“BASIS”) for interrogation of large “-omics” data to identify metabolic/signaling pathways and their downstream metabolites disrupted in ALD. The novelty of this proposal relies on the use of cutting-edge approaches that will allow identification of novel biomarkers and their cellular sources in predictable animal models of ALD. Such information will form a basis for more effective diagnosis and prediction of the progression of ALD. Successful completion of the proposed studies will form a foundation upon which studies in human biological fluids will be conducted in the future. In addition, it is anticipated that our studies will also lay the foundation for examination of the molecular mechanisms associated with other forms of alcohol-induced tissue injury. Such knowledge will facilitate the development of more effective treatments of alcohol abuse. NARRATIVE Excessive alcohol consumption induces alcoholic liver disease (ALD). Unfortunately, no definitive diagnostic markers exist for ALD (or its different phases), and diagnosis requires a liver biopsy which itself carries significant risk. Because the early stages of ALD can potentially be reversed by sobriety, regular screening of the general population and early diagnosis are essential. The overarching goal of this application is to establish metabolomic analyses and tissue imaging mass spectrometry coupled with integrative machine learning to identify novel pathways in and biomarkers for ALD that will be applied in humans.","Identification of Biomarkers and Novel Pathways of Alcoholic Liver Disease by Leveraging Metabolomics, Tissue Imaging Mass Spectrometry, and Integrative Machine Learning",9995686,R21AA028432,"['Alcohol-Induced Disorders', 'Alcoholic Hepatitis', 'Alcoholic Liver Diseases', 'Alcoholic liver damage', 'Alcohols', 'Animal Model', 'Biochemical', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Cell physiology', 'Cells', 'Chronic', 'Complex', 'Coupled', 'Data', 'Data Analyses', 'Data Analytics', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Drug or chemical Tissue Distribution', 'Early Diagnosis', 'Early treatment', 'Ethanol', 'Fatty Liver', 'Fibrosis', 'Foundations', 'Functional disorder', 'Future', 'General Population', 'Goals', 'Heavy Drinking', 'Hepatitis', 'Hepatocyte', 'Histologic', 'Histology', 'Human', 'Hybrids', 'Image', 'Individual', 'Inflammation', 'Investigation', 'Knowledge', 'Label', 'Link', 'Lipids', 'Liquid substance', 'Liver', 'Liver Cirrhosis', 'Liver diseases', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Metabolic', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Monoclonal Antibody R24', 'National Institute on Alcohol Abuse and Alcoholism', 'Pathogenicity', 'Pathologic', 'Pathway interactions', 'Pattern', 'Phase', 'Plasma', 'Process', 'Proteomics', 'Publishing', 'Recovery', 'Resources', 'Risk', 'Sampling', 'Signal Pathway', 'Source', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Steatohepatitis', 'Structure', 'System', 'Tissue imaging', 'Tissues', 'alcohol abuse therapy', 'alcohol research', 'biobank', 'biomarker identification', 'candidate marker', 'cohort', 'diagnostic biomarker', 'effective therapy', 'global health', 'human subject', 'improved', 'insight', 'intrahepatic', 'liver biopsy', 'liver imaging', 'metabolome', 'metabolomics', 'molecular imaging', 'mouse model', 'novel', 'novel diagnostics', 'novel marker', 'potential biomarker', 'predictive marker', 'protein metabolite', 'screening', 'small molecule', 'sobriety', 'specific biomarkers', 'tissue injury', 'tool']",NIAAA,YALE UNIVERSITY,R21,2020,178842,-0.07487547868222778
"Application of advanced methodology to osteoarthritis phenotyping Osteoarthritis (OA) is highly prevalent, contributes to substantial morbidity in the population, and lacks effective interventions to prevent onset and progression. Importantly, and like many other chronic conditions, OA is not a single disease but rather a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying pathophysiological mechanisms. It is becoming increasingly clear that consideration of specific OA phenotypes in clinical studies and trials is critically needed to move the field forward. The overall goal of this line of work is to identify and understand potential phenotypes of knee osteoarthritis (KOA) to better inform future research efforts and treatments; this exploratory R21 project using OA Initiative (OAI) data will investigate novel methodology to support phenotyping in KOA. Successful treatments for OA will need to be targeted to, and tested in, specifically chosen OA phenotypes. Our hypothesis is that an understanding of KOA phenotypes, a key step toward Precision Medicine in OA, will lead to more successful clinical studies in the long-term. To approach this important clinical problem, we propose a project in which we will apply innovative machine learning methods and validation strategies to data from the large, publicly available OAI cohort. We will leverage this large dataset, along with local expertise in statistics, biostatistics and machine learning methodology, to tackle the problem of phenotyping this heterogeneous disease. In Aim 1, we will utilize a data-driven, unsupervised learning approach, to cluster features that best define and discriminate among phenotypes of KOA in the OAI dataset, using biclustering and a novel significance test (SigClust) developed by co-I Marron. For Aim 2, we will test specific hypotheses of relevance to OA outcomes, such as differences between those with and without OA, or those who do or do not develop new or worsening disease, using another set of machine learning methods (Direction-projection-permutation [DiProPerm] hypothesis testing, and Distance-Weighted Discrimination [DWD]), also developed by co-I Marron, in the full cohort and in any identified clusters from Aim 1. In order to address these aims, this proposal involves interdisciplinary collaborations among experts in statistics, biostatistics, computer science, rheumatology, and epidemiology. This work will significantly impact the field by fulfilling a critical need to accurately define OA phenotypes, discover the key features associated with these phenotypes, link phenotype subgroups to underlying mechanisms and use this information to inform and focus future clinical studies. In the long term, we expect that this strategy will lead to more personalized and successful management of the millions of people affected by OA. Project narrative Osteoarthritis is an enormous and increasing public health problem, and like many other chronic conditions it is not a single disease but a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying mechanisms. The lack of appreciation of this heterogeneity has contributed to the failure of all attempts to date to develop disease-modifying osteoarthritis drugs; future trials will need to target specific OA phenotypes. There is a critical need to define and understand phenotypes in OA and link these to outcomes, leading to more personalized and successful management of this common and debilitating disease.",Application of advanced methodology to osteoarthritis phenotyping,9889390,R21AR074685,"['Address', 'Affect', 'Age-Years', 'Arthritis', 'Biomechanics', 'Biometry', 'Cartilage', 'Chronic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Discrimination', 'Disease', 'Epidemiology', 'Etiology', 'Failure', 'Fibrinogen', 'Future', 'General Population', 'Goals', 'Heterogeneity', 'Individual', 'Inflammation', 'Injury', 'Intervention', 'Joints', 'Knee Injuries', 'Knee Osteoarthritis', 'Link', 'Machine Learning', 'Meniscus structure of joint', 'Methodology', 'Morbidity - disease rate', 'Non obese', 'Obesity', 'Outcome', 'Pain', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Progressive Disease', 'Public Health', 'Randomized', 'Research Methodology', 'Resources', 'Rheumatology', 'Risk Factors', 'Structure', 'Subgroup', 'Symptoms', 'Syndrome', 'Synovial Membrane', 'Techniques', 'Testing', 'Time', 'Tissues', 'Validation', 'Visit', 'Work', 'base', 'bone', 'cohort', 'common treatment', 'computer science', 'demographics', 'design', 'disability', 'drug development', 'effective intervention', 'experience', 'improved', 'injured', 'innovation', 'interdisciplinary collaboration', 'joint destruction', 'large datasets', 'loss of function', 'machine learning method', 'novel', 'precision medicine', 'prevent', 'statistics', 'unsupervised learning']",NIAMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2020,200007,-0.0134320286843119
"DUET: Rapid dual-mode microscopy for quantitative slide-based renal fibrosis evaluation Contact PD/PI: Fereidouni, Farzad Abstract Kidneys, like other organs, have an inherent capacity to recover from acute injury; however, severe or recurrent injury can result in chronic kidney disease (CKD), the sequelae of which result in 82,000 deaths annually in the US alone. Regardless of the etiology of the initial injury, the common final pathway leading to- end stage renal disease is closely connected to fibrosis(excess or aberrant collagen distribution), one of the most important determinants of renal disease severity and prognosis. Histology is the gold standard for evaluation, typically through the use of histochemical stains such as trichrome and PAS that highlight the presence of collagens and basement membrane, respectively. Nevertheless, these stains are not completely specific, can be technically challenging to perform well and reproducibly, and thus contribute to interobserver variability and a concomitant decrease in diagnostic precision. Moreover, they also require the preparation of extra slides and additional staining procedures, and thus increase cost and can prolong the diagnostic process. We propose to optimize, deploy and test a new kind of microscope, DUET (DUal mode Emission and Transmission microscopy), developed at UC Davis, that will be a low-cost and very rapid solution for detection and digital characterization of the presence and distribution of collagen and other macromolecules, directly from standard formalin-fixed, paraffin-embedded hematoxylin and eosin-stained slides. Specifically, we will finalize the design of the hardware and software components of the instrument itself, validate imaging performance against standard histology and immunohistochemical stains for collagen and other components, and with the assistance of scientists at our partnering institutions (John Hopkins University and University of Buffalo) develop robust tools for analysis and quantitation of fibrosis. DUET instrument hardware will be shared with JHU to ensure that the methods are technically reproducible across multiple sites. The application leverages the expertise across three institutions in optics, biomedical engineering, renal pathology and novel artificial intelligence approaches. The goal of the project is development and validation of DUET, which promises to be a robust, inexpensive and practical approach for the rapid and accurate evaluation of fibrosis, extensible to other renal pathologies, and indeed across other organs systems, with significant positive impact on disease research, clinical practice, and patient outcomes. Page 6 Project Summary/Abstract Project Narrative Evaluation of fibrosis and tubular atrophy from chemically stained kidney biopsies are essential for diagnosis and disease-severity assessment, but current techniques are time-consuming, somewhat non-specific and contribute to interobserver variability and imprecision, affecting care. We propose to optimize and test a new kind of microscope (“DUET”) that can visualize fibrosis (scarring) and other tissue abnormalities directly from standard slides to enable high-quality reproducible fibrosis scoring and evaluation. This multi-site project will also provide a unique opportunity to perform a retrospective study from hundreds of existing H&E slides with associated months to years of clinical follow-up data, and to create a method with demonstrated utility in more than one institution.",DUET: Rapid dual-mode microscopy for quantitative slide-based renal fibrosis evaluation,10261643,R56DK124873,"['Acute', 'Affect', 'Agreement', 'Algorithms', 'Allografting', 'Archives', 'Artificial Intelligence', 'Atrophic', 'Basement membrane', 'Biomedical Engineering', 'Biopsy', 'Buffaloes', 'Caring', 'Cessation of life', 'Chemicals', 'Chronic Kidney Failure', 'Cicatrix', 'Clinical', 'Collagen', 'Computer software', 'Computers', 'Consumption', 'Data', 'Data Science', 'Data Sources', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'End stage renal failure', 'Ensure', 'Etiology', 'Evaluation', 'Fibrillar Collagen', 'Fibrosis', 'Fluorescence', 'Formalin', 'Goals', 'Gold', 'Hematoxylin and Eosin Staining Method', 'Histologic', 'Histology', 'Image', 'Image Analysis', 'Individual', 'Injury', 'Institution', 'Interobserver Variability', 'Kidney', 'Kidney Diseases', 'Kidney Failure', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Microscope', 'Microscopy', 'Natural History', 'Optics', 'Organ', 'Paraffin Embedding', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Preparation', 'Procedures', 'Process', 'Property', 'Quantitative Microscopy', 'Recurrence', 'Renal Replacement Therapy', 'Renal function', 'Reproducibility', 'Research', 'Retrospective Studies', 'Running', 'Scientist', 'Severity of illness', 'Signal Transduction', 'Sirius Red F3B', 'Site', 'Slide', 'Staging', 'Stains', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissue Embedding', 'Tissues', 'Trichrome stain method', 'Tubular formation', 'Universities', 'Validation', 'base', 'body system', 'clinical care', 'clinical practice', 'clinically significant', 'cohort', 'cost', 'cost effective', 'design', 'digital', 'digital pathology', 'follow-up', 'histological stains', 'instrument', 'instrumentation', 'kidney biopsy', 'kidney fibrosis', 'macromolecule', 'novel', 'outcome forecast', 'personalized diagnostics', 'predict clinical outcome', 'predictive modeling', 'prognostic', 'prognostic value', 'software development', 'stem', 'tool', 'transmission process']",NIDDK,UNIVERSITY OF CALIFORNIA AT DAVIS,R56,2020,91725,-0.01247474224653935
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9952129,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'computational platform', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'diverse data', 'feature selection', 'federated computing', 'guided inquiry', 'hands-on learning', 'heterogenous data', 'in silico', 'indexing', 'innovation', 'insight', 'interest', 'large datasets', 'machine learning method', 'multidimensional data', 'multimodality', 'patient population', 'patient subsets', 'post-traumatic stress', 'post-traumatic symptoms', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'supervised learning', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2020,501996,-0.0008885220460793425
"Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled Iifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. n/a",Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry,10023190,R01GM135927,"['Accelerometer', 'Address', 'Adopted', 'Algorithmic Software', 'Algorithms', 'Behavior monitoring', 'Behavioral', 'Big Data', 'Cellular Phone', 'Classification', 'Coupled', 'Data', 'Data Analyses', 'Development', 'Devices', 'Diabetes Mellitus', 'Elements', 'Generations', 'Geometry', 'Goals', 'Growth', 'Harvest', 'Health', 'Home environment', 'Hour', 'Human Activities', 'Intervention', 'Life', 'Location', 'Machine Learning', 'Mathematics', 'Measurement', 'Modeling', 'Observational Study', 'Outcome', 'Pattern', 'Periodicity', 'Physical activity', 'Process', 'Regimen', 'Research', 'Research Activity', 'Sampling', 'Series', 'Stroke', 'Techniques', 'Technology', 'Time', 'Work', 'base', 'heart rate monitor', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning algorithm', 'mathematical algorithm', 'mathematical sciences', 'multimodality', 'sensor', 'signal processing', 'statistics', 'tool', 'wearable device']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,311680,-0.011757090233976067
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,10021429,R01GM135926,"['Address', 'Algorithms', 'Architecture', 'Basic Science', 'Behavior', 'Biological', 'Biological Process', 'Clinical', 'Collection', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Developmental Process', 'Dimensions', 'Disease Progression', 'Drosophila genus', 'Etiology', 'Expression Profiling', 'Gene Proteins', 'Genes', 'Grouping', 'Immune System Diseases', 'Immune response', 'Innate Immune Response', 'Knowledge', 'Lead', 'Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pattern', 'Process', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Scientist', 'Series', 'Silicon Dioxide', 'Structure', 'System', 'Techniques', 'Time', 'Uncertainty', 'Validation', 'Variant', 'base', 'biological systems', 'clinical practice', 'dynamic system', 'experimental study', 'high dimensionality', 'innovation', 'insight', 'learning algorithm', 'learning strategy', 'mouse model', 'novel', 'open source', 'organ growth', 'outcome forecast', 'protein protein interaction', 'random forest', 'temporal measurement']",NIGMS,CORNELL UNIVERSITY,R01,2020,344345,-0.03157461902003728
"Phenotype screens of Chlamydia Inclusions Abstract Chlamydia trachomatis is a major health concern with over 200 million people with active urogenital or ocular infection each year worldwide. Chlamydia are obligate intracellular bacteria with a unique biphasic developmental cycle. A better understanding of that biphasic cycle can lead to inhibitors that are specific for chlamydial infection in order to avoid overuse of antibiotics. Individual Chlamydia are too small and tightly packed to be spatially separated with conventional light microscopes, and 3D SEM is too labor-intensive for inhibitor studies. We will use a new sample preparation method that physically expands the sample with polymers termed ""Expansion Microscopy"" or ExM. Expanded samples can then be imaged with a traditional confocal microscope, and high-content analysis performed automatically using machine learning methods such as pixel classification and novelty detection. Prepared samples can be imaged and analyzed in under an hour instead of the multiple days required for 3D SEM. This R03 grant will develop an innovative high-content screening platform, called Expansion Microscopy Aided Phenotyping (ExMAP), for the quantification of changes in Chlamydia development after treatment. ExMAP can be paired with Chlamydia transformed with promoters for EUO and IhtA (RB cell types) and the promoters for HctB and Tarp (EB cell types). The combination of expansion microscopy, machine learning, and chlamydial transformation will make ExMAP a powerful tool for research on both the developmental cycle and new therapy development. Project Narrative This project will develop a new high-content platform, termed ExMAP, that will physically expand the sample of interest and then utilize machine learning for image analysis. At the completion of the project, we expect to have developed a new method for the study of the Chlamydia developmental cycle and inhibitors that disrupt that cycle.",Phenotype screens of Chlamydia Inclusions,9979523,R03AI146437,"['3-Dimensional', 'Acrylates', 'Aftercare', 'Agonist', 'Antibiotics', 'Applications Grants', 'Bacteria', 'Cells', 'Chlamydia', 'Chlamydia Infections', 'Chlamydia genome', 'Chlamydia trachomatis', 'Chloramphenicol', 'Classification', 'Clinical', 'Computer software', 'Confocal Microscopy', 'Consumption', 'Data', 'Detection', 'Development', 'Developmental Gene', 'Drug Costs', 'Drug Screening', 'Electron Microscopy', 'Eye Infections', 'Gel', 'Genitourinary System Infection', 'Grant', 'Growth', 'Health', 'Hour', 'Human', 'Image', 'Image Analysis', 'Individual', 'Iron Chelating Agents', 'Lead', 'Light Microscope', 'Machine Learning', 'Malaria', 'Measurement', 'Methods', 'Microbiology', 'Microscope', 'Microscopy', 'Morbidity - disease rate', 'PF4 Gene', 'Penicillins', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phenotype', 'Polymers', 'Preparation', 'Research', 'Resolution', 'SIRT1 gene', 'Sampling', 'Sodium', 'Techniques', 'Time', 'Work', 'cell type', 'chlamydia vaccine', 'human pathogen', 'inhibitor/antagonist', 'innovation', 'interest', 'machine learning method', 'novel', 'novel therapeutics', 'pathogen', 'promoter', 'screening', 'small molecule', 'small molecule inhibitor', 'therapy development', 'tool']",NIAID,WAKE FOREST UNIVERSITY,R03,2020,77243,-0.032633224689998123
"SBIR Phase I Topic 402 -  Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring This project aims to develop an interpretable, physician-in-the-loop AI-aided software that accurately delineates glioma boundaries in MRIs, computes volumetric curves, and statistically quantifies the tumor growth in longitudinal studies. The current clinical practice of visually analyzing and manually contouring tumors is subjective, time-consuming, and often inconsistent. The novelty of MRIMath's explainable, trustworthy, and physician-in-the-loop AI system is multi-fold. First, we introduce a multi-scale feature extraction framework using the inception modules in contracting and expanding paths of the U-Net image segmentation neural network architecture. Second, we propose a new loss function based on the modified Dice similarity coefficient. Third, we train and test the AI system using two learning regimes: learning to segment intra-tumoral structures and learning to segment glioma sub-regions. Finally, we produce heat maps to visualize the features extracted by the AI, thus offering physicians a view of AI's attention patterns and activation maps that were triggered during AI's decision-making. An intuitive and interactive User Interface will allow the physician to review contouring results, make adjustments and approve contours, visualize AI's explanations and volumetric measurements, and finally review the results of the statistical analysis. Any modifications made by the physician will be used later to re-train AI. n/a","SBIR Phase I Topic 402 -  Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring",10269837,5N91020C00049,"['Artificial Intelligence', 'Attention', 'Computer software', 'Consumption', 'Contracts', 'Data', 'Data Sources', 'Decision Making', 'Diagnosis', 'Glioma', 'Human', 'Intuition', 'Learning', 'Longitudinal Studies', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measurement', 'Modality', 'Modification', 'Monitor', 'Pattern', 'Phase', 'Physicians', 'Small Business Innovation Research Grant', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'System', 'Testing', 'Time', 'TimeLine', 'Training', 'base', 'cancer imaging', 'cancer prevention', 'clinical practice', 'design', 'feature extraction', 'imaging Segmentation', 'imaging software', 'imaging system', 'loss of function', 'neural network architecture', 'prototype', 'tumor', 'tumor growth', 'usability']",NCI,"MRIMATH, LLC",N43,2020,400000,-0.04725665743780132
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9975882,K08HL136928,"['Affect', 'Bioinformatics', 'Biological', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Multiomic Data', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'machine learning method', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2020,172800,-0.01847737165619277
"Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Summary The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled lifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. Project Number: 1R01GM135927-01 Title: Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Narrative In this revision application, we seek to submit an equipment supplement to our existing R01 referenced above. As our project progressed, we found that it is important to consider the role of new emerging feature-learning approaches to extract downstream time-series features. To fully develop our approach and conduct additional experiments, we need significant GPU computational resources that will be dedicated to this project.",Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry,10135658,R01GM135927,"['Accelerometer', 'Address', 'Adopted', 'Algorithmic Analysis', 'Algorithmic Software', 'Algorithms', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Big Data', 'Cellular Phone', 'Classification', 'Coupled', 'Data', 'Data Analyses', 'Development', 'Devices', 'Diabetes Mellitus', 'Dimensions', 'Elements', 'Equipment', 'Generations', 'Geometry', 'Goals', 'Growth', 'Harvest', 'Health', 'Home environment', 'Hour', 'Human Activities', 'Intervention', 'Learning', 'Life', 'Location', 'Machine Learning', 'Mathematics', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Observational Study', 'Outcome', 'Pattern', 'Periodicity', 'Physical activity', 'Process', 'Regimen', 'Research', 'Research Activity', 'Role', 'Running', 'Sampling', 'Series', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Technology', 'Time', 'Time Series Analysis', 'Validation', 'Walking', 'Work', 'analysis pipeline', 'base', 'computing resources', 'density', 'experimental study', 'heart rate monitor', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning algorithm', 'mathematical algorithm', 'mathematical sciences', 'multimodality', 'preservation', 'sedentary lifestyle', 'sensor', 'signal processing', 'statistics', 'tool', 'wearable device', 'wearable sensor technology']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,69169,-0.00971198696836043
"A Novel Cryopreservation Technology for Large Skin Grafts to Facilitate Tissue Banking and Allograft Transplantations SUMMARY As a result of recent and rapid advances of immunogenetical, artificial intelligence (AI) and big data technologies, the accuracy and efficiency of donor-recipient immunogenetical matching are expected to be significantly improved. As such, traditional tissue banking practices are being severely challenged due to low cryopreservation throughput. Skin allograft transplantations are currently utilized for numerous clinical applications. In such applications, donor tissues are now typically utilized as “expensive bandages”, i.e. for temporary coverage, thereby requiring painful and damaging follow-on autotransplantation procedures. With tremendously improved skin graft cryopreservation methods coupled with use of HLA matching based on AI network, future tissue banking platforms can potentially provide grafts to be used as permanent and definitive treatment for patients, thereby bringing high impact to clinical applications and the healthcare industry. Based on CryoCrate’s novel ultra-fast (106 K/min on a sample surface) cooling technology platform (PCT/US2019/26162), this project aims to develop a highly efficient (approaching 100% post-thaw viability) tissue graft cryopreservation prototype system that requires no cryoprotectant. The post-thaw viability and functionality will be assessed by standard in vitro assays using porcine and human skin (research exempt) grafts and in vivo assays using a mouse model. After achieving these aims, in future phase II and III stages, CryoCrate and its academic collaborator will collaborate with a local Organ Procurement Organization (Mid-America Transplant) and a wound healing clinic (University of Missouri Hospital) for porcine and human skin transplantation studies and clinical trials. Narrative A novel skin graft cryopreservation technology that requires no cryoprotectant will be developed to significantly improve efficiencies for tissue banking and allograft transplantation practices. This project initiates collaboration between CryoCrate LLC and local stakeholders, and thereby paves a path to commercialization of a new generation of ultra‐fast tissue cryopreservation devices.",A Novel Cryopreservation Technology for Large Skin Grafts to Facilitate Tissue Banking and Allograft Transplantations,10011745,R43AI155070,"['Allografting', 'Americas', 'Animal Model', 'Area', 'Artificial Intelligence', 'Autologous Transplantation', 'Bandage', 'Big Data', 'Biological Assay', 'Clinic', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Coupled', 'Cryopreservation', 'Cryopreserved Tissue', 'Devices', 'Engineering', 'Excision', 'Family suidae', 'Freeze Drying', 'Freezing', 'Fresh Tissue', 'Future', 'Gases', 'Generations', 'Goals', 'Healthcare Industry', 'Hospitals', 'Human', 'Immunogenetics', 'Impairment', 'In Situ Nick-End Labeling', 'In Vitro', 'Liquid substance', 'Mechanics', 'Methods', 'Missouri', 'Modeling', 'Mus', 'Nitrogen', 'Organ Procurements', 'Outcome', 'Pain', 'Patients', 'Performance', 'Phase', 'Procedures', 'Process', 'Research', 'Research Project Grants', 'Sampling', 'Skin', 'Skin Tissue', 'Skin Transplantation', 'Skin graft', 'Stains', 'Surface', 'System', 'Technology', 'Testing', 'Thick', 'Thinness', 'Time', 'Tissue Donors', 'Tissue Grafts', 'Tissue Transplantation', 'Tissues', 'Toxic effect', 'Transplantation', 'Universities', 'Variant', 'base', 'biobank', 'clinical application', 'commercialization', 'design', 'experimental study', 'improved', 'in vitro Assay', 'in vivo', 'invention', 'medical schools', 'mouse model', 'novel', 'prototype', 'scale up', 'skin allograft', 'success', 'transplant model', 'two-dimensional', 'wound healing']",NIAID,"CRYOCRATE, LLC",R43,2020,160371,-0.006371408388746009
"DIGITAL HEALTH SOLUTIONS FOR COVID-19: COVID-19 ONGOING MONITORING (COMMUNITY) The goal of this proposal is to develop a COVID-19 detection algorithm based on self-report survey data and wearable sensor data. Data from 25K COVID-19 Experiences participants and 25K Large-scale Flu Surveillance (COVID-19 Questions added March 2020) will be used with an existing machine learning model to develop this new detection algorithm, which will be validated in a large-scale pilot population to identify individuals with undiagnosed COVID-19. Evidation will incorporate the model into an established web and multi-platform (Android, iOS) smartphone platform called Achieve which allows users to share person-generated health data (PGD) from their everyday lives. Data collected under this project will be deidentified and securely transmitted to an NIH data hub. n/a",DIGITAL HEALTH SOLUTIONS FOR COVID-19: COVID-19 ONGOING MONITORING (COMMUNITY),10274140,5N91020C00034,"['Algorithms', 'Android', 'COVID-19', 'Cellular Phone', 'Communities', 'Data', 'Detection', 'Goals', 'Health', 'Individual', 'Internet', 'Machine Learning', 'Modeling', 'Monitor', 'Participant', 'Patient Self-Report', 'Persons', 'Population', 'Secure', 'Surveys', 'United States National Institutes of Health', 'base', 'data hub', 'digital', 'experience', 'health data', 'influenza surveillance', 'wearable sensor technology']",NCI,"EVIDATION HEALTH, INC.",N01,2020,240000,-0.007509215752528077
"Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity PROJECT SUMMARY/ABSTRACT  Experimental animal and clinical testing to evaluate hepatotoxicity demands extensive resources and long turnaround times. Utilization of computational models to directly predict the toxicity of new compounds is a promising strategy to reduce the cost of drug development and to screen the multitude of industrial chemicals and environmental contaminants currently lacking safety assessments. However, the current computational models for complex toxicity endpoints, such as hepatotoxicity, are not reliable for screening new compounds and face numerous challenges. Our recent studies have shown that traditional Quantitative Structure-Activity Relationship modeling is applicable for relatively simple properties or toxicity endpoints with a clear mechanism, but fails to address complex bioactivities such as hepatotoxicity. The primary objective of this proposal is to develop novel mechanism-driven Virtual Adverse Outcome Pathway (vAOP) models for the fast and accurate assessment of hepatotoxicity in a high-throughput manner The resulting vAOP models will be experimentally validated using a complement of in vitro and ex vivo testing. We have generated a preliminary vAOP model based on the antioxidant response element (ARE) pathway that has undergone initial validation and refinement using in vitro testing. To this end, our project will generate novel predictive models for hepatotoxicity by applying 1) a virtual cellular stress pathway model to mechanism profiling and assessment of new compounds; 2) computational predictions to fill in the missing data for specific targets within the pathway; 3) in vitro experimental validation with three complementary bioassays; and 4) ex vivo experimental validation with pooled primary human hepatocytes capable of biochemical transformation. The scientific approach of this study is to develop a universal modeling workflow that can take advantage of all available short-term testing information, obtained from both computational predictions using novel machine learning approaches and in vitro experiments, for target compounds of interest. We will validate and use our modeling workflow to directly evaluate the hepatotoxicity of new compounds and prioritize candidates for validation in pooled primary human hepatocytes. The resulting workflow will be disseminated via a web portal for public users around the world with internet access. Importantly, this study will pave the way for the next generation of chemical toxicity assessment by reconstructing the modeling process through a combination of big data, computational modeling, and low cost in vitro experiments. To the best of our knowledge, the implementation of this project will lead to the first publicly available mechanisms-driven modeling and web- based prediction framework for complex chemical toxicity based on publicly-accessible big data. These deliverables will have a significant public health impact by not only prioritizing compounds for safety testing or new chemical development, but also revealing toxicity mechanisms. PROJECT NARRATIVE Hepatotoxicity is a leading safety concern in the development of new chemicals. We will create virtual “Adverse Outcome Pathway” models that will directly evaluate the hepatotoxicity potentials of chemicals using massive public toxicity data. The primary deliverable of this project will be a publically-accessible, web-based search engine to evaluate new chemicals for risk of hepatotoxicity.",Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity,9864299,R01ES031080,"['Address', 'Animal Model', 'Animal Testing', 'Antioxidants', 'Big Data', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Cellular Stress', 'Chemical Injury', 'Chemical Structure', 'Chemicals', 'Clinical', 'Complement', 'Complex', 'Computer Models', 'Computer software', 'Computers', 'Cryopreservation', 'Custom', 'Data', 'Data Pooling', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Drug Costs', 'Ensure', 'Environment', 'Environmental Pollution', 'Evaluation', 'Face', 'Generations', 'Hepatocyte', 'Hepatotoxicity', 'Human', 'In Vitro', 'Industrialization', 'Injury', 'Internet', 'Libraries', 'Liver', 'Luciferases', 'Machine Learning', 'Marketing', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Nutraceutical', 'Online Systems', 'Pathway interactions', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Population', 'Process', 'Property', 'Proteomics', 'PubChem', 'Public Health', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Personnel', 'Resources', 'Response Elements', 'Risk', 'Safety', 'Signal Transduction', 'Source', 'Statutes and Laws', 'System', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Toxicology', 'Translating', 'Validation', 'Vertebrates', 'adverse outcome', 'base', 'candidate validation', 'cell injury', 'combat', 'computational toxicology', 'computer framework', 'computerized tools', 'cost', 'data mining', 'deep neural network', 'design', 'developmental toxicity', 'drug development', 'endoplasmic reticulum stress', 'experimental study', 'hepatocellular injury', 'improved', 'in vitro Assay', 'in vitro testing', 'in vivo', 'interest', 'knowledge base', 'large datasets', 'liver injury', 'next generation', 'novel', 'pre-clinical', 'predictive modeling', 'reproductive toxicity', 'research clinical testing', 'safety assessment', 'safety testing', 'screening', 'search engine', 'tool', 'toxicant', 'transcriptomics', 'virtual', 'web portal']",NIEHS,RUTGERS THE STATE UNIV OF NJ CAMDEN,R01,2020,465692,-0.020509436379674183
"An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics Immune-repertoire sequence, which consists of an individual's millions of unique antibody and T-cell receptor (TCR) genes, encodes a dynamic and highly personalized record of an individual's state of health. Our long- term goal is to develop the computational models and tools necessary to read this record, to one day be able diagnose diverse infections, autoimmune diseases, cancers, and other conditions directly from repertoire se- quence. The key problem is how to find patterns of specific diseases in repertoire sequence, when repertoires are so complex. Our hypothesis is that a combination of bottom-up (sequence-level) and top-down (systems- level) modeling can reveal these patterns, by encoding repertoires as simple but highly informative models that can be used to build highly sensitive and specific disease classifiers. In preliminary studies, we introduced two new modeling approaches for this purpose: (i) statistical biophysics (bottom-up) and (ii) functional diversity (top-down), and showed their ability to elucidate patterns related to vaccination status (97% accuracy), viral infection, and aging. Building on these studies, we will test our hypothesis through two specific aims: (1) We will develop models and classifiers based on the bottom-up approach, statistical biophysics; and (2) we will de- velop the top-down approach, functional diversity, to improve these classifiers. To achieve these aims, we will use our extensive collection of public immune-repertoire datasets, beginning with 391 antibody and TCR da- tasets we have characterized previously. Our team has deep and complementary expertise in developing computational tools for finding patterns in immune repertoires (Dr. Arnaout) and in the mathematics that under- lie these tools (Dr. Altschul), with additional advice available as needed regarding machine learning (Dr. AlQuraishi). This proposal is highly innovative for how our two new approaches address previous issues in the field. (i) Statistical biophysics uses a powerful machine-learning method called maximum-entropy modeling (MaxEnt), improving on past work by tailoring MaxEnt to learn patterns encoded in the biophysical properties (e.g. size and charge) of the amino acids that make up antibodies/TCRs; these properties ultimately determine what targets antibodies/TCRs can bind, and therefore which sequences are present in different diseases. (ii) Functional diversity fills a key gap in how immunological diversity has been measured thus far, by factoring in whether different antibodies/TCRs are likely to bind the same target. This proposal is highly significant for (i) developing an efficient, accurate, generative, and interpretable machine-learning method for finding diagnostic patterns in repertoire sequence; (ii) applying a robust mathematical framework to the measurement of immuno- logical diversity; (iii) impacting clinical diagnostics; and (iv) adding a valuable new tool for integrative/big-data medicine. The expected outcome of this proposal is an integrated pair of robust and well validated new tools/models for classifying specific disease exposures directly from repertoire sequence. This proposal in- cludes plans to make these tools widely available, to maximize their positive impact across medicine. The proposed research is relevant to public health because B cells/antibodies and T cells play vital roles across such a vast range of health conditions, from infection, to autoimmunity, to cancer, that the ability to de- code what they are doing would be an important step forward for diagnosing these conditions. The proposed research is relevant to the NIH's mission of fostering fundamental creative discoveries, innovative research strategies, and their applications as a basis for ultimately protecting and improving health, specifically relating to the diagnosis of human diseases.",An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics,10050030,R01AI148747,"['Address', 'Affect', 'Aging', 'Amino Acid Motifs', 'Amino Acids', 'Antibodies', 'Autoimmune Diseases', 'Autoimmunity', 'B-Lymphocytes', 'Base Sequence', 'Big Data', 'Binding', 'Biophysics', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Code', 'Collection', 'Complex', 'Computer Models', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Entropy', 'Fostering', 'Gene Frequency', 'Genes', 'Goals', 'Health', 'Human', 'Immune', 'Immunology', 'Individual', 'Infection', 'Influenza vaccination', 'Intuition', 'Learning', 'Letters', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Physics', 'Play', 'Population Heterogeneity', 'Privatization', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Role', 'Sample Size', 'Sampling', 'Sampling Errors', 'Signs and Symptoms', 'Speed', 'Statistical Study', 'System', 'T-Cell Receptor', 'T-Cell Receptor Genes', 'T-Lymphocyte', 'Testing', 'United States National Institutes of Health', 'Vaccination', 'Virus Diseases', 'Work', 'base', 'biophysical properties', 'clinical diagnostics', 'computerized tools', 'diagnostic accuracy', 'human disease', 'immunological diversity', 'improved', 'information model', 'innovation', 'machine learning method', 'multidisciplinary', 'multilevel analysis', 'novel', 'novel strategies', 'tool']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,535171,-0.027849024799155738
"Multi-Study Integer Programming Methods for Human Voltammery Project Summary/Abstract  The development of treatments for addiction requires the characterization of neural mechanisms underlying reward. Studying reward in humans requires assays that can detect changes in neurotransmitter levels with high chemical specificity. Recently, fast-scan cyclic voltammetry (FSCV) has been implemented in humans to measure dopamine with high temporal and spatial resolution. This technological achievement was enabled in large part through the novel application of machine learning methods. FSCV relies on statistical tools since FSCV records an electrochemical response which must be converted into concentration estimates via a statistical model. The validity of the scientific conclusions from human FSCV studies therefore depends heavily on the reliability of these statistical models to generate accurate dopamine concentration estimates.  In human FSCV, models are fit on in vitro training sets as making in vivo training sets in humans is infeasible. Producing accurate estimates thus requires that models trained on in vitro training sets generalize to in vivo brain recordings. Combining data from multiple training sets is the standard approach human FSCV researchers have employed to improve model generalizability. This proposal extends work that shows that multi-study machine learning methods improve dopamine concentration estimates by combining training sets from different electrodes such that the resulting average signal (“cyclic voltammogram” or CV) is similar to the average CV of the electrode used in the brain. However, this approach relies on random resampling. This is problematic because the randomness limits the extent to which estimate accuracy can be improved and the slow speed of the resampling approach precludes the generation of estimates during data collection, which is critical to experiment success.  This proposal details the development of methods that leverage mixed integer programming to optimally generate training sets that combine data from multiple electrodes. By generating training sets that are specifically tailored to the electrode used for brain measurements, one can vastly improve dopamine concentration estimate accuracy. The speed of the integer programming methods will enable the use of this approach during data collection. This work will include validation of the methods on in vitro data as well as on data from published in vivo and slice experiments in rodents. By applying methods to published optogenetic experiments, one can compare estimates from the proposed methods and from standard methods. The asymptotic properties of the proposed methods will be characterized analytically assuming a linear mixed effects model and empirically through application of the methods to data simulated under this model.  This work will be conducted at the highly collaborative and innovative Harvard School of Public Health. The fellowship will support growth in statistical, computing and collaborative skills, and prepare the trainee for a productive career as a biostatistics professor who develops methods for neuroscience and addiction research. Project Narrative  Fast-scan cyclic voltammetry in humans offers an invaluable tool to study the neural mechanisms underlying reward by allowing for sub-second detection of dopamine during cognitive-behavioral tasks. However, conducting voltammetry in humans presents distinct statistical challenges that must be overcome to ensure optimal dopamine concentration estimates. We propose novel statistical methods that use mixed integer optimization and extend preliminary work that shows multi-study machine learning methods substantially improve dopamine concentration estimate accuracy.",Multi-Study Integer Programming Methods for Human Voltammery,10067624,F31DA052153,"['Achievement', 'Address', 'Algorithms', 'Behavioral', 'Biological Assay', 'Biometry', 'Brain', 'Cells', 'Chemicals', 'Cognitive', 'Complex Mixtures', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Dopamine', 'Electrodes', 'Ensure', 'Fellowship', 'Generations', 'Goals', 'Grant', 'Growth', 'Human', 'In Vitro', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Neurosciences', 'Neurotransmitters', 'Nucleus Accumbens', 'Performance', 'Periodicity', 'Property', 'Public Health Schools', 'Publications', 'Publishing', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Rewards', 'Rodent', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Specificity', 'Speed', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Training', 'Validation', 'Work', 'addiction', 'algorithm training', 'career', 'effective therapy', 'experimental study', 'improved', 'in vivo', 'innovation', 'insight', 'machine learning method', 'method development', 'multiple data sources', 'neuromechanism', 'novel', 'optogenetics', 'predictive modeling', 'professor', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool']",NIDA,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2020,37235,-0.0060389736950201426
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9889134,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated algorithm', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'machine learning method', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2020,356625,-0.03215507790089247
"Automated Phenotyping in Epilepsy There are 65 million people worldwide with epilepsy and 150,000 new cases of epilepsy are diagnosed in the US annually. However, treatment options for epilepsy remain inadequate, with many patients suffering from treatment-resistant seizures, cognitive comorbidities and the negative side effects of treatment. A major obstacle to progress towards the development of new therapies is the fact that preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). Recently, the Datta lab showed that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales and arranged according to specific rules (“grammar”). These syllables can be detected without observer bias using a method called motion sequencing (MoSeq) that employs video imaging with a 3D camera combined with artificial intelligence (AI)-assisted video analysis to characterize behavior. Through collaboration between the Soltesz and Datta labs, exciting data were obtained that demonstrated that MoSeq can be adapted for epilepsy research to perform objective, inexpensive and automated phenotyping of mice in a mouse model of chronic temporal lobe epilepsy. Here we propose to test and improve MoSeq further to address long-standing, fundamental challenges in epilepsy research. This includes the development of an objective alternative to the Racine scale, testing of MoSeq as an automated anti-epileptic drug (AED) screening method, and the development of human observer- independent behavioral biomarkers for seizures, epileptogenesis, and cognitive comorbidities. In addition, we plan to dramatically extend the epilepsy-related capabilities of MoSeq to include the automated tracking of finer-scale body parts (e.g., forelimb and facial clonus) that are not possible with the current approach. Finally, we propose to develop the analysis pipeline for MoSeq into a form that is intuitive, inexpensive, user-friendly and thus easily sharable with the research community. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user- independent, inexpensive analysis of both acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will develop breakthrough technologies involving artificial intelligence (AI)-assisted analysis of 3-dimensional video data of mouse behavior to address long-standing, fundamental challenges in preclinical epilepsy research. If successful, this innovative approach is expected to have a significant and sustained impact on epilepsy research by enabling investigators to perform objective, automated, inexpensive, reproducible assessment of epilepsy in experimental animals to aid the testing of anti-seizure drugs and other novel therapies.",Automated Phenotyping in Epilepsy,10024094,R01NS114020,"['3-Dimensional', 'Address', 'Animal Behavior', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological Markers', 'Body part', 'Cannabidiol', 'Carbamazepine', 'Cells', 'Chronic', 'Clonazepam', 'Clonus', 'Cognitive', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Diagnosis', 'Drug Screening', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Face', 'Forelimb', 'Future', 'Genetic', 'Genetic Models', 'Head', 'Hippocampus (Brain)', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Intuition', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Mus', 'Observer Variation', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Phenytoin', 'Pilocarpine', 'Probability', 'Reproducibility', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Rest', 'Seizures', 'Specificity', 'Stereotyping', 'Structure', 'Tail', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-dimensional analysis', 'Time', 'Treatment Side Effects', 'Valproic Acid', 'analysis pipeline', 'base', 'behavior test', 'behavioral phenotyping', 'comorbidity', 'drug use screening', 'improved', 'innovation', 'kainate', 'method development', 'mouse model', 'novel therapeutics', 'optogenetics', 'pre-clinical', 'pre-clinical research', 'selective expression', 'user-friendly']",NINDS,STANFORD UNIVERSITY,R01,2020,422740,-0.009401378755816236
"Georgia Clinical & Translational Science Alliance (GaCTSA) EFFECTIVE ALLOCATION OF TEST CENTERS FOR COVID-19 USING MACHINE LEARNING AND  ADAPTIVE SAMPLING ABSTRACT A critical task in managing and dealing with COVID-19 in communities is to perform diagnostic and/or antibody tests to identify diseased individuals. This information is critical to public health officials to estimate prevalence and transmission, and to effectively plan for required resources such as ICU beds, ventilators, personal protective equipment, and medical staff. Additionally, information on the number of infected people can be used to develop probabilistic and statistical models to estimate the reproduction number of the disease, and to predict the likely spatial and temporal trajectories of the outbreak. This provides vital information for planning actions and preparing policies and guidelines for social-distancing, school closures, remote work, community lockdown, etc. Despite the importance of diagnostic testing and identification of the positive cases, broad-scale testing is a challenging task particularly due to the limited number of test kits and resources. Our proposed research focuses on the development machine learning-based allocation strategies for determining the optimal location of COVID-19 test centers, including mobile and satellite centers, to minimize the local and global prediction uncertainties, maximize geographic coverage, associated with projections of spatio-temporal outbreak trajectories, and to improve efficient identification of diseased cases. EFFECTIVE ALLOCATION OF TEST CENTERS FOR COVID-19 USING MACHINE LEARNING AND  ADAPTIVE SAMPLING NARRATIVE Diagnostic and antibody tests for COVID-19 can provide invaluable information on prevalence and transmission of the disease. However, due to limited test capacity, broad-scale testing is currently not feasible. Consequently, there is a pressing need for a systematic and data-driven approach to defining testing strategies, in particular, determining the number and location of satellite and mobile testing centers (e.g., drive-through test locations). Our research program develops machine learning approaches to effectively allocate test centers for COVID-19 at the city, county, and state levels to accurately and reliably estimate the disease prevalence and its trajectory for resource planning and policy making, and to efficiently identify cases for treatment.",Georgia Clinical & Translational Science Alliance (GaCTSA),10158891,UL1TR002378,"['Active Learning', 'Antibodies', 'Area', 'Beds', 'Biology', 'COVID-19', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Cities', 'Clinical Sciences', 'Communities', 'Contracts', 'County', 'Data', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Ecology', 'Ensure', 'Epidemic', 'Epidemiology', 'Equipment', 'Federal Government', 'Future', 'Geography', 'Goals', 'Guidelines', 'Hybrids', 'Individual', 'Local Government', 'Location', 'Machine Learning', 'Medical Staff', 'Methods', 'Modeling', 'Monitor', 'Neighborhood Health Center', 'Neurology', 'Pattern', 'Performance', 'Policies', 'Policy Making', 'Population', 'Prevalence', 'Process', 'Public Health', 'Readiness', 'Reproduction', 'Research', 'Research Project Grants', 'Resistance', 'Resources', 'Sampling', 'Scheme', 'Schools', 'Series', 'Social Distance', 'Statistical Models', 'Testing', 'Time', 'Translational Research', 'Uncertainty', 'Update', 'Ventilator', 'Virus', 'Work', 'base', 'case finding', 'disease transmission', 'environmental justice', 'evidence base', 'experience', 'flexibility', 'improved', 'metropolitan', 'multidisciplinary', 'novel', 'pandemic disease', 'programs', 'racial and ethnic', 'racial diversity', 'sociodemographics', 'socioeconomics', 'spatiotemporal', 'transmission process']",NCATS,EMORY UNIVERSITY,UL1,2020,225579,-0.001784409537809378
"Improving Critical Congenital Heart Disease Screening and Detection of ""Secondary"" Targets PROJECT SUMMARY/ABSTRACT  We propose to develop an automated critical congenital heart disease (CCHD) screening algorithm using machine learning techniques to combine non-invasive measurements of perfusion and oxygenation. Oxygen saturation (SpO2)-based screening is the current standard for CCHD screening, however it fails to detect up to 50% of asymptomatic newborns with CCHD or nearly 900 newborns in the United States annually. The majority of newborns missed by SpO2 screening have defects with aortic obstruction, such as coarctation of the aorta (CoA), that do not result in deoxygenated blood entering circulation. Non-invasive measurements of perfusion such as perfusion index (PIx) and pulse oximetry waveform analysis is expected to improve the detection of newborns with defects such as CoA, which is currently the most commonly missed CCHD by SpO2 screening. Both PIx and pulse oximetry waveforms can be measured non-invasively and with the same equipment used for SpO2 screening.  Members of our team recently showed that the addition of PIx, a non-invasive measurement of pulsatile blood flow, has the potential to improve CCHD detection otherwise missed by SpO2 screening. However, variability of PIx over brief time periods (seconds) and human error in its interpretation limit its clinical capabilities. Additionally, human error in interpretation of the current SpO2 screening algorithm leads to missed diagnoses and inappropriate testing in healthy newborns. Therefore, an automated SpO2-PIx screening algorithm is needed to both simplify the screening process, and improve detection of defects that are missed with SpO2 screening. In order to achieve that, we will identify the optimal PIx waveforms to create a metric that discriminates between newborns with and without CCHD. We will perform pulse oximetry waveform analysis to identify other non-invasive components with discriminatory capacity for newborns with CCHD. Additionally, we will apply supervised machine learning techniques to automate the algorithm interpretation.  The proposed research is significant because an automated SpO2-PIx screening algorithm could save the lives of hundreds of newborns with CCHD that are not diagnosed by SpO2 screening. Additionally, this is innovative as it will be the first automatic interpretation of PIx measurement among newborns with CCHD and merging of automated PIx and SpO2, which will allow for easy implementation at later steps. Through collaboration with four pediatric cardiac centers, we will establish the infrastructure and necessary multidisciplinary relationships to conduct future multicenter studies to evaluate this novel combined SpO2-PIx algorithm on a large scale involving thousands of newborns. Improving the detection of CCHD will require a multidisciplinary approach among all the individuals involved in the care and screening of newborns with CCHD. Additionally, collaboration with engineering and computer sciences will be necessary to automate the SpO2-PIx CCHD screening algorithm. PROJECT NARRATIVE A screening approach that improves earlier detection of critical congenital heart defects with systemic obstruction is critically necessary. This application seeks to develop a screening algorithm that will combine the current screening standard, oxygen saturation, with non-invasive measurements of perfusion. This high risk, high reward approach is fundamentally different from other approaches as it will use machine learning techniques, and is expected to improve the detection of critical congenital heart defects with systemic obstruction and automate the interpretation of the screening results.","Improving Critical Congenital Heart Disease Screening and Detection of ""Secondary"" Targets",10018507,R21HD099239,"['Affect', 'Algorithms', 'American Heart Association', 'Aortic coarctation', 'Automatic Data Processing', 'Blood', 'Blood Circulation', 'Blood Pressure', 'Blood flow', 'California', 'Cardiac', 'Caring', 'Cessation of life', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Congenital Abnormality', 'Critical Congenital Heart Defects', 'Critical Illness', 'Data', 'Defect', 'Detection', 'Diagnosis', 'Early Diagnosis', 'Engineering', 'Equipment', 'Evaluation', 'Funding', 'Future', 'Goals', 'Individual', 'Infant', 'Infrastructure', 'Interruption', 'Intervention', 'Lesion', 'Lower Extremity', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Morbidity - disease rate', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'National Institute of Child Health and Human Development', 'Neonatal Screening', 'New York', 'Newborn Infant', 'Obstruction', 'Oxygen', 'Perfusion', 'Physiologic pulse', 'Population', 'Process', 'Pulsatile Flow', 'Pulse Oximetry', 'Research', 'Savings', 'Screening Result', 'Screening procedure', 'Sensitivity and Specificity', 'Specificity', 'Techniques', 'Testing', 'Time', 'Ultrasonography', 'United States', 'Upper Extremity', 'Validation', 'aortic arch', 'automated algorithm', 'base', 'clinical application', 'cohort', 'computer science', 'congenital heart disorder', 'high reward', 'high risk', 'human error', 'improved', 'indexing', 'infant death', 'innovation', 'interdisciplinary approach', 'member', 'mortality', 'multidisciplinary', 'neonatal period', 'novel', 'prenatal', 'prevent', 'screening', 'supervised learning']",NICHD,UNIVERSITY OF CALIFORNIA AT DAVIS,R21,2020,192195,-0.03353294122156179
"BDD CIS: Big Data Driven Clinical Informatics & Surveillance - A Multimodal Database Focused Clinical, Community, & Multi-Omics Surveillance Plan for COVID19 Abstract With South Carolina’s population already being vulnerable to poor health as evidenced by poor national health rankings, challenging rural geography and health professional shortages, the impact of the novel Coronavirus Disease 2019 (COVID-19) will be long lasting in the state. Patient morbidity and mortality rates already continue to increase, with ongoing economic damage to health systems and businesses. The speed of transmission and geographical spread of COVID-19 across South Carolina and the United States is alarming, which combined with the novel nature of the disease justifies the need for accelerated research to combat this pandemic. As clinicians and frontline health workers battle to save lives, creating a data environment that accelerates research is key, and necessary to battle the disease. Access to such information will equip frontline health workers to continue the fight against the disease. This proposal will build the capacity for accelerated research and intelligence gathering by coalescing multiple state partners and leveraging relevant data for discoveries around COVID-19. To accomplish this, this proposal aims to (1) create a de-identified linked database system via REDCap and a mobile application (app) to collate surveillance, clinical, multi-omics and geospatial data on both COVID-19 patients and health workers treating COVID-19 patients in South Carolina; (2) examine the natural history of COVID-19 including transmission dynamics, disease progression, and geospatial visualization; and (3) identify important predictors of short- and long-term clinical outcomes of COVID-19 patients in South Carolina using machine learning algorithms. These aims will be accomplished through collaborations with multiple state agencies and stakeholders relevant to COVID-19 and the creation of a REDCap database and mobile app that allow for coalescing relevant data in a timely fashion, combined with leveraging of statewide integrated data warehouse capabilities. Project Narrative COVID-19 represents an opportunity to create and deploy a research system that allows accelerated research on any pandemic. While South Carolina is rural in nature, and has low health rankings, it has a powerful integrated health data infrastructure that allows for tracking short-and long-term clinical and health system impacts of pandemics like the novel coronavirus (COVID-19). The creation of multiple data sources at the individual level, coupled with innovative big data science techniques will advance important discoveries in disease surveillance, transmission, natural history and progression important for treatment and necessary for targeted intervention purposes in South Carolina.","BDD CIS: Big Data Driven Clinical Informatics & Surveillance - A Multimodal Database Focused Clinical, Community, & Multi-Omics Surveillance Plan for COVID19",10136980,R01AI127203,"['2019-nCoV', 'Active Learning', 'Affect', 'Algorithms', 'American', 'Architecture', 'Archives', 'Big Data', 'Big Data Methods', 'Businesses', 'COVID-19', 'COVID-19 pandemic', 'Chest', 'Classification', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Common Cold', 'Communities', 'Complex', 'Computer software', 'Confidentiality of Patient Information', 'Coupled', 'Data', 'Data Analytics', 'Data Discovery', 'Data Sources', 'Database Management Systems', 'Databases', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease Surveillance', 'Disease susceptibility', 'Early Diagnosis', 'Economics', 'Ensure', 'Environment', 'Future', 'Geography', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Professional', 'Health Sciences', 'Health system', 'High Resolution Computed Tomography', 'Image', 'Individual', 'Intelligence', 'Intervention', 'Learning', 'Life Style', 'Link', 'Literature', 'Monitor', 'Morbidity - disease rate', 'Multiple Partners', 'Natural History', 'Nature', 'Outcome', 'Patients', 'Pattern', 'Population', 'Precision Health', 'Provider', 'Recovery', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Rural', 'SARS coronavirus', 'Secure', 'Series', 'Source', 'South Carolina', 'Speed', 'System', 'Techniques', 'Time', 'United States', 'Virus', 'Visualization', 'Work', 'big-data science', 'combat', 'data infrastructure', 'data management', 'data warehouse', 'deep learning', 'demographics', 'disability', 'disease natural history', 'disease transmission', 'experience', 'fight against', 'gene therapy', 'health data', 'imaging modality', 'improved', 'innovation', 'machine learning algorithm', 'mobile application', 'mortality', 'multimodal data', 'multimodality', 'multiple data sources', 'multiple omics', 'novel', 'novel coronavirus', 'novel virus', 'pandemic disease', 'patient privacy', 'precision medicine', 'response', 'systems research', 'time use', 'transmission process', 'trend', 'virology']",NIAID,UNIVERSITY OF SOUTH CAROLINA AT COLUMBIA,R01,2020,626275,-0.02631896648539399
"Public Insurance Design and Health at Older Ages PROJECT ABSTRACT Health insurance has been at the forefront of US public policy debate throughout the last decade. For elderly Americans, who benefit from nearly universal coverage under Medicare, decisions about the scope of their insurance coverage have been a central policy concern since Medicare’s inception in 1965. Medicare’s coverage is extensive, but by statute the program only covers medical services that are “reasonable and necessary,” which can be a controversial definition. Some decisions about what Medicare should cover have engendered intense debate, underscoring the importance of the program’s coverage decisions for millions of patients and doctors. The debate around the scope of Medicare coverage is likely to intensify in the coming decades as options for medical treatment, and testing, expand rapidly, the population ages, and public insurance systems grapple with how to address inequality in access to medical innovations. Yet, despite the importance of decisions about the scope of insurance coverage in and outside of Medicare, we know little about how the scope of coverage (as opposed to patient cost-sharing), affects treatment choices, clinical practice, and health outcomes for the elderly. The research outlined in this proposal aims to start filling this gap. The project investigates how the presence or lack of insurance coverage for specific procedural or pharmacologic therapies affects treatment decisions and health outcomes for the elderly with Alzheimer’s Disease and related Dementias (ADRD). This is a population that may be particularly vulnerable to changes and limits in insurance coverage, as the patients may have limited decision-making capacity and may be disproportionately exposed to treatments that are deemed experimental and lacking effectiveness to clear the “reasonable and necessary” threshold. Aim 1 of the project is to estimate the average effect of coverage decisions across prescription drugs and outpatient procedures on treatment decisions and health outcomes of elderly Medicare enrollees with ADRD. Aim 2 is to predict and characterize the subgroups of ADRD patients that are most likely to be affected by decisions that restrict the scope of insurance coverage using machine learning methods. The proposed empirical method is to use quasi- experimental variation that arises from natural experiments of abrupt changes in insurance coverage within different parts of the Medicare program. The analysis takes advantage of variation in the scope of formularies across Medicare Part D plans, as well as the variation in local coverage decisions for physician services and outpatient procedures under Medicare Part B. These sources of variation coupled with methods for quasi- experimental estimation of treatment effects and machine learning methods for heterogeneity analyses, allow estimating the response of treatment decisions and health of the elderly with ADRD across different drugs, procedures, and subgroups of ADRD patients. PROJECT NARRATIVE For elderly Americans, who benefit from nearly universal coverage under Medicare, decisions about the scope of their insurance coverage have been a central policy concern since Medicare’s inception in 1965 - Medicare’s coverage is extensive, but nevertheless incomplete as by statute the program only covers medical services that are “reasonable and necessary,” which can be hard to define. The debate around the scope of Medicare coverage is likely to intensify in the coming decades as options for medical treatment, and testing, expand rapidly, the population ages, and public insurance systems grapple with how to address inequality in access to medical innovations. In this project I investigate how changes in coverage for prescription drugs, physician office visits, and outpatient procedures affect the treatment received by elderly patients with Alzheimer’s Disease and related dementias (ADRD), who often face coverage limitations due to experimental nature of many ADRD treatments, and yet may be most vulnerable to unintended consequences of any coverage restrictions.",Public Insurance Design and Health at Older Ages,9928348,K01AG059843,"['Address', 'Admission activity', 'Affect', 'Age', 'Alzheimer&apos', 's disease related dementia', 'American', 'Amyloid', 'Anxiety', 'Barbiturates', 'Benzodiazepines', 'Caring', 'Characteristics', 'Clinical Medicine', 'Cost Sharing', 'Coupled', 'Data', 'Diagnostic', 'Diagnostic Procedure', 'Drug Prescriptions', 'Drug Targeting', 'Economics', 'Effectiveness', 'Elderly', 'Ethnic Origin', 'Exposure to', 'Face', 'Family Physicians', 'Formularies', 'Fracture', 'Frequencies', 'Future', 'Geography', 'Geriatrics', 'Gerontology', 'Goals', 'Health', 'Health Benefit', 'Health Insurance', 'Heart', 'Heterogeneity', 'Hospitalization', 'Individual', 'Inequality', 'Insurance', 'Insurance Coverage', 'Investigation', 'Knowledge', 'Learning Skill', 'Length of Stay', 'Light', 'Machine Learning', 'Measures', 'Medical', 'Medicare', 'Medicare Part B', 'Mental Depression', 'Mentors', 'Methods', 'Movement', 'Natural experiment', 'Nature', 'Office Visits', 'Outcome', 'Outcome Measure', 'Outpatients', 'Pain', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Physicians', 'Policies', 'Policy Maker', 'Population', 'Positron-Emission Tomography', 'Procedures', 'Public Policy', 'Race', 'Recording of previous events', 'Research', 'Services', 'Skilled Nursing Facilities', 'Source', 'Subgroup', 'System', 'Testing', 'Therapeutic procedure', 'Time', 'Training', 'Universal Coverage', 'Variant', 'aging population', 'alternative treatment', 'bariatric surgery', 'base', 'clinical practice', 'comorbidity', 'decision-making capacity', 'design', 'experience', 'high dimensionality', 'innovation', 'insurance claims', 'machine learning method', 'mortality', 'older patient', 'patient population', 'patient subsets', 'precision medicine', 'predictive modeling', 'programs', 'public health insurance', 'response', 'socioeconomics', 'theories', 'tool', 'treatment choice', 'treatment effect', 'treatment response']",NIA,STANFORD UNIVERSITY,K01,2020,126700,-0.018887955647897144
"Machine Learning and Network Science for Predicting Kidney Transplant Survival  Chronic kidney disease affects about 10% of adults in the United States and 7-12% of the population worldwide. It may lead to irreversible loss of kidney function, known as end-stage renal disease (ESRD). For patients with ESRD, kidney transplantation is the preferred treatment compared to dialysis in terms of patient survival, quality of life and cost. Despite the advantages of kidney transplants, most patients with ESRD are treated with dialysis primarily because there exist an insufficient number of compatible donors for patients. The human leukocyte antigens (HLAs) of the organ donor and recipient are known to be a significant contributing factor to transplanted organ survival times due to immunogenicity, the immune response of the recipient to the transplanted organ. Mismatches between donor and recipient HLAs are associated with shorter survival times; however, it is extremely rare to identify donors that have a perfect match with recipients, so most transplants involve mismatched HLAs. Our main objective is to accurately predict survival times for kidney transplants by incorporating both data- driven models of HLA compatibility based on outcomes of past transplants and biologically-driven models of HLA immunogenicity. Accurate prediction of survival times can improve patient transplant outcomes by enabling more efficient allocation of donors and recipients, particularly by reducing the number of repeat transplants due to graft failure with a poorly matched donor. We propose to estimate HLA compatibilities using high-dimensional variable selection techniques applied to outcomes of past transplants and through a novel donor-recipient latent space model for the HLA compatibility network. We then propose to incorporate these predicted compatibilities along with biologically-driven models of HLA immunogenicity using amino acid sequences and epitopes into a multi-task classification-based survival prediction algorithm. Our proposed approach for learning integrated data- and biologically-driven models of transplant survival generalizes broadly to organ transplantation (liver, heart, pancreas, lungs) and possibly to bone marrow transplantation. Finding better methods for human leukocyte antigen (HLA) matching between donor and recipient may redefine and improve clinical outcomes for organ and tissue transplants. In fact, new approach may revolutionize the selection of donors with recipients, thereby producing significantly improved long-term organ and tissue transplant survivals.",Machine Learning and Network Science for Predicting Kidney Transplant Survival ,9985180,R01LM013311,"['Adult', 'Affect', 'Amino Acid Sequence', 'Antigens', 'Anus', 'Biological', 'Bone Marrow Transplantation', 'Chronic Kidney Failure', 'Classification', 'Clinical', 'Data', 'Dialysis procedure', 'Donor Selection', 'End stage renal failure', 'Epitopes', 'HLA Antigens', 'Heart', 'Immune response', 'Kidney Transplantation', 'Lead', 'Learning', 'Liver', 'Lung', 'Machine Learning', 'Methods', 'Modeling', 'Organ Donor', 'Organ Survival', 'Organ Transplantation', 'Outcome', 'Pancreas', 'Patients', 'Plants', 'Population', 'Quality of life', 'Renal function', 'Research', 'Science', 'Space Models', 'Techniques', 'Time', 'Tissue Transplantation', 'Transplant Recipients', 'Transplantation', 'United States', 'base', 'cost', 'discrete time', 'graft failure', 'hazard', 'high dimensionality', 'human model', 'immunogenicity', 'improved', 'learning network', 'multitask', 'novel', 'novel strategies', 'prediction algorithm', 'survival prediction', 'transplant model', 'treatment comparison']",NLM,UNIVERSITY OF TOLEDO,R01,2020,277764,-0.02256161208212046
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9981804,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Models', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in silico', 'in vivo', 'insight', 'intracranial artery', 'microSPECT', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2020,601275,-0.00953104851564133
"Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria Project Summary/Abstract Fungal and bacterial pathogens are a major threat to human health. Few therapeutics exist to treat fungal infections while bacteria are becoming increasingly resistant to existing therapeutics. Humans have been using natural products to treat infections for thousands of years, long before the causal agents of infection were understood. Natural products have continued to be used as therapeutics in the modern age of medicine. Rates of rediscovery of known natural products have increased in traditional sources of natural products, such as soil bacteria. Recently, symbiotic Actinobacteria from insect agricultural systems have been recognized as a promising source of bioactive compounds, especially antifungal agents. These bacteria often produce natural products that defend an insect’s fungal crop from pathogenic fungus. The work proposed here will use chemical biology approaches such as phenotypic interaction screens, genomics, and a new bioinformatics approach to systematically search for bioactive natural products produced by Actinobacteria symbionts and other organisms in insect agricultural systems. The first part of this proposal focuses on using existing techniques to identify new bioactive natural products. Phenotypic interaction screens can identify bioactive natural products by determining if a symbiotic bacteria produces a natural product that inhibits the growth of a fungal pathogen and vice-versa. We will then use genomic sequencing, bioinformatics, and heterologous expression to identify and characterize biosynthetic gene clusters (BGCs) that are not expressed in the phenotypic interaction screens. The second part of the proposed work involves the use of a new bioinformatics technique to identify interesting bioactive natural products. Existing bioinformatics techniques identify BGCs and predict the most likely chemical structure of the corresponding natural product. However, they do not conclude anything concerning the functional role that the natural product plays. The technique developed here will use machine learning to predict the function that the natural product fulfills in the ecological context of the organism. This algorithm will facilitate the identification of bioactive natural products with therapeutically relevant functions. Project Narrative Fungal infections are an underappreciated threat to human health with high mortality rates and few effective therapeutic agents for treatment. Symbiotic Actinobacteria from insect agricultural systems are a promising source of antifungal agents since they often produce natural products with antifungal activity protecting an insect’s fungal crop from pathogenic fungus. The work proposed here will use phenotypic interaction screens, genome sequencing, and the development of a novel bioinformatics method to systematically mine Actinobacteria for antifungal and antibacterial products – leading to the discovery of new bioactive small molecules along with a deeper understanding of how natural products mediate the interaction between species in insect agricultural systems.",Bioinformatics and Chemical Biology Approaches for Identifying Bioactive Natural Products of Symbiotic Actinobacteria,9963295,F32GM128267,"['Actinobacteria class', 'Age', 'Agriculture', 'Algorithms', 'Anti-Bacterial Agents', 'Antibiotics', 'Antifungal Agents', 'Ants', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bioinformatics', 'Biological Assay', 'Biology', 'Breathing', 'Chemical Structure', 'Chemicals', 'Collaborations', 'Computational Biology', 'Computing Methodologies', 'Data Set', 'Development', 'Ecosystem', 'Gene Cluster', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Infection', 'Insecta', 'Learning', 'Life', 'Literature', 'Machine Learning', 'Mediating', 'Medicine', 'Methods', 'Mining', 'Modernization', 'Molecular Structure', 'Mycoses', 'Natural Products', 'Organism', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Public Health', 'Resistance', 'Role', 'Soil', 'Source', 'Structure', 'System', 'Techniques', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'Validation', 'Work', 'algorithm development', 'base', 'bioactive natural products', 'drug discovery', 'fungus', 'genetic information', 'genome sequencing', 'human disease', 'machine learning algorithm', 'mortality', 'novel', 'pathogen', 'pathogenic bacteria', 'pathogenic fungus', 'post-doctoral training', 'prediction algorithm', 'small molecule', 'symbiont']",NIGMS,HARVARD MEDICAL SCHOOL,F32,2020,65310,-0.018911710638758843
"Automated Object Contouring Methods & Software for Radiotherapy Planning Abstract In 2015, 1,658,370 new cancer cases are estimated to occur in the US, where nearly two-thirds will have radiation therapy (RT). Given that there are over 2,300 RT centers in the US, and current systems for contouring organs at risk (OARs) rely mostly on manual methods, there is a strong commercial opportunity for producing a software system that can contour OARs in medical images at a high degree of automation and for impacting current practice of RT planning. Encouraged by our strong Phase I results in thoracic and head and neck (H&N) body regions compared to current industry systems, we seek the accuracy, efficiency, and clinical acceptance of the contours output by our software product to significantly exceed those of existing systems. Our overall aim for Phase II is to advance the algorithms and prototype software developed in Phase I into a leading commercial software product, and demonstrate its efficacy in multiple medical centers across the country with diverse populations. Phase II specific aims are three-fold: (1) Further advance the automatic anatomy recognition algorithms from Phase I using advanced deep learning techniques. (2) Develop a cloud-based software auto contouring service. (3) Perform clinical evaluation of the new software on H&N and thoracic cases. Aim 1 will be accomplished in three stages: (a) Automating the process of defining the body region on given patient CT studies, which is currently done manually in our system, via a new concept of virtual landmarks using deep learning techniques. (b) Improving object recognition/ localization accuracy from the current 2 voxels for “good” quality data sets to 1 voxel and from 4-5 voxels for “poor” quality data sets to 2-3 voxels by using virtual landmarks to learn object relationships. (c) Improving object delineation by combining object localization methods with deep learning techniques applied to the vicinity of the localized objects to bring boundary distance accuracy within 1 voxel. Aim 2 will be achieved by developing a cloud-based Software-as-a-Service model to implement the software that incorporates the algorithms. To accomplish Aim 3, an evaluation study involving four academic RT centers will be undertaken to assess the efficiency, accuracy, and acceptability of the contours output by the new software. To assess efficiency, contouring time taken by the current clinical process will be compared to the time taken by the new software method plus any manual adjustment needed. Accuracy will be assessed by comparing software output to carefully prepared ground truth contours. Acceptability will be determined by conducting a blinded reader study, where an acceptability score (1-5) is given by radiation oncologists to software produced contours, ground truth contours, and contours produced by the normal clinical process, and comparing these scores. Expected clinical outcomes are significantly improved clinical efficiency/ acceptability of contouring compared to current practice. There is a strong commercial opportunity for producing a software system that can contour organs at risk in medical images at a high degree of automation for impacting current practice of radiation therapy planning. Encouraged by strong competitive results from the Phase I part of this project, in this grant, further technical advances and a cloud-based software service are proposed. A multicenter clinical evaluation of the new product is also planned to assess the clinical efficacy of the system.",Automated Object Contouring Methods & Software for Radiotherapy Planning,10241562,R42CA199735,"['Adoption', 'Algorithms', 'Anatomy', 'Automation', 'Back', 'Blinded', 'Body Regions', 'Chest', 'Clinical', 'Collaborations', 'Computer software', 'Country', 'Data Set', 'Development', 'Digital Imaging and Communications in Medicine', 'Evaluation', 'Evaluation Studies', 'Grant', 'Head and neck structure', 'Health Personnel', 'Image', 'Industry', 'Inferior', 'Investigation', 'Investments', 'Learning', 'Location', 'Malignant Neoplasms', 'Malignant neoplasm of thorax', 'Manuals', 'Medical Imaging', 'Medical center', 'Methods', 'Modeling', 'Morphologic artifacts', 'Names', 'Object Attachment', 'Organ', 'Outcome', 'Output', 'Pathology', 'Patients', 'Phase', 'Population Heterogeneity', 'Process', 'Protocols documentation', 'Radiation Oncologist', 'Radiation therapy', 'Reader', 'Research', 'Risk', 'Scanning', 'Service delivery model', 'Services', 'Slice', 'Specific qualifier value', 'System', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Time', 'Visualization', 'Work', 'base', 'clinical efficacy', 'cloud based', 'deep learning', 'image processing', 'improved', 'interest', 'learning network', 'learning strategy', 'neural network', 'novel strategies', 'object recognition', 'prototype', 'research clinical testing', 'software as a service', 'software development', 'software systems', 'treatment center', 'treatment planning', 'virtual']",NCI,"QUANTITATIVE RADIOLOGY SOLUTIONS, LLC",R42,2020,2500,-0.04573214646133128
"Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility Traumatic brain injury (TBI) is a leading cause of neurological disorders and affects over 2.5 million people each year, yet no treatment has successfully translated from bench to clinic. TBI is a broad term and encompasses an extremely heterogeneous set of injuries differing by cause, severity, biomechanics, and the varied, complex secondary injury responses that collectively result in chronic disabilities. Current preclinical research circumvents the issue of TBI heterogeneity by relying on specific preclinical animal models that mimic subpopulations of patients and particular secondary injury mechanisms with each study focusing on limited, individual pathways. This proposal instead aims to tackle TBI heterogeneity by approaching TBI as a “big data” problem and aggregating and analyzing the multidimensional data collectively. A framework for data harmonization and curation will be developed, and datasets from a consortium of preclinical labs employing a variety of preclinical TBI models will be collected and curated into an open data commons (ODC-TBI). Utilizing machine learning and multidimensional analytics, the proposed research will directly leverage TBI heterogeneity in the merged dataset to identify persistent features of TBI to empower translational research. By creating a preclinical TBI ODC and applying machine learning to integrate the heterogeneity of preclinical TBI models, the project will reveal multidimensional features of TBI across heterogeneous injuries and characterize how diverse secondary injury mechanisms interact and ultimately affect injury outcome. Throughout the project's timeline, new datasets will continue to be harmonized into the ODC-TBI according to the established framework. The ODC-TBI will be the first open multicenter, multi-model repository of preclinical TBI data and will enable the application of data science to the field of TBI. Furthermore, the ODC-TBI and the methods implemented throughout the project will be openly shared to improve reproducibility of TBI research. Together with the multidimensional analysis that will provide quantitative and qualitative understanding of TBI heterogeneity, the project aims to ultimately accelerate data- driven discovery and precision medicine for TBI. Reflecting the complexities of clinical traumatic brain injury (TBI), preclinical TBI research is confounded by the extreme heterogeneity prevalent across possible injury models and resulting biological responses. The proposed research will aggregate and curate an extensive open data commons (ODC) of preclinical TBI research with multiple TBI models and utilize machine learning to tackle TBI heterogeneity directly. The project will create an ODC for preclinical TBI research to improve data sharing and scientific reproducibility, and will empower translational TBI research by identifying multidimensional features of TBI that best predict functional outcome.",Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility,10042756,F32NS117728,"['Address', 'Affect', 'Animal Model', 'Big Data', 'Biological', 'Biological Markers', 'Biomechanics', 'Brain region', 'Chronic', 'Clinic', 'Clinical', 'Closed head injuries', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Element', 'Data Science', 'Data Set', 'Development', 'Foundations', 'Goals', 'Heterogeneity', 'Incidence', 'Individual', 'Inflammation', 'Informatics', 'Injury', 'Institutes', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Multivariate Analysis', 'National Institute of Neurological Disorders and Stroke', 'Outcome', 'Pathway interactions', 'Pattern', 'Pharmacologic Substance', 'Population', 'Positioning Attribute', 'Pre-Clinical Model', 'Principal Component Analysis', 'Publishing', 'Reproducibility', 'Research', 'Severities', 'Standardization', 'Synaptic plasticity', 'Therapeutic', 'TimeLine', 'Translating', 'Translational Research', 'Translations', 'Traumatic Brain Injury', 'behavioral outcome', 'bench to bedside', 'biomarker discovery', 'controlled cortical impact', 'data curation', 'data framework', 'data harmonization', 'data sharing', 'disability', 'experimental study', 'functional outcomes', 'genetic manipulation', 'improved', 'insight', 'multidimensional data', 'multiple datasets', 'nerve injury', 'nervous system disorder', 'neuroinflammation', 'open data', 'patient subsets', 'pre-clinical', 'pre-clinical research', 'precision medicine', 'repository', 'response', 'response to injury']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2020,69810,-0.017478566000164706
"Exploring the role of gonadotropins in Down syndrome PROJECT SUMMARY/ABSTRACT Down syndrome (DS) is a common chromosomal disorder that results from the triplication of chromosome 21. Comorbid medical conditions frequently occur among individuals with DS; however, opthalamic disorders such as keratoconus occurs 6 times more often among this population as compared to the general population. Keratoconus is a known multifactorial, progressive, degenerative disease of the cornea; however, the etiology of the disease remains unclear. Our recent work has found that hormone-regulated Prolactin-Induced Protein (PIP) is a potential biomarker of keratoconus but the link between DS and keratoconus remains unclear. Pathway analysis has further linked comorbid DS and keratoconus through gonadotropins and serum amyloid A1 (SAA), a protein linked in a series of studies to cognitive dysfunction among adults with DS. The proposed study aims to determine and validate the role of gonadotropins in the DS population with the keratoconus co-morbidity with the long-term goal to better understand the underlying mechanisms that result in increased risk for ocular co-morbidities such as keratoconus among this population. This pilot study will include n=90 participants (n=30 DS; n=30 comorbid DS and Keratoconus; n=30 healthy controls). All participants will provide biofluid samples of tears and plasma, which will be assayed on an electrochemiluminescence (ECL) platform. Analyses will be performed using R and SPSS 24 (IBM) in a series of steps. Support Vector Machine (SVM) analyses will be applied to examine the use of biomarkers to detect disease presence (i.e. keratoconus comorbidity). Least absolute shrinkage and selection operator (LASSO) analyses will help with variable selection in order to enhance the prediction accuracy of the models. Biomarkers will include hormone specific (LSH, LH, GNRH, LHR, FSHR, GNRHR) as well as a panel of plasma proteins including biomarkers of inflammation linked with cognitive dysfunction (SAA, CRP, ICAM1, VCAM1, IL-6, IL-10, TNF-α, IL-5, IL-7, Eotaxin-3, TARC, A2M, B2M, FVII, TNC, Adiponectin, FABP-3, IL-18, PPY, TPO and I-309). Diagnosis of keratoconus will be utilized as the dependent variable. If successful, data obtained from this study could help facility the development of a screening tool for detecting individuals with DS who are at risk for development of keratoconus, with the goal of defining potential enrollment into clinical trials. PROJECT NARRATIVE Vision disorders occur in nearly all individuals with Down syndrome (DS) with development of ocular co-morbidities such as keratoconus, a cornea dystrophy, occurring 6 times more often in this population as compared to the general population. Recent work from our group has found differences in sex hormones (i.e. androgen and estrogen levels) among individuals with keratoconus as compared to healthy controls but this difference was impacted by age and gender. Information gained from this study will (1) reveal the relationship between gonadotropins among individuals with DS and ocular comorbidities, specifically keratoconus, and (2) provide insight into the association between known plasma biomarkers including serum amyloid A1 and gonadotrophins among those with DS as well as those with comorbid keratoconus and DS.",Exploring the role of gonadotropins in Down syndrome,10108508,R21EY032320,"['Adult', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Amyloid', 'Androgens', 'Biochemical', 'Biological Assay', 'Biological Markers', 'Biology', 'Birth', 'Blepharitis', 'Blood', 'CCL1 gene', 'CCL17 gene', 'CCL26 gene', 'Cataract', 'Characteristics', 'Chromosome 21', 'Clinical Trials', 'Congenital chromosomal disease', 'Cornea', 'Corneal Diseases', 'Corneal dystrophy', 'Data', 'Degenerative Disorder', 'Development', 'Diagnosis', 'Disease', 'Down Syndrome', 'Down-Regulation', 'Enrollment', 'Environmental Risk Factor', 'Estrogens', 'Etiology', 'Eye Development', 'Follicle Stimulating Hormone', 'Future', 'GNRH1 gene', 'GNRH2 gene', 'GNRHR gene', 'Gender', 'General Population', 'Genes', 'Genetic Diseases', 'Goals', 'Gonadal Hormones', 'Gonadal Steroid Hormones', 'Gonadotropins', 'Health', 'Hormonal', 'Hormones', 'ICAM1 gene', 'Impaired cognition', 'Incidence', 'Individual', 'Inflammation', 'Intellectual functioning disability', 'Interleukin-10', 'Interleukin-18', 'Interleukin-5', 'Interleukin-6', 'Interleukin-7', 'Keratoconus', 'Lacrimal Duct Obstruction', 'Link', 'Longevity', 'Luteinizing Hormone', 'Medical', 'Modeling', 'Nature', 'Participant', 'Pathogenesis', 'Pathologic Nystagmus', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pilot Projects', 'Plasma', 'Plasma Proteins', 'Play', 'Population', 'Populations at Risk', 'Prolactin', 'Proteins', 'Proteomics', 'Publishing', 'Quality of life', 'Readiness', 'Refractive Errors', 'Research', 'Risk', 'Role', 'Saliva', 'Sampling', 'Screening procedure', 'Series', 'Serum', 'Severities', 'Strabismus', 'TNF gene', 'Thinness', 'Thyroid Hormones', 'Time', 'Vision Disorders', 'Visual Acuity', 'Weight', 'Work', 'adiponectin', 'base', 'clinically relevant', 'comorbidity', 'design', 'high reward', 'high risk', 'insight', 'link protein', 'mild cognitive impairment', 'novel', 'potential biomarker', 'programs', 'receptor', 'sex', 'support vector machine']",NEI,UNIVERSITY OF NORTH TEXAS HLTH SCI CTR,R21,2020,403584,-0.018382140593150274
"Designing neutralization antibodies against Sars-Cov-2 Project Summary COVID-19 has become a worldwide pandemic whose rapid spread and mortality rate threatens millions of lives and the global economic system. Developing effective treatment such as neutralization antibodies is an urgent need. We propose here to develop a new method to design antibodies strongly bind to the SARS-CoV-2 receptor binding domain (RBD) that is necessary for viral entrance to human cells. We will develop a novel approach that combines directed evolution, deep sequencing and interpretable neural network models to efficiently identify strong and specific antibodies. This method will allow analyzing large sequencing data sets of antibody variants against the SARS-CoV-2 RBD in order to derive superior binders that do not exist in the original library. Iteration through directed evolution and computational design will efficiently identify neutralization antibody candidates that can be used as potent therapeutics to treat COVID-19. Narrative: Developing neutralization antibodies is critical to provide effective treatment for Covid-19.",Designing neutralization antibodies against Sars-Cov-2,10173204,R21AI158114,"['2019-nCoV', 'Affinity', 'Amino Acids', 'Antibodies', 'Binding', 'COVID-19', 'Cells', 'Cessation of life', 'Clinical Trials', 'Consumption', 'Data', 'Data Set', 'Development', 'Directed Molecular Evolution', 'Economics', 'Epitopes', 'Future', 'Gene Library', 'Histones', 'Human', 'Human Engineering', 'Immunoglobulin G', 'Lead', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Mutate', 'Mutation', 'Nature', 'Network-based', 'Neural Network Simulation', 'Peptides', 'Positioning Attribute', 'Process', 'Reporting', 'Resistance', 'Screening procedure', 'Solubility', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Variant', 'Viral', 'Virus', 'Virus Diseases', 'base', 'clinical efficacy', 'data archive', 'deep learning', 'deep sequencing', 'design', 'drug candidate', 'effective therapy', 'machine learning method', 'mortality', 'mutant', 'neural network', 'neutralizing antibody', 'novel strategies', 'pandemic disease', 'receptor binding', 'screening', 'trend']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2020,433750,-0.027431012179267304
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,10124880,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,50000,-0.023947337918730344
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9980967,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,1100000,-0.023947337918730344
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,-0.02421501756440073
"Maternal Antecedents and Electronic Fetal Monitoring in Term Asphyxia (MAESTRA) Neonatal hypoxic-ischemic encephalopathy (HIE) is a neurologic syndrome that results from reduced flow of oxygenated blood to the fetal or newborn brain. HIE occurs in 1-3 per 1,000 term births and may cause death or neurologic disabilities such as cerebral palsy. Electronic fetal monitoring (EFM) was developed in the 1970's to assess the adequacy of fetal oxygenation as a strategy to prevent HIE, and is now standard of care. Yet clinical trials report that EFM usage has not reduced the rate of CP, perinatal death or HIE, but is associated with a dramatic increase in cesarean deliveries. The currently used 3 Category fetal heart rate (FHR) classification system, based on simple rules designed to be easy to apply at the bedside, has some utility in predicting HIE. However, Category II FHR patterns that make up the vast majority of tracings are poorly predictive of HIE and confer “indeterminate” risk. Category III patterns are also of limited use in predicting HIE due to low sensitivity. There is an urgent need to develop better objective methods to assess EFM that would identify more fetuses at risk of HIE in time for corrective actions. Uterine tachysystole, or excessive frequency of uterine contractions, has been implicated as a preventable cause of HIE; yet studies report conflicting results. EFM research has been limited by an inability to access and manually analyze the large datasets needed to study HIE. We now have the ability to analyze digital EFM signals using automated methods to measure standard FHR patterns as well as to discover novel aspects of the tracing that may not be readily detectable by a clinician at the bedside. We hypothesize that modern signal processing and machine learning techniques can create highly predictive models of HIE by analyzing established and novel features of EFM tracings, in combination with demographic and pertinent clinical information from the mother and fetus. We propose a population-based retrospective cohort study of 350,000 infants born at ≥ 36 weeks gestation at Kaiser Permanente Northern California in 2010-19. Our specific aims are: 1) To create the MAESTRA Cohort dataset that links EFM recordings to HIE and neonatal acidosis among 350,000 infants born at ≥ 36 weeks gestation in 2010-19 at Kaiser Permanente Northern CA; 2) Using modern signal processing and machine learning techniques, to extract established and novel FHR and uterine contractility features from the EFM recordings, and to determine which of these features are most predictive of HIE and acidosis when combined with maternal and fetal clinical data; and 3) To perform external validation by applying the final predictive models to a historical dataset. We anticipate that machine learning techniques incorporating novel FHR and uterine contractility patterns over time, as well as pre- and perinatal clinical characteristics, will improve the predictive value of the EFM data that are already being collected as part of routine care. Our results will inform future clinical trials. Such an unprecedented large-scale multidisciplinary study will lead to improvements in our ability to use EFM data to prevent neonatal brain injury while minimizing unnecessary cesarean sections. MAESTRA Project Narrative Hypoxic-ischemic encephalopathy (HIE) occurs when a baby gets reduced oxygen and blood flow to the brain, and can lead to death or long-term disabilities such as cerebral palsy. During labor and delivery, doctors are able to continuously record the heart rate of the fetus. This study will determine how best to use the heart rate information so that we can reduce the number of infants who develop this severe brain condition.",Maternal Antecedents and Electronic Fetal Monitoring in Term Asphyxia (MAESTRA),9972526,R01HD099216,"['Acidosis', 'Address', 'Apgar Score', 'Asphyxia', 'Blood', 'Blood flow', 'Brain', 'California', 'Categories', 'Cause of Death', 'Cerebral Palsy', 'Cesarean section', 'Cessation of life', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cohort Studies', 'Computerized Medical Record', 'Conflict (Psychology)', 'Data', 'Data Set', 'Discipline of obstetrics', 'Educational workshop', 'Fetal Heart Rate', 'Fetal Monitoring', 'Fetus', 'Frequencies', 'Future', 'Heart Rate', 'Infant', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Metabolic Brain Diseases', 'Metabolic acidosis', 'Methods', 'Modeling', 'Modernization', 'Mothers', 'National Institute of Child Health and Human Development', 'Neonatal', 'Neonatal Brain Injury', 'Neurologic', 'Newborn Infant', 'Observational Study', 'Outcome', 'Oxygen', 'Pattern', 'Perinatal', 'Perinatal anoxic ischemic brain injury', 'Perinatal mortality demographics', 'Population', 'Positioning Attribute', 'Predictive Value', 'Pregnancy', 'Preventive Intervention', 'Records', 'Reporting', 'Research', 'Retrospective cohort study', 'Risk', 'Seizures', 'Sensitivity and Specificity', 'Signal Transduction', 'Syndrome', 'System', 'Techniques', 'Term Birth', 'Testing', 'Time', 'Uterine Contraction', 'Uterus', 'Validation', 'base', 'cohort', 'computerized', 'design', 'digital', 'disability', 'effectiveness evaluation', 'falls', 'fetal', 'fetus at risk', 'high risk', 'hypoxia neonatorum', 'improved', 'large datasets', 'multidisciplinary', 'neonatal hypoxic-ischemic brain injury', 'novel', 'population based', 'predictive modeling', 'prevent', 'routine care', 'signal processing', 'standard measure', 'standard of care', 'uterine contractility']",NICHD,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2020,667049,-0.04414334520836935
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.029284822477105635
"Antibodies as Drugs ABSTRACT Support is requested for a Keystone Symposia conference entitled Antibodies as Drugs organized by Drs. Pierre Bruhns, Patrick C. Wilson, Esther Breij and David P. Humphreys. The conference will be held in Whistler, British Columbia from January 27-30, 2021. Therapeutic antibodies continue to largely populate the list of top biologic drugs. They revolutionize the treatment of pathologies falling into major disease areas, which include cancer, chronic inflammatory diseases, allergy and infectious diseases. As more antibodies are being tested to treat an expanding number of pathologies, there has also been a concomitant increase in the frequency of resistance and escape mechanisms. Thus, researchers are exploring alternative concepts to prevent unresponsiveness or treat resistant patients. Therefore, not only are new methodological and technological advances being developed, paradigm-shifting concepts are being developed in order to face these challenges. This conference in the Keystone Symposia series on Antibodies as Drugs aims to present the state-of-the-art in antibody therapeutics, repertoires and deep learning, bispecific antibodies and engineering. This conference will provide a forum for extensive interactions between investigators from both industry and academia. Goals of this conference include discussion of recent breakthroughs in identifying therapeutics from patients, understanding vaccination and mode of delivery, modalities for half-life extensions and involvement of complement in antibody mode of action. It is anticipated that research in the field will showcase the rise of bispecific antibodies and highlight the technologies to develop them, improve their functionalities, and boost clinical efficacies, particularly in immune-oncology. PROJECT NARRATIVE The success of early antibody approaches has been tremendous and now they have revolutionized treatment for many major disease areas, such as cancer, chronic inflammatory diseases, allergy and infectious diseases. However, as more antibodies are being tested to treat an expanding number of pathologies, then more resistance and escape mechanisms are being found, forcing scientists to search for alternative concepts to prevent unresponsiveness or treat resistant patients. Therefore, this conference will gather scientists from many different disciplines, including structural and cellular immunology, biology, engineering, chemistry and medicine, to share ideas and help develop novel antibody therapeutics.",Antibodies as Drugs,10071067,R13AI154980,"['Academia', 'Antibodies', 'Area', 'Biological', 'Biology', 'Bispecific Antibodies', 'British Columbia', 'Cellular Immunology', 'Chemistry', 'Collaborations', 'Communicable Diseases', 'Complement', 'Discipline', 'Disease', 'Educational workshop', 'Engineering', 'Face', 'Goals', 'Half-Life', 'Hypersensitivity', 'Immunooncology', 'Industry', 'Knowledge', 'Learning', 'Life Extension', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Methodology', 'Modality', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Research', 'Research Personnel', 'Resistance', 'Scientist', 'Series', 'Structure', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic antibodies', 'Vaccination', 'antibody engineering', 'career', 'chronic inflammatory disease', 'clinical efficacy', 'clinical practice', 'deep learning', 'falls', 'improved', 'insight', 'novel', 'novel therapeutics', 'posters', 'prevent', 'resistance frequency', 'success', 'symposium']",NIAID,KEYSTONE SYMPOSIA,R13,2020,11500,-0.029779917144992837
"Longitudinal neuroimaging and neurocognitive assessment of risk and protective factors across the schizophrenia spectrum PROJECT SUMMARY Schizotypal personality disorder (SPD) is similar to schizophrenia (SZ), but with fewer and attenuated abnormalities, thus representing an important yet understudied intermediate SZ-spectrum phenotype. Examination of abnormalities in SPD will provide information regarding etiology, genetics, treatment and risk factors associated with psychosis. Although individuals with SPD demonstrate marked temporal lobe abnormalities that resemble SZ, we hypothesize that relative “sparing” or “functional enhancement” in the frontal lobes (e.g., dorsolateral prefrontal cortex), may protect these individuals from frank psychosis and the severe social and cognitive deficits typically observed in SZ. Studying SPD is powerful as antipsychotic medication and hospitalization confounds observed in SZ are not present. Moreover, there is no study examining neurobiological changes in the SZ-spectrum that incorporates individuals with SPD using a longitudinal design as proposed here. This novel approach will help disentangle potential risk and protective factors for psychosis in the SZ spectrum. This is the first longitudinal study to utilize multimodal MR imaging and Research Domain Criteria (RDoC) approaches in SZ-spectrum disorders to identify aberrant neural circuitry along a continuum from healthy controls (HCs) to SPD to SZ and examine changes in these measures in relationship to impairments in symptom severity, neurocognition and functional outcome. We propose studying three groups (80 in each) of demographically matched and rigorously diagnosed individuals (age 18- 40): HCs (no Axis I or personality disorder), unmedicated individuals with SPD (and no Axis I disorder), and early-onset (first 2 years of illness) SZ patients at baseline, 9-, and 18-month follow-up. Measures assessing frontal and temporal lobe integrity include multimodal MR imaging (structural MRI, DTI, resting-state fMRI, and task-based fMRI with a nonverbal event related working-memory task; baseline and 18-months) and neuropsychological assessment (all three timepoints). We will utilize dynamic causal modeling to test competing neurobiological models involving abnormal frontotemporal connectivity in the SZ-spectrum and machine learning approaches to integrate multimodal neuroimaging, neurocognitive, and clinical assessment data. We focus on three specific aims: (1) Investigate the longitudinal course of frontal-temporal lobe/circuitry abnormalities in the SZ-spectrum using multimodal MR imaging; (2) Investigate the longitudinal course of neurocognition, clinical, and functional outcome in the SZ spectrum; (3) Determine which factor or combination of factors differentiate groups in the SZ-spectrum to identify those that are associated with risk for and protection from SZ using machine learning. PROJECT NARRATIVE Schizotypal personality disorder (SPD) is similar to schizophrenia (SZ), but with fewer and attenuated abnormalities, representing an important yet understudied intermediate SZ-spectrum phenotype that provides important information regarding etiology, genetics, treatment and risk factors associated with psychosis. This study utilizes a Research Domain Criteria (RDoC) approach coupled with longitudinal multimodal MR imaging and neurocognitive assessment in SZ-spectrum disorders to identify aberrant neural circuitry and cognition along a continuum from healthy controls to SPD (in the SZ spectrum but not fully psychotic) to SZ (psychosis). Changes in frontal-temporal regions/circuitry and neurocognitive measures over time will be examined together using novel approaches (e.g., machine learning) to predict diagnostic group, symptom severity, and functional outcome for the identification of key factors associated with risk and resilience in the SZ spectrum.",Longitudinal neuroimaging and neurocognitive assessment of risk and protective factors across the schizophrenia spectrum,9985547,R01MH121411,"['Age', 'Antipsychotic Agents', 'Attenuated', 'Brain', 'Brain region', 'Brodmann&apos', 's area', 'Clinical', 'Clinical assessments', 'Cognition', 'Cognitive deficits', 'Coupled', 'Data', 'Deterioration', 'Diagnosis', 'Diagnostic', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Etiology', 'Event', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hospitalization', 'Impairment', 'Individual', 'Investigation', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Neurobiology', 'Neurocognition', 'Neurocognitive', 'Neuropsychology', 'Onset of illness', 'Outcome', 'Pathogenesis', 'Patients', 'Pattern', 'Performance', 'Personality Disorders', 'Pharmaceutical Preparations', 'Phenotype', 'Prefrontal Cortex', 'Psychotic Disorders', 'Research Domain Criteria', 'Rest', 'Risk', 'Risk Assessment', 'Risk Factors', 'Scanning', 'Schizophrenia', 'Schizotypal Personality Disorder', 'Severities', 'Short-Term Memory', 'Structure', 'Symptoms', 'Temporal Lobe', 'Testing', 'Time', 'Treatment Factor', 'Ursidae Family', 'base', 'causal model', 'early onset', 'follow-up', 'frontal lobe', 'functional outcomes', 'improved', 'longitudinal course', 'longitudinal design', 'multimodality', 'neural circuit', 'neuroimaging', 'novel strategies', 'predictive modeling', 'protective factors', 'resilience', 'social deficits', 'white matter']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,839437,-0.04496729682783442
"Evaluation of molecular mechanisms of treatment response in late-life depression DESCRIPTION: Over the past decades, antidepressants and psychotherapy have been the first-line treatments for LLD. Despite being safe and well-tolerated, a large number of patients do not achieve full and persistent remission after initial treatment. About 50% of patients with LLD do not respond after two antidepressant trials, meeting the consensus definition of treatment resistance (TR-LLD). The persistence of chronic and elevated depressive symptoms in older adults has significant clinical and public health implications. This has been correlated to poor general health, reduced quality of life, and a higher risk of mortality when compared to those with sustained remission after treatment. Despite the relevance to public health of TR-LLD, there is little information about the biological mechanisms and no robust clinical prediction model to evaluate at the outset of antidepressant therapy who will or will not respond to treatment. Leveraging an NIMH funded clinical trial, the Incomplete Response in Late-Life Depression: Getting to Remission” (IRL-GREY), across 3 sites, in this study, we propose to evaluate the biological mechanisms related to treatment response in late-life depression and to develop a machine learning based algorithm for prediction of treatment response in these subjects. We will carry out a comprehensive, multiplexed proteomic analysis on 542 samples from patients who completed phase 1 and phase 2 of the clinical trial. We hypothesise that ageing-related biological pathways (i.e. inflammatory response control, proteostasis control, cell damage response, endothelial function) will be associated with poorer treatment response in LLD. Moreover, we hypothesize that a machine learning derived biomarker panel will have sensitivity and specificity greater than 80% to predict treatment response in LLD. Finally, we will evaluate the biological mechanisms related to different depressive symptoms trajectories after treatment. This work will set the stage for a biologically-driven model of treatment response that will be useful to guide, at the outset of antidepressant treatment, those who will benefit more from a specific treatment. If successful, our work can accelerate therapeutic efforts and innovation targeting depression and reduce suffering for large numbers of elderly and their families. Using advanced molecular approaches, we will determine the biological mechanisms related to treatment response in late-life depression. Our proposed study will also develop a predictive tool combining neurocognitive, neuroimaging, and protein data to identify who with late-life depression is more likely to have no benefit from antidepressant treatment at the outset of therapy. The goal of the study is to clarify the mechanisms of treatment response in late-life depression, and whether we can effectively identify those who will benefit from antidepressant therapy at the outset of treatment.",Evaluation of molecular mechanisms of treatment response in late-life depression,9966041,R01MH118311,"['Address', 'Aftercare', 'Aging', 'Antidepressive Agents', 'Bioinformatics', 'Biological', 'Biological Aging', 'Biological Markers', 'Biological Process', 'Blood specimen', 'Brain', 'Cell Cycle', 'Chronic', 'Clinical', 'Clinical Trials', 'Consensus', 'Data', 'Disease remission', 'Double-Blind Method', 'Elderly', 'Endothelium', 'Evaluation', 'Family', 'Funding', 'Geroscience', 'Goals', 'Growth Factor', 'Health', 'Immune', 'Impaired cognition', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Machine Learning', 'Mental Depression', 'Modeling', 'Molecular', 'National Institute of Mental Health', 'Neurocognitive', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Phase', 'Phase II Clinical Trials', 'Placebos', 'Population', 'Prediction of Response to Therapy', 'Prevalence', 'Proteins', 'Proteomics', 'Psychotherapy', 'Public Health', 'Quality of life', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Structure', 'Testing', 'Therapeutic', 'Tissues', 'Work', 'angiogenesis', 'aripiprazole', 'base', 'biomarker panel', 'cell injury', 'cohort', 'depressive symptoms', 'design', 'endothelial dysfunction', 'geriatric depression', 'high risk', 'innovation', 'meetings', 'mortality risk', 'neuroimaging', 'new therapeutic target', 'novel', 'novel marker', 'novel strategies', 'open label', 'prediction algorithm', 'predictive modeling', 'predictive tools', 'proteostasis', 'recruit', 'response', 'therapy resistant', 'treatment response', 'treatment-resistant depression', 'venlafaxine']",NIMH,CENTRE FOR ADDICTION AND MENTAL HEALTH,R01,2020,516549,-0.024394922418315895
"Understanding the role of the Complement Proteome in progressive Diabetic Kidney Disease PROJECT SUMMARY / ABSTRACT There is a critical need to identify novel mechanisms of diabetic kidney disease (DKD) that will provide targets for new interventions. Chronic inflammation is one plausible mechanism. Using untargeted high-throughput aptamer proteomics, our recently published study has shed new light on specific, key inflammatory drivers of DKD. This was a large prospective three-cohort study that identified a novel and extremely robust circulating signature (KRIS) associated with risk of ESRD in diabetes. Our pilot study points to the data-driven connection between circulating KRIS and urinary profiles of the Complement pathway. Our hypothesis is that the Complement involvement in the kidney is a downstream effect of the systemic inflammatory processes mediating an increased DKD risk. The overarching goal of this proposal is to provide a high-resolution view of the involvement of the Complement proteome in progressive diabetic kidney disease. Aim 1 will comprehensively evaluate the etiological role of the urinary Complement proteome in progressive DKD leading to ESRD. This evaluation will leverage a prospective two-cohort population of Joslin Kidney Study (JKS) participants with an overt DKD at baseline followed for 10 years (primary outcome – incident ESRD). Measurements will utilize an aptamer proteomic technology (SOMAscan). Aim 2 will extend generalizability of the urinary Complement proteome to earlier DKD stages. The proposed study will be conducted in participants of the Preventing Early Renal Loss (PERL) clinical trial with predominantly normal renal function at baseline followed for 3 years (primary outcome - renal slope). Aim 3 proposes to gain direct insight into the intra-renal Complement proteome by targeted and untargeted protein studies in diabetic kidney tissue (Susztaklab Biobank). This project focuses on a significant public health problem, leverages the progressiveness of the disease, employs an innovative proteomic technology and stems from strong preliminary data. Advances in this project will pinpoint missing key components of DKD etiology, thereby accelerating drug development strategies for patients with diabetes. PROJECT NARRATIVE Diabetes accounts for approximately 45% of prevalent ESRD cases in the United States, therefore new interventions to prevent or decelerate development of kidney failure are critical in order to improve the health of patients with diabetes who comprise a large sector of the US population. This study will advance our knowledge regarding the etiology of diabetic kidney complications evaluating specific components of systemic (KRIS) and local, kidney inflammation (Complement). These advances offer long-term potential for the development of new therapies that will ultimately improve clinical outcomes of patients with diabetes.",Understanding the role of the Complement Proteome in progressive Diabetic Kidney Disease,9866226,R01DK123459,"['Address', 'Biological Markers', 'CCL2 gene', 'Characteristics', 'Chronic', 'Clinical', 'Clinical Trials', 'Cohort Studies', 'Complement', 'Complex', 'Data', 'Data Reporting', 'Development', 'Diabetes Mellitus', 'Diabetic Nephropathy', 'Disease Progression', 'End stage renal failure', 'Ethnic Origin', 'Etiology', 'Evaluation', 'Goals', 'Health', 'Histology', 'Immunoglobulin G', 'Inflammation', 'Inflammatory', 'Injury to Kidney', 'Intervention', 'Kidney', 'Kidney Failure', 'Knowledge', 'Light', 'Machine Learning', 'Measurement', 'Mediating', 'Mediation', 'Medicine', 'Nature', 'Nested Case-Control Study', 'Participant', 'Pathway interactions', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phenotype', 'Pilot Projects', 'Population', 'Process', 'Progressive Disease', 'Proteins', 'Proteome', 'Proteomics', 'Public Health', 'Publishing', 'Renal function', 'Resolution', 'Risk', 'Role', 'Severity of illness', 'Signaling Protein', 'Source', 'Technology', 'Tissue Banks', 'Tissues', 'Tubular formation', 'Tumor Necrosis Factor Receptor', 'United States', 'aptamer', 'biobank', 'cohort', 'complement pathway', 'complement system', 'diabetic', 'disorder risk', 'drug development', 'follow-up', 'improved', 'indexing', 'innovation', 'insight', 'member', 'nephrogenesis', 'non-diabetic', 'novel', 'novel therapeutics', 'prevent', 'primary outcome', 'prospective', 'random forest', 'stem', 'study population', 'tool', 'transcriptomics', 'urinary']",NIDDK,JOSLIN DIABETES CENTER,R01,2020,498936,-0.02008909091088646
"APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY Abstract  Acute myeloid leukemia (AML) accounts for half of all pediatric leukemia deaths and is the leading cause of leukemia-related death in adulthood. One reason for worse outcomes is the inability to properly assess for minimal residual disease (MRD) following therapy. Unlike ALL, AML presents with multiple subclonal populations without a singular clonal surface marker, and surface markers can change during therapy. The current gold standard for AML MRD is multi-parameter flow cytometry (MPFC), which is predictive of outcomes to frequencies of 0.001, yet 30% of MPFC-MRD-negative patients still relapse. Alternatively, every AML case harbors leukemia-specific mutations that could be markers of disease, except that next-generation sequencing has high error rate of ~1%. In this proposal, we will implement a novel, validated error-corrected sequencing (ECS) strategy, developed by the Druley lab in collaboration with Illumina, to improve MRD assessment of AML subclonal heterogeneity in 990 pediatric de novo AML cases from the Children's Oncology Group AAML1031 study. We hypothesize that using a highly sensitive sequencing method will improve identification of residual AML, provide important insights on subclonal heterogeneity in pediatric AML, improve understanding of the role of germline variability and gene function on relapses or refractory disease and facilitate personalized medicine. To interrogate this hypothesis, we propose the following aims: 1. Define subclonal heterogeneity at diagnosis and end of Induction 1 (EOI1) in 990 pediatric de novo  AML patients (n=1890). By using the largest prospective study of pediatric AML that has ever been  performed, we will perform ECS on 94 genes that are the most frequently mutated genes in pediatric and  adult AML at diagnosis and EOI1 to identify patterns of mutation associated with relapsed disease, FAB  subtypes or other cytogenetic features. 2. Correlate ECS-MRD with existing EOI1 MPFC-MRD for all participants in the COG AAML1031 study.  A major question is whether the “different from normal” cell population identified as residual disease by  MPFC is actually the same population(s) identified by ECS. We will define residual disease by ECS and  compare results to MPFC status (positive/negative), actual MPFC percentages (<0.001) and the clinical  outcomes (relapse risk, disease-free survival and overall survival) of study participants. 3. Integrate germline variation and all subclonal mutations into mechanistic groups that are frequently  mutated in pediatric AML and correlate with outcomes using unbiased machine learning  algorithms. Preliminary data tells us that every patient will have multiple subclones at diagnosis and EOI1  as well as germline variants in AML-associated genes, which may be important for outcome. In this aim, we  will take these mutations into account as well as MPFC, clinical features and cytogenetics for probabilistic  risk assessment using unsupervised machine learning algorithms for improved outcome prognostication. Narrative We have developed Error-Corrected Sequencing (ECS) that enables the highly accurate detection of leukemia- specific mutations in heterogeneous DNA samples to a limit of 0.0001. We will perform ECS with a panel of 94 frequently mutated genes in adult and pediatric AML in matched diagnostic and end of Induction 1 bone marrow samples from 990 pediatric de novo AML patients enrolled on the Children's Oncology Group AAML1031 protocol, which is the largest pediatric AML trial ever performed in North America. With these data, we can truly understand subclonal heterogeneity in pediatric AML, significantly improve minimal residual disease testing in pediatric AML, and integrate these data into an unbiased multivariate probability platform taking into account individual sequencing, flow cytometry, cytogenetic and clinical features to provide truly personalized cancer care.",APPLYING ERROR-CORRECTED SEQUENCING TO DETECT MINIMAL RESIDUAL IN THE AAML 1031 STUDY,9844413,R01CA211711,"['Acute Myelocytic Leukemia', 'Adult', 'Adult Acute Myeloblastic Leukemia', 'Alleles', 'BAY 54-9085', 'Biological Markers', 'Bone Marrow', 'Bortezomib', 'Categories', 'Cessation of life', 'Childhood', 'Childhood Acute Myeloid Leukemia', 'Childhood Leukemia', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Cytogenetics', 'DNA', 'DNA Sequence Alteration', 'DNA sequencing', 'Data', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Marker', 'Disease-Free Survival', 'Enrollment', 'Epigenetic Process', 'Exons', 'FLT3 gene', 'Family', 'Flow Cytometry', 'Frequencies', 'Future', 'Genes', 'Genetic', 'Goals', 'Gold', 'Hematology', 'Immunophenotyping', 'Individual', 'Investigation', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Mutate', 'Mutation', 'Normal Cell', 'North America', 'Oncogenes', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pediatric Oncology Group', 'Phase', 'Point Mutation', 'Population', 'Probability', 'Prospective Studies', 'Protein Tyrosine Kinase', 'Protein phosphatase', 'Protocols documentation', 'Reagent', 'Recurrent disease', 'Refractory Disease', 'Relapse', 'Residual Neoplasm', 'Residual Tumors', 'Residual state', 'Resolution', 'Risk', 'Risk Assessment', 'Risk stratification', 'Role', 'Sampling', 'Site', 'Spliceosomes', 'Surface', 'Testing', 'Time', 'Treatment Efficacy', 'Tumor Suppressor Proteins', 'United States National Institutes of Health', 'Variant', 'analytical method', 'base', 'chemotherapy', 'cohesion', 'design', 'digital', 'falls', 'functional group', 'gene function', 'improved', 'improved outcome', 'insight', 'leukemia', 'leukemia treatment', 'machine learning algorithm', 'next generation', 'next generation sequencing', 'novel', 'nucleocytoplasmic transport', 'outcome prediction', 'pediatric patients', 'personalized cancer care', 'personalized medicine', 'prognostic', 'prospective', 'randomized trial', 'relapse risk', 'response', 'specific biomarkers', 'subclonal heterogeneity', 'transcription factor', 'unsupervised learning']",NCI,WASHINGTON UNIVERSITY,R01,2020,352587,-0.03699468490481828
"Southern California Clinical Center of the Type 1 Diabetes in Acute Pancreatitis Consortium This application describes a robust Southern California-based Clinical Center for participation in the Type 1 Diabetes in Acute Pancreatitis Consortium (T1DAPC). Proposed protocols address the metabolic mechanisms and the genetic, protein, and imaging signature of patients with acute pancreatitis (AP) and recurrent acute pancreatitis (RAP) who are at high risk for future development of diabetes. AP is the most common cause of pancreatogenic diabetes. While meta-analyses have revealed an incidence rate of 23% for diabetes arising after AP, they have not shed light on the type of diabetes that develops, which may comprise autoimmune or idiopathic type 1 diabetes (T1DM), type 2 diabetes (T2DM), or a unique diabetes pathobiology. A detailed understanding of diabetes developing after AP will yield great benefit by facilitating novel approaches to predict, prevent, and treat this form of diabetes. The following aims are proposed to address these goals:  Specific Aim 1. Recruit a cohort of non-diabetic patients with a recent episode of AP or RAP and prospectively characterize their islet autoimmunity and glucose/insulin homeostasis using the frequently sampled intravenous glucose tolerance test and mixed meal tolerance tests performed 1 month after hospital discharge, and at 3, 6, 12, 18, and 24 months, and yearly thereafter. The goals of this aim are to (a) determine the incidence of diabetes after AP, (b) identify the types of diabetes that develop after AP, (c) identify early metabolic trajectories associated with post-AP diabetes, (d) assemble the cohort that will be the platform for Aims 2-4.  Specific Aim 2. Evaluate genetic and protein risk factors for diabetes in patients with AP or RAP. This Aim will evaluate association of genetic risk scores for T1DM and T2DM with post AP diabetes. Thirteen candidate proteins, associated with post AP diabetes in preliminary studies, will be assessed for association with incident diabetes after AP, yielding a key set of proteins with utility not only in diabetes prediction but also targets for future preventive or therapeutic measures.  Specific Aim 3. Characterize the imaging phenotype that predicts development of diabetes after AP or RAP. Retrospective CT scans obtained during hospitalization for AP as well as CT and novel multiparametric MRI scans obtained 1 and 12 months afterward will undergo artificial intelligence analysis to identify the imaging biomarkers that signal diabetes risk.  Specific Aim 4. Develop a multi-factorial model to predict development of diabetes after AP or RAP. A wealth of data will be collected from Aims 1-3, which will be combined with clinical factors to build and validate (in independent datasets) an integrative predictive model of post AP diabetes. The goal is to create a model that can be used in clinical settings to identify those at highest risk, facilitating targeted measures to prevent diabetes.  This innovative research will be conducted by an experienced team of investigators in endocrinology, gastroenterology, imaging, physiology, and epidemiology to solve a problem of great public health significance. This is an application for the establishment of a Southern California Clinical Center to participate in the Type 1 Diabetes in Acute Pancreatitis Consortium, comprised of an expert team to carry out studies of the Consortium in Los Angeles County, the most populous and ethnically diverse county in the United States. Acute pancreatitis, one of the most frequent causes of hospitalization in the USA, greatly increases the risk of future development of diabetes, yet the type of diabetes is unclear. We propose studies to determine the metabolic mechanisms and the genetic, protein, and imaging signature of diabetes occurring after pancreatitis, which will provide the means to understand, prevent, and treat this form of diabetes.",Southern California Clinical Center of the Type 1 Diabetes in Acute Pancreatitis Consortium,10128180,U01DK127403,"['Address', 'Artificial Intelligence', 'Autoantibodies', 'Autoimmune Process', 'Autoimmunity', 'Biological Assay', 'Blood', 'California', 'Clinical', 'Collection', 'County', 'Data', 'Data Set', 'Defect', 'Development', 'Diabetes Mellitus', 'Endocrinology', 'Enrollment', 'Epidemiology', 'Etiology', 'Fasting', 'Frequencies', 'Future', 'Gastroenterology', 'Genetic', 'Genetic Risk', 'Genotype', 'Glucose', 'Goals', 'Health', 'Homeostasis', 'Hormonal', 'Hospitalization', 'Hospitals', 'Image', 'Incidence', 'Individual', 'Insulin', 'Insulin Resistance', 'Insulin-Dependent Diabetes Mellitus', 'Intake', 'Lead', 'Light', 'Liquid substance', 'Los Angeles', 'MRI Scans', 'Magnetic Resonance Imaging', 'Measures', 'Mediator of activation protein', 'Meta-Analysis', 'Metabolic', 'Methods', 'Modeling', 'Necrosis', 'Non-Insulin-Dependent Diabetes Mellitus', 'Pancreas', 'Pancreatitis', 'Participant', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Preventive', 'Proteins', 'Proteomics', 'Protocols documentation', 'Public Health', 'Recurrence', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Sampling', 'Scanning', 'Set protein', 'Severities', 'Signal Transduction', 'Swelling', 'Testing', 'Therapeutic', 'United States', 'X-Ray Computed Tomography', 'acute pancreatitis', 'base', 'chronic pancreatitis', 'clinical center', 'cohort', 'design', 'diabetes risk', 'ethnic diversity', 'experience', 'genetic association', 'genetic profiling', 'genome wide association study', 'genome-wide', 'high risk', 'imaging biomarker', 'improved', 'innovation', 'insulin secretion', 'intravenous glucose tolerance test', 'islet', 'non-diabetic', 'novel', 'novel strategies', 'pancreas imaging', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'recruit', 'response', 'type I and type II diabetes']",NIDDK,CEDARS-SINAI MEDICAL CENTER,U01,2020,324613,-0.019945476998901188
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9851853,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2020,161422,-0.05801156850670986
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9942277,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data harmonization', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'machine learning method', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'supervised learning', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,658144,-0.01134259697656707
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9838229,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Models', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'random forest', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2020,474671,-0.023602032190622083
"Determination of structure, dynamics and energetics of enzyme reactions Project Summary  Understanding enzyme mechanisms is of paramount importance from both the basic biophysics perspective of understanding life processes and the role of enzymes in diseases. To achieve a detailed understanding of enzyme catalysis, the effects of protein structure and dynamics on the reaction energetics need to be elucidated. We propose a combined computational and experimental approach that combines the synthetic, computational and structural biology expertise of a team of investigators that has been working together for >15 years to create a “molecular movie” where the position, movement and energy of every atom in the system followed over the entire reaction pathway. The proposal exploits the emerging convergence of timescales accessible by molecular simulation using GPUs and time resolved structural biology. Specific Aim 1 describes the simulation of the complete reaction pathway of Pseudomonas mevalonii (Pm) HMGCoA Reductase (HMGR) and will use transition state force fields (TSFFs) generated by the quantum guided molecular mechanics method to allow the µsec MD simulations of the chemical steps. TSFFs not only circumvent the well-known boundary problem of QM/MM, but are also 102-104 times faster. This allows a realistic modeling of the coupling of µsec dynamics and catalysis that was demonstrated in the last grant period to be essential for understanding the reaction. Together with accelerated MD simulations of the conformational changes involved in the reaction using standard force fields, these computational studies cover the fsec to µsec timescale. In Specific Aim 2, the computational results will be merged with the results of a three-tiered approach to obtain structural snapshots with progressively increasing time resolution: (i) “Frozen” intermediates that map out the overall pathway on long timescales, (ii) time resolved Laue crystallography using pH jump initiation on the msec timescale and (iii) use of photocaged substrates to allow time resolved Laue experiments on the µsec timescale. This approach will be applied to the study of HMGR, an enzyme of high biophysical and biomedical significance that has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. The project is highly innovative because it (i) uses a combination of MD simulations using TSFFs and time resolved crystallography to span timescales of at least 12 orders of magnitude, (ii) iteratively couples the Markov State analysis of long timescale trajectories to the Singular Value Decomposition used to analyze time resolved crystallography data, thus providing new tools to generate and experimentally validate trial structures (iii) applies global optimization and machine learning techniques to allow the automated fitting of TSFFs for proteins, which will enhance the application of this powerful method to other proteins and (iv) provides new photocaged substrates for the study of enzyme mechanisms to the chemical biology community. All tool compounds, methods and codes developed in this project will be made available to the scientific community. Public Health Statement  The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism-based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG CoA Reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of high mechanistic complexity. 1","Determination of structure, dynamics and energetics of enzyme reactions",9897100,R01GM111645,"['Active Sites', 'Anti-Bacterial Agents', 'Biochemistry', 'Biological', 'Biology', 'Biophysics', 'Catalysis', 'Chemicals', 'Cholesterol', 'Code', 'Collaborations', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computational Biology', 'Computing Methodologies', 'Couples', 'Coupling', 'Crystallization', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymatic Biochemistry', 'Enzymes', 'Equilibrium', 'Free Energy', 'Freezing', 'Goals', 'Grant', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Movement', 'Mutagenesis', 'Nature', 'Oxidoreductase', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protein Dynamics', 'Proteins', 'Pseudomonas', 'Public Health', 'Reaction', 'Research Personnel', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Science', 'Structure', 'System', 'Techniques', 'Time', 'Validation', 'Work', 'base', 'biophysical chemistry', 'cofactor', 'computer studies', 'design', 'electron density', 'enzyme mechanism', 'experience', 'experimental study', 'improved', 'innovation', 'machine learning method', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'molecular scale', 'movie', 'new therapeutic target', 'particle', 'protein structure', 'quantum', 'simulation', 'structural biology', 'synthetic biology', 'theories', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2020,335664,-0.048919363180967
"Optimal dynamic treatment strategies for controlling alcohol use: novel methods for selecting and incorporating effect modifiers Project Summary: The cyclical and heterogeneous nature of many substance use disorders highlights the need to adapt the type or the dose of treatment to accommodate the specific and changing needs of individuals. This proposal is motivated by the Extending Treatment Effectiveness of Naltrexone (ExTENd) trial, a sequential multiple assignment randomized trial (SMART) designed to find a (personalized) rescue treatment for those who are non-responsive to initial Naltrexone. One of the main challenges in this trial is the presence of the many variables available for consideration when making treatment decisions at each stage of the trial. This feature has made it virtually impossible for investigators to fully explore the possibility of building high quality treatment strategies using the data. Our overarching aim is to address this particular challenge through developing and subsequently applying new statistical methods to the ExTENd trial data. A SMART trial is a multi-stage trial that can inform the design of a dynamic treatment regime (DTR) which formalizes an individualized treatment plan and where current treatment strategy can depend on a patient's past medical and treatment history. An optimal DTR is one that maximizes a specified health outcome of interest. Q-learning can be used with data from both SMARTs and observational studies to estimate an optimal DTR. However, like other model-based approaches, model misspecification can seriously affect the results and lead to the identification of suboptimal DTRs. The potential for misspecification increases with the number of variables that may influence treatment decisions through, e.g., incorrect assumptions on the relationship of variables to the outcome and the inclusion (exclusion) of unimportant (important) variables. These features represent the main analytical challenges for the ExTENd trial. We propose a new approach to Q-learning that leverages machine learning approaches to reduce the chances of misspecifying the relationship between the expected outcome and a given set of variables. We also develop a variable selection technique specifically designed for Q-learning that enables investigators to select the important variables from a long list of possibilities (e.g., genetic and demographic information, medical history over time) when estimating an optimal DTR. In both settings, we will develop new methods for conducting valid inferences (e.g., confidence intervals and p-values), including when there exist patients for whom treatment is neither beneficial nor harmful at a given decision stage (i.e., when an important technical assumption, “uniqueness”, is violated). Finally, we will develop easy-to-use, publicly available software in the R language that implements our methods. This will allow re-analysis of the ExTENd trial data with a goal of constructing a DTR that improves upon the current rescue treatment strategy for those non-responsive to initial Naltrexone. It will also provide an expandable platform that will assist researchers in developing new optimal DTRs for patients suffering from alcoholism and other substance use disorders. Narrative: This project aims to address the need for robust, rigorous and computationally efficient methods for estimating the optimal treatment regime in substance use disorder. We will develop a variable selection technique that enables investigators to select important variables for the decision- making process among a long list of variables (e.g., patient’s genetic information, demographic characteristics, and medical history over time). We will also leverage statistical machine learning approaches to improve the quality of the constructed optimal treatment regimes while providing valid statistical inference.",Optimal dynamic treatment strategies for controlling alcohol use: novel methods for selecting and incorporating effect modifiers,9953933,R21AA027571,"['Address', 'Adoption', 'Affect', 'Alcohol consumption', 'Alcohol dependence', 'Alcoholism', 'Behavior Therapy', 'Characteristics', 'Code', 'Computer software', 'Confidence Intervals', 'Data', 'Decision Making', 'Dose', 'Exclusion', 'Fibrinogen', 'Future', 'Genetic', 'Goals', 'Health', 'Heterogeneity', 'Individual', 'Language', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Medical Care Costs', 'Medical History', 'Methodology', 'Methods', 'Modeling', 'Naltrexone', 'Nature', 'Observational Study', 'Outcome', 'Patients', 'Periodicity', 'Phenotype', 'Process', 'Prognostic Factor', 'Randomized', 'Recording of previous events', 'Research', 'Research Personnel', 'Sequential Multiple Assignment Randomized Trial', 'Software Tools', 'Specific qualifier value', 'Statistical Methods', 'Subgroup', 'Substance Use Disorder', 'Techniques', 'Testing', 'Time', 'Treatment Effectiveness', 'Treatment Protocols', 'Work', 'alcohol use disorder', 'base', 'care providers', 'clinical care', 'clinical decision-making', 'design', 'dosage', 'expectation', 'falls', 'genetic information', 'high dimensionality', 'improved', 'individualized medicine', 'interest', 'novel', 'novel strategies', 'open source', 'optimal treatments', 'overtreatment', 'personalized medicine', 'repository', 'response', 'statistical and machine learning', 'treatment planning', 'treatment strategy', 'trial design', 'virtual']",NIAAA,UNIVERSITY OF ROCHESTER,R21,2020,185591,-0.010775909401038637
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9916801,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'diverse data', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2020,341471,-0.06905573186628432
"Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy In this proposed project, we plan to fill the knowledge gap of the relationships between microscopic self-assembled structures, collagen-molecule interactions and macroscopic fiber morphologies of type-I collagen, the primary component of most human tissues and a commonly used biomaterial for tissue engineering. By investigating collagen-water and collagen-protein interactions in in vitro systems that mimic basic aspects of physiologically relevant three- dimensional fibrillar tissue architectures, we aim to fill knowledge gaps in fundamental collagen research. We will achieve this goal by developing a hyperspectral imaging technique – vibrational sum frequency generation (VSFG) microscopy – at high repetition rates (400 kHz) and apply it to collagen. The long-term vision is to develop new biophysics methods to reveal molecular-level structures and interactions for pericellular space research and other complex biological environments, and eventually applying it to study various pericellular environment related diseases. In order to correlate spectral features to microscopic and macroscopic structures of type I collagen, we plan to apply machine-learning techniques to analyze our data and extract spectral signatures of collagen’s micro/macrostructures. We will two major scientific focuses: (A) understanding molecular signatures of microscopic self-assembly fibrils structures and its relationship to the macroscopic morphology (plan 1 and 2); and (B) investigating molecular level collagen-molecule interactions (plan 3 and 4). Specific plans include:  1. Obtaining hyperspectral VSFG images of collagen tissues to study their morphology in a  label free and non-invasive manner  2. Establishing molecular spectral signatures of self-assembled collagen fibril structures  3. Understanding collagen-water interaction in first solvation layer of collagen fibers.  4. Imaging spatial locations of chemicals and peptides that interact with collagens. If successful, the significance is that a label free, vibrational mode specific imaging technique specific for pericellular space will be available, which can reveal molecular level insights of collagen structures and its interactions with surrounding molecules, pertinent to fibrosis and cell— pericellular space interaction related diseases. This proposed project contributes to the scope of NIGMS by developing new technology to reveal fundamental molecular-level principle, mechanism and signatures related to morphology of collagen I at both micro- and macroscopic scales, and collagen-molecule interactions, laying foundations for biophysical/biochemical principles for future biomedical applications related to collagens. This proposed development of vibrational sum frequency generation microscopy, in the short term, will spatially resolve collagen tissues with chemical structure and molecular interaction information in a complicated environment. Machine learning and simulation approaches will be employed to build a data base to convert hyperspectral images of collagen into a spatial map with microscopic structures and molecular interaction information. In the long term, the fundamental biochemical knowledge learned from this development will lay foundations for rationally design biomedical approaches to monitor and control pericellular spaces and its interaction with cells, and further advance treatment to diseases related to it.",Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy,10028946,R35GM138092,"['3-Dimensional', 'Architecture', 'Binding', 'Biochemical', 'Biocompatible Materials', 'Biological', 'Biophysics', 'Cells', 'Chemical Structure', 'Chemicals', 'Collagen', 'Collagen Fiber', 'Collagen Fibril', 'Collagen Type I', 'Complex', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Fiber', 'Fibrosis', 'Foundations', 'Frequencies', 'Future', 'Generations', 'Goals', 'Image', 'Imaging Techniques', 'In Vitro', 'Knowledge', 'Label', 'Location', 'Machine Learning', 'Maps', 'Microscopic', 'Microscopy', 'Molecular', 'Molecular Profiling', 'Monitor', 'Morphology', 'National Institute of General Medical Sciences', 'Peptides', 'Physiological', 'Proteins', 'Research', 'Structure', 'Sum', 'System', 'Techniques', 'Tissue Engineering', 'Tissues', 'Vision', 'Water', 'biophysical techniques', 'design', 'human tissue', 'image reconstruction', 'insight', 'molecular imaging', 'new technology', 'self assembly', 'simulation', 'vibration']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R35,2020,393958,-0.040530996401616295
"N3C & All of Us Research Program Collaborative Project Project Summary/Abstract The COVID-19 pandemic presents unprecedented clinical and public health challenges. Though institutions collect large amounts of clinical data about COVID-19 cases, these datasets individually might not be diverse enough to draw population level conclusions. Also, statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. To tackle this problem, NCATS introduced the National COVID Cohort Collaborative (N3C), an open science, community-based initiative to share patient level data for analysis. The initiative requires participating institutions to share information about their COVID-19 patients in a standard-driven way, including demographics, vital signs, diagnoses, laboratory results, medications, and other treatments. The data from multiple institutions will be merged and consolidated, and access will be provided to investigators through a centralized analytical platform. The COVID-19 data sharing collaboration with the N3C initiative offers a mechanism to initiate collaborations with other NIH sponsored data sharing programs, such as the All of Us Research Program (AoURP). This administrative supplement will support efforts to clean and standardize data at VCU, and to transfer it to the N3C data repository. The supplement will also assist in introducing new services at the Wright Center to support our investigators to use the N3C resources. It will also enable collaboration with the AoURP by establishing a pipeline to collect and transmit consented patients' EHR data and by building on existing community outreach pathways to recruit additional participants for the AoURP. The project will be overseen by the PI/Executive Committee and supervised by the Director of Research Informatics. Procedures and services developed at our local CTSA hub will be shared and disseminated to the CTSA network. Project Narrative NIH/NCATS has been working on the National COVID Cohort Collaborative (N3C), which aims to build a centralized national data resource to be used by the research community to study the COVID-19 pandemic and identify potential treatments as the pandemic continues to evolve. The COVID-19 data sharing collaboration with the N3C initiative also offers a mechanism to initiate collaborations with the All of Us Research Program (AoURP). This administrative supplement will support the creation and management of a data extraction and transfer pipeline to the N3C and AoURP data repositories from VCU.",N3C & All of Us Research Program Collaborative Project,10217339,UL1TR002649,"['Administrative Supplement', 'All of Us Research Program', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Outreach', 'Consent', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Effectiveness', 'Funding Opportunities', 'Goals', 'Health', 'Health Status', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Laboratories', 'Outcomes Research', 'Participant', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Procedures', 'Public Health', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Services', 'Supervision', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'base', 'biomedical informatics', 'clinical center', 'cohort', 'coronavirus disease', 'data resource', 'data sharing', 'data standards', 'data warehouse', 'demographics', 'design', 'improved', 'informatics infrastructure', 'innovation', 'large scale data', 'multi-site trial', 'network informatics', 'open data', 'pandemic disease', 'parent grant', 'programs', 'recruit', 'response', 'statistical and machine learning', 'tool']",NCATS,VIRGINIA COMMONWEALTH UNIVERSITY,UL1,2020,346608,-0.01766491981864577
"Elucidating Sensorial and Functional Characteristics of Topical Formulations Abstract In addition to the defined therapeutic effect caused by an active drug in a product, there is also a placebo and, potentially, a nocebo effect associated with that product. In topical products, these latter effects may account for 30% to 50% of the overall response for some products. They may also explain why some topical products with apparently identical bioavailability are associated with different patient outcomes. This application seeks to address the question of when do subtle excipient and manufacturing changes in a topical product cause a sensorial perception by subjects such that the “feel” of a product has changed either before and/or after it is applied to human skin. A second question is whether the “feel” of a product both before and after application can be quantified by instrumental rheology, tribology and texture analysis methods and whether these, in turn, can be related to the reported sensorial behaviour. We will manufacture topical formulations that systematically vary in Q1, Q2, and/or Q3 attributes and have large and borderline perceptive differences. We will then characterize these products using a range of rheology, tribology and texture analysis methods along with characterization of rate of drying, particle and globule size. In parallel, these products will be evaluated by perceptive testing focus groups, with controls, for their sensory properties or the ‘feel’ of the products. We will then relate these sensorial findings with the variations in formulation nature, composition and manufacture, and their resulting instrumental test results. Our goals are, firstly, to understand the relationships between product nature, instrumental findings and sensorial analyses and, secondly, to derive criteria for instrument tests that indicate what product composition subjects suggest do not differ, uncertain if they differ and do differ in their sensorial behaviour. It is anticipated that we can define the simplest, robust test that accurately and robustly aligns with sensory perceptions. A range of statistical methods, including (potentially) sophisticated, machine learning and deep learning tools will then be used to model the most appropriate instrumental analysis that can, with reasonable confidence predict perceptive attributes. A key outcome is a potential regulatory guideline advocating that generic products should exhibit similar sensorial behaviour as a reference listed drug product, giving boundaries in rheology, tribology and texture analysis as defined by Q1, Q2 and Q3 differences when sensorial behaviour between topical products is likely to be different. Narrative Generic and reference-listed topical products have the potential to have differing placebo and nocebo effects, i.e. effects beyond those of the active drug. This project aims to understand what subtle changes in Q1,Q2 or Q3 between different product formulations lead to a subject reporting sensorial perceptions suggesting that the “feel” of two product is either different or they can no longer perceive a meaningful difference. Compositions (reference and generic) will be manufactured with variations in Q1, Q2 and Q3, characterised by instrumental measures and the results related to sensorial analysis findings.",Elucidating Sensorial and Functional Characteristics of Topical Formulations,9999507,U01FD006700,[' '],FDA,UNIVERSITY OF QUEENSLAND,U01,2020,499990,-0.03494562974371788
"Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center The “clinical high risk” (CHR) for psychosis syndrome is an antecedent period characterized by attenuated psychotic symptoms that are marked by subtle deviations from normal development in thinking, motivation, affect, behavior, and a decline in functioning. Early intervention in this CHR population is critical to prevent psychosis onset as well as other adverse outcomes. However, the presentation of symptoms and subsequent course is highly variable, and there is a paucity of biomarkers to guide treatment development. Thus, to improve predictive models that are clinically relevant, several issues need to be addressed: 1) focusing on outcomes beyond psychosis; 2) taking into account heterogeneity in samples and outcomes; and 3) integrating data sets with a broad array of variables using innovative algorithms to overcome variability across studies. To address these challenges, the proposed “Psychosis Risk Evaluation Data Integration and Computational Technologies: Data Processing, Analysis, and Coordination Center” (PREDICT-DPACC) brings together a multidisciplinary team of highly experienced researchers with proven capabilities in all aspects of large-scale studies, CHR studies, as well as computational expertise. The ultimate goal is to identify new CHR biomarkers, and CHR subtypes that will enhance future clinical trials. To do so, the PREDICT-DPACC will 1) aggregate extant CHR- related data sets from legacy datasets; 2) provide collaborative management, direction, data processing and coordination for new U01 multisite network(s); and 3) develop and apply advanced algorithms to identify biomarkers that predict outcomes, and to stratify CHR into subtypes based on outcome trajectories, first from the extant data and then refined and applied to the new data. The PREDICT-DPACC team has the broad, comprehensive, and robust infrastructure that is sufficiently flexible to accommodate the inclusion of multiple data types and to optimally address the needs of the CHR U01 network(s). Carefully selected extant data will be rapidly obtained, processed, and uploaded to the NIMH Data Archive (NDA). Proposed analysis methods are powerful and robust, leveraging the expertise and experience of computer scientist developers, and experienced clinical researchers. The U01 network(s) will be coordinated by a team that is experienced in managing large studies, familiar with the needs of such studies, flexible, and is knowledgeable in all aspects of CHR studies, including measures, outcomes, biomarkers, and cohorts. Upon meeting the goals of this U24, and the supported U01 network(s), the expected outcomes of the PREDICT-DPACC will be new predictive biomarkers for CHR outcomes, new definitions of CHR subtypes that are clinically useful, and new curated and comprehensive CHR datasets (extant and new) as well as processing tools and prediction algorithms that are shared with the research community through the NIMH Data Archive. NARRATIVE The “Clinical High Risk” (CHR) for psychosis syndrome in young people represents an opportune window for early intervention to prevent the onset of psychosis and other disorders, and to forestall disability; however, clinical heterogeneity and the paucity of biomarkers have hampered the development of effective intervention. To address these challenges, working with NIMH and key stakeholders, we will harmonize and aggregate existing “legacy” CHR data, and guide and coordinate the collection of new data across a network of sites, to develop biomarker algorithms that can predict individual trajectories for diverse outcomes. This proposal leverages a multidisciplinary team with broad and CHR-specific experience in large-scale multisite and multimodal studies (including clinical trials), along with expertise in data type-specific processing, coordination, analysis, and computational analyses (e.g., machine and deep learning tools from artificial intelligence, and advanced statistical approaches), ethics, community outreach, and data dissemination, all of which will ensure the success of this project.","Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center",10092398,U24MH124629,"['Address', 'Adolescent', 'Affect', 'Algorithms', 'Anxiety Disorders', 'Artificial Intelligence', 'Attenuated', 'Behavior', 'Big Data', 'Biological Markers', 'Child', 'Clinical', 'Clinical Trials', 'Collection', 'Common Data Element', 'Communities', 'Community Outreach', 'Computer Analysis', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease remission', 'Early Intervention', 'Early identification', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'FAIR principles', 'Follow-Up Studies', 'Funding', 'Future', 'Goals', 'Heterogeneity', 'Human Resources', 'Impaired cognition', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Intervention', 'Lead', 'Leadership', 'Longterm Follow-up', 'Machine Learning', 'Measures', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Monitor', 'Moods', 'Motivation', 'National Institute of Mental Health', 'Online Systems', 'Outcome', 'Output', 'Perception', 'Procedures', 'Process', 'Protocols documentation', 'Psychotic Disorders', 'Quality Control', 'Recovery', 'Research', 'Research Personnel', 'Risk', 'Risk stratification', 'Safety', 'Sampling', 'Scientist', 'Secure', 'Site', 'Social Functioning', 'Standardization', 'Substance Use Disorder', 'Suggestion', 'Symptoms', 'Technology', 'Thinking', 'Time', 'Training', 'Transact', 'United States', 'Validation', 'Visualization software', 'adverse outcome', 'analytical tool', 'attenuated psychosis syndrome', 'base', 'bioinformatics infrastructure', 'candidate marker', 'clinical heterogeneity', 'clinical risk', 'clinical subtypes', 'clinically relevant', 'cloud based', 'cohort', 'computerized data processing', 'data acquisition', 'data archive', 'data dictionary', 'data dissemination', 'data harmonization', 'data infrastructure', 'data integration', 'data tools', 'deep learning', 'demographics', 'design', 'disability', 'effective intervention', 'experience', 'flexibility', 'functional decline', 'functional disability', 'high risk', 'high risk population', 'improved', 'inclusion criteria', 'innovation', 'meetings', 'member', 'multidisciplinary', 'multimodal data', 'multimodality', 'multiple data types', 'outcome prediction', 'persistent symptom', 'prediction algorithm', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'psychotic symptoms', 'quality assurance', 'recruit', 'research study', 'resilience', 'response', 'success', 'therapy development', 'tool', 'working group']",NIMH,BRIGHAM AND WOMEN'S HOSPITAL,U24,2020,3935239,-0.011361628575769866
"Familial hypercholesterolemia screening in children: population impact of phenotype, genotype, and cascade approaches Project Summary  Familial hypercholesterolemia (FH) is a common genetic disorder, affecting every 200-1000 people, depending on the population and diagnostic criteria. FH leads to lifetime raised low-density lipoprotein (LDL) cholesterol, a high risk for premature atherosclerosis and downstream coronary heart disease. FH is designated as Tier 1 disease by the Center for Disease Control and Prevention, notably one of only three such diseases, because it is common, is associated with a high risk of premature illness, and is treatable with lifestyle or medications. Great uncertainty exists about the optimal approach to FH screening, which is reflected in conflicting recommendations in national screening guidelines.  We propose to synthesize high quality data from national surveys and population-based cohort studies in a health policy computer simulation model comparing the health and economic value of different FH screening strategies. This study will prioritize the optimal approaches to FH screening in the U.S. population, identifying optimal initial screening age and defining the role of genetic testing in screening.  We have assembled a team of experts in pediatric preventive cardiology, decision analysis, cardiovascular disease epidemiology, population genetics, biostatistics, health economic evaluation, and computer simulation modeling in order to evaluate and compare different FH screening strategies in children and adults. We aim to use this expertise and these methods in order to:   Quantify diagnostic yield, clinical effectiveness, and economic value of universal FH phenotype  screening in childhood or adulthood, and the added value of FH genotype screening   Compare universal FH screening to the alternatives of using family history or a Big Data-based  algorithm to direct targeted screening limited to children and adults with possible FH diagnosis   Quantify the health and economic value of cascade screening families of FH cases  We hypothesize that FH screening in childhood will be the highest value screening strategy in the U.S. population, and that genetic testing will improve diagnosis and treatment decisions most in cases of diagnostic uncertainty (e.g., borderline high cholesterol or absent family history). We hypothesize that a machine-learning algorithm will avoid the costs and complexity of universal screening, while yielding a similar case yield, as long as cholesterol testing is sufficiently common in children.  This study will identify the optimal approach to FH screening in the U.S. population and the most influential data based on current knowledge and set the stage for efficiently designed clinical trials of FH screening. This study will be a test case for the concept of a “precision” population health approach to screening for genetically-determined diseases in the general population. Familial hypercholesterolemia (FH) is a common genetic disorder characterized by lifetime elevated cholesterol, which, if uncontrolled, is associated with premature atherosclerotic cardiovascular disease. Conflicting current national guidelines highlight that the optimal approach to FH screening in the U.S. population is controversial: it is unclear if screening should start in childhood or adulthood, or if it should include genetic testing. We propose to synthesize data from national surveys, high quality cohort studies, and clinical trials of cholesterol lowering interventions in a lifetime cardiovascular disease risk computer simulation model to project the life time health impact of different FH screening approaches and identify optimal screening strategies in children and adults.","Familial hypercholesterolemia screening in children: population impact of phenotype, genotype, and cascade approaches",9883835,R01HL141823,"['Adult', 'Adverse effects', 'Advisory Committees', 'Affect', 'Age', 'Algorithms', 'Atherosclerosis', 'Big Data', 'Biometry', 'Cardiology', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Childhood', 'Cholesterol', 'Clinical Trials', 'Clinical Trials Design', 'Clinical effectiveness', 'Cohort Studies', 'Computer Simulation', 'Conflict (Psychology)', 'Coronary heart disease', 'County', 'Data', 'Decision Analysis', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Event', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Future', 'General Population', 'Genetic Diseases', 'Genetic Screening', 'Genotype', 'Guidelines', 'Health', 'Health Benefit', 'Health Policy', 'Hepatocyte', 'Influentials', 'Intervention', 'Knowledge', 'LDL Cholesterol Lipoproteins', 'Laboratories', 'Life', 'Life Style', 'Low Prevalence', 'Low-Density Lipoproteins', 'Medical Care Costs', 'Methods', 'Mutation', 'Parents', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Population Genetics', 'Prevalence', 'Preventive', 'Preventive service', 'Preventive treatment', 'Proxy', 'Puberty', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recording of previous events', 'Role', 'Serum', 'Surveys', 'Testing', 'Time', 'Uncertainty', 'Visit', 'Youth', 'base', 'cardiovascular disorder epidemiology', 'cardiovascular disorder risk', 'clinical practice', 'cost', 'cost effective', 'cost effectiveness', 'diagnostic accuracy', 'economic evaluation', 'economic value', 'genetic testing', 'health economics', 'high risk', 'improved', 'improved outcome', 'lifestyle intervention', 'machine learning algorithm', 'machine learning method', 'models and simulation', 'pediatric patients', 'population based', 'population health', 'premature', 'premature atherosclerosis', 'prevent', 'screening', 'screening guidelines', 'screening program', 'treatment strategy', 'uptake']",NHLBI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,804498,-0.032174991098022916
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,10002324,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'advanced analytics', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'complex data ', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'feature extraction', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2020,869698,-0.013884110688931132
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,-0.02183440442085335
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10058921,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2020,710101,-0.009092285744277414
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer’s disease, Parkinson’s disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these challenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymorphisms, whole brain neuroimaging, biofluids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mechanisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Specifically, in Aim 1, we develop efficient methods for multi-level semiparametric transformation models to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efficiency. Our methods do not rely on full pedigree genotyping and provide family-specific substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic systems through ordinary differential equations with random inflections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker profiles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efficient machine learning methods to identify predictive markers, estimate optimal individualized therapies, and identify subgroups who may receive the greatest benefit from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art analytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research community. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9927686,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Non-linear Models', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'machine learning method', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'statistical learning', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,340025,-0.02120611141109557
New Approaches to Dementia Heterogeneity  ,New Approaches to Dementia Heterogeneity,10053649,P30AG062422,"['Aging', 'Area', 'Artificial Intelligence', 'Astrocytes', 'Behavior', 'Big Data', 'Biological Markers', 'Blood', 'Blood Vessels', 'Brain', 'Brain imaging', 'Cells', 'Cognition', 'Cognitive', 'Data', 'Data Analytics', 'Data Collection', 'Degenerative Disorder', 'Dementia', 'Disease', 'Disease Progression', 'Dissection', 'Early Diagnosis', 'Emotional', 'Endothelium', 'Etiology', 'Failure', 'Fibrosis', 'Functional disorder', 'Funding', 'Goals', 'Heterogeneity', 'Homeostasis', 'Image', 'Impaired cognition', 'Impairment', 'Inflammation', 'Injury', 'Intervention', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Mentors', 'Mentorship', 'Modeling', 'Molecular', 'National Institute of Neurological Disorders and Stroke', 'Natural regeneration', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neuroglia', 'Neurons', 'Outcome', 'Pathologic Neovascularization', 'Pathway interactions', 'Patients', 'Pattern', 'Perfusion', 'Phase', 'Phenotype', 'Play', 'Population', 'Proteomics', 'Research Personnel', 'Risk', 'Role', 'Scientist', 'Site', 'Synapses', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Training', 'Vascular Dementia', 'Vascular Diseases', 'Veterans', 'angiogenesis', 'base', 'biomedical informatics', 'cerebrovascular biology', 'clinical phenotype', 'cognitive testing', 'cohort', 'disorder subtype', 'drug development', 'endophenotype', 'exosome', 'hypoperfusion', 'molecular marker', 'multidimensional data', 'neuroinflammation', 'neurovascular', 'novel', 'novel strategies', 'phenotypic biomarker', 'precision medicine', 'predictive signature', 'prospective', 'proteostasis', 'resilience', 'retinal imaging', 'tau Proteins', 'tool', 'unsupervised learning', 'vascular cognitive impairment and dementia']",NIA,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",P30,2020,298759,-0.025640715758755636
"Antibody isotyping for discrimination of disease stage and diagnosis of early Lyme disease. ABSTRACT More than 3 million tests are performed each year to support the laboratory diagnosis of human Lyme disease (LD). While the CDC conventional standard two-tier (CSTT) approach for serodiagnosis of LD has worked relatively well when used as recommended, there is plenty of room for improvement. Of a number of weaknesses associated with the supplemental immunoblot of the CSTT the most significant is low reproducibility due to the subjective visual interpretation of results. To overcome these weaknesses the CDC recently updated its recommendations based on a modified STT (MSTT) in that a second EIA can replace the immunoblot. The major goal of this project is to develop an objective, quantitative, multiplex EIA that can detect four antibody isotypes (IgM/D/G/A) and all four IgG subclasses (IgG1/2/3/4) to leverage acquisition of simultaneous antibody profile information on multiple B. burgdorferi antigens to build an assay that can discriminate Lyme disease stage with increased overall sensitivity without incurring in loss of specificity. The novelty of this study relies on: 1) evaluation of B. burgdorferi antigen-specific antibody isotypes and IgG subclasses that can be correlated with Lyme disease stage; and 2) development of new diagnostic tools using machine learning techniques to train and integrate all data and produce an objective result to discriminate early Lyme from early disseminated/late Lyme disease. We expect this Phase I SBIR to allow us to develop a new EIA for serodiagnosis of Lyme disease (isoEIAplex-Ld) and to further an ongoing collaboration with DCN diagnostics for the adaptation of our biomarkers to a new rapid Lateral Flow Assay (see Letter of Support) for a follow up Phase II SBIR . NARRATIVE More than 3 million tests are performed each year to support the laboratory diagnosis of human Lyme disease. While the CDC conventional standard two-tier approach for serodiagnosis of Lyme disease has worked relatively well when used as recommended, there is plenty of room for improvement. We propose to develop an objective multiplex enzyme immunoassay that can detect four antibody isotypes (IgM/D/G/A) and all four IgG subclasses (IgG1/2/3/4) to leverage acquisition of simultaneous antibody profile information on multiple B. burgdorferi antigens and build an assay that can discriminate Lyme disease stage with increased overall sensitivity without incurring in loss of specificity.",Antibody isotyping for discrimination of disease stage and diagnosis of early Lyme disease.,10080461,R43AI155211,"['Acute', 'Acute Disease', 'Affinity', 'Antibodies', 'Antibody Response', 'Antigens', 'Arthritis', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Borrelia burgdorferi', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Collaborations', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Specificity', 'Discrimination', 'Disease', 'Early Diagnosis', 'Enzyme Immunoassay', 'Evaluation', 'GTP-Binding Protein alpha Subunits, Gs', 'Genetic Recombination', 'Goals', 'Grant', 'High Prevalence', 'Human', 'IgA1', 'IgA2', 'IgE', 'IgG1', 'IgG2', 'IgG3', 'IgG4', 'Immune response', 'Immunodominant Antigens', 'Immunoglobulin A', 'Immunoglobulin D', 'Immunoglobulin G', 'Immunoglobulin Isotypes', 'Immunoglobulin M', 'Immunoglobulins', 'Infection', 'Iowa', 'Laboratories', 'Laboratory Diagnosis', 'Lateral', 'Lesion', 'Letters', 'Licensing', 'Lyme Arthritis', 'Lyme Disease', 'Machine Learning', 'OspC protein', 'Patients', 'Peptidoglycan', 'Performance', 'Phase', 'Proteins', 'ROC Curve', 'Recommendation', 'Reproducibility', 'Research', 'Serum', 'Small Business Innovation Research Grant', 'Specificity', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'V(D)J Recombination', 'Visual', 'Work', 'antigen binding', 'base', 'commercialization', 'disease diagnosis', 'erythema migrans', 'follow-up', 'improved', 'novel diagnostics', 'pathogen', 'tool']",NIAID,"IMMUNO TECHNOLOGIES, INC.",R43,2020,298123,-0.014025759744315703
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.02789190501927242
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,10002192,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'data standards', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2020,427122,-0.03069807948960138
"Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers PROJECT SUMMARY/ABSTRACT Candidate: Dr. Adel El Boueiz is a pulmonary and critical care physician-scientist completing a period of T32- funded support at the Channing Division of Network Medicine (CDNM) and Harvard Medical School (HMS). He received a Master's of Medical Science in Biomedical Informatics from HMS in May 2016. He will be promoted to Instructor of Medicine at the CDNM and HMS on July 1, 2017. His principal research interests are the genetic epidemiology of chronic obstructive pulmonary disease (COPD) and the translation of genomic discoveries into clinical practice and public health. His long-term goal is to be an independent investigator with expertise in imaging phenotyping, genomics, and predictive analytics of the regional heterogeneity of the various aspects of COPD (emphysema, airway disease, and pulmonary vascular remodeling). Environment: Dr. El Boueiz will continue to pursue his research and career development in the rich and multidisciplinary environment of the CDNM and the Brigham and Women's Hospital Applied Chest Imaging Lab (ACIL). He will be mentored by Drs. Edwin K. Silverman, Peter J. Castaldi, and Raúl San José Estépar, leaders in the field of COPD quantitative imaging, genetic epidemiology, and predictive analytics with excellent track records of mentoring young investigators towards independent research careers. His career development will also be overseen by an advisory committee with expertise related to key areas of his proposal. Research: COPD is a major cause of morbidity and mortality that is of increasing public health importance. COPD is a heterogeneous disease and this heterogeneity complicates the identification of the predictors of disease progression and consequently, the development of effective therapies. Emphysema distribution is an important COPD-related phenotype that emerged as a strong predictor of the response to lung volume reduction procedures. Despite the availability of advanced texture-based CT quantification methods, global threshold-based quantitative metrics have to date been the cornerstone for the radiological characterization of emphysema distribution with inability to differentiate centrilobular, panlobular, and paraseptal emphysema patterns. In this project, we will apply a texture-based CT quantification method to discover novel imaging biomarkers of the regional heterogeneity of centrilobular, panlobular, and paraseptal emphysema in a large cohort of well-characterized smokers and identify their genetic determinants using whole genome sequencing and integrative genomics analyses. The results will be considered for inclusion along with other rich phenotypic and imaging data in COPD disease progression machine learning predictive models. Relevance: Through improved radiographic phenotyping of emphysema distribution, better understanding of disease pathobiology, and more accurate prediction of disease progression, the proposed work will open new avenues of investigation for the development of personalized and improved COPD therapeutic strategies. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a common disease that affects up to 24 million people in the United States, is associated with considerable and increasing morbidity and mortality, and for which there is no available disease-modifying therapy. COPD is associated with significant variation in radiographic, symptomatic and physiologic presentation and exhibits variability in progression. Currently, there is no satisfactory method for progression prediction. This project will identify novel imaging biomarkers of the regional distribution of centrilobular, panlobular, and paraseptal emphysema with particular emphasis on their associations with clinical relevant COPD-related outcomes, their genetic determinants, and their ability to improve prediction of COPD disease progression, above and beyond that provided by the traditional clinical, radiographic, and genetic features. This is an important area of research as predicting those patients who will remain stable from those who will have rapid disease progression is critical in defining prognosis and selecting patients for specific therapeutic interventions.","Clinical significance and genetic determinants of novel imaging measures of emphysema distribution in 9,743 smokers",9975215,K08HL141601,"['ACVR1B gene', 'Accounting', 'Advisory Committees', 'Affect', 'Airway Disease', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological Process', 'Chest', 'Chronic Obstructive Airway Disease', 'Clinical', 'Clinical Trials', 'Clinical/Radiologic', 'Cohort Studies', 'Collection', 'Complex', 'Computing Methodologies', 'Critical Care', 'Data', 'Detection', 'Development', 'Diagnostic radiologic examination', 'Disease', 'Disease Progression', 'Dyspnea', 'Environment', 'Evaluation', 'Exhibits', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Polymorphism', 'Genomic approach', 'Genomics', 'Goals', 'Heterogeneity', 'Hospitals', 'Image', 'Investigation', 'Lobar', 'Lobe', 'Lung', 'Lung Volume Reductions', 'Lung diseases', 'Machine Learning', 'Measures', 'Medical', 'Medical Genetics', 'Medical Research', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Outcome', 'Pathologic', 'Patients', 'Pattern', 'Phenotype', 'Physicians', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Procedures', 'Public Health', 'Pulmonary Emphysema', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Quantitative Trait Loci', 'Radiology Specialty', 'Records', 'Research', 'Research Personnel', 'Resources', 'Respiratory physiology', 'Science', 'Scientist', 'Smoker', 'Structure of parenchyma of lung', 'Subgroup', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Therapeutic Intervention', 'Tissues', 'Translations', 'United States', 'Variant', 'Vascular remodeling', 'Visual', 'Walking', 'Woman', 'Work', 'X-Ray Computed Tomography', 'attenuation', 'base', 'biomedical informatics', 'career', 'career development', 'clinical practice', 'clinically relevant', 'clinically significant', 'cohort', 'comorbidity', 'data mining', 'disease heterogeneity', 'disease phenotype', 'disorder risk', 'disorder subtype', 'effective therapy', 'genetic architecture', 'genetic association', 'genetic epidemiology', 'genetic predictors', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic predictors', 'imaging biomarker', 'imaging genetics', 'improved', 'instructor', 'interest', 'machine learning method', 'medical schools', 'mortality', 'multidisciplinary', 'novel', 'outcome forecast', 'personalized care', 'predicting response', 'predictive modeling', 'prognostic', 'quantitative imaging', 'rare variant', 'research and development', 'respiratory', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2020,170639,-0.05135543274143036
"Imaging Mass Spectrometry for metabolome mapping SUMMARY In response to NOT-GM-20-013, we are requesting a supplement to our R01 5R01GM120033-04 for an MALDI imaging source unit to be attached to an existing Q ExactiveMass Spectrometer (Ultra-High Mass Range Hybrid Quadrupole-Orbitrap™) for spatial mapping of metabolites in thin tissue sections. Within our R01 award, to analyze NMR metabolome data we are developing two novel, powerful, and automated algorithms that capitalize on recent developments in machine learning. We have coded these algorithms and tested their sensitivity and specificity on both synthesized and real data. We then applied these methods to human disease models and identified putative biomarkers. To validate these biomarkers, we have developed methods to analyze animal tissues and human brain organoids using imaging mass spectrometry (IMS), which permits spatial localization of metabolites without labeling. This targeted IMS metabolic phenotyping approach complements our untargeted NMR methods: it allows us to determine whether the individual metabolites identified by NMR represent bona fide biomarkers and to develop metabolic hypotheses for their association with disease. We submit this request for imaging mass spectrometer hardware because a nearby IMS facility on which we have relied has closed and no other IMS facility exists in greater Houston area. Performing the IMS studies ourselves, with the help of collaborators, will accelerate our discovery about the role small molecules and metabolites play in health and disease. This instrument will help us better i) perform metabolome screens to identify the effects of SARS-CoV-2 on neural cell types in human brain organoid models; ii) perform high-throughput drug screening to stimulate neural stem cells to produce new neurons in the brain organoid models to regenerate damaged tissue; and iii) use our NMR algorithms to develop a protocol for quantitative imaging. None of these studies will be possible without the imaging mass spectrometer. Given our access to state-of-the-art equipment, data-collection expertise, and new analytical algorithms that are especially sensitive and specific to NMR spectral data, we are uniquely positioned to advance biomarker and diagnostics tools and screening methods for metabolites and synthetic small molecules. Using an imaging mass spectrometer to map metabolite distribution may help us discover diagnostic and prognostic biomarkers not only for SARS-CoV-2, but for a broad spectrum of brain disorders that lead to neurodegeneration. Such broad usage of our platform would be transformative for neuroscientists, neurologists, and their patients. NARRATIVE The metabolome is a dynamic and sensitive biological system that reflects both innate processes and environmental influences, and can therefore tell us much about an organism's health and homeostasis. In our R01, we are developing two novel, powerful, and automated algorithms to analyze NMR metabolome data. We are requesting an MALDI imaging source unit to attach to an existing Q ExactiveMass Spectrometer (Ultra- High Mass Range Hybrid Quadrupole-Orbitrap™) to validate our ongoing NMR studies and accelerate the translation of our biomarker discoveries to the clinical realm.",Imaging Mass Spectrometry for metabolome mapping,10175695,R01GM120033,"['2019-nCoV', 'Algorithms', 'Area', 'Award', 'Biological Markers', 'Brain', 'Brain Diseases', 'Clinical', 'Code', 'Complement', 'Data', 'Data Collection', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Equipment', 'Health', 'Homeostasis', 'Human', 'Hybrids', 'Image', 'Individual', 'Label', 'Lead', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Metabolic', 'Methods', 'Modeling', 'Nerve Degeneration', 'Neurologist', 'Neurons', 'Organism', 'Organoids', 'Patients', 'Play', 'Positioning Attribute', 'Process', 'Prognostic Marker', 'Protocols documentation', 'Role', 'Sensitivity and Specificity', 'Source', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Testing', 'Thinness', 'Tissues', 'Translations', 'animal tissue', 'automated algorithm', 'biological systems', 'biomarker discovery', 'cell type', 'diagnostic biomarker', 'high-throughput drug screening', 'human disease', 'instrument', 'mass spectrometer', 'metabolic phenotype', 'metabolome', 'nerve stem cell', 'novel', 'quantitative imaging', 'response', 'screening', 'small molecule', 'targeted imaging', 'tissue regeneration', 'tool']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2020,209619,-0.06789134101646727
"Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design PROJECT SUMMARY/ABSTRACT Molecular simulation is a powerful tool to predict the properties of biomolecules, interpret biophysical experiments, and design small molecules or biomolecules with therapeutic utility. However, a number of obstacles have impeded the development of quantitative, cloud-scale research workﬂows involving biomolecular simulation. Two main ob- stacles are the insufﬁcient accuracy of current atomistic models for biomolecules and small molecule therapeutics and the lack of interoperability in simulation toolchains used in both academic and industrial biomolecular research. Our original R01, “Open Data-driven Infrastructure for Building Biomolecular Force Fields for Predictive Bio- physics and Drug Design,” seeks to solve the ﬁrst problem. It helps fund our effort, the Open Force Field Initiative (https://openforceﬁeld.org) to develop open, extensible, and shared software and data infrastructure, implementing statistically robust methods of parameterizing force ﬁelds and choosing new force ﬁelds in a statistically sound manner. This work is designed to create not just a new generation of force ﬁelds, but an open technology to continue advancing force ﬁeld science. However, even with improved molecular models, putting together complete workﬂows of biomolecular simulations involves interfacing substantial numbers of different tools. However the majority of the existing molecular simulation workﬂows are mutually incompatible, with differing representations of the molecular models. The Open Force Field Initiative effort already includes the development of molecular data structures that we can ex- port into existing molecular simulation tools. We propose to extend the existing scope of our R01 to create an extensible common molecular simulation representation and translators to and from this representation. Such a set of tools will immediately make it signiﬁcantly easier to combine the disparate workﬂows developed for different sets of molecular simulation tools. Researchers will be able to set up and build the biophysical simulations using their usual tools, but run and analyze them with currently incompatible tools, enabling better matching of computational resources and methods to problems. It will help avoid trapping in a single software framework, and enable combinations of functionalities previously impossible without substantial developer time and effort. We will (Aim 1) work with partners to generalize our modular, extensible object model for representing parameterized biomolecular systems in a manner that accommodates the force ﬁeld terms currently supported by most popular biomolecular simulation packages. We will engineer it to be extensible to advanced interaction forms, such as polarizability and other multibody terms, and machine learning models for intermolecular forces. We will (Aim 2): enable easy conversion between components of molecular simulation workﬂows by allowing other molecular simulation packages to easily store their representations in this data model, developing converters that can import/export this object model to multiple popular ﬁle formats, focusing initially on OpenMM, AMBER, CHARMM, and GROMACS. We will demonstrate the utility of this interface in cloud-ready workﬂows. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This supplement will make it much easier for molecular simulation workﬂows to interoperate with each other in large-scale workﬂows.",Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design,10166314,R01GM132386,"['Affinity', 'Binding', 'Biophysics', 'COVID-19', 'Collaborations', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA', 'Development', 'Drug Design', 'Ecosystem', 'Engineering', 'Funding', 'Generations', 'Human', 'Individual', 'Industrialization', 'Infrastructure', 'Language', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Problem Solving', 'Property', 'Proteins', 'Pythons', 'RNA', 'Readability', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Scientist', 'Software Framework', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'Writing', 'biomaterial interface', 'computing resources', 'data infrastructure', 'data modeling', 'design', 'experimental study', 'file format', 'improved', 'interoperability', 'molecular modeling', 'open data', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'sound', 'structured data', 'tool']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,225000,-0.033022775724174044
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,10005472,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Axon', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'automated algorithm', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'large scale data', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional', 'web portal']",NIMH,UNIVERSITY OF WASHINGTON,R01,2020,628430,-0.019117037755523182
"Coordinating Research on Emerging Arboviral Threats Encoing the Neotropics (CREATE-NEO) Project Summary In recent decades, Central and South America have experienced spillover of endemic arthropod-borne viruses (arboviruses) from wildlife reservoirs into humans, exchange and recombination of emerging arboviruses within the region, resurgence of arboviruses previously controlled by vaccination or vector control, introduction and spread of novel arboviruses, and exportation of viruses to other regions. Furthermore, there is great concern that newly-introduced Zika virus may spill back into an enzootic transmission cycle in the Americas. Central and South America encompass enormous vertebrate and invertebrate biodiversity, and these species harbor a broad range of arboviruses whose risk of spillover and spread in humans is presently unknown. Increases in the rates of global travel, invasion of novel vector species, urban expansion, deforestation, and global climate change all elevate the risk of further arbovirus emergence, as does the breakdown of public health structures in Venezuela.  The Coordinating Research on Emerging Arboviral Threats Encompassing the Neotropics (CREATE- NEO) project will provide a network of surveillance sites in the neotropics coupled to cutting-edge modeling approaches in order to anticipate and counter emerging arboviruses. Aim 1 will identify novel and known arboviruses as well as the host-vector networks that sustain transmission of these viruses within the neotropics, map the spatial distribution of these transmission networks, and characterize virus transmission dynamics within these networks. To do so, we will collect mosquitoes and other vectors as well as non-human primates and other vertebrate hosts at multiple sites in areas of high and varied biodiversity in Panama and Brazil and screen these samples for known and novel arboviruses. These data will then be analyzed using niche modeling, machine learning to predict undiscovered hosts and vectors, and dynamical transmission models. Aim 2 will focus on prospective and retrospective analysis of human infection and disease. To do so, we will leverage ongoing human clinical cohorts at multiple sites in Brazil and Panama. We will extend and expand these cohorts, with a particular focus on the immune-mediated interactions among multiple arboviruses at sites of hyperendemicity. We will also develop novel diagnostics to capture known and novel arboviruses and model the impact of human and non-human primate movement on spillover and spillback of target arboviruses.  Data and models generated via these two aims will forewarn local, regional and global public health agencies of arboviruses within Central and South America that pose particularly high risk of spillover, emergence into transmission among humans, and/or international spread. Moreover CREATE-NEO will build local capacity to predict, detect and respond to emerging arboviruses at their point of origin, thereby maximizing the potential to avert full-blown emergence. Project Narrative Arthropod-borne viruses, such as dengue, Zika and Mayaro, are emerging at an accelerating rate in Central and South America. The Coordinating Research on Emerging Arboviral Threats Encompassing the Neotropics (CREATE-NEO) project will provide a nimble and flexible network of surveillance sites in Central and South America coupled to cutting-edge modeling approaches in order to anticipate and counter these threats to public health.",Coordinating Research on Emerging Arboviral Threats Encoing the Neotropics (CREATE-NEO),9968994,U01AI151807,"['Acute', 'Address', 'Age', 'Americas', 'Animals', 'Arbovirus Infections', 'Arboviruses', 'Area', 'Back', 'Biodiversity', 'Biological', 'Blood Circulation', 'Brazil', 'Central America', 'Chikungunya virus', 'Cities', 'Clinical', 'Cohort Studies', 'Collection', 'Coupled', 'Culicidae', 'Data', 'Deforestation', 'Dengue', 'Detection', 'Diagnostic', 'Disease', 'Emergency Situation', 'Enzyme-Linked Immunosorbent Assay', 'Frequencies', 'Genetic Recombination', 'Genetic Variation', 'Habitats', 'Human', 'Immune', 'Infection', 'International', 'Invertebrates', 'Machine Learning', 'Malaria', 'Maps', 'Measles', 'Mediating', 'Modeling', 'Movement', 'Panama', 'Public Health', 'Research', 'Risk', 'Route', 'Sampling', 'Site', 'South America', 'Spatial Distribution', 'Structure', 'Technology', 'Testing', 'Time', 'Travel', 'Vaccination', 'Vector-transmitted infectious disease', 'Venezuela', 'Virus', 'West Nile virus', 'Yellow Fever', 'ZIKA', 'Zika Virus', 'Zoonoses', 'chikungunya', 'climate change', 'cohort', 'design', 'enzootic', 'experience', 'flexibility', 'high risk', 'insight', 'nanobodies', 'nonhuman primate', 'novel', 'novel diagnostics', 'pathogen', 'prospective', 'seroconversion', 'surveillance network', 'transmission process', 'vector', 'vector control', 'viral transmission']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,U01,2020,1615615,-0.028472201308132637
"Evidence to improve heat warning effectiveness in reducing morbidity and mortality. Project Summary/Abstract  While exposure to high ambient temperature (i.e., heat) has long been recognized as a threat to public health, the burden of illness and death attributable to heat in the US remains high. In an effort to reduce heat- related mortality and morbidity, the US National Weather Service (NWS) issues heat alerts in advance of forecasted extreme heat events to communicate these risks to the public and government officials. However, it is largely unknown: (1) what are the optimal metrics of heat stress to inform when to issue heat alerts, (2) how effective are heat alerts in protecting the public’s health and (3) what factors make heat alerts comparatively more or less effective in some places or in some people versus others. In the absence of such information, we will fail to maximize the public health benefits of heat alerts.  The goals of this proposal are to identify the optimal health-based and location-specific metrics for issuing heat alerts, to estimate the causal benefits of heat alerts, and to identify characteristics of individuals or communities associated with the greatest reductions in morbidity or mortality following heat alerts. Specifically, using national claims data on deaths and hospital admissions among the large, geographically diverse population of >60 million US Medicare beneficiaries age ≥65 years enrolled between 2001 and 2015, and on emergency department visits among >130 million participants of all ages from one of the nation’s largest health insurers, we propose to: (Aim 1) Use novel machine learning methods to identify the heat metric(s) (e.g., heat index, ambient temperature, spatial synoptic classification, wet bulb globe temperature, absolute humidity) that best predict excess heat-related deaths, emergency hospitalizations, and emergency departments visits in each location, (Aim 2) estimate the causal effects of NWS heat alerts on rates of mortality, hospitalizations, and emergency department visits across the country and within groups stratified by health outcome, sex, and age group, and (Aim 3) assess how the benefits of heat alerts vary across characteristics of communities.  Key innovations of this proposal include a very large sample size, geographic diversity encompassing the entire US, the assessment across multiple health endpoints and age groups, and the use of sophisticated methods in statistical learning and causal inference. Collectively, the findings from this proposal will provide meteorologists, public health and emergency management officials, and local policy-makers with critical information to better protect public health during extreme heat events and guide more targeted future research on strategies to mitigate the adverse health effects of heat. Project Narrative Extreme heat is recognized as an important threat to public health, and the burden of illness and death attributable to heat in the US remains high. In an effort to reduce heat-related mortality and morbidity, the US National Weather Service (NWS) issues heat alerts in advance of forecast extreme heat events to communicate these risks to the public and local government officials, although it remains largely unknown whether such alerts are effective in protecting people. We propose to provide forecasters and public health officials across the country with location-specific, health-based evidence as to the effectiveness of heat alerts and insights into how to improve them to better protect the public’s health.",Evidence to improve heat warning effectiveness in reducing morbidity and mortality.,10164490,R01ES029950,"['Age', 'Air Conditioning', 'Cessation of life', 'Characteristics', 'Classification', 'Communities', 'Country', 'Data', 'Effectiveness', 'Emergency Situation', 'Emergency department visit', 'Enrollment', 'Ethnic Origin', 'Event', 'Exposure to', 'Geography', 'Goals', 'Government Agencies', 'Government Officials', 'Health', 'Health Benefit', 'Heat Stress Disorders', 'Hospitalization', 'Humidity', 'Individual', 'Insurance Carriers', 'Knowledge', 'Local Government', 'Location', 'Medicare', 'Methods', 'Morbidity - disease rate', 'Outcome', 'Participant', 'Policy Maker', 'Population Heterogeneity', 'Predisposition', 'Prevalence', 'Public Health', 'Race', 'Research Personnel', 'Risk', 'Sample Size', 'Seasons', 'Services', 'Socioeconomic Status', 'Structure', 'Temperature', 'United States', 'Weather', 'age group', 'attributable mortality', 'base', 'beneficiary', 'burden of illness', 'comparative', 'evidence base', 'extreme heat', 'human old age (65+)', 'improved', 'indexing', 'innovation', 'insight', 'learning strategy', 'machine learning method', 'mortality', 'mortality risk', 'novel', 'prevent', 'public health emergency', 'response', 'sex', 'statistical learning']",NIEHS,BOSTON UNIVERSITY MEDICAL CAMPUS,R01,2020,613553,-0.025258999776884877
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,-0.016862843280812247
"Detection and characterization of critical under-immunized hotspots Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots,9887876,R01GM109718,"['Affect', 'Bayesian Method', 'Behavioral Model', 'California', 'Characteristics', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Disease model', 'Economic Burden', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Funding', 'Geography', 'Growth', 'Health', 'Health Personnel', 'Herd Immunity', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Measles-Mumps-Rubella Vaccine', 'Medical', 'Methodology', 'Methods', 'Minnesota', 'Modeling', 'New Jersey', 'New York', 'Oregon', 'Outcome', 'Pathway interactions', 'Policies', 'Policy Maker', 'Population', 'Population Analysis', 'Privatization', 'Public Health', 'Records', 'Registries', 'Resolution', 'Resource Allocation', 'Resources', 'Risk', 'Scanning', 'Schools', 'Science', 'Source', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Uncertainty', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Washington', 'Work', 'base', 'data mining', 'demographics', 'diverse data', 'economic cost', 'economic outcome', 'health care delivery', 'health disparity', 'health organization', 'improved', 'interest', 'novel', 'novel strategies', 'population based', 'provider networks', 'public health intervention', 'social', 'social media', 'spatiotemporal', 'statistics', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,324178,-0.013286421975916703
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9924432,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device', 'wearable sensor technology']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,298890,-0.006311311755648708
"Multiscale models of fibrous interface mechanics PROJECT SUMMARY Interfaces between tissues either transfer load (requiring toughness) or provide a smooth surface (requiring low friction). Fibrous interfaces are very effective at transferring load between tissues, e.g., at connective tissue-bone interfaces (“entheses”), peritoneal-mesentery interfaces, interfaces between layers of the vasculature, and the pia mater. These interfaces require toughness to resist high stresses associated with material mismatches. Surgical repair can lead to smooth interfaces becoming fibrous, (e.g., following hernia surgery) or to tough interfaces becoming weak (e.g., following tendon- and ligament-to-bone repair). In older patients with large rotator cuff repairs, for example, where the desired attachment is not reformed, up to 94% of surgical repairs fail. These challenges arise in part because the features that endow fibrous interfaces with toughness are not known. We therefore propose to develop a comprehensive modeling and experimental approach for studying the factors underlying the transition from tough to weak in a fibrous interface. Our previous work motivates the hypothesis that disorder is a key toughening feature of fibrous attachments. We will focus initially on the example of tendon attaching to bone, in which microscale disorder underlies the ordered macroscale, graded transition between the two tissues, as a foundation for studying the general problem of adhesion throughout the body. We predict that disorder enhances energy absorption by distributing failure processes and energy absorption over larger volumes of tissue. We propose this as a fundamental mechanism by which fibrous interfaces in the body transfer load effectively. We will test these ideas through two aims: (1) Identify and model the mechanisms of fibrous attachment toughening ex vivo. We will model and experimentally validate how disorder across length scales toughens the tendon-to-bone attachment. Hierarchical molecular dynamics-to- continuum models, enriched by machine learning, will be validated in vitro, in systems with nanoscale control of mineral distributions, and ex vivo, in tissue samples of fibrous attachments. (2) Identify and model the loss of fibrous attachment toughness due to pathologic settings in vivo using murine rotator cuff tendinopathy models. In both aims, nano- through milli-scale characterization will be performed to define the mechanisms driving mechanical behavior. We will test the hypothesis that pathology- induced changes at multiple length scales will predict changes in failure mode. These models and experiments will test the global hypothesis that energy absorption across hierarchies is a fundamental toughening mechanism by which fibrous interfaces resist injury level loads. Taken together, we believe that these new models of fibrous attachment will enable an understanding of how the order and complexity of fibrous attachments leads to effective attachment of tissues. PROJECT NARRATIVE Tough fibrous interfaces between tissues are a common location of injury and source of pathology, pain and disability. Surgical repairs of interfaces that are desirable proceed in the absence of knowledge of the mechanisms that endow fibrous interfaces with toughness, and unsurprisingly have very high failure rates of up to a nearly unbelievable 94%. By developing and testing the first mathematical models for the toughness of this class of interfaces, we hope to identify the mechanisms of toughening definitively, and more broadly to provide technology that enables design of improved surgical procedures.",Multiscale models of fibrous interface mechanics,10037326,R01AR077793,"['Adhesions', 'Adhesives', 'Animal Model', 'Automobile Driving', 'Behavior', 'Biological Models', 'Bone Regeneration', 'Bone Tissue', 'Botulinum Toxin Type A', 'Brain', 'Characteristics', 'Collagen', 'Collagen Diseases', 'Collagen Fiber', 'Connective Tissue', 'Disease', 'Environment', 'Failure', 'Fiber', 'Foundations', 'Friction', 'Hernia', 'In Vitro', 'Injury', 'Intra-abdominal', 'Knowledge', 'Laparotomy', 'Lead', 'Length', 'Ligaments', 'Location', 'Machine Learning', 'Mechanics', 'Meniscus structure of joint', 'Mesentery', 'Minerals', 'Modeling', 'Modification', 'Mus', 'Musculoskeletal', 'Nature', 'Nervous system structure', 'Operative Surgical Procedures', 'Pain', 'Paralysed', 'Pathologic', 'Pathology', 'Patients', 'Peritoneal', 'Physiological', 'Pia Mater', 'Process', 'Rotator Cuff', 'Running', 'Skin', 'Slide', 'Source', 'Specimen', 'Stress', 'Structure', 'Surface', 'System', 'Technology', 'Tendinopathy', 'Tendon structure', 'Testing', 'Tissue Sample', 'Tissues', 'Work', 'absorption', 'bone', 'cranium', 'crosslink', 'design', 'disability', 'experimental study', 'improved', 'in vivo', 'mathematical model', 'mechanical behavior', 'millimeter', 'molecular dynamics', 'multi-scale modeling', 'nano', 'nanocrystal', 'nanoscale', 'older patient', 'repaired', 'treadmill']",NIAMS,WASHINGTON UNIVERSITY,R01,2020,545403,-0.020031626803505082
"Diagnostic and prognostic biomarkers for subtypes of addiction-related circuit dysfunction Project Summary Substance use disorders (SUDs) are increasing in prevalence and are already a leading cause of disability, due in part to the fact that our understanding of the underlying pathophysiology is incomplete. Like most neuropsychiatric syndromes, SUDs are highly heterogeneous, and distinct mechanisms may be operative in some individuals but not in others, even within a single diagnostic category. Furthermore, SUDs frequently co- occur with depression, anxiety, and other psychiatric syndromes, complicating efforts to identify molecular and circuit-level mechanisms, and disentangle them from those involved in mood and anxiety disorders. Diagnostic heterogeneity is thus a fundamental obstacle to developing better treatments, identifying biomarkers for quantifying risk for different forms of addiction, and predicting treatment response and relapse. Recently, we developed and validated an approach to discovering and diagnosing subtypes of depression using fMRI measures of functional connectivity, which in turn predicted subtype-specific clinical symptom profiles and treatment outcomes. Here, in response to PAR-18-062, we propose a secondary data analysis that would extend this approach to SUDs, leveraging multiple deeply characterized and large-scale neuroimaging datasets. Our central hypothesis is that individual differences in mechanisms underlying impairments in response inhibition and salience attribution (iRISA) are mediated by distinct forms of dysfunctional connectivity in addiction-related circuits, which in turn interact and give rise to distinct neurophysiological addiction subtypes. In Aim 1, we will use statistical clustering and machine learning methods to delineate these subtypes and optimize classifiers (fMRI biomarkers) for diagnosing them in individual patients, focusing initially on cocaine addiction. In Aim 2, we will validate these subtype-specific biomarkers by first replicating them in a new dataset and then evaluating their longitudinal stability and predictive utility. In Aim 3, we will test whether subtype-specific circuit mechanisms generalize to mediate iRISA functions in other forms of addiction, and define their interactions with distinct mechanisms mediating anhedonia and anxious arousal in patients with comorbid depression and anxiety. Project Narrative Biomarkers have transformed how physicians care for patients with cancer, cardiovascular disease, and a host of other medical conditions by providing quantitative tools for diagnosing disease processes and individualizing treatment decisions. In contrast, biomarkers for addictions remain relatively elusive. This project will test a new strategy for developing neuroimaging (brain scan) biomarkers for diagnosing novel subtypes of addiction in individual patients and then investigate how dysfunction in specific circuits gives rise to specific addiction- related behaviors and clinical symptoms.",Diagnostic and prognostic biomarkers for subtypes of addiction-related circuit dysfunction,9985102,R01DA047851,"['Abstinence', 'Amygdaloid structure', 'Anhedonia', 'Animal Model', 'Anterior', 'Anxiety', 'Anxiety Disorders', 'Arousal', 'Behavior', 'Biological Markers', 'Brain', 'Brain scan', 'Cardiovascular Diseases', 'Categories', 'Clinical', 'Cocaine', 'Cocaine Dependence', 'Cognition', 'Cognitive', 'Communities', 'Corpus striatum structure', 'Data Analyses', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Dorsal', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Habits', 'Heterogeneity', 'Impairment', 'Individual', 'Individual Differences', 'Lateral', 'Lead', 'Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medial', 'Mediating', 'Medical', 'Mental Depression', 'Molecular', 'Mood Disorders', 'Motivation', 'Negative Reinforcements', 'Neurobiology', 'Nicotine', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacology', 'Physicians', 'Prediction of Response to Therapy', 'Prevalence', 'Process', 'Prognostic Marker', 'Psychiatric Diagnosis', 'Regulation', 'Relapse', 'Research', 'Research Personnel', 'Rest', 'Rewards', 'Risk', 'Scanning', 'Subgroup', 'Substance Use Disorder', 'Symptoms', 'Syndrome', 'System', 'Testing', 'Transcend', 'Treatment outcome', 'Ventral Striatum', 'Withdrawal', 'addiction', 'animal data', 'anxious', 'base', 'biobank', 'biomarker validation', 'cocaine use', 'comorbid depression', 'comorbidity', 'craving', 'diagnostic biomarker', 'disability', 'disease classification', 'disease diagnosis', 'drug seeking behavior', 'individual patient', 'individualized medicine', 'interest', 'large scale data', 'machine learning method', 'negative affect', 'negative emotional state', 'network dysfunction', 'neuroimaging', 'neurophysiology', 'neuropsychiatry', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'patient subsets', 'reinforcer', 'response', 'specific biomarkers', 'substance misuse', 'tool']",NIDA,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2020,603973,-0.015364100134528963
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,10011756,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'effectiveness evaluation', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2020,247413,-0.001050658270197014
"TEMPORAL DIETARY PATTERNS: DEVELOPMENT AND EVALUATION AGAINST ADIPOSITY AND METABOLIC BIOMARKERS ABSTRACT There is a growing interest in dietary patterns that capture the overall quality of diet as well as its constituent foods and nutrients. Commonly used dietary patterns are a priori diet score/index based on a set of dietary recommendations for a healthy diet (e.g., Mediterranean diet, Healthy Eating Index) or data-driven dietary patterns (e.g., prudent diet, western diet). Numerous studies have shown that those dietary patterns were related to the risk of chronic diseases such as heart disease, diabetes, and cancer. However, none of these dietary patterns incorporates eating behavior such as when we eat (i.e., eating time) and how often we eat (i.e. eating frequency) during a day. Since the amount of foods and nutrients consumed at one eating occasion influences the food consumption at the subsequent eating occasion and overall intake of the day, eating time and frequency are integral parts of dietary patterns. Furthermore, several lines of evidence consistently suggest that eating time and frequency as well as a meal composition play roles in body weight regulation and metabolic health and also regulate circadian rhythms, all of which may lead to metabolic dysfunctions and ultimately chronic diseases. Given a clear need to expand the dietary patterns framework and close a gap in dietary patterns methodological work, we propose to 1) develop a “temporal” dietary patterns based on temporal distribution of eating time and frequency during a day; and 2) evaluate if the identified temporal dietary patterns are associated with i) overall diet quality and nutrient intakes, ii) adiposity (e.g., BMI, waist circumference), and iii) metabolic biomarkers (e.g., insulin, HOMA-IR, LDL-cholesterol, c-reactive protein). To overcome a limitation that a conventional statistical method cannot capture multidimensional aspects of temporal dietary patterns (e.g., 24-dimensional feature vectors, multivariate dietary intake time-series data), we will use a novel approach combining nutrition and systems science—machine learning method. The Interactive Diet and Activity Tracking in AARP (IDATA) study that repeatedly collected diet, anthropometry, and blood samples from 1,021 men and women, 50-74 years old will be used. During one year, the IDATA study collected 24-hour recalls with clock time for each eating occasion, every other month (total six 24-hour recalls); measured anthropometry three times (baseline and at month 6 and 12); and collected blood twice, 6-month apart. Successful completion of our proposed study will identify temporal dietary patterns that are related to diet quality and metabolic health and validate the utility of temporal dietary patterns as a new tool for future research on diet-health relations and prevention of chronic diseases. NARRATIVE Eating behaviors and its impact on health are complex and multidimensional. The proposed study provides an excellent opportunity to develop new dietary patterns that capture eating behaviors such as when we eat and how often we eat during a day. The findings of the study about healthy eating patterns will also improve dietary recommendations by adding messages on when and how often to eat during a day.",TEMPORAL DIETARY PATTERNS: DEVELOPMENT AND EVALUATION AGAINST ADIPOSITY AND METABOLIC BIOMARKERS,9830609,R01CA226937,"['Advisory Committees', 'Affect', 'Algorithms', 'Animals', 'Anthropometry', 'Biological Markers', 'Blood', 'Blood specimen', 'Body Weight', 'C-reactive protein', 'Calories', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Circadian Rhythms', 'Complex', 'Consumption', 'Data', 'Development', 'Diabetes Mellitus', 'Diet', 'Diet Habits', 'Dietary Practices', 'Dietary intake', 'Dimensions', 'Eating', 'Eating Behavior', 'Energy Intake', 'Evaluation', 'Fasting', 'Fatty acid glycerol esters', 'Food', 'Frequencies', 'Health', 'Healthy Eating', 'Heart Diseases', 'Hour', 'Human', 'Individual', 'Insulin', 'Intake', 'LDL Cholesterol Lipoproteins', 'Lead', 'Macronutrients Nutrition', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediterranean Diet', 'Metabolic', 'Metabolic dysfunction', 'Metabolic syndrome', 'Methodology', 'Modeling', 'Nutrient', 'Obesity', 'Outcome', 'Pattern', 'Persons', 'Physical activity', 'Play', 'Population', 'Positioning Attribute', 'Prevention', 'Recommendation', 'Regulation', 'Risk', 'Role', 'Science', 'Series', 'Statistical Methods', 'System', 'Techniques', 'Time', 'Waist-Hip Ratio', 'Weight maintenance regimen', 'Woman', 'Work', 'base', 'cardiovascular disorder risk', 'dietary guidelines', 'doubly-labeled water', 'epidemiology study', 'food consumption', 'good diet', 'improved', 'indexing', 'interest', 'machine learning method', 'men', 'novel', 'novel strategies', 'nutrient metabolism', 'nutrition', 'obesity risk', 'prudent diet', 'tool', 'vector', 'waist circumference', 'western diet']",NCI,WASHINGTON UNIVERSITY,R01,2020,396585,-0.011474661815043734
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,-0.017647185564944778
"Nathan Shock Center for Excellence in Basic Biology of Aging OVERALL—PROJECT SUMMARY Healthspan is a complex trait, influenced by many interacting polymorphic alleles and environmental factors that may accelerate or delay aging, reduce or increase disease risk, and/or promote extended lifespan. Thus, assessing the role of genetic variation in aging requires an experimental strategy capable of modeling the genetic and biological complexity of human populations while allowing for efficient identification and validation of candidate genes. With this proposal, the JAX NSC seeks support to further develop and disseminate the next generation of genetic, phenotyping, and information resources necessary to enable a systems-wide approach to understanding healthy aging. Over the past 15 years, The JAX NSC has transformed aging research both at JAX and across the geroscience community, providing central resources to support investigators that have resulted in 26 peer-reviewed publications in the last funding period. The Center has developed nascent regional and national resources for aging research, including aging mouse resources and tissues that support our numerous collaborations and external researchers. All JAX NSC data and tools are publicly disseminated on the Mouse Phenome Database and the JAX NSC website, thus ensuring that the resources generated and expertise acquired through the Center is readily available to the aging research community. In this renewal, we will advance towards our goal by providing unique resources, tools, and support to geroscience investigators while leveraging JAX's unparalleled expertise in the large-scale identification and functional validation of complex polygenic traits in mice. We will do this by providing effective Center administration and enhancing the utility of JAX NSC resources throughout the aging community (Aim 1); expanding the research focus on aging, healthspan and age-related diseases through a robust Research Development Core (Aim 2); increasing the diversity of mouse resources available for aging research, including a new study to, for the first time, investigate the effect of genetic variation on cellular senescence and treatment with senolytic drugs (Aim 3); strengthening the data and computational and support available to the aging community (Aim 4); expanding the use of machine learning technologies in interpretation of aging pathologies (Aim 5). The Center will be led by a highly experienced team of Principal Investigators and Core Leaders who, with oversight from an External Advisory Board, will provide effective management to facilitate the goals and objectives of the Center. The Center will leverage unparalleled institutional resources, facilities and expertise of The Jackson Laboratory, a globally renowned institution for mouse genetics research, to enhance its goals and the utility of the resources it generates for the aging research community. OVERALL—PUBLIC HEALTH RELEVANCE Human aging is influenced by genetic factors, whereby differences in longevity as well as changes in health and disease risk with time are linked to variation in individuals' genetic codes. The Jackson Laboratory Nathan Shock Center will develop resources to encourage the use of a wider range of mouse models in aging research. Resources—including aged mouse models that mirror human genetic variation, metabolic and microbiome data, and methods to reveal genetic factors tied to human aging—will be available to the scientific community, accelerating research to understand and ultimately prolong healthy human aging.",Nathan Shock Center for Excellence in Basic Biology of Aging,10045024,P30AG038070,"['Advisory Committees', 'Aging', 'Alleles', 'Animals', 'Biological', 'Biology of Aging', 'Candidate Disease Gene', 'Cell Aging', 'Collaborations', 'Communities', 'Complex', 'Computer Assisted', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Educational workshop', 'Ensure', 'Environmental Risk Factor', 'Funding', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Models', 'Genetic Research', 'Genetic Variation', 'Geroscience', 'Goals', 'Health', 'Heart', 'Histologic', 'Human', 'Human Genetics', 'Image Analysis', 'Inbred Strain', 'Individual', 'Information Resources', 'Institution', 'Joints', 'Laboratories', 'Leadership', 'Link', 'Liver', 'Longevity', 'Lung', 'Machine Learning', 'Maps', 'Mentorship', 'Metabolic', 'Methods', 'Mus', 'Pathology', 'Peer Review', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Pilot Projects', 'Polygenic Traits', 'Population', 'Principal Investigator', 'Process', 'Protocols documentation', 'Publications', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Shock', 'Statistical Methods', 'Structure', 'System', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Validation', 'Variant', 'Visit', 'age related', 'aged', 'animal tissue', 'behavioral phenotyping', 'candidate validation', 'career development', 'data dissemination', 'data management', 'data tools', 'disorder risk', 'experience', 'healthspan', 'healthy aging', 'insight', 'microbiome', 'mouse genetics', 'mouse model', 'next generation', 'novel', 'open source', 'phenome', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'senescence', 'symposium', 'tool', 'trait', 'user-friendly', 'web site']",NIA,JACKSON LABORATORY,P30,2020,1069526,-0.03451658729939082
"Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine A fundamental challenge in precision medicine is to understand the patterns of differentiation between individuals. To address this challenge, we propose to go beyond the traditional `one disease--one model' view of bioinformatics and pursue a new view built upon personalized patient models that facilitates precision medicine by leveraging both commonalities within a patient cohort as well as signatures unique to every individual patient. With the emergence of large-scale databases such as The Cancer Genome Atlas (TCGA), the International Cancer Genome Consortium (ICGC), and the Gene Expression Omnibus (GEO), which collect multi-omic data on many different diseases, a new “pan-omics” and “pan-disease” paradigm has emerged to jointly analyze all patients in a disease cohort while accounting for patient-specific effects. An example of this is the recently released Pan-Cancer Atlas. At the same time, next generation statistical tools to accurately and rigorously draw the necessary inferences are lacking. In this project we propose a series of mathematically rigorous, statistically sound, and computationally feasible approaches to infer sample-specific models, providing a more complete view of heterogeneous datasets. By bringing together ideas from the machine learning, statistics, and mathematical optimization communities, we provide a rigorous framework for precision medicine via sample-specific statistical models. Crucially, we propose to analyze this framework and prove strong theoretical guarantees under weak assumptions--this dramatically distinguishes our framework from much of the existing literature. Towards these goals, we propose the following aims: Aim 1: Discovery of new molecular profiles with sample-specific statistical models. We propose a general framework for inferring sample-specific models with low-rank structure based on the novel concept of distance-matching. This allows us to infer statistical models at the level of a single patient without overfitting, and is general enough to be applied for prediction, classification, and network inference as well as a variety of diseases and phenotypes. Aim 2: Multimodal approaches to personalized diagnosis--contextually interpretable models for actionable clinical decision support. In order to translate these models into practice, we propose a novel interpretable predictive model that supports complex, multimodal data types such as images and text combined with high-level interpretable features such as SNP data, gender, age, etc. This framework simultaneously boosts the accuracy of clinical predictions by exploiting sample heterogeneity while providing human-digestable explanations for the predictions being made. Aim 3: Next-generation precision medicine--algorithms and software for personalized estimation. To put our models into practical use, we will develop new algorithms for interpretable prediction of personalized clinical outcomes and visualization of personalized statistical models. All of our tools will be combined into a user-friendly software package called PrecisionX that will be freely available to researchers and clinicians everywhere. RELEVANCE (See instructions): Personalization with data is a critical challenge whenever decisions must be made at scale, and has applications that go beyond precision medicine; businesses, educational institutions, and financial institutions are among the many players that have acknowledged a stake in this complex problem. We expect the proposed work to provide a rigorous foundation for personalization with large and high-dimensional datasets, finding use throughout the broader scientific community as well as with industry and educational institutions. Alongside our collaboration with Pitt/UPMC, we will work with physicians and data scientists for practical feedback as well as provide training in the methods developed. n/a",Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine,10133782,R01GM140467,"['Accounting', 'Address', 'Age', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Bioinformatics', 'Businesses', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Scientist', 'Data Set', 'Disease', 'Feedback', 'Foundations', 'Gender', 'Gene Expression', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Industry', 'Institution', 'Instruction', 'International', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Portraits', 'Research Personnel', 'Sampling', 'Series', 'Statistical Models', 'Structure', 'Text', 'The Cancer Genome Atlas', 'Time', 'Training', 'Translating', 'Visualization', 'Work', 'base', 'cancer genome', 'clinical decision support', 'clinically actionable', 'cohort', 'disease phenotype', 'heterogenous data', 'high dimensionality', 'individual patient', 'large-scale database', 'molecular modeling', 'multimodal data', 'multimodality', 'next generation', 'novel', 'personalized diagnostics', 'personalized predictions', 'precision medicine', 'predictive modeling', 'sound', 'statistics', 'tool', 'user friendly software']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,305566,-0.013263001598731776
"Computational and Statistical Framework to Model Tissue Shape and Mechanics PROJECT SUMMARY  The morphologic and mechanical characteristics of a tissue are fundamental to understanding the development, homeostasis, and pathology of the human body. During the previous period of funding, we developed statistical shape modeling (SSM) methods and applied these to the study of structural hip disease. We also developed the initial framework to integrate SSM with finite element (FE) analysis to enable the study of shape and mechanics together. If incorporated into clinical practice, SSM and FE analysis could identify features of the anatomy likely responsible for injury, remodeling, or repair. Geometry needed for SSM and FE models is typically generated by segmentation of volumetric imaging data. This step can be painstakingly slow, error prone, and cost prohibitive, which hampers clinical application of these computational techniques. We have created a deep machine learning algorithm ‘DeepSSM’ that uses a convolutional neural network to establish the correspondence model directly from unsegmented images. In Aim 1 we will apply DepSSM to improve clinical understanding of structural hip disease by characterizing differences in anatomy between symptomatic and asymptomatic individuals; these morphometric comparisons will identify anatomic features most telling of disease, thereby guiding improvements in diagnosis. Computational advancements have simplified the process to generate patient-specific FE models, enabling clinically focused research. However, there is no framework to collectively visualize, compare, and interpret (i.e., post-process) results from multiple FE models. Currently, inter-subject comparisons require oversimplifications such as averaging results over subjectively defined regions. In Aim 2 we will develop new post-processing methods to collectively visualize, interpret and statistically analyze FE results across multiple subjects and study groups. We will map FE results to synthetic anatomies representing statistically meaningful distributions using the correspondence model. Statistical parametric mapping will be applied to preserve anatomic detail through statistical testing. We will use our published FE models of hip joint mechanics as the test system. Finally, volumetric images provide a wealth of information that is delivered to physicians in a familiar format. Yet, tools are not available to interpret model data with clinical findings from volumetric images. In Aim 3, we will develop methods that evaluate relationships between shape, mechanics, and clinical findings gleaned from imaging through integrated statistical tests and semi-automatic medical image annotation tools that utilize standard ontologies. Quantitative CT and MRI images of the hip, which estimate bone density and cartilage ultrastructure, respectively, will be evaluated as test datasets. To impart broad impact, we will disseminate our methods to the community as open source software that will call core functionality provided by existing, open source software that has a large user base (FEBio, ShapeWorks). PROJECT NARRATIVE The proposed technology will provide the methodologies necessary to increase the clinical acceptance and applicability of computer models. These models measure three-dimensional tissue shape and estimate tissue mechanics, providing information that cannot be measured conventionally. We will implement these methods into software that can be used by the public free-of-charge.",Computational and Statistical Framework to Model Tissue Shape and Mechanics,9972694,R01EB016701,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anatomy', 'Architecture', 'Bone Density', 'Cardiology', 'Cartilage', 'Characteristics', 'Charge', 'Clinical', 'Communities', 'Computational Technique', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Deformity', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Finite Element Analysis', 'Foundations', 'Funding', 'Geometry', 'Glean', 'Grooming', 'Hip Joint', 'Hip region structure', 'Homeostasis', 'Human Pathology', 'Human body', 'Image', 'Individual', 'Injury', 'Intuition', 'Libraries', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Neurology', 'Ontology', 'Orthopedics', 'Pathology', 'Patient imaging', 'Patients', 'Performance', 'Physicians', 'Procedures', 'Process', 'Publishing', 'Quantitative Evaluations', 'Research', 'Resources', 'Scheme', 'Shapes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Training', 'Validation', 'X-Ray Computed Tomography', 'annotation  system', 'base', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data modeling', 'disease diagnosis', 'improved', 'in vivo', 'machine learning algorithm', 'novel', 'open source', 'predictive modeling', 'preservation', 'relating to nervous system', 'repaired', 'shape analysis', 'simulation', 'three-dimensional modeling', 'tool']",NIBIB,UNIVERSITY OF UTAH,R01,2020,563658,-0.01974563469914804
"Multimodal Approaches to Neurobiology of Traumatic Dissociation Dissociative symptoms in traumatized individuals are common, debilitating, and costly; however, little is known about how its biological mechanisms interact with PTSD treatment. Traumatic dissociation broadly encompasses a range of distinct, yet clinically interrelated symptoms: depersonalization, derealization, amnesia, numbing, flashbacks, passive influence phenomena, and identity disturbances. Either alone or in various combinations, these symptoms serve as diagnostic criteria and commonly associated features across multiple psychiatric disorders. Traumatic dissociation is also associated with significant personal and societal burden. Traumatized individuals with dissociative symptoms typically have co-occurring psychiatric conditions, high rates of self-destructive behaviors and suicidality, and are disproportionate treatment utilizers. In addition, they are at increased risk for attrition, non-response and relapse following treatment interventions. Despite the significant and disabling nature of traumatic dissociative symptoms, little is known about the neurobiology of these processes and targeted interventions do not exist.  PTSD treatment studies have neither looked at neural intermediate phenotypes of dissociation, nor how these are associated with psychophysiological and digital phenotypes. Compared to clinical symptom measures, these biological and in-the-moment digital markers of dissociation may more robustly map onto the underlying core aspects of the disorder differentiating dissociation subtypes following childhood and adult trauma. We propose to build upon our prior Exploratory R21 to now capture longitudinal multimodal phenotype data related to dissociation, pre-, post- and during PTSD treatment modalities that include empirically-derived, exposure-based components.  The goals of this study will be 1) to understand the differential biomarkers that map onto dissociative symptoms, and 2) to understand how these biomarkers may best predict trajectory of response to empirically based standard-of-care treatments. For each of these Aims, we will collect Neuroimaging, Physiology, and Digital Phenotyping data, applying computational modeling with multimodal data to provide machine-learning based, unbiased predictive models of dissociative intermediate phenotypes at baseline and longitudinally. This naturalistic study will allow us to map the biology of dissociation, and importantly, the change in dissociative symptoms and underlying biomarkers over time, using naturalistic evidenced-based treatment for PTSD in 130 treatment-seeking patients with PTSD, and a range of dissociative symptoms.  Successful completion of these Aims will provide a novel and powerful understanding of the biological markers of dissociation subtypes following trauma exposure, and will identify biological mechanisms for understanding and treating PTSD with dissociation. Pathological trauma-related dissociation is a significant personal and societal burden. This proposal will examine behavioral and biological correlates of dissociation across imaging, physiology, and digital assessments. Results from this proposal will inform how to identify those at risk for these debilitating symptoms, potential brain regions to target for treatment, and may support a new standard of clinical care for PTSD with dissociation. ",Multimodal Approaches to Neurobiology of Traumatic Dissociation,9965404,R01MH119227,"['Address', 'Adult', 'Aftercare', 'Amnesia', 'Anterior', 'Arousal', 'Attention', 'Behavioral', 'Biological', 'Biological Markers', 'Biology', 'Brain', 'Brain region', 'Childhood', 'Clinical', 'Clinical Trials', 'Complex', 'Computer Models', 'Data', 'Depersonalization', 'Derealizations', 'Diagnosis', 'Diagnostic', 'Disease', 'Dissociation', 'Dorsal', 'Ecological momentary assessment', 'Emotional', 'Environment', 'Equipment and supply inventories', 'Evidence based treatment', 'Exhibits', 'Fright', 'Future', 'Goals', 'Image', 'Individual', 'Intervention', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Medial', 'Mental disorders', 'Modality', 'Modeling', 'Nature', 'Neurobiology', 'Numbness', 'Participant', 'Pathologic', 'Patient Self-Report', 'Patients', 'Phase', 'Phenotype', 'Physiological', 'Physiology', 'Post-Traumatic Stress Disorders', 'Prefrontal Cortex', 'Process', 'Psychometrics', 'Psychophysiology', 'Recording of previous events', 'Recovery', 'Reflex action', 'Refractory', 'Regulation', 'Relapse', 'Rest', 'Risk', 'Role', 'Self Destructive Behavior', 'Sinus Arrhythmia', 'Suicide', 'Symptoms', 'System', 'Time', 'Trauma', 'Work', 'base', 'cingulate cortex', 'clinical care', 'cost', 'digital', 'disabling symptom', 'emotion regulation', 'heart rate variability', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pediatric trauma', 'phenotypic data', 'predictive modeling', 'relating to nervous system', 'respiratory', 'response', 'skills', 'standard of care', 'targeted biomarker', 'targeted therapy trials', 'time use', 'trauma exposure']",NIMH,MCLEAN HOSPITAL,R01,2020,760581,-0.012309348177351694
"Noninvasive assessment of the cornea by diffusion OCT Keratoconus is a degenerative disease of the cornea that is a major cause of reduced vision-related quality of life in the United States, often leading to corneal transplantation. Ectasia after refractive surgery is a vision-threatening complication that can occur in apparently low-risk patients despite current screening technology. The biomechanical properties of the cornea play a central role in these diseases, but diagnostics are still rooted in shape measures because doctors lack direct measures of biomechanical change. While several methods have shown early promise for addressing this gap, most require contact with or perturbation of the cornea, cannot spatially resolve biomechanical properties, or involve expensive optical systems.  To address the need for direct biomechanical measurement of the cornea and the limitations of other approaches, we introduce phase-decorrelation OCT (phd-OCT). Phd-OCT makes use optical coherence tomography (OCT) to quantify random nanoscale mobility that is related to the strength and cohesion of the cornea. Preliminary results strongly support the rationale, feasibility and potential advantages of the approach. Our objectives are: (1) to refine our method to optimize detection sensitivity and speed, and (2) to determine if the technique is clinically useful. We will achieve these objectives through the following aims: 1. To develop and validate mobility-sensitive phd-OCT for corneal imaging. Spectral-domain OCT will be used for anterior segment phase-decorrelation imaging and the analysis algorithm will be optimized. The system will be validated using phantoms and torsional rheometry. 2. To investigate the potential influence of physiological factors (intraocular pressure (IOP), hydration, and temperature). Factorial design experiments in porcine and human donor globes will establish the sensitivity of phd-OCT measurements to potential confounders. 3. To develop and validate data acquisition and processing methods to enable clinical testing. GPU processing and machine learning will be configured to minimize processing time. Scan patterns will be optimized to minimize scan time and return clinically useful data. 4. To assess feasibility and preliminary diagnostic performance of clinical mobility-sensitive OCT imaging of the cornea. A first-in-human study will characterize repeatability and test the hypotheses that phd-OCT mobility measurements are increased in the region of a LASIK flap and decreased in CXL.  Expected Outcomes: The proposed studies will establish a new, non-contact method for imaging corneal biomechanical properties with the potential to address many shortcomings of current and emerging methods. Success could lead to earlier detection of of ectasia risk and allow more appropriate timing and customization of corneal treatments. Future integration of data into computational models has the potential to greatly impact the field by driving a shift from empirical planning and risk analysis to patient-specific strategies. The mechanical properties of the cornea, the transparent front of the eye, are important for diseases that affect vision as well as for corrective treatments like crosslinking therapy and LASIK. We have developed a new optical coherence tomography (OCT)-based method to detect and map corneal mechanical properties that has advantages that may make it readily translatable to clinical use. The objective of this proposal is to determine whether this technique is clinically feasible and useful by refining the technology, testing it in animal and human donor corneas, and performing studies in patients with corneal conditions of interest.",Noninvasive assessment of the cornea by diffusion OCT,9942461,R01EY028667,"['Achievement', 'Address', 'Affect', 'Algorithmic Analysis', 'Algorithms', 'Animals', 'Anterior', 'Automobile Driving', 'Biomechanics', 'Clinical', 'Clinical Research', 'Complication', 'Computational Technique', 'Computer Models', 'Cornea', 'Corneal Diseases', 'Custom', 'Data', 'Degenerative Disorder', 'Detection', 'Development', 'Diagnostic', 'Diffusion', 'Disease', 'Doctor of Philosophy', 'Early Diagnosis', 'Ensure', 'Exhibits', 'Eye', 'Family suidae', 'Feedback', 'Future', 'Human', 'Hydration status', 'Image', 'Keratoconus', 'Keratoplasty', 'Laser In Situ Keratomileusis', 'Lead', 'Life', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathological Dilatation', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic Intraocular Pressure', 'Physiological', 'Plant Roots', 'Play', 'Procedures', 'Property', 'Proxy', 'Quality of life', 'Risk', 'Role', 'Scanning', 'Shapes', 'Speed', 'Structure', 'Surgical Flaps', 'Surgical incisions', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Torsion', 'United States', 'Validation', 'Vision', 'Visual impairment', 'base', 'clinical practice', 'cohesion', 'computerized data processing', 'crosslink', 'data acquisition', 'data integration', 'design', 'experimental study', 'first-in-human', 'human study', 'imaging modality', 'interest', 'mechanical properties', 'nanoscale', 'novel', 'outcome prediction', 'research clinical testing', 'screening', 'simulation', 'success', 'tool', 'treatment effect']",NEI,CASE WESTERN RESERVE UNIVERSITY,R01,2020,391938,-0.01601674057797006
"Driver Genes for Engineered Rotator Cuff Development Rotator cuff tears affect over 15% of Americans and impair shoulder joint biomechanics and function. Following repair of symptomatic tears, functional deficits frequently persist and re-tears are common, due to the complex anatomy and high functional demands on the rotator cuff tendons. Rotator cuff tendon tissue engineering research is focused on devices to improve immediate mechanical support to the repair and to stimulate early and rapid tendon regeneration rather than scarring and fibrosis, particularly for the supraspinatus tendon (SST), the most commonly torn tendon in the rotator cuff. Aligned electrospun scaffolds that mimic both the highly aligned medial region of the SST, and bi-axially aligned electrospun scaffolds that mimic the multi-axially aligned isotropic anterior region of the SST have been evaluated with promising results when seeded with adipose- derived stem cells (ASCs). However, progress in this area of rotator cuff tendon engineering and in other areas of tendon research is hindered by the lack of definitive markers for SST or for its regional heterogeneity, the lack of understanding to what extent ASCs are tenogenic and can assume the identity of tendon fibroblasts, the lack of specific markers for tendon fibroblast identity and tenogenic differentiation, and by a lack of markers for tendon maturation and response to mechanical loading in engineered tendon. Therefore, is it difficult to assess how successful current tendon tissue engineering approaches really are, or to predict how well tendon tissue engineered approaches will function in translation when autologous or allogeneic ASCs from diverse human populations are used to enhance rotator cuff repair via augmentation or interposition with engineered tendon devices. These studies will evaluate the epigenome (methylome), transcriptome, proteome, lipidome, metabolome and phenome (phenotype) of native human SST and donor-matched tissue engineered tendon produced from SST fibroblasts and ASCs. Bioinformatics approaches will be used to integrate the data to an integrated multiome, which will then be used with machine learning approaches to extract key causal ‘driver’ genes, or tendon specific genes or molecules responsible for: 1) SST heterogeneity between medial and anterior regions. 2) Tendon cell identity and the extent of tenogenesis by ASCs on electrospun scaffolds. 3) The heterogenetic response by ASCs on uni- vs. bi-axially aligned electrospun scaffolds that mimic the native heterogeneity of the SST. 4) The response of engineered tendon to dynamic loading. Identified driver genes or molecules will be validated though over-expression or silencing approaches, thus providing therapeutic targets for manipulation to enhance tenogenesis, and engineered tendon development and maturation. Together these innovative studies will provide a template for improved external validity of benchtop tendon tissue engineering and pre-clinical studies towards successful translation in diverse patient populations. In addition, the bioinformatics and multiomics toolboxes and assays that result from this work will be invaluable to not only the tendon research community, but also to the wider musculoskeletal and regenerative medicine fields. A small number of driver genes in other medical fields are responsible for changes in expression and protein levels in hundreds of other genes, and subsequent tissue metabolism. The process of tissue engineered rotator cuff tendon development and formation as it relates to adult tendon after surgical repair is not well understood, despite rotator cuff tears being an important clinical problem and representing almost one third of orthopedic injuries. Evaluating changes in the ability of genes to be translated to proteins at the DNA, RNA, small metabolite, and protein level of tissue engineered tendon formation under different loading conditions compared to normal adult tendon will identify driver genes and potential new ways to improve currently available options for rotator cuff repair.",Driver Genes for Engineered Rotator Cuff Development,9894766,R01AR073882,"['Address', 'Adipose tissue', 'Adult', 'Affect', 'Allogenic', 'American', 'Anatomy', 'Anterior', 'Area', 'Autologous', 'Biochemical', 'Biocompatible Materials', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Biomechanics', 'Cadaver', 'Cells', 'Cicatrix', 'Clinical', 'Communities', 'Complex', 'Computational Technique', 'Cues', 'DNA', 'Data', 'Data Set', 'Development', 'Devices', 'Engineered Gene', 'Engineering', 'Evaluation', 'Extracellular Matrix', 'Fibroblasts', 'Fibrosis', 'Genes', 'Genetic', 'Harvest', 'Head', 'Heterogeneity', 'Human', 'Impairment', 'In Vitro', 'Injury', 'Investigation', 'Joint structure of shoulder region', 'Knowledge', 'Machine Learning', 'Measures', 'Mechanical Stimulation', 'Mechanics', 'Medial', 'Medical', 'Mesenchymal', 'Mesenchymal Stem Cells', 'Metabolism', 'Methylation', 'Modeling', 'Modification', 'Molecular', 'Musculoskeletal', 'Natural regeneration', 'Operative Surgical Procedures', 'Orthopedics', 'Outcome', 'Phenotype', 'Population', 'Process', 'Property', 'Proteins', 'Proteome', 'RNA', 'Regenerative Medicine', 'Research', 'Resources', 'Rotator Cuff', 'Seeds', 'Techniques', 'Tendon structure', 'Testing', 'Tissue Engineering', 'Tissues', 'Translating', 'Translations', 'Work', 'design', 'epigenome', 'improved', 'innovation', 'interest', 'mechanical load', 'mechanical properties', 'metabolome', 'methylome', 'multiple omics', 'overexpression', 'patient population', 'phenome', 'phenomics', 'preclinical study', 'progenitor', 'regenerative', 'repaired', 'response', 'rotator cuff tear', 'scaffold', 'spatial relationship', 'stem cells', 'supraspinatus muscle', 'tendon development', 'therapeutic target', 'transcriptome', 'translation to humans']",NIAMS,PURDUE UNIVERSITY,R01,2020,491199,-0.04016178527581582
"Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Summary:  Spinal cord injury (SCI) patients experience limited functional recovery, owing in part to the paucity of axon regrowth from injured CNS neurons. Effective treatments are lacking, likely because of multiple factors, intrinsic and extrinsic, that inhibit axon growth. Thus we require agents that target more than one source of regeneration failure.  Kinases are ubiquitous signal transducers that regulate most cellular processes, including axon growth. To begin to identify compounds that positively regulate axon growth, we screened 1600 small-molecule kinase inhibitors (KIs) in an in vitro CNS neurite outgrowth assay and identified “hit” KIs that reproducibly and strongly promote outgrowth. Due to homology of catalytic domains, KIs typically inhibit multiple kinases. This makes it difficult to identify the kinase(s) that mediate a KI's effects on cells. We used information theory and machine learning to analyze the inhibition profiles of KIs in relation to their effects on neurite outgrowth. This enabled us to identify, and later validate via siRNA knockdown in primary neurons, multiple kinase targets (i.e. kinases that should be inhibited to promote neurite outgrowth). These included previously known targets that regulate intrinsic and extrinsic inhibitor factors, in addition to several novel candidates. Conversely, we identified kinases whose activity is critical for neurite outgrowth, and whose inhibition must be avoided (anti-targets). We discovered several KIs that inhibit multiple targets and no anti-targets. These KIs strongly promoted neurite outgrowth in vitro.  We tested the KI, RO48, that had the largest effect in vitro in two in vivo models. Our preliminary experiments indicate that RO48 is remarkably effective in vivo. It promoted robust axonal growth of the corticospinal tract (CST) in three separate models of CST injury (pyramidotomy, funiculotomy, dorsal hemisection), and in the dorsal hemisection model, improved forelimb function. We propose to build on these remarkable results to test the working hypothesis that the simultaneous inhibition of RO48's five target kinases (ROCK, PKC, PRKG1, PRKX, and RPS6K) promotes sprouting and regeneration of CST axons. This will be accomplished using viral vectors to knock down expression of the different target kinases individually and in combination. We will do knockdown in CST neurons in the cortex. We will assess CST axon growth at the injury site using light microscopy. We will also perform experiments to determine if RO48-induced CST axon growth promotes axon sprouting, regeneration, or both, and whether RO48 improves behavioral outcomes such as grasping and walking after a contusion injury.  These experiments will 1) validate novel kinases as in vivo targets for future development of SCI therapeutics 2) determine whether these kinases regulate CST axon sprouting, regeneration, or both, and 3) confirm whether the substantial stimulation of axon growth induced by treatment with RO48 improves motor outcomes in a clinically relevant contusion model.  Title: Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury Project Narrative: The proposed experiments aim to understand how small-molecule drug-like compounds increase the ability of nerve cells to grow long processes and re-form connections. Validating the molecular targets of these compounds for in vivo nerve growth will enable future drug discovery projects focused on these targets.",Targeting Multiple Kinases to Treat Experimental Spinal Cord Injury,9917854,R01NS100531,"['Axon', 'Behavioral', 'Biochemical', 'Biological', 'Biological Assay', 'Catalytic Domain', 'Cell physiology', 'Cells', 'Cervical', 'Complement 5a', 'Confocal Microscopy', 'Control Animal', 'Contusions', 'Corticospinal Tracts', 'Data', 'Development', 'Distal', 'Dorsal', 'Dose', 'Failure', 'Forelimb', 'Future', 'Gold', 'Growth', 'In Vitro', 'Individual', 'Information Theory', 'Injury', 'Institution', 'Label', 'Lesion', 'Light', 'Machine Learning', 'Mediating', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Target', 'Morphology', 'Motor', 'Motor Cortex', 'Mus', 'Natural regeneration', 'Nerve', 'Neurites', 'Neurons', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphotransferases', 'Process', 'Rattus', 'Recovery of Function', 'Role', 'Signal Pathway', 'Signal Transduction', 'Site', 'Small Interfering RNA', 'Source', 'Spinal', 'Spinal Cord', 'Spinal cord injury', 'Spinal cord injury patients', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transducers', 'Viral Vector', 'Walking', 'axon growth', 'axon regeneration', 'axonal sprouting', 'behavior test', 'behavioral outcome', 'central nervous system injury', 'clinically relevant', 'design', 'drug discovery', 'effective therapy', 'experience', 'experimental study', 'grasp', 'gray matter', 'improved', 'in vivo', 'in vivo Model', 'inhibitor/antagonist', 'injured', 'insight', 'kinase inhibitor', 'knock-down', 'light microscopy', 'novel', 'reconstruction', 'regenerative', 'screening', 'small molecule', 'targeted agent', 'therapeutic target']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2020,465464,-0.026815890371606463
"Clinical Evaluation of Burns using Spatial Frequency Domain Imaging Program Director/Principal Investigator (Last, First, Middle): Durkin, Anthony J. Abstract The central aim of this 3 year competing R01 renewal is to characterize and apply a new, compact, clinic- friendly Spatial Frequency Domain Imaging (SFDI) device to objectively and non-invasively classify burn severity (burn grade) over a large areas of skin. Delays in determining burn severity directly impacts patient treatment plans (including decisions whether to graft), rates of infection and scarring, duration of hospitalization and ultimately cost of care. Currently, the primary method of determining burn severity continues to be clinical assessment, which is highly subjective. While both superficial thickness and full-thickness burns are typically readily diagnosed based on visual clinical impression, partial thickness burns are difficult to classify and carry with them considerable potential for complications. Burn severity classification accuracy, even by experts, is only 60–80%. Our research in animal models demonstrates that SFDI data can successfully be used to classify different regions of burn severities. Typically, these differences are not apparent to the unaided eye and a great deal of training and experience is required in order for clinicians to accurately differentiate them Our work using a research grade, hybrid-SFDI device suggests that objective parameters provided by SFDI can be used within 24 hours after injury, to accurately classify burn severity. Specifically, we have demonstrated in a porcine burn model that the research grade SFDI outperforms laser speckle imaging and thermal imaging at 24 hours post-burn, in terms of predicting whether a burn will require a graft or not. However, translating these results to the clinic has been difficult due to several device limitations. The research grade SFDI device has slow acquisition times that can result in motion artifacts. It is also sensitive to ambient light which is often an issue in a clinical setting. Additionally, the SFDI device generates so much diverse data (oxygenated and deoxygenated hemoglobin, water fraction, reduced scattering coefficients at multiple wavelengths), there is no obvious way to present it to a clinical user to make a quick decision. To this end, we propose to methodically investigate an improved next generation SFDI device that addresses these issues by using brighter LEDs and fewer wavelengths to rapidly collect data in a way that reduces motion artifacts and is independent of clinical lighting conditions. In addition, we will develop a machine learning based classification framework that will provide the clinical with actionalble diagnostic information. The central aim of this 3 year competing R01 renewal is to characterize and then modify a new clinic-friendly SFDI device (Clarifi) to objectively classify in- vivo regions of different burn severity over large areas. The proposed research seeks to investigate this via the following Specific Aims: 1) Test & Validate Clinical SFDI Instrument, 2) Compare Clinical SFDI Instrument to other Modalities on a Long Term Swine Model of Graded Burns, 3) Develop Spatially Resolved Classification Maps of Burn Severity based on SFDI Data, 4) Conduct Clinical Measurements of Burn Severity using the new SFDI device and Spatially Resolved Burn Severity Classification Maps based on SFDI data. Program Director (Last, first, middle): Durkin, Anthony J. PROJECT NARRATIVE Burn injuries rank in the top 15 causes of global burden of disease. Burn severity assessment, which is a critical step in treatment planning, is subjective, depending on the experience of the treating physician. This leads to misdiagnosis and increased days of hospitalization and cost. In order to address this, we propose to test, validate and apply a novel optical imaging device in order to provide noninvasive objective assessment of burn wound severity. This has the potential to improve management of burn patients and reduce rates of complications.",Clinical Evaluation of Burns using Spatial Frequency Domain Imaging,10052657,R01GM108634,"['Address', 'Animal Model', 'Area', 'Biometry', 'Blood Vessels', 'Burn Centers', 'Burn injury', 'Cicatrix', 'Classification', 'Clinic', 'Clinical', 'Clinical assessments', 'Collaborations', 'Custom', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Enrollment', 'Eye', 'Family suidae', 'Female', 'Hemoglobin', 'Hospital Costs', 'Hospitalization', 'Hour', 'Hybrids', 'Image', 'Imaging Device', 'Injury', 'Laser Speckle Imaging', 'Lasers', 'Light', 'Lighting', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Medical center', 'Methods', 'Modality', 'Modeling', 'Morphologic artifacts', 'Motion', 'Noise', 'Optics', 'Output', 'Patients', 'Physicians', 'Principal Investigator', 'Property', 'Reporting', 'Research', 'Severities', 'Side', 'Signal Transduction', 'Skin', 'Spatial Frequency Domain Imaging', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Translating', 'Ulcer', 'Variant', 'Visual', 'Water', 'Work', 'base', 'burden of illness', 'burn model', 'burn wound', 'care costs', 'clinical imaging', 'cost', 'data acquisition', 'data integrity', 'data modeling', 'diverse data', 'experience', 'healing', 'human data', 'imaging system', 'impression', 'improved', 'in vivo', 'infection rate', 'male', 'next generation', 'novel', 'optical imaging', 'pre-clinical', 'programs', 'research clinical testing', 'stability testing', 'treatment planning']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,430325,-0.012281744640750113
"In-Office, Ultrasound-Based Breakage and Removal of Urinary stones OVERALL SUMMARY  The main focus of this Program Project Grant is to discover the foundational and translational knowledge needed to create an office-based handheld ultrasound device to target, detach, break, and expel stones and stone fragments from the urinary space to facilitate natural clearance. This system will obviate costly and inefficient emergency department visits that typically include repetitive exposure to ionizing radiation from diagnostic imaging, and will significantly reduce the often lengthy (days to weeks) wait time patients must endure before procedures for stone removal can be scheduled and performed. As the proposed therapy system is entirely noninvasive, patients will be treated on an outpatient basis. Further, as the system is designed to efficiently and painlessly break stones of any size and expel the fragments from the kidney, the treatment of both symptomatic and asymptomatic stones using this technology will reduce the high retreatment and stone event recurrence rates associated with current surgical interventions for stone removal.  In this effort, we will combine stone breakage by burst wave lithotripsy (BWL), clearance of fragments by ultrasonic propulsion (UP), and stone-specific ultrasound imaging (S-mode) into an integrated system in which exposure strategies are adapted during treatment in response to real-time acoustic feedback to enhance comminution efficiency and patient safety. We will tailor treatment by investigating numerically and in lab tests the primary mechanisms - cavitation and elastic waves - involved in the comminution process over a broad parameter space. We will develop acoustics-based feedback including model- based, machine learning and passive acoustic mapping (PAM) of the bubble field to signal the need to adjust the energy output. We will investigate the morphological and functional response of the kidney in living animals and in ex vivo perfused porcine kidneys, and pursue tissue protective treatment strategies such as power ramping.  These studies will include the first in-human test of BWL in which we will compare the comminution effectiveness and safety of treatment with and without adaptive output control in response to acoustic feedback. In addition, we will conduct a randomized controlled trial of the benefits and risks of fragmenting and expelling symptomatic and asymptomatic stones in the clinic. Toward application of the system for use in humans, we will refine and validate the use of UP and S-mode together to improve stone and fragment detection. With our eye on the future of stone management, we will develop and validate in vivo an extracorporeal acoustic tractor beam to grasp and carry fragments through the complex three- dimensional path of the urinary space and out of the kidney. OVERALL NARRATIVE Urinary stone disease, which affects 1 in 11 Americans, is one of the most painful diseases and also the costliest non-malignant urologic disease, because current management is limited to observation for the stone to pass or performing surgery. The main focus of this Program Project Grant is to discover the foundational and translational knowledge needed to create an office-based, handheld ultrasound device to target, detach, break, and expel stones and stone fragments from the urinary space to facilitate natural clearance. This system will obviate costly and temporizing emergency department visits and radiation exposing imaging while the patient waits in pain for days to weeks for the stone to pass or be surgically removed.","In-Office, Ultrasound-Based Breakage and Removal of Urinary stones",10005352,P01DK043881,"['3-Dimensional', 'Acoustics', 'Address', 'Affect', 'American', 'Anatomy', 'Animals', 'Benefits and Risks', 'Biostatistics Core', 'Bowman&apos', 's space', 'Calibration', 'Clinic', 'Clinical', 'Collaborations', 'Complex', 'Data Set', 'Detection', 'Devices', 'Diagnostic Imaging', 'Disease', 'Effectiveness', 'Emergency department visit', 'Engineering', 'Ensure', 'Event', 'Excision', 'Exposure to', 'Eye', 'Family suidae', 'Feedback', 'Foundations', 'Fracture', 'Future', 'Health', 'Human', 'Image', 'Ionizing radiation', 'Kidney', 'Knowledge', 'Lithotripsy', 'Machine Learning', 'Measures', 'Methodology', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Non-Malignant', 'Operative Surgical Procedures', 'Outpatients', 'Output', 'Pain', 'Painless', 'Patients', 'Procedures', 'Process', 'Productivity', 'Program Research Project Grants', 'Publications', 'Radiation exposure', 'Ramp', 'Randomized Controlled Trials', 'Recurrence', 'Renal Tissue', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Retreatment', 'Safety', 'Sample Size', 'Schedule', 'Signal Transduction', 'Statistical Methods', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Model', 'Tissues', 'Ultrasonics', 'Ultrasonography', 'Urinary Calculi', 'Urologic Diseases', 'Wait Time', 'base', 'biomechanical model', 'calcification', 'complex data ', 'cost', 'design', 'experimental study', 'first-in-human', 'grasp', 'improved', 'in vivo', 'individualized medicine', 'next generation', 'patient safety', 'programs', 'response', 'safety study', 'simulation', 'structured data', 'success', 'treatment strategy']",NIDDK,UNIVERSITY OF WASHINGTON,P01,2020,1567293,-0.024870587683835408
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9920211,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'data quality', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,709525,-0.0009505744286558409
"Evaluation of multiple medication exposures concurrently using a novel algorithm PROJECT SUMMARY The development of large observational health databases (OHD) has expanded the data available for analysis by pharmacoepidemiology research. The efficiency of these studies may be improved by simultaneously studying the association of multiple medications with a disease of interest. Unfortunately, prior research has demonstrated that it is difficult to distinguish true-positive from false-positive results when studying multiple exposures simultaneously, thus limiting the conclusions drawn from these types of studies and representing a major gap in the field. The objective of this proposal, which is the first step in achieving the applicant's long- term goal of improving the diagnosis and treatment of gastrointestinal diseases using insights derived from OHD, is to evaluate and validate medication class enrichment analysis (MCEA), a novel set-based signal-to- noise enrichment algorithm developed by the applicant to analyze multiple exposures from OHD with high sensitivity and specificity. The central hypothesis of this proposal is that MCEA has equal sensitivity and greater specificity compared to logistic regression, the most widely used analytic method for OHD, for identifying true associations between medications and clinical outcomes. The applicant will complete the following two interrelated specific aims to test the hypothesis: Aim 1 – to calculate the sensitivity and specificity of medication class enrichment analysis (MCEA) and logistic regression (LR) for identifying medication associations with Clostridium difficile infection (CDI) and Aim 2 – to calculate the sensitivity and specificity of MCEA and LR for identifying medication associations with gastrointestinal hemorrhage (GIH). The rationale for these aims is that by reproducing known medication-disease associations without false positives, MCEA can be used to identify novel pharmacologic associations with gastrointestinal diseases in future studies. The expected outcome for the proposed research is that it will demonstrate MCEA as a valid method for pharmacoepidemiology research, opening new research opportunities for the study of multi-exposure OHD. These new research opportunities may lead to more rapid identification of potential pharmacologic causes of emerging diseases and discovery of unanticipated beneficial medication effects, allowing such medications to be repurposed for new indications. To attain the expected outcome, the applicant will complete additional coursework that builds on his Master of Science in Clinical Epidemiology to learn computational biology, machine learning, and econometrics techniques. With the support of this grant and his institution, he will also directly apply these techniques to pharmacoepidemiology applications under the close mentorship of a carefully selected team of faculty with extensive experience in gastroenterology, pharmacoepidemiology, medical informatics, and mentoring prior K-award grant recipients. Through these activities, the applicant will develop the skills necessary to obtain NIH R01-level funding and become a leader in developing novel techniques for application to the epidemiologic study of gastrointestinal diseases. PROJECT NARRATIVE Traditionally, research studying medications associated with diseases are limited to analyzing one medication at a time. This novel proposal will validate medication class enrichment analysis, a recently developed algorithm to study multiple medications simultaneously for association with a disease of interest. Validation of this method will allow researchers to use existing medical databases to more rapidly identify potential medication causes of emerging diseases and identify medications with unanticipated beneficial effects, allowing such medications to be repurposed for new indications.",Evaluation of multiple medication exposures concurrently using a novel algorithm,9853783,K08DK119475,"['Address', 'Algorithms', 'Aminoglycosides', 'Antibiotics', 'Anticoagulants', 'Antiplatelet Drugs', 'Big Data to Knowledge', 'Biological', 'Carbapenems', 'Case-Control Studies', 'Cephalosporins', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Computational Biology', 'Computer software', 'Data', 'Databases', 'Development', 'Development Plans', 'Diagnosis', 'Digestive System Disorders', 'Disease', 'Electronic Health Record', 'Epidemiologic Methods', 'Evaluation', 'Faculty', 'Fluoroquinolones', 'Funding', 'Future', 'Gastroenterology', 'Gastrointestinal Diseases', 'Gastrointestinal Hemorrhage', 'Generations', 'Genomics', 'Goals', 'Grant', 'Health', 'Infection', 'Informatics', 'Inpatients', 'Institution', 'K-Series Research Career Programs', 'Lead', 'Learning', 'Logistic Regressions', 'Machine Learning', 'Master of Science', 'Medical', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Noise', 'Non-Steroidal Anti-Inflammatory Agents', 'Outcome', 'Penicillins', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacology', 'Research', 'Research Design', 'Research Personnel', 'Sensitivity and Specificity', 'Signal Transduction', 'Specificity', 'Techniques', 'Testing', 'Time', 'United Kingdom', 'United States National Institutes of Health', 'Validation', 'Veterans', 'analytical method', 'base', 'beta-Lactams', 'career development', 'clinical epidemiology', 'econometrics', 'epidemiology study', 'experience', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'research study', 'simulation', 'skills', 'usability']",NIDDK,UNIVERSITY OF PENNSYLVANIA,K08,2020,167900,-0.011018525149680873
"Biomarkers of Alcoholic Hepatitis Abstract Our overarching goal is to develop minimally invasive approaches to better predict outcome and novel mechanisms in alcoholic hepatitis (AH). AH is characterized by acute hepatic decompensation and multiple organ failure. Although supportive care for AH has improved, short-term mortality has largely remained unchanged (30-40%) for decades. Effective approaches to predict risk hamper the treatment of AH. The hepatic extracellular matrix (ECM) responds dynamically to organ injury and ECM turnover increases; we propose to take advantage of this to develop new biomarkers for AH. The peptidome, low molecular weight peptides in biologic fluids, includes not only synthesized peptides, but fragments of degraded proteins (i.e., ‘degradome’). We hypothesize that the ECM degradome in plasma will yield new biomarkers to predict outcome and mechanisms in AH. We will test this hypothesis via the following Specific Aims: 1). To identify key changes in the peptidome as predictive biomarkers of outcome in AH. Unbiased peptidomics and multivariate analyses will identify degradomic features independently linked to prognosis. Protease activity that could produce significantly changed peptides will be predicted using Proteasix. We will also determine the mechanistic role of ECM turnover in the in parallel established models of alcohol-induced liver injury. 2) To develop probabilistic graphical models to predict outcome in AH. Whereas we expect the results of Aim 1 to establish that the peptidome profile in patients correlates with overall outcome, biomarkers alone are often insufficient to accurately predict individual patient outcome. We will therefore employ machine learning methods like probabilistic graphical models (PGMs) over mixed data types to integrate peptidomic and individual patient clinical data, into a single probabilistic graphical framework. The resulting graphs will then be used to infer causal interactions between variables, select informative biomarkers that will more specifically predict the outcome, and gain new mechanistic insight into the biology of AH (hypothesis generation). 3) To validate the use of the peptidome as a predictive tool for determining outcome in AH. Using a large prospectively-designed patient cohort with established outcomes, we will test the ability of the algorithms and biomarkers generated in this study to predict outcome. The successful completion of the proposed work will produce significant results at various levels: (1) Biomarker discovery: we will identify biomarkers and conditional biomarkers for AH prognosis. (2) Mechanistic understanding of AH: our models will generate hypotheses about the interactions between variables at different scales (molecular, individual) that will provide insights on the proteins that are involved in AH. (3) Algorithm development: through this project we will extend our mixed data graph learning algorithms to include censored variables (i.e., survival data). As a result of the above, this project is likely to yield novel diagnostic tools for AH that may also translate to other liver diseases. Narrative Alcoholic hepatitis (AH) is a severe acute form of alcoholic liver disease with a very high mortality rate. Despite years of research, the standard-of-care and mortality rate has not changed dramatically in over 50 years. Our goal is to develop new computational and experimental methods and discover new biomarkers for AH outcome that can efficiently identify at-risk individuals. A second goal of the project is to identify molecular mechanisms of AH, which could be targeted therapeutically.",Biomarkers of Alcoholic Hepatitis,9995661,R01AA028436,"['Acute', 'Address', 'Alcoholic Hepatitis', 'Alcoholic Liver Diseases', 'Alcohols', 'Algorithms', 'Area', 'Biological', 'Biological Markers', 'Biology', 'Cells', 'Clinical Data', 'Collagen', 'Data', 'Development', 'Disease', 'Extracellular Matrix', 'Extracellular Matrix Proteins', 'Generations', 'Goals', 'Graph', 'Health', 'Hepatic', 'Homeostasis', 'Human', 'Individual', 'Informatics', 'Injury', 'Lead', 'Link', 'Liquid substance', 'Liver Fibrosis', 'Liver diseases', 'Matrix Metalloproteinases', 'Mediating', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Weight', 'Multiple Organ Failure', 'Multivariate Analysis', 'Neoplasm Metastasis', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Peptide Fragments', 'Peptide Hydrolases', 'Peptides', 'Plasma', 'Population', 'Predictive Value', 'Process', 'Production', 'Prognostic Marker', 'Protease Inhibitor', 'Protein Fragment', 'Proteins', 'Research', 'Risk', 'Role', 'Severities', 'Signal Transduction', 'Supportive care', 'Surrogate Markers', 'Testing', 'Tissues', 'Translating', 'Work', 'acute liver injury', 'algorithm development', 'biomarker discovery', 'chronic alcohol ingestion', 'cohort', 'design', 'improved', 'individual patient', 'insight', 'interest', 'learning algorithm', 'machine learning method', 'minimally invasive', 'mortality', 'mouse model', 'new therapeutic target', 'novel', 'novel diagnostics', 'organ injury', 'outcome forecast', 'outcome prediction', 'predictive marker', 'predictive tools', 'profiles in patients', 'prospective', 'standard of care', 'targeted treatment', 'tool']",NIAAA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2020,615104,-0.016290567565937702
"Assessment of ultrasound features of knee osteoarthritis in a population-based community cohort Project summary Our long-term goal is to demonstrate the utility of ultrasound for OA assessment, standardize its acquisition and scoring, and promote increased uptake of US for use in clinical, research, and trial settings. Knee osteoarthritis (KOA) is highly prevalent and frequently debilitating. Development of potential treatments has been hampered by the heterogenous nature of this common chronic condition, which is characterized by a number of subgroups, or phenotypes, with different underlying pathophysiological mechanisms. Imaging, genetics, biochemical biomarkers, and other features can be used to characterize phenotypes, but variations in data types can make it difficult to harmonize definitions. While radiography is widely used in KOA imaging, it is limited in its ability to assess early disease (when interventions are most likely to succeed) and is insensitive to change. Ultrasound (US) is a widely accessible, time-efficient and cost-effective imaging modality that can provide detailed and reliable information about all joint tissues (e.g., cartilage, meniscus, synovium, bone), and could therefore inform phenotypes in KOA (e.g., by presence of synovitis, effusion, cartilage damage, calcium crystal deposition, and popliteal cysts). Use of US is currently limited by the lack of systematically performed studies in well-characterized non-clinical populations. To address this gap and further the use of this advantageous imaging modality for KOA, we will obtain standardized US and radiography in the population- based Johnston County Health Study (JoCoHS), the new enrollment phase of the 25+ year Johnston County OA Project which includes white, African American, and Hispanic men and women aged 35-70, to achieve three aims. In Aim 1, we will determine the population prevalence (n~3000) of knee US features including cartilage and meniscal damage, synovitis/effusion, calcium crystal deposition, popliteal cysts and osteophytes overall and in key subgroups by age, sex, race/ethnicity, and symptom status. Aim 2 will allow quantification of the associations between these US features and radiographic findings and symptom scores overall and in key subgroups (e.g., those with and without radiographic KOA, by sex, by race/ethnicity). For Aim 3, we will apply novel machine learning methodologies (e.g., Direction-projection-permutation [DiProPerm] hypothesis testing, Joint and Individual Variation [JIVE], and Distance-Weighted Discrimination [DWD]) to a) develop an overall US score for symptomatic KOA and b) identify the contribution of US variables to phenotypes relevant to KOA based on general health, physical activity, and functional assessments. This study is a crucial step to establish the foundation for US as an assessment tool for clinical use, research, and clinical trials in KOA, providing unique population-based cross-sectional data regarding the utility of US and forming the basis for future longitudinal work evaluating its value and performance characteristics related to incident and progressive KOA. Project narrative Osteoarthritis is an enormous and increasing public health problem that, like many other chronic conditions, is not a single disease but a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying mechanisms. Ultrasound is an accessible, time-efficient, and cost-effective imaging modality that provides invaluable data about all joint tissues involved in osteoarthritis and has the potential to identify important phenotypes. The proposed work is relevant to the NIAMS mission and represents a crucial step to establish the foundation for ultrasound as an assessment tool for use in clinics, research, and clinical trials in osteoarthritis.",Assessment of ultrasound features of knee osteoarthritis in a population-based community cohort,9944803,R01AR077060,"['Address', 'African American', 'Age', 'Area', 'Assessment tool', 'Bilateral', 'Biochemical', 'Biological Markers', 'Bone Spur', 'Calcium', 'Cartilage', 'Categories', 'Characteristics', 'Chronic', 'Claustrophobias', 'Clinic', 'Clinical', 'Clinical Assessment Tool', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Cohort Studies', 'Communities', 'County', 'Crystal Formation', 'Crystallization', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Development', 'Diagnostic radiologic examination', 'Discrimination', 'Disease', 'Enrollment', 'Ethnic Origin', 'Etiology', 'Foundations', 'Future', 'General Population', 'Goals', 'Health', 'Hispanics', 'Image', 'Implant', 'Individual', 'Infrastructure', 'Intervention', 'Joints', 'Knee', 'Knee Osteoarthritis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Meniscus structure of joint', 'Methodology', 'Mission', 'Modality', 'Musculoskeletal', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Nature', 'Outcome', 'Pain', 'Participant', 'Pathology', 'Performance', 'Phase', 'Phenotype', 'Physical activity', 'Popliteal Cyst', 'Population', 'Population Study', 'Prevalence', 'Public Health', 'Race', 'Receiver Operating Characteristics', 'Research', 'Risk Factors', 'Sex Differences', 'Specialist', 'Standardization', 'Subgroup', 'Symptoms', 'Syndrome', 'Synovial Membrane', 'Synovitis', 'Testing', 'Time', 'Tissues', 'Ultrasonography', 'Variant', 'Woman', 'Work', 'aged', 'base', 'bone', 'cohort', 'cost', 'cost effective', 'effusion', 'follow-up', 'imaging genetics', 'imaging modality', 'individual variation', 'interest', 'men', 'novel', 'point of care', 'population based', 'recruit', 'rheumatologist', 'sex', 'uptake']",NIAMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,342100,-0.008723474844005293
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9925811,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Management Resources', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'emotional functioning', 'executive function', 'experience', 'follow-up', 'hedonic', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'large datasets', 'machine learning method', 'multidimensional data', 'multimodal data', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2020,745640,-0.02362866887395924
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9888378,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelial', 'Epithelium', 'Esophageal Adenocarcinoma', 'Esophageal Tissue', 'Esophagitis', 'Esophagus', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'intelligent algorithm', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'stem cells', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2020,171720,-0.04199193537594426
"Combinatorial matrix-mimetic recombinant proteins as engineered nerve guidance conduits ABSTRACT Over 500,000 Americans suffer from peripheral nerve injury (PNI), and despite surgical interventions, most suffer permanent loss of motor function and sensation. Current clinical options for long nerve gap PNI include naturally- derived grafts, which provide native matrix cues to regenerate neurons but suffer from very limited supply and batch-to-batch variability, or synthetic nerve guidance conduits (NGCs), which are easy to manufacture but often fail due to lack of regenerative cues. The main challenge with using any NGC for treatment of PNI is the immense trade-off between providing the complex matrix cues necessary for optimal nerve regeneration while providing a conduit that is readily available, reproducible, and easily fabricated. To overcome this challenge, we propose an entirely new type of biomaterial: a computationally optimized, protein-engineered recombinant NGC (rNGC). This rNGC combines the reliability of synthetic NGCs with the presentation of multiple regenerative matrix cues of natural NGCs. Because current understanding of cell-matrix interactions is insufficient to enable to direct design of a fully functional rNGC, we hypothesize that the use of machine learning, computational optimization methods will allow identification of an rNGC that promotes nerve regeneration similar to the current gold standard autograft. We utilize a family of protein-engineered, elastin-like proteins (ELPs) that are reproducible, with predictable, consistent material properties, and fully chemically defined for streamlined FDA approval. Due to ELPs’ modular design, they have biomechanical (i.e. matrix stiffness) and biochemical (i.e. cell-adhesive ligand) properties that are independently tunable over a broad range. While numerous studies detail the effects of individual biomechanical or biochemical matrix cues on neurite outgrowth using single-variable approaches, their combinatorial effects have been largely unexplored as insufficient knowledge exists to make accurate predictions of their interactions a priori. This fundamentally prohibits the direct design of combinatorial matrix cues. We hypothesize that optimized presentation of biomechanical and biochemical cues will create a microenvironment that better mimics the native ECM milieu, resulting in synergistic ligand cross-talk to improve nerve regeneration. In Aim 1, we use computational optimization methods to identify the combination of ligand identities, ligand concentrations, and matrix stiffness that best enhances neurite outgrowth. We will develop and characterize a library of ELP variants with distinct cell-adhesive ligands derived from native ECM, and assess their ability to support neurite outgrowth from rat dorsal root ganglia (DRG). In Aim 2, we will validate our in vitro optimization results in a preclinical, rat sciatic nerve injury model. A core-shell, ELP-based rNGC with an inner core matrix of the optimized ELP formulation from Aim 1 will be fabricated and evaluated for its ability to enhance therapeutic outcome. Controls include reversed nerve autograft, hollow silicone conduit, and non-optimized ELP- based rNGC. This study would represent the first use of computational optimization methods to design a reproducible, reliable, recombinant biomaterial with multiple regenerative matrix cues. PROJECT NARRATIVE The main challenge with using nerve guidance conduits (NGCs) to bridge long peripheral nerve gap injuries is the immense trade-off between providing the complex matrix cues necessary for optimal nerve regeneration while providing a conduit that is readily available, reproducible, and easily fabricated. To address this challenge, here we utilize (1) computational optimization methods to identify the optimal biochemical and biomechanical matrix cues for nerve regeneration, and (2) advanced protein-engineering strategies to incorporate these cues into a recombinant NGC (rNGC). Our rNGC combines the reliability of synthetic NGCs with the matrix cues of naturally-derived NGCs to make an affordable, off-the-shelf rNGC that promotes nerve regeneration.",Combinatorial matrix-mimetic recombinant proteins as engineered nerve guidance conduits,9872885,R21NS114549,"['Address', 'Adhesives', 'Allografting', 'American', 'Amino Acids', 'Autologous Transplantation', 'Axon', 'Behavioral', 'Biochemical', 'Biocompatible Materials', 'Biomechanics', 'Blood Vessels', 'Cell Surface Receptors', 'Cells', 'Chemicals', 'Cholinergic Receptors', 'Chronic', 'Clinical', 'Collagen', 'Complex', 'Cues', 'Data', 'Elastin', 'Electron Microscopy', 'Encapsulated', 'Engineering', 'Esthesia', 'Extracellular Matrix', 'Fibronectins', 'Formulation', 'Gold', 'Immunofluorescence Immunologic', 'Immunohistochemistry', 'In Vitro', 'Individual', 'Injury', 'Knowledge', 'Label', 'Laminin', 'Libraries', 'Ligands', 'Machine Learning', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Modulus', 'Motor', 'Motor Neurons', 'Muscle', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Neurites', 'Neuroglia', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Peripheral Nerves', 'Peripheral nerve injury', 'Process', 'Property', 'Protein Engineering', 'Protein Family', 'Proteins', 'Rattus', 'Recombinant Proteins', 'Recombinants', 'Recovery of Function', 'Reporting', 'Reproducibility', 'Signal Pathway', 'Signal Transduction', 'Silicones', 'Spinal', 'Spinal Ganglia', 'Stains', 'Synaptophysin', 'Tenascin', 'Tissues', 'Tolonium chloride', 'Variant', 'Walking', 'alpha Bungarotoxin', 'base', 'combinatorial', 'comparative', 'design', 'exhaustion', 'experimental study', 'gel electrophoresis', 'improved', 'in vivo evaluation', 'mimetics', 'motor control', 'myelination', 'nerve autograft', 'nerve gap', 'nerve injury', 'nerve supply', 'postsynaptic', 'pre-clinical', 'regenerative', 'sciatic nerve', 'therapy outcome', 'tomato lectin', 'transcriptional coactivator p75']",NINDS,STANFORD UNIVERSITY,R21,2020,436238,-0.057770259049864346
"2/2 Leveraging electronic health records for pharmacogenomics of psychiatric diorders Abstract  Schizophrenia (SCZ) and major depressive disorder (MDD) are highly heritable, debilitating diseases with lifetime prevalences of ~1% and 15%, respectively. Both disorders carry substantial morbidity and mortality and are associated with severe societal and personal costs. Despite the availability of efficacious treatments for both disorders, ~1/3 of individuals will not achieve symptomatic improvement even after multiple rounds of medication. Identifying individuals at greater risk for such treatment nonresponse, or treatment resistance, could facilitate more targeted interventions for these individuals.  A burgeoning literature has identified genomic variation associated with treatment response. In particular, antidepressant response has been suggested to be highly heritable; convergent data from rodent studies likewise suggest that antipsychotic and antidepressant response phenotypes are influenced by genetic variation. However, treatment studies to date have had minimal success in identifying variants associated with psychotropic response, likely as a result of limited sample sizes: prior efforts required sequential treatment trials and prospective assessment to characterize outcomes. Longitudinal electronic health records (EHR) data provide an opportunity to efficiently characterize treatment response in many individuals in real-world settings. Coupled with large and expanding biobanks, these cohorts allow for low- cost, large-scale genomic studies that finally achieve sufficient power to detect realistic effect sizes.  The investigators now propose to apply these approaches to the EHRs of two large regional health systems, each linked to a large biobank, to investigate treatment resistance in SCZ and MDD. They will apply canonical indicators of treatment resistance - clozapine treatment for SCZ, and electroconvulsive therapy (ECT) for MDD - to identify coded and uncoded clinical features associated with high probability of treatment resistance in EHR data. These predictors will themselves provide a useful baseline for identifying high risk individuals. Then, they will apply these to study the entire affected population of each biobank, extending existing genomic data with additional genome-wide association, yielding more than 25,000 antidepressant-treated individuals and 2,200 antipsychotic-treated individuals. Rather than simply conducting a case-control study, they will examine treatment resistance as a quantitative trait, applying a method developed by the investigators and shown to substantially increase power for such traits.  The project combines expertise in clinical informatics, machine learning, and analysis of large scale genomics, as well as domain-specific expertise in psychiatric treatment resistance. Spanning two distinct health systems, the algorithms and methods developed have maximal portability, facilitating next- step investigations. Successful identification of risk variants will facilitate efforts at clinical risk stratification as well as investigation of the biology underlying treatment resistance. Public Health Statement Despite the availability of effective treatments for schizophrenia and major depressive disorder, around 1/3 of individuals will not have symptomatic improvement after multiple treatment trials. If individuals at high risk for treatment resistance could be identified early, it might be possible to find more effective treatments for them. In particular, this study seeks to use electronic health records and biobanks to identify genomic variation associated with treatment resistance.",2/2 Leveraging electronic health records for pharmacogenomics of psychiatric diorders,9861268,R01MH116269,"['Academic Medical Centers', 'Address', 'Affect', 'Antidepressive Agents', 'Antipsychotic Agents', 'Biology', 'Case-Control Studies', 'Clinical', 'Clinical Informatics', 'Clozapine', 'Code', 'Consumption', 'Coupled', 'DNA', 'Data', 'Development', 'Discrimination', 'Disease', 'Effectiveness', 'Electroconvulsive Therapy', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'General Hospitals', 'Generations', 'Genetic', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Heritability', 'Hospitals', 'Individual', 'Intervention', 'Investigation', 'Label', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Major Depressive Disorder', 'Massachusetts', 'Measures', 'Medical Genetics', 'Mental disorders', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Outcome', 'Patient Triage', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacology', 'Pharmacotherapy', 'Phenotype', 'Population', 'Prevalence', 'Probability', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Public Health', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Risk', 'Risk stratification', 'Rodent', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Sequential Treatment', 'Site', 'Structure', 'Suicide attempt', 'Supervision', 'System', 'Therapeutic', 'Time', 'Treatment Failure', 'Treatment Step', 'Treatment outcome', 'Variant', 'Work', 'adverse outcome', 'algorithmic methodologies', 'base', 'biobank', 'biomedical resource', 'clinical predictors', 'clinical risk', 'cohort', 'cost', 'design', 'determinants of treatment resistance', 'effective therapy', 'genetic association', 'genome wide association study', 'genomic data', 'genomic predictors', 'genomic variation', 'high risk', 'improved', 'in silico', 'mortality', 'neuropsychiatric disorder', 'personalized intervention', 'portability', 'predictive modeling', 'prevent', 'prospective', 'recruit', 'response', 'risk variant', 'success', 'symptomatic improvement', 'therapy resistant', 'trait', 'treatment response', 'treatment risk', 'treatment strategy', 'treatment trial']",NIMH,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2020,425000,-0.023242221090145804
"1/2 Leveraging electronic health records for pharmacogenomics of psychiatric disorders Schizophrenia (SCZ) and major depressive disorder (MDD) are highly heritable, debilitating diseases with lifetime prevalences of ~1% and 15%, respectively. Both disorders carry substantial morbidity and mortality and are associated with severe societal and personal costs. Despite the availability of efficacious treatments for both disorders, ~1/3 of individuals will not achieve symptomatic improvement even after multiple rounds of medication. Identifying individuals at greater risk for such treatment nonresponse, or treatment resistance, could facilitate more targeted interventions for these individuals.  A burgeoning literature has identified genomic variation associated with treatment response. IN particular, antidepressant response has been suggested to be highly heritable; convergent data from rodent studies likewise suggest that antipsychotic and antidepressant response phenotypes are influenced by genetic variation. However, treatment studies to date have had minimal success in identifying variants associated with psychotropic response, likely as a result of limited sample sizes: prior efforts required sequential treatment trials and prospective assessment to characterize outcomes. Longitudinal electronic health records (EHR) data provide an opportunity to efficiently characterize treatment response in many individuals in real-world settings. Coupled with large and expanding biobanks, these cohorts allow for low- cost, large-scale genomic studies that finally achieve sufficient power to detect realistic effect sizes.  The investigators now propose to apply these approaches to the EHRs of two large regional health systems, each linked to a large biobank, to investigate treatment resistance in SCZ and MDD. They will apply canonical indicators of treatment resistance - clozapine treatment for SCZ, and electroconvulsive therapy (ECT) for MDD - to identify coded and uncoded clinical features associated with high probability of treatment resistance in EHR data. These predictors will themselves provide a useful baseline for identifying high risk individuals. Then, they will apply these to study the entire affected population of each biobank, extending existing genomic data with additional genome-wide association, yielding more than 26,000 antidepressant-treated individuals and 2,500 antipsychotic-treated individuals. Rather than simply conducting a case-control study, they will examine treatment resistance as a quantitative trait, applying a method developed by the investigators and shown to substantially increase power for such traits.  The project combines expertise in clinical informatics, machine learning, and analysis of large scale genomics, as well as domain-specific expertise in psychiatric treatment resistance. Spanning two distinct health systems, the algorithms and methods developed have maximal portability, facilitating next- step investigations. Successful identification of risk variants will facilitate efforts at clinical risk stratification as well as investigation of the biology underlying treatment resistance. Public Health Statement Despite the availability of effective treatments for schizophrenia and major depressive disorder, around 1/3 of individuals will not have symptomatic improvement after multiple treatment trials. If individuals at high risk for treatment resistance could be identified early, it might be possible to find more effective treatments for them. In particular, this study seeks to use electronic health records and biobanks to identify genomic variation associated with treatment resistance.",1/2 Leveraging electronic health records for pharmacogenomics of psychiatric disorders,9843528,R01MH116270,"['Academic Medical Centers', 'Address', 'Affect', 'Antidepressive Agents', 'Antipsychotic Agents', 'Biology', 'Case-Control Studies', 'Clinical', 'Clinical Informatics', 'Clozapine', 'Code', 'Consumption', 'Coupled', 'DNA', 'Data', 'Development', 'Discrimination', 'Disease', 'Effectiveness', 'Electroconvulsive Therapy', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'General Hospitals', 'Generations', 'Genetic', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Heritability', 'Hospitals', 'Individual', 'Intervention', 'Investigation', 'Label', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Major Depressive Disorder', 'Massachusetts', 'Measures', 'Medical Genetics', 'Mental disorders', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Outcome', 'Patient Triage', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacology', 'Pharmacotherapy', 'Phenotype', 'Population', 'Prevalence', 'Probability', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Public Health', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Risk', 'Risk stratification', 'Rodent', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Sequential Treatment', 'Site', 'Structure', 'Suicide attempt', 'Supervision', 'System', 'Therapeutic', 'Time', 'Treatment Failure', 'Treatment Step', 'Treatment outcome', 'Variant', 'Work', 'adverse outcome', 'algorithmic methodologies', 'base', 'biobank', 'biomedical resource', 'clinical predictors', 'clinical risk', 'cohort', 'cost', 'design', 'determinants of treatment resistance', 'effective therapy', 'genetic association', 'genome wide association study', 'genomic data', 'genomic predictors', 'genomic variation', 'high risk', 'improved', 'in silico', 'mortality', 'neuropsychiatric disorder', 'personalized intervention', 'portability', 'predictive modeling', 'prevent', 'prospective', 'recruit', 'response', 'risk variant', 'success', 'symptomatic improvement', 'therapy resistant', 'trait', 'treatment response', 'treatment risk', 'treatment strategy', 'treatment trial']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,421250,-0.023527320921341053
"Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia PROJECT SUMMARY Preeclampsia (PE) is a devastating hypertensive disorder of pregnancy that is a leading cause of maternal and fetal mortality and morbidity worldwide. It affects 2-10% of women and accounts for 16% of maternal deaths related to childbirth in developed countries. PE is difficult to study because it is a multifactorial disorder with varying molecular mechanisms associated with the development of the spectrum of clinical symptoms associated with early onset, late onset, and severe PE. A personalized medicine approach applied to dissecting the underlying mechanisms of PE may be beneficial to reduce clinical incidence of this devastating syndrome. The placenta plays a key role in the development of PE, leading to widespread maternal endothelial dysfunction, hypertension, and systemic multi-organ failure in PE. Extracellular vesicles (EVs) containing protein, RNA, and lipid cargo are continuously extruded from the placenta, and are capable of interacting with maternal organs including the kidney. PE is primarily associated with placental and renal dysfunction, and PE is the most common cause of acute kidney injury during pregnancy. However, no studies have investigated the potential of placenta-derived RNA cargo as a link between placental and renal dysfunction in PE. Urinary EVs are derived from multiple tissue types and represent a trove of biomarkers that are increasingly being utilized to diagnose renal disorders. Further, urine samples can be obtained throughout pregnancy non-invasively and could potentially be utilized to identify biomarkers related to placental dysfunction in PE. In the proposed research, during the mentored phase, cutting-edge RNA-Seq technology coupled with computational biological and machine learning approaches will be applied to profile the transcriptome of urinary EVs in women with PE compared to normal pregnancy. Preliminary data indicates that it is possible to isolate and profile the transcriptome of urinary EVs from maternal urine throughout normal gestation, and that placenta-derived and placenta-specific mRNA and miRNA can be detected within the urinary EV population. This presents a novel technique that has potential to identify biomarkers as well as provide information on placental dysfunction in PE in a non-invasive manner. During the independent phase, the candidate will utilize an in vitro approach to investigate the effect of uptake of placenta-derived EVs with miRNA cargo associated with PE on the function of proximal tubule epithelial cells and cortical collecting duct cells. These two renal-specific cell types are involved in tubular reabsorption in the nephron, a process that is compromised leading to increased excretion of protein in the urine in some preeclamptic pregnancies. This proposal is multidisciplinary, utilizing basic biology, clinical research, and high-performance computing applied to investigating placental dysfunction in PE. These experiments are significant because they will generate novel information on the role of placenta- derived EVs in renal dysfunction in PE, as well as point the way towards preventative and therapeutic targets that may be transformative and clinically relevant. PROJECT NARRATIVE The proposed research will examine the transcriptome of urinary extracellular vesicles obtained non-invasively as a source of placenta-derived nucleic acids reflective of the development of preeclampsia (PE), and will con- duct in vitro experiments to determine the role of placental extracellular vesicles in renal dysfunction in PE.",Extracellular Vesicles as a Link Between Placental and Renal Dysfunction in Preeclampsia,10266768,K99HD096125,"['Acute Renal Failure with Renal Papillary Necrosis', 'Affect', 'Apoptosis', 'Biological', 'Biological Markers', 'Biology', 'Blood Circulation', 'Brain', 'Cells', 'Characteristics', 'Childbirth', 'Chromosomes', 'Clinical', 'Clinical Research', 'Coupled', 'Data', 'Developed Countries', 'Development', 'Diagnosis', 'Disease', 'Disease Pathway', 'Ductal Epithelial Cell', 'Epithelial Cells', 'Etiology', 'Excretory function', 'Failure', 'Fetal Mortality Statistics', 'Functional disorder', 'Genetic Transcription', 'Gestational Age', 'Goals', 'Health Care Costs', 'High Performance Computing', 'Human', 'Hypertension', 'Hypoxia', 'Immune response', 'Immunohistochemistry', 'In Vitro', 'Incidence', 'Injury', 'Kidney', 'Kidney Diseases', 'Libraries', 'Link', 'Lipids', 'Liver', 'Lung', 'Machine Learning', 'Maternal Mortality', 'Mentors', 'Messenger RNA', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Disease', 'Morbidity - disease rate', 'Mus', 'Nephrons', 'Nucleic Acids', 'Organ', 'Pathogenesis', 'Pathologic', 'Pathology', 'Phase', 'Placenta', 'Play', 'Population', 'Population Heterogeneity', 'Pre-Eclampsia', 'Pregnancy', 'Process', 'Proteins', 'Proteinuria', 'RNA', 'Research', 'Role', 'Sampling', 'Small RNA', 'Source', 'Stress', 'Syndrome', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Transcript', 'Tubular formation', 'Urine', 'Woman', 'associated symptom', 'biobank', 'cell injury', 'cell type', 'clinically relevant', 'early onset', 'endothelial dysfunction', 'experimental study', 'extracellular vesicles', 'improved', 'innovation', 'kidney dysfunction', 'maternal serum', 'mortality', 'multidisciplinary', 'neonatal morbidity', 'novel', 'personalized medicine', 'pregnancy disorder', 'premature', 'prevent', 'response', 'stillbirth', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'uptake', 'urinary']",NICHD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2020,60858,-0.026268381858144466
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9965942,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,989602,-0.024931959307730275
"Multi-Resolution Docking Methods for Electron Microscopy Summary In the past decade, we have witnessed a revolutionary progress in camera technology and the attainable resolution of macromolecular assemblies via cryogenic electron microscopy (cryo-EM) and in the development of computational algorithms that relate the resulting 3D maps to atomic resolution structures. Whereas single- particle cryo-EM today is capable of directly solving atomic structures of biomolecular assemblies in isolation, electron tomography (ET) in unstained frozen-hydrated samples is widely used to capture the 3D organization of supramolecular complexes in their native (organelle, cell, or tissue) environments. We have identified three inter-related research areas where our computational modeling experience (historically rooted in pre-revolution multi-scale approaches) offers the biggest value to today's post-revolution EM community: (1) medium resolution cryo-EM modeling, (2) the segmentation and denoising of cryo-ET data, and (3) the validation of atomic models and their corresponding maps. The first aim is an extension of promising new ideas in flexible fitting as well as secondary structure prediction for medium resolution maps, which have been our key research areas in the past. medium resolution (5-10Å) maps are still widely used in EM and can be of significant biological importance. This is particularly true in the case of cryo-ET maps, which are harder to read than single particle cryo-EM maps because they often exhibit considerable noise, anisotropic resolution, and anisotropic density variations due to the low dose requirements and the missing wedge in the Fourier space. In the case of tightly packed or crowded macromolecular structures, the fusion of nearby biomolecular densities prevents an automated segmentation of geometric shapes, requiring a labor-intensive manual tracing by human experts. We are currently developing novel computational approaches to provide a more objective strategy for missing wedge correction in homogeneous specimen areas of tomograms. Our hybrid approach combines deconvolution and denoising with template matching in a unified mathematical framework that allows modeling constraints to be imposed in a least-squares optimization process. Our approach can also be extended to the flexible refinement of atomic structures using our damped dynamics flexible fitting approach by tuning the internal point-spread functions to the missing wedge of the ET data. To support these aims, we will quantitatively measure the fitness of an atomic model in local density regions and characterize the fitness of maps with reliable reference structures. The collaborative efforts supported by this grant will include the refinement of cytoskeletal filaments, molecular motors, bacterial chemoreceptor arrays, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established Internet-based mechanisms used by the Situs and Sculptor packages and as plugins for the popular UCSF Chimera graphics program. Project Narrative This project will help biological electron microscopists bridge a broad range of resolution levels, from the atomic to the living organism. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,10120245,R01GM062968,"['3-Dimensional', 'Algorithms', 'Architecture', 'Area', 'Biological', 'Cells', 'Characteristics', 'Chemoreceptors', 'Chimera organism', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational algorithm', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Crowding', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Databases', 'Deposition', 'Detection', 'Development', 'Docking', 'Dose', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Elements', 'Environment', 'Equilibrium', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Human', 'Hybrids', 'Hydration status', 'Internet', 'Laboratories', 'Least-Squares Analysis', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Mathematics', 'Measures', 'Medical', 'Methods', 'Microscope', 'Modeling', 'Modernization', 'Molecular Motors', 'Molecular Structure', 'Morphologic artifacts', 'Nature', 'Noise', 'Organelles', 'Organism', 'Pattern', 'Plant Roots', 'Research', 'Resolution', 'Sampling', 'Shapes', 'Specimen', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Tomogram', 'Training', 'Validation', 'Variant', 'Visualization software', 'Work', 'algorithmic methodologies', 'automated segmentation', 'base', 'beta pleated sheet', 'computer code', 'cryogenics', 'data warehouse', 'deep learning', 'denoising', 'density', 'electron tomography', 'experience', 'feature detection', 'fitness', 'flexibility', 'fundamental research', 'heuristics', 'high standard', 'image reconstruction', 'improved', 'interest', 'learning network', 'macromolecular assembly', 'novel', 'particle', 'prevent', 'process optimization', 'programs', 'reconstruction', 'structured data', 'theories', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2020,313572,-0.03210990079282909
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10258317,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,157500,-0.024931959307730275
"FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control PROJECT SUMMARY In this proposal we plan to contribute addressing the above foundational and operational challenges by advancing the science of influenza modeling and contributing novel methods and data sources that will increase the accuracy and availability of seasonal and pandemic influenza models. To address these challenges, we plan to build on the unique mechanistic spatially structured modeling approaches developed by our consortium, that includes stochastic metapopulation models and fully developed agent-based models nested together in our global epidemic and mobility modeling (GLEAM) approach. The objective of this project is to generate novel and actionable scientific insights from dynamic transmission models of influenza transmission that effectively integrate key socio-demographic indicators of the focus population, as well as a wide spectrum of pharmaceutical and non-pharmaceutical interventions. Our proposed work in specific aim 1 (A1) will leverage our global modeling (from the global to local scale) framework that can be used to explore the multi-year impact of influenza vaccination, antiviral prophylaxis/treatment, and community mitigation during influenza seasons and pandemics. Our specific aim 2 (A2) will focus on using high quality data to model heterogeneous transmission drivers and novel contact pattern stratifications that will allow us to guide mitigation strategies and prioritization for interventions. In our Aim 3 (A3) we will use artificial intelligence approaches to identify interventions that are particularly synergistic and well-suited to particular epidemic scenarios, for seasonal and pandemic influenza. Our overarching goal is to provide a modeling portfolio with flexible and innovative mathematical and computational approaches. We aim to address several questions commonly asked about seasonal and pandemic influenza and match these with analytical methods and outbreak projections. The modeling and data developed in this project can help facilitate and justify transparent public health decisions, while contributing to the definition of standard methods for model selection and validation. Finally, our influenza modeling platform can also benefit the broader network of modeling teams and can be used to improve result sharing and harmonization of modeling approaches. The objective of this proposal is to advance the science of modeling and contribute novel methods and data analytics tools that will increase the understanding of seasonal and pandemic influenza in the context of the network of modeling teams coordinated by the CDC. To address these challenges, we plan to develop a novel global modeling framework, contribute new data and methods for improve the accuracy and validation of flu modeling approaches, and evolve successful methodologies to advance the analysis of layered intervention with artificial Intelligence.",FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control,10071782,U01IP001137,[' '],NCIRD,NORTHEASTERN UNIVERSITY,U01,2020,371721,0.0006065048920643425
"Metal-nutrient mixtures in epidemiologic and toxicologic studies of cardiovascular disease Abstract This proposal is designed to extend and complement our R01-funded case-cohort study of cadmium (Cd) and acute myocardial infarction (AMI). Herein we will add arsenic (As), calcium (Ca), and magnesium (Mg) to our case-cohort study and include epidemiologic and toxicologic mixtures analyses. The four elements we have selected are compelling for their independent role in cardiovascular disease (CVD) and as a mixture. As increases plaque formation and adhesion to endothelium and Cd also induces endothelial dysfunction and atherosclerosis; yet it is not clear if the effects of As and Cd are synergistic or competing. To potentially counteract these processes, Mg is important for modulating endothelial function. While Ca’s role is equivocal, its role in calcification of the arteries is undeniable, making it important to consider as well. Our efficient case-cohort study design includes 810 cases of AMI and a comparison subcohort of 600 men and 600 women selected randomly from never smokers at risk of AMI at the start of follow-up, leveraging the prospective population-based Danish Diet Cancer and Health Cohort. We are already funded to measure Cd, creatinine, osmolality, and cotinine in baseline urine samples. We now propose to additionally analyze As species, Mg, and Ca in urine among ~2000 participants selected into this case-cohort study, along with pre-existing food frequency questionnaire data on Mg and Ca. In Aim 1 we will evaluate the association between each of As, Ca, and Mg, and incidence of AMI. This will be one of the largest prospective studies of these elements in relation to AMI. In Aim 2 we will apply mixtures methods (Bayesian kernel machine regression, weighted quantile sum regression, random forests) to evaluate the interactive and joint effects of Cd, As, Ca, and Mg in relation to AMI risk. In Aim 3 we will apply in vitro and in vivo approaches to study combined effects of exposure to these elements to investigate the toxicologic mechanisms and pathways of activity. Each Aim is independently compelling and will provide important scientific contributions but together the complementary approaches have the potential to provide evidence of consistency in findings across the distinct approaches. Triangulating data across in vitro, in vivo and epidemiologic analyses represents a translational bridge as depicted in the NIEHS translational research framework. The scope of this virtual consortium will enrich our understanding of the relationships between Cd, As, Mg, Ca, and CVD. Other innovative features of our study include leveraging an existing efficient case-cohort design, large sample size, a large number of incident AMI events, controlling for tobacco smoking, in vitro assays of pro-atherogenic mechanisms, and in vivo studies of atherosclerosis. Sources of exposure to these elements are well known, therefore the identification of mixtures of these elements as cardiovascular risk/protective factors can have major implications for the prevention and control of CVD. Public Health Relevance Mechanistic and epidemiologic studies suggest a mixture of elements including cadmium, arsenic, magnesium, and calcium may play an important role in cardiovascular disease as risk and protective factors. We propose to create a virtual consortium to study chemical mixtures of these elements in relation to risk of cardiovascular disease using state-of-the-art techniques in exposure science, epidemiology, biostatistics, and cardio-toxicology. Sources of exposure to these elements are well known, therefore the identification of mixtures of these elements as cardiovascular risk/protective factors can have major implications for the prevention and control of cardiovascular disease.",Metal-nutrient mixtures in epidemiologic and toxicologic studies of cardiovascular disease,10063369,R01ES030938,"['Acute myocardial infarction', 'Adhesions', 'Apolipoprotein E', 'Arsenic', 'Arteries', 'Atherosclerosis', 'Behavioral', 'Biological Markers', 'Biometry', 'Cadmium', 'Calcium', 'Cardiovascular Diseases', 'Cell Adhesion', 'Chemicals', 'Cholesterol', 'Clinical Data', 'Cohort Studies', 'Collection', 'Complement', 'Cotinine', 'Creatinine', 'Data', 'Diabetes Mellitus', 'Elements', 'Endothelial Cells', 'Endothelium', 'Epidemiology', 'Event', 'Exposure to', 'Food', 'Frequencies', 'Funding', 'Generations', 'Health', 'Hypertension', 'In Vitro', 'Incidence', 'Joints', 'Knockout Mice', 'Lipids', 'Magnesium', 'Measures', 'Metals', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Institute of Environmental Health Sciences', 'Nutrient', 'Osmolalities', 'Participant', 'Pathway interactions', 'Physical activity', 'Play', 'Population', 'Prevention', 'Process', 'Prospective Studies', 'Prospective cohort', 'Questionnaires', 'Reactive Oxygen Species', 'Regression Analysis', 'Reporting', 'Research Design', 'Risk', 'Risk Factors', 'Role', 'Sample Size', 'Sampling', 'Science', 'Side', 'Smoking', 'Source', 'Spottings', 'Sum', 'Techniques', 'Tobacco smoking behavior', 'Toxicology', 'Translational Research', 'Urine', 'Woman', 'calcification', 'cardiovascular disorder risk', 'cardiovascular risk factor', 'cohort', 'design', 'diet and cancer', 'endothelial dysfunction', 'epidemiology study', 'follow-up', 'hazard', 'in vitro Assay', 'in vivo', 'innovation', 'macrophage', 'men', 'mortality', 'mouse model', 'never smoker', 'novel', 'population based', 'prospective', 'protective factors', 'public health relevance', 'random forest', 'recruit', 'urinary', 'virtual']",NIEHS,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2020,618191,-0.03303809764482351
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,-0.01675202652059165
"Learning to Predict Delayed Cerebral Ischemia with Novel Continuous Cerebral Arterial State Index Project Summary  Delayed cerebral ischemia (DCI) is the most devastating complication after aneurysmal subarachnoid hemorrhage (aSAH) and has an incidence rate of 30%. Current practice relies on intermittent assessment of neurological status and daily cerebral blood flow velocity (CBFV) by Transcranial Doppler ultrasound (TCD) to guide medical management to prevent DCI. Only after medical management fails, is endovascular treatment (EVT) including intraarterial vasodilator infusion and/or intracranial angioplasty initiated. This reactive practice does not account for early predictors of DCI and may miss the optimal EVT window at an early stage of DCI development before symptoms or severe deviations from normal hemodynamics. The goal of this project is to develop algorithms to predict DCI and related targets at an early stage in their development. An accurate prediction of DCI will enable a more proactive strategy to prevent and treat the underlying cause of DCI.  The following three aims will be pursued towards the goal of the project: 1) Develop aSAH-specific intracranial pressure (ICP) pulse-based cerebral arterial state index; 2) Develop and validate predictive models of targets related to delayed cerebral ischemia after aSAH; 3) Conduct a prospective institution- specific adaption and validation of the developed models.  Our DCI predictive algorithms only need data available in current clinical practice hence they can be readily adopted. If validated, these algorithms will enable clinicians to monitor risk of DCI continuously and to proactively deliver appropriate treatment. The proposed prospective study of algorithm implementation and adaptation will well prepare future clinical trials to test the efficacy of algorithm-informed interventions. Project Narrative  Delayed cerebral ischemia (DCI) is a devastating complication after aneurysmal subarachnoid hemorrhage (aSAH) and has an incidence rate of 30%. Current practice relies on intermittent assessment of neurological status and daily cerebral blood flow velocity (CBFV) by Transcranial Doppler ultrasound (TCD) to guide medical management to prevent DCI. The goal of this project is to develop algorithms to predict DCI and other related targets at an early stage in their development to enable a more proactive strategy to prevent and treat the underlying cause of DCI.",Learning to Predict Delayed Cerebral Ischemia with Novel Continuous Cerebral Arterial State Index,10070930,R01NS113541,"['Acute', 'Adopted', 'Algorithms', 'Aneurysmal Subarachnoid Hemorrhages', 'Angioplasty', 'Appearance', 'Area', 'Blood Flow Velocity', 'Cerebral Ischemia', 'Cerebral perfusion pressure', 'Cerebrovascular Circulation', 'Cerebrum', 'Characteristics', 'Chronic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Complication', 'Data', 'Data Analyses', 'Data Reporting', 'Development', 'Diagnosis', 'Dilatation - action', 'Distal', 'Electronic Health Record', 'Ensure', 'Evaluation', 'Event', 'Future', 'Goals', 'Hydrocephalus', 'Incidence', 'Individual', 'Infusion procedures', 'Injury', 'Institution', 'Intervention', 'Intracranial Pressure', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Modeling', 'Monitor', 'Morphology', 'Nature', 'Neurologic', 'Neurological status', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physiologic Monitoring', 'Physiological', 'Procedures', 'Process', 'Prospective Studies', 'Pulse Pressure', 'Recurrence', 'Reproducibility', 'Research', 'Risk', 'Shapes', 'Signal Transduction', 'Source', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Training', 'Transcranial Doppler Ultrasonography', 'Validation', 'Vasodilator Agents', 'base', 'clinical practice', 'constriction', 'data streams', 'diagnostic accuracy', 'efficacy testing', 'electronic data', 'hemodynamics', 'improved', 'indexing', 'machine learning algorithm', 'novel', 'prediction algorithm', 'predictive modeling', 'prevent', 'prospective', 'recurrent neural network', 'relating to nervous system', 'temporal measurement', 'vector']",NINDS,DUKE UNIVERSITY,R01,2020,612984,-0.015420688459379422
"Dissecting the role of the direct and indirect pathways in moment-to-moment action selection. Brains transform sensory information into decisions and decisions into behaviors, which ultimately determine fitness. Behavior can be broken down into a set of discrete chunks of movement, called actions. The basal ganglia (BG) and in particular the input nucleus of the BG, the striatum, is critical for the proper sequencing and selection of actions. At a cellular level, the striatum is comprised of spiny projection neurons (SPNs) that constitute the direct pathway (dSPNs) and indirect pathway (iSPNs). Under the center-surround model of action selection, dSPNs are thought to facilitate the expression of an action while iSPNs are thought to inhibit the expression of other actions. However, it is not clear how each pathway contributes to action selection due to methodological constraints in acquiring an objectively quantitative description of behavior. Our lab has recently developed a pipeline, known as MoSeq, that acquires high-resolution behavioral data and uses an unsupervised algorithm to model stereotyped pose dynamics (actions or “syllables”). Here I propose to combine this state-of- the-art behavioral acquisition and detection technology with both cellular-resolution imaging and optogenetic perturbation to study the population dynamics underlying action selection in the striatum. I hypothesize that SPNs exhibit syllable-specific tuning, where dSPNs are tightly tuned to facilitate the expression of related syllables, while iSPNs are more broadly tuned to suppress the simultaneous expression of other syllables. I will dissect these two processes by recording and manipulating each SPN class during specific syllable expression. In aim 1, I will perform cellular-resolution recordings of the direct or indirect pathway using genetically encoded calcium indicators and miniaturized microendoscopy in the striatum. I will examine the differential roles of the direct and indirect pathways in the context of behavioral tuning. My preliminary data suggest that dSPNs are more sparsely tuned than iSPNs. In aim 2, I will functionally test the center-surround model via direct and indirect pathway inhibition. I will use the inhibitory anion-conducting rhodopsin, ACR2. Using a system capable of detecting syllable expression in real-time, I will perturb each pathway triggered upon the expression of specific syllables to compare the same selection context across many trials. In summary, the experiments proposed here will contribute to a mechanistic understanding of how the BG performs action selection on a moment-to-moment timescale. This proposal is the first to test the predictions made by the center- surround model and will advance our understanding of how the BG encodes actions. Behavior is constructed from actions that are placed into sequences that enable animals to achieve ethologically relevant goals. The striatum, the input nucleus of the basal ganglia, is critical for the proper selection and expression of actions. My work aims to understand how striatal circuits perform action selection on a moment- to-moment timescale.",Dissecting the role of the direct and indirect pathways in moment-to-moment action selection.,10026026,F31NS113385,"['Address', 'Algorithms', 'Animals', 'Anions', 'Basal Ganglia', 'Behavior', 'Behavioral', 'Brain', 'Cell Nucleus', 'Corpus striatum structure', 'Data', 'Detection', 'Endoscopes', 'Environment', 'Exhibits', 'Goals', 'Human', 'Image', 'Individual', 'Methodology', 'Modeling', 'Motion', 'Movement', 'Neurons', 'Organism', 'Output', 'Pathway interactions', 'Pattern', 'Play', 'Population', 'Population Dynamics', 'Postural adjustments', 'Process', 'Property', 'Resolution', 'Rhodopsin', 'Rodent', 'Role', 'Sensory', 'Series', 'Stereotyping', 'Substantia nigra structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'calcium indicator', 'complement pathway', 'experimental study', 'fitness', 'indexing', 'microendoscopy', 'miniaturize', 'optogenetics', 'tool', 'unsupervised learning']",NINDS,HARVARD MEDICAL SCHOOL,F31,2020,33352,-0.030257308375492727
"Combining Voice and Genetic Information to Detect Heterogeneity in Major Depressive Disorder PROJECT SUMMARY This application aims to advance our understanding of major depressive disorder (MDD) by combining genetic information and analyzing speech patterns of those with MDD to identify subtypes. MDD is the leading cause of disability throughout the world, yet, relative to other common disorders, less is known about its origins. There are less effective treatments and much less is spent on trying to understand how it arises and how to cure it. Current treatments are relatively ineffective, with up 50% of patients refractory and many suffering severe recurrence. Understanding the mechanisms underlying MDD has been recognized as a grand challenge in global mental health. Thus, developing new treatments for MDD is a major priority for public health. A major challenge for MDD research is the presence of heterogeneity. The existence of multiple subtypes of MDD has been suspected for a long time, and likely confounds the ability to treat the disorder appropriately with existing treatments, as well as making it hard to identify the causes of MDD as a prelude to developing new treatments. However finding subtypes has been hard. Given that the way people talk can reflect alterations in mood, we expect voice to be able to predict mood, and hence potentially be used as biomarker to recognize heterogeneity. In preliminary data show that in combination with genetic data high-dimensional vocal features extracted from recordings can be used to identify subtypes. Furthermore, the use of genetic data allows us to impute voice features into large biobanks where no recordings exist, making it possible to explore the relationship between vocal features and a rich array of clinically important indicators. We explore the power of voice to make a diagnosis of MDD, to predict severity and other clinical features. Applying our approach to will inform clinical management, improving diagnosis, refine treatment and aid the development of new treatments PROJECT NARRATIVE The research proposed here will contribute to an understanding of major depressive disorder, the commonest psychiatric disorder and a leading cause of disability throughout the world. The proposal will combine information from voice recordings and genetics to identify subtypes of depression and develop robust predictors of mood, severity of illness and other clinical indicators. Our research will thereby provide new insights into disease, and well enable the more effective targeting of therapy to those who will most benefit at the appropriate time.",Combining Voice and Genetic Information to Detect Heterogeneity in Major Depressive Disorder,9943508,R01MH122569,"['Affect', 'Alleles', 'Anxiety', 'Behavioral Genetics', 'Biological Markers', 'Biology', 'Case-Control Studies', 'China', 'Chinese People', 'Classification', 'Clinical', 'Clinical Management', 'Collection', 'Data', 'Data Set', 'Depressed mood', 'Development', 'Diagnosis', 'Disease', 'Disease remission', 'Engineering', 'Ensure', 'Far East', 'Frequencies', 'Genetic', 'Genetic study', 'Genomics', 'Genotype', 'Heritability', 'Heterogeneity', 'Interview', 'Investigation', 'Linkage Disequilibrium', 'Major Depressive Disorder', 'Manuals', 'Maps', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Moods', 'Morphologic artifacts', 'Neurobiology', 'Participant', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacological Treatment', 'Phenotype', 'Population', 'Psychiatry', 'Psychological Transfer', 'Recurrence', 'Refractory', 'Research', 'Resources', 'Sampling', 'Scheme', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Suicide', 'System', 'Testing', 'Time', 'Ursidae Family', 'Voice', 'Voice Quality', 'Woman', 'accurate diagnosis', 'base', 'biobank', 'clinical application', 'clinical phenotype', 'comorbidity', 'computer science', 'data sharing', 'deep neural network', 'disability', 'disorder subtype', 'effective therapy', 'flexibility', 'genetic analysis', 'genetic architecture', 'genetic information', 'genetic predictors', 'improved', 'innovation', 'insight', 'long short term memory', 'multidimensional data', 'neuroimaging', 'preservation', 'psychogenetics', 'public health priorities', 'statistics', 'targeted treatment', 'trait', 'treatment response', 'vector']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,733478,-0.027334875502796917
"An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases. Project Summary / Abstract Diagnostic errors are increasingly recognized as a cause of pain, suffering and increased healthcare costs. Diagnostic delays are an important class of diagnostic errors. While many diagnostic errors occur in hospital settings, emergency departments visits may be especially important to consider because they treat critically ill patients and because most decisions to admit patients to the hospital are made in emergency departments. Thus, to enable a more complete understanding of diagnostic delays requires consideration of healthcare visits across a range of healthcare settings including clinic visits, emergency department visits and hospitalizations. Delays in diagnosing infectious diseases are important to consider. For contagious infectious diseases, diagnostic delays increase the risk of additional exposures, potentially generating more cases. Second, many infectious diseases can be effectively treated, but even short delays in treatment lead to worse clinical outcomes. However, with the exception of a few infectious diseases (e.g., tuberculosis), diagnostic delays for infectious diseases are understudied. Thus, there is a critical need to investigate the incidence, risk factors and clinical impact for diagnostic delays for infectious diseases. The overarching goal of our research is to investigate diagnostic delays associated with infectious diseases using existing data along with methods from the fields of computer science and statistics. While our research relies upon “big data”, we will also use clinical experts to review and contribute to all of our results. Our subject matter experts incorporate expertise in infectious diseases, emergency medicine, acute care, medical education, diagnostic reasoning, healthcare epidemiology, public health, industry, and professional infectious disease societies. Specifically, we will 1) determine the incidence of diagnostic delays for a wide range of infectious diseases; 2) identify the risk factors associated with diagnostic delays for infectious diseases that are frequently delayed or have serious outcomes; and 3) estimate the impact of diagnostic delays in terms of healthcare costs and mortality. With our data, methods and clinical experts, we will be able to translate our results into future interventions designed to decrease diagnostic delays and improve healthcare outcomes. In addition, while our proposal focuses on infectious diseases, the methods and approaches that we will develop can be adopted to investigate non-infectious diseases and conditions. Project Narrative Diagnostic delays for infectious diseases contribute to worse clinical outcomes, increased healthcare costs and, for some infectious diseases, outbreaks of great public health importance. We will use existing large data sets along with machine-learning techniques and expert clinical guidance to detect patterns of healthcare visits representing diagnostic delays. Our goal is to characterize the incidence, risk factors and clinical impact of diagnostic delays for a wide range of infectious diseases to inform future interventions.","An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases.",10017203,R01HS027375,[' '],AHRQ,UNIVERSITY OF IOWA,R01,2020,496235,-0.018284400235122046
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.034031930278022295
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,-0.019103655783134218
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,10021018,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2020,255238,-0.022648281012801964
"Organ Procurement and Information Process Optimization Project Summary: Organ Procurement and Information Process Optimization A recent White House initiative titled Advancing American Kidney Health outlines the current administration's plans to increase the supply of kidneys in the US, along with other measures that attempt to correct the imbalance between supply and demand for kidneys. In alignment with this White House initiative, the principal investigator (PI) of this project will develop algorithms to assist Organ Procurement Organizations (OPOs) make donor disposition decisions with the goal of increasing the supply of transplantable organs. The PI will harness the vast amount of data collected by OPOs, and customize as well as develop new Artiﬁcial Intelligence (AI) algorithms to identify good donors. These algorithms will reduce case coordinators workload and improve the accuracy of their decisions. The PI's objective is to quantity the impact of AI-assisted decision-making on increasing the supply of kidneys by ﬁnding missed opportunities on account of the current manual processes. Even a few more potential donors per OPO will help reduce the organ shortage problem when scaled to all 58 OPOs across the country.  This project will leverage unique data sets to generate and test hypotheses concerning which fac- tors aﬀect donor disposition decisions, and which information processing protocols produce greater referral-to-donor conversion rate. It will also customize classiﬁcation algorithms to assist case co- ordinators make donor disposition decisions. The focus will be on improving both the accuracy and speed of such decisions. As a follow up of this project, the PI will conduct a broader study involving at least ﬁve additional OPOs. The ﬁve OPOs will be selected to represent the diversity in size, population, and geography among the 58 OPOs across the country. Results of this and the follow-up study will be shared freely with all OPOs. The proposed research has the potential to increase the number of donors, free up staﬀ time, and lower OPO labor costs.  The potential impact of this project lies in the formulation and testing of hypotheses that can beneﬁt OPO decision-making and people on transplant wait list, customization of existing machine learning algorithms and computational techniques for sequential decision-making in a novel setting, and laying the groundwork for the development of new methods in the future.  The PI has the necessary disciplinary expertise, and management and leadership experience to succeed in the proposed eﬀorts. He has access to advanced computing resources and administrative help from the University of Texas. Finally, he has obtained the necessary IRB approval to proceed with this project. 1 Project Narrative: Organ Procurement and Information Process Optimization The widespread use of electronic medical record systems by hospitals, and electronic capture of referrals data by the organ procurement organizations (OPOs) makes it possible to harness large amounts of data to augment human decision-making and ask whether this can increase the supply of organs, speciﬁcally kidneys. The PI in this project will focus on improving the speed and accuracy of case coordinators' donor-disposition decisions by developing and testing customized machine learning and sequential decision-making algorithms using data from two OPOs. These eﬀorts will help quantify the potential beneﬁt of leveraging OPO data to increase kidney supply and pave the way for a larger study focusing on multiple OPOs.",Organ Procurement and Information Process Optimization,10042096,R03HS027671,[' '],AHRQ,"UNIVERSITY OF TEXAS, AUSTIN",R03,2020,41981,-0.01793680863011859
"A Multi-Omic Platform for Polycystic Ovary Syndrome Characterization and Management Project Summary/Abstract Polycystic ovarian syndrome (PCOS) is the most common endocrine disorder in women of reproductive age. Due to the complex nature of PCOS, diagnosis is often delayed and options for treatment and self-management are limited. With recent developments in high-throughput, next- generation and metabolomics technologies, there is now a unique opportunity to utilize a cost- effective, multi-omic approach to study individuals with PCOS. In this application, we propose to apply these technologies to examine women with PCOS, developing a platform that is cost-effective, scalable, integrative, and highly accurate. Our multi- omic platform (FemBio) will integrate patient’s medical records, genomics, microbiome (shotgun metagenomics), and metabolomics to create a molecular profile of women with and without PCOS. This approach has the potential to provide a more comprehensive characterization of PCOS, to lead to the identification of improved diagnostic markers, and to allow for the discovery of novel targets for treatment. Moreover, we propose a prospective observational cohort study including whole genome sequencing, shotgun metagenomic sequencing of the gut microbiome, fecal metabolomics, serum and urine hormone levels, and clinical indices in a sample of individuals with PCOS and a healthy control group. This platform (FemBio) will enable broader accessibility to the latest in molecular assays, multi- omic quantification, and computational modeling for the general public, which currently is lacking. In turn, this will improve options for PCOS characterization, long-term management, and open potentially new treatment options for the millions of women suffering from this disorder, as well as serve as a large, annotated multi-omic data set that can help the PCOS field more broadly. Project Narrative Polycystic ovarian syndrome (PCOS) is the most common endocrine disorder in women of reproductive age. Due to the complex nature of PCOS, diagnosis is often delayed and options for treatment and self-management are limited. With recent developments in sequencing and molecular technologies for studying blood and urine, there is a unique opportunity to utilize a multi-omic approach to study individuals and disease and improve options for monitoring and therapy.",A Multi-Omic Platform for Polycystic Ovary Syndrome Characterization and Management,10080951,R43HD103568,"['Age', 'Animals', 'Anovulation', 'Artificial Intelligence', 'Bacteria', 'Bacteriophages', 'Biological Assay', 'Blood', 'Cardiovascular system', 'Clinical', 'Clinical Trials', 'Cohort Studies', 'Complex', 'Computational algorithm', 'Computer Models', 'Consensus', 'Control Groups', 'Custom', 'DNA', 'Data', 'Data Reporting', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Endocrine System Diseases', 'Evaluation', 'Functional disorder', 'General Population', 'Genomics', 'Goals', 'Health', 'Home environment', 'Hormones', 'Hyperandrogenism', 'Individual', 'Infertility', 'Irritable Bowel Syndrome', 'Lead', 'Literature', 'Measures', 'Medical', 'Metabolic', 'Metagenomics', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Monitor', 'Morphology', 'Multiomic Data', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Oligonucleotides', 'Ovarian', 'Patient Self-Report', 'Patients', 'Phenotype', 'Plants', 'Polycystic Ovary Syndrome', 'Pregnancy Complications', 'Prevalence', 'Prevention', 'Probiotics', 'Recommendation', 'Risk', 'Sampling', 'Self Management', 'Serum', 'Shotguns', 'Spontaneous abortion', 'Symptoms', 'System', 'Technology', 'Testing', 'Urine', 'Variant', 'Virus', 'Vitamins', 'Woman', 'Work', 'base', 'cardiovascular health', 'convolutional neural network', 'cost effective', 'design', 'diagnostic biomarker', 'diet and exercise', 'genome sequencing', 'gut microbiome', 'improved', 'indexing', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'multiple omics', 'next generation', 'next generation sequencing', 'novel', 'ovarian dysfunction', 'patient health information', 'personalized intervention', 'prebiotics', 'prospective', 'reproductive', 'small molecule', 'symptom management', 'whole genome']",NICHD,ONEGEVITY HEALTH LLC,R43,2020,155473,-0.03706170813250357
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,-0.017558939412014626
"Eliminating the human factor from stereotaxic surgeries Project Summary: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. Advancing a tool such as an electrode, injection pipette or optical fiber through a small hole in the cranium, sometimes over long distances, and placing it precisely in a particular brain area, often much less than one millimeter in diameter, is a significant experimental challenge. Any time an investigator misses the target brain area and the experiment fails as a result, a significant amount of work is lost, additional animals get sacrificed, materials are wasted, and the pace of scientific discovery has been slowed. Even in cases when experiments succeed, they can be difficult to reproduce because many research groups rely on their most experienced lab members and their “special touch” to perform these procedures – thereby adding an element of non- quantitativeness to the procedures, effectively making the experiment less reproducible. We propose to develop a novel stereotaxic apparatus which will overcome many of these shortcomings. Our device features a radically different mechanical design which is natively compatible with both traditional and novel in-vivo techniques. We propose to combine computer 3D vision and robotics for automatic and software guided adjustments of the animal's skull. Landmarks are measured with 3D vision, based on structured illumination at a level of accuracy that has not been accomplished by any of the existing devices. This information will guide a robotic platform to position the animal for the experiment. Finally, we propose to develop an open software platform for neuronavigation that will allow investigators to use the platform with any small animal species they desire to use. Brain atlas systems for neuronavigation can either be downloaded from a cloud based site, or produced de-novo by the investigator by preparing a single set of MRI and CT scans from one sample animal. Our device will help make stereotaxic procedures more accurate and less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Narrative: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. These devices will help make stereotaxic procedures less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Most importantly, they will help reduce or eliminate failed experiments due to mistargeted interventions, thereby accelerating the pace of scientific discovery.",Eliminating the human factor from stereotaxic surgeries,10080673,R41NS119079,"['3-Dimensional', 'Animal Experimentation', 'Animal Experiments', 'Animals', 'Area', 'Atlases', 'Base of the Brain', 'Brain', 'Caliber', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Dorsal', 'Electrodes', 'Elements', 'Ensure', 'Frustration', 'Goals', 'Human', 'Image', 'Injections', 'Intervention', 'Laboratories', 'Lighting', 'Location', 'MRI Scans', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Mechanics', 'Monitor', 'Neuronavigation', 'Operative Surgical Procedures', 'Persons', 'Positioning Attribute', 'Procedures', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Robotics', 'Sampling', 'Savings', 'Scanning', 'Side', 'Site', 'Speed', 'Structure', 'Surgical sutures', 'System', 'Techniques', 'Technology', 'Time', 'Touch sensation', 'Translations', 'Vision', 'Work', 'X-Ray Computed Tomography', 'age group', 'base', 'bone', 'bone imaging', 'brain tissue', 'cloud based', 'cost effective', 'cranium', 'design', 'experimental study', 'genetic strain', 'hexapod', 'in vivo', 'laboratory experience', 'member', 'millimeter', 'novel', 'operation', 'optical fiber', 'programs', 'prototype', 'soft tissue', 'software development', 'tool', 'virtual', 'wasting']",NINDS,POPNEURON LTD.,R41,2020,251960,-0.015571937659786919
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9970407,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2020,453846,-0.022634024570818742
"Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences Abstract A major approach in causal inference literature aimed at mitigating bias due to unmeasured confounding is the so- called instrumental variable (IV) design which relies on identifying a variable which (i) influences the treatment process, (ii) has no direct effect on the outcome other than through the treatment, and (iii) is independent of any unmeasured confounder. IV methods are very well developed and widely used in social and health science, although validity of IV inferences may not be reliable if any of required assumptions (i)-(iii) is violated. This proposal aims to develop (a) new IV methods robust to violation of any of (i)-(iii); (b) New negative control methods that can be used to detect and sometimes to nonparametrically account for unmeasured confounding bias; (c) New bracketing methods for partial inference about causal effects in comparative interrupted time series studies. The proposed methods will be used to address current scientific queries in three major substantive public health areas:(1) to understand the health effects of air pollution; (2) to quantify the causal effects of modifiable risk factors for Alzheimer's disease and related disorders; (3) To uncover the mechanism by which a randomized package of interventions produced a substantial reduction of HIV incidence in a recent major cluster randomized trial of treatment as prevention in Botswana, Africa. Our proposal will provide the best available analytical methods to date to resolve confounding concerns in these high impact public health applications and more broadly in observational studies in the health sciences. Summary This proposal aims to develop new causal inference methods to tame bias due to hidden confounding factors in obser- vational studies as well as in randomized experiments subject to non-adherence. The proposed methods are firmly grounded in modern semiparametric theory which will be used to obtain more robust and efficient inferences about causal effects in a broad range of public health applications including in Epidemiology of Aging, Environmental Health Epidemiology and HIV/AIDS Prevention.",Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences,9859751,R01AG065276,"['AIDS prevention', 'Address', 'Adherence', 'Africa', 'Aging', 'Air Pollution', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Area', 'Blood Pressure', 'Botswana', 'Clinical Treatment', 'Cluster randomized trial', 'Data', 'Diabetes Mellitus', 'Disease', 'Environmental Health', 'Epidemiology', 'Genetic', 'HIV', 'Health', 'Health Sciences', 'Incidence', 'Interruption', 'Intervention', 'Learning', 'Linkage Disequilibrium', 'Literature', 'Machine Learning', 'Masks', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Observational Study', 'Outcome', 'Participant', 'Prevention', 'Process', 'Public Health', 'Public Health Applications Research', 'Randomized', 'Research Design', 'Research Personnel', 'Risk Factors', 'Series', 'Social Sciences', 'Testing', 'Thromboplastin', 'Time', 'ambient air pollution', 'analytical method', 'c new', 'comparative', 'design', 'experimental study', 'genetic variant', 'high dimensionality', 'intervention effect', 'modifiable risk', 'mortality', 'novel', 'pleiotropism', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'uptake', 'user friendly software']",NIA,UNIVERSITY OF PENNSYLVANIA,R01,2020,502013,-0.05329065231942433
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.03440224674106881
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9924443,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Models', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2020,7143424,-0.007503923584936272
"Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents PROJECT ABSTRACT  Antimicrobial resistance is a critical public health issue. Infections with drug resistant pathogens are estimated to cause an additional eight million hospitalization days annually over the hospitalizations that would be seen for infections with susceptible agents. The use of antibiotics (in both clinical and agricultural settings) is being viewed as precursor for these infections and thus, is a major public health concern—particularly as outbreaks become more frequent and severe. However, scientiﬁc evidence describing the hazards associated with antibiotic use is lacking due to inability to quantify the risk of these practices. One promising avenue to elucidate this risk is to use shotgun metagenomics to identify the AMR genes in samples taken through systematic spatiotemporal surveillance. The goal of this proposed work is to develop algorithms that will provide such a means for analysis. The algorithms need to be scalable to very large datasets and thus, will require the development and use succinct data structures.  In order to achieve this goal, the investigative team will develop the theoretical foundations and applied meth- ods needed to study AMR through the use of shotgun metagenomics. A major focus of the proposed work is developing algorithms that can handle very large datasets. To achieve this scalability, we will create novel means to create, compress, reconstruct and update very large de Bruijn graphs that metagenomics data in a manner needed to study AMR. In addition, we will pioneer the study of AMR through long read data by proposing new algorithmic problems and solutions that use data. For example, identifying the location of speciﬁc genes in a metagenomics sample using long read data has not been proposed or studied. Thus, the algorithmic ideas and techniques developed in this project will not only advance the study of AMR, but contribute to the growing domain of big data analysis and pan-genomics.  Lastly, we plan to apply our methods to samples collected from both agricultural and clinical settings in Florida. Analysis of preliminary and new data will allow us to conclude about (1) the public risk associated with antimicro- bial use in agriculture; (2) the effectiveness of interventions used to reduce resistant bacteria, and lastly, (3) the factors that allow resistant bacteria to grow, thrive and evolve. A–1 PROJECT NARRATIVE  Antibiotic use in agriculture is a major public health concern that is receiving a lot of media attention, par- ticularly as antibiotic-resistant infections in become more frequent and severe. This research will build a novel bioinformatics framework for determining how antimicrobial resistant genes evolve, grow, and persist in a system that has been affected by antibiotic use. This will, in turn, facilitate the development of effective intervention methods that reduce resistant pathogens in clinical and agricultural settings. N–1",Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents,9828618,R01AI141810,"['Affect', 'Agriculture', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Attention', 'Bacteria', 'Base Pairing', 'Big Data', 'Bioinformatics', 'Clinical', 'Collaborations', 'Combating Antibiotic Resistant Bacteria', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Compression', 'Data Set', 'Development', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Florida', 'Food production', 'Foundations', 'Genes', 'Genomics', 'Goals', 'Graph', 'Hospitalization', 'Infection', 'International', 'Investigation', 'Length', 'Location', 'Measures', 'Memory', 'Metagenomics', 'Methods', 'Monitor', 'Noise', 'Organism', 'Pathogenicity', 'Plasmids', 'Prevention', 'Public Health', 'Research', 'Resistance', 'Risk', 'Sampling', 'Shotguns', 'Surveillance Methods', 'System', 'Techniques', 'Time', 'Translating', 'Update', 'Work', 'antibiotic resistant infections', 'bacterial resistance', 'base', 'combinatorial', 'drug resistant pathogen', 'effective intervention', 'foodborne outbreak', 'genetic variant', 'hazard', 'improved', 'large datasets', 'machine learning algorithm', 'method development', 'microbial', 'microbiome analysis', 'microbiome research', 'multiple datasets', 'novel', 'pathogen', 'petabyte', 'reconstruction', 'research and development', 'resistance gene', 'spatiotemporal', 'standard care', 'structured data']",NIAID,UNIVERSITY OF FLORIDA,R01,2020,422334,-0.017834945590245965
"MUFA-SIRT1 signaling as a central node regulating healthspan PROJECT SUMMARY Macronutrients serve a multitude of roles beyond provision of energy, with numerous nutrients and/or their downstream metabolites acting as signaling molecules to coordinate cellular metabolism and function. Indeed, numerous nutrient sensing pathways (e.g. mTOR, AMPK and sirtuins) have evolved allowing us to respond to specific nutrients/metabolites, which in turn impacts healthspan. Sirtuins are largely thought to be driven by redox, whereby high levels of NAD, a cofactor in the sirtuin reaction and indicator of low energy charge, drives sirtuin-catalyzed deacylation of target proteins. SIRT1, the most-studied sirtuin, is a key nutrient sensing node that regulates a plethora of cellular functions to promote lifespan extension and healthy aging. As a result, there is immense interest in the use of SIRT1 activating compounds (STACs) to prevent or treat a wide range of aging-related disease. The links between dietary macronutrients, nutrient sensing and healthspan have historically focused upon caloric or protein restriction with limited attention given to dietary lipids. However, a small and growing body of literature has linked monounsaturated fatty acids (MUFAs) to improved healthspan. In addition to positive effects on lifespan and healthy aging in model organisms, dietary MUFAs have been linked to wide-ranging health benefits in epidemiological studies and, since they are a primary constituent of olive oil, thought to contribute to the benefits of the Mediterranean Diet. Despite these studies, little is known about the biological underpinnings through which MUFAs elicit their beneficial health effects. We have previously shown that lipid droplet catabolism (i.e. lipolysis) increases SIRT1 and downstream PGC-1a/PPAR- a signaling as a means to increase mitochondrial biogenesis and function during times of nutrient deprivation. Our preliminary data show for the first time that MUFAs released specifically from lipolysis are trafficked to the nucleus where they allosterically activate SIRT1 towards select acetylated peptide substrates. This discovery makes MUFAs the first-known endogenous allosteric activators of SIRT1. Moreover, we show that MUFAs activate SIRT1 through a similar mechanism to resveratrol suggesting that MUFA signaling may modulate the response to exogenous SIRT1 activators. Based on these preliminary data, the objective of this application is to further characterize the role of MUFAs as endogenous SIRT1 activators. We hypothesize that MUFAs selectively activate SIRT1 to modulate the response to numerous dietary interventions known to impact healthspan. To test our objective, we propose the following aims: Aim 1: To define how MUFAs modulate SIRT1 substrate selectivity. Aim 2: To characterize the SIRT1-dependent effects of MUFAs/olive oil on healthspan. Aim 3: To determine the contribution of MUFAs in mediating the response to STACs or caloric restriction. Upon completion of the proposes studies, we will have further expanded our understanding of SIRT1 biology allowing for refined approaches to activate SIRT1 to promote healthy aging. NARRATIVE The proposed studies will advance our understanding into the underlying biology linking dietary factors to healthspan. The data gleaned from these studies will help refine therapeutic or nutritional avenues to modulate lifespan and aging-related diseases resulting in a direct, positive impact on human health.",MUFA-SIRT1 signaling as a central node regulating healthspan,10092409,R01AG069768,"['Aging', 'Animal Model', 'Animals', 'Attention', 'Biogenesis', 'Biological', 'Biology', 'Caloric Restriction', 'Catabolism', 'Cell Nucleus', 'Cell physiology', 'Charge', 'Clinical Trials', 'Data', 'Deacetylation', 'Development', 'Diet', 'Dietary Factors', 'Dietary Fats', 'Dietary Intervention', 'Disease', 'Dose', 'FRAP1 gene', 'Fasting', 'Glean', 'Gold', 'Health', 'Health Benefit', 'Human', 'Link', 'Lipids', 'Lipolysis', 'Literature', 'Longevity', 'Machine Learning', 'Macronutrients Nutrition', 'Maps', 'Mediating', 'Mediterranean Diet', 'Metabolism', 'Mitochondria', 'Modeling', 'Monounsaturated Fatty Acids', 'Mus', 'Nutrient', 'Nutritional', 'Oils', 'Olive oil preparation', 'Olives - dietary', 'Outcome', 'Oxidation-Reduction', 'PPAR alpha', 'Pathway interactions', 'Peptides', 'Pharmacologic Substance', 'Proteins', 'Proteomics', 'Reaction', 'Research', 'Resveratrol', 'Role', 'SIRT1 gene', 'Signal Transduction', 'Signaling Molecule', 'Sirtuins', 'Source', 'Testing', 'Therapeutic', 'Time', 'Work', 'analog', 'base', 'cofactor', 'deacylation', 'detection of nutrient', 'epidemiology study', 'healthspan', 'healthy aging', 'improved', 'innovation', 'interest', 'middle age', 'mutant mouse model', 'novel', 'nutrient deprivation', 'polyphenol', 'prevent', 'red wine', 'response']",NIA,UNIVERSITY OF MINNESOTA,R01,2020,315700,-0.022097406079530108
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,9941499,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,348397,-0.011826931579564375
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,9851457,R37DA009757,"['Address', 'Alcohol or Other Drugs use', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2020,382893,-0.009297986707867988
"Investigation of arterial changes in the Circle of Willis during intracranial aneurysm growth in humans PROJECT SUMMARY/ABSTRACT Prevalence of saccular intracranial aneurysms (IA) in western populations is estimated at around ~3%. Clinically, IA present a dilemma, in that they are usually asymptomatic; however, IA are extremely dangerous if they rupture, causing subarachnoid hemorrhage (~50% mortality). There is convincing evidence that continued IA growth increases the risk of rupture (12-24 times). To better monitor and predict IA progression, there is a compelling need to better understand clinical IA growth (aneurysmal remodeling). 90% of IA occur within the arteries of the Circle of Willis (CoW). Despite there being overwhelming evidence connecting CoW vascular remodeling and IA disease, the majority of IA research focuses only on the IA site, and does not consider the contribution of connected arteries. Specific vascular remodeling in the CoW arteries may provide an additional indicator for monitoring IA progression. The biochemical processes that occur at the IA site include inflammation and extracellular matrix remodeling leading to cell death and vessel wall degeneration. Analyses in animal models have strongly connected arterial wall shear stress (WSS) as a trigger of these processes, leading to IA initiation and remodeling. Patient-oriented research has further linked areas of low WSS with IA growth. Because CoW vasculature can change during IA growth, the blood flow entering IA changes and may create a new level of WSS to stabilize the remodeling process. Better understanding how human IA may naturally stabilize is highly relevant to predicting IA progression, and the role of changing WSS will be investigated in this grant. In our recent study of 520 clinically monitored IA, we found that while many IA grew consistently, following a projected growth path, others became stable. We also found that IA growth speed is significantly faster in women. Given the association of IA with sex, family history, and disease, different patterns of vascular remodeling may occur within groups with different genetics or medical history. We propose a clinical translational study to study IA growth in different genetic and medical history groups. We hypothesize IA growth may associate with patterns of vascular remodeling within the CoW. We will test our hypothesis with the following specific aims: (1) Is IA growth a local phenomenon or it associated with vascular remodeling within the CoW? (2) Do genetically similar individuals undergo similar patterns of vascular remodeling? (3) Does blood flow within the CoW associate with vascular remodeling? By identifying how IA disease progression may associate with other remodeling within the CoW, this study can identify new imaging biomarkers that enable improved IA treatment decisions. This proposal is significant because there is an unmet need to accurately assess IA disease progression and changes in risk. This proposal is innovative because it will extend existing IA studies to include more, relevant cerebrovascular arteries and longitudinal data, while implementing several technical innovations specific to this problem which can translate to clinical tracking of cerebrovascular changes PROJECT NARRATIVE Intracranial aneurysms are extremely dangerous when they rupture. When one is detected, it may be treated or monitored through imaging, with aneurysm growth a strong indication of increased risk of rupture. This project will study how remodeling in the arteries of the Circle of Willis associates with aneurysm growth, and how blood flow may determine locations of remodeling, in order to identify new biomarkers to improve tracking and assessment of aneurysm progression.",Investigation of arterial changes in the Circle of Willis during intracranial aneurysm growth in humans,9947821,R01HL152270,"['Affect', 'Aneurysm', 'Angiography', 'Animal Model', 'Area', 'Arteries', 'Autosomal Dominant Polycystic Kidney', 'Biochemical Process', 'Biological Markers', 'Blood Vessels', 'Blood flow', 'Caliber', 'Cell Death', 'Characteristics', 'Circle of Willis', 'Clinical', 'Dangerousness', 'Data', 'Data Set', 'Disease', 'Disease Progression', 'Extracellular Matrix', 'Family', 'Gender', 'Genetic', 'Geometry', 'Grant', 'Growth', 'Human', 'Hypertension', 'Image', 'Individual', 'Inflammation', 'Intracranial Aneurysm', 'Investigation', 'Length', 'Lesion', 'Link', 'Liquid substance', 'Location', 'Machine Learning', 'Medical History', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Prevalence', 'Process', 'Prospective cohort', 'Recording of previous events', 'Research', 'Risk', 'Risk Factors', 'Role', 'Rupture', 'Site', 'Speed', 'Statistical Models', 'Stroke', 'Subarachnoid Hemorrhage', 'Techniques', 'Testing', 'Time', 'Translating', 'United States', 'Validation', 'Vascular remodeling', 'Woman', 'Work', 'arterial tortuosity', 'cerebrovascular', 'clinical risk', 'data modeling', 'demographics', 'hemodynamics', 'imaging biomarker', 'improved', 'innovation', 'mortality', 'novel', 'patient oriented research', 'sex', 'shape analysis', 'shear stress', 'side effect', 'tool', 'translational study']",NHLBI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,693928,-0.01627009110428837
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9965720,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2020,741796,-0.03446756189115408
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,-0.015512950440719141
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9855035,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'disease phenotype', 'driver mutation', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'machine learning method', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistical learning', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,305167,-0.019155018576707244
"Flexible Bayesian approaches to causal inference with multilevel survival data and multiple treatments Project Summary Combining comparative effectiveness research (CER) and dissemination and implementation research is playing an increased role in public health and health care service by allowing practitioners to make informed decisions about treatments and improving adoption of evidence-based practices. In circumstances where CER questions do not lend themselves to direct experimentation or in implementation trials where incomplete adoption of in- tervention occurs, causal inference tools for “ﬁeld data” are recommended for evaluating treatment effects. The increased complexities in large national electronic health databases pose challenges for statistical analyses and demand approaches beyond conventional causal inference techniques, which have traditionally focused on bi- nary treatment. Given the wealth of information captured in large-scale data, it is rare that treatment regimens are deﬁned in terms of two treatments only. The data are typically pooled from treating facilities across the nation with considerable variability in the institutional effect. Although it has been established that popular tools for bi- nary treatment are inappropriate for the multiple treatment setting, and that ignoring the multilevel data structure can bias the estimate of the treatment effect, few alternative methods have been proposed to deal with both complications simultaneously. The ﬁrst aim of our proposed project is to develop a novel and ﬂexible Bayesian approach to estimating the causal effects of multiple treatments on survival with clustered data. We then fully investigate the operating characteristics of our proposed method in a variety of simulated scenarios and contrast it with approaches often used in practice. For causal estimates to be unbiased, researchers commonly make the assumption of no unmeasured confounding (UMC). Though highly recommended with binary treatment, there is no known implementation or framework for sensitivity analysis with multiple treatments and multilevel survival data. The second aim of our project is to develop and apply a ﬂexible and interpretable Bayesian approach to assessing the sensitivity of causal estimates to possible departures from the assumption of no UMC, at both cluster- and individual-level. This approach is capable of gauging the amount of unobserved confounding needed to change the direction of the observed treatment effects Our project will apply the developed methods in the ﬁrst two aims to a large representative high-risk localized prostate cancer population, drawn from the National Cancer Data Base, to evaluate the average causal effects of three popular treatment options on survival and evaluate how unmeasured confounding might alter causal conclusions. We also will estimate treatment heterogeneity and identify distinct subgroups of patients for which a treatment is effective or harmful. Our methods will establish the effectiveness component and lay the groundwork for building the cost-effectiveness models, and provide evidence for further investigations of variations in intervention implementation and modiﬁcations in recommendations for treatments leading to different patient outcomes. To facilitate the dissemination of our work, we will share the underlying statistical code via an R package. Project Narrative As public health comparisons often involve more than two treatments on patients from multiple care centers, comparative effectiveness research and implementation research call for advanced causal inference techniques allowing for the estimation of multiple treatment effects while respecting the multilevel data structure. We de- velop a new approach to simultaneously contrast the effectiveness of multiple treatments on clustered survival outcomes, propose and apply a new framework for testing the assumptions behind these estimates, and propose a strategy for estimating treatment effect heterogeneity and identifying distinct subgroups of patients for which a treatment is harmful or effective. Our work will provide practitioners clarity with respect to estimating causes and effects from complicated health care databases in which the comparative assessment of multiple treatment options from multilevel survival data is challenging but imperative.",Flexible Bayesian approaches to causal inference with multilevel survival data and multiple treatments,10056850,R21CA245855,"['Address', 'Adoption', 'Androgens', 'Bayesian Method', 'Bayesian Modeling', 'Brachytherapy', 'Caring', 'Characteristics', 'Clinical', 'Clinical Practice Patterns', 'Code', 'Comparative Effectiveness Research', 'Complex', 'Data', 'Databases', 'Effectiveness', 'Event', 'Evidence based practice', 'Health', 'Healthcare', 'Heterogeneity', 'Individual', 'Intervention', 'Investigation', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Output', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Play', 'Population', 'Prostate Cancer therapy', 'Public Health', 'Radiation therapy', 'Radical Prostatectomy', 'Recommendation', 'Research Personnel', 'Role', 'Source', 'Statistical Data Interpretation', 'Subgroup', 'Techniques', 'Testing', 'Time', 'Treatment Protocols', 'Trees', 'Variant', 'Work', 'base', 'cancer epidemiology', 'comparative', 'complex data ', 'cost effectiveness', 'data structure', 'data tools', 'deprivation', 'design', 'dissemination research', 'effective therapy', 'epidemiology study', 'flexibility', 'health care service', 'high risk', 'implementation research', 'implementation trial', 'improved', 'interest', 'large scale data', 'novel', 'novel strategies', 'patient subsets', 'regression trees', 'simulation', 'structured data', 'survival outcome', 'tool', 'treatment arm', 'treatment choice', 'treatment effect', 'user-friendly']",NCI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R21,2020,459480,-0.017259379316819628
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9973174,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,726322,-0.02642508460211758
"Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease Abstract: Hepatocellular carcinoma (HCC) is the third most common cause of cancer-related death worldwide and is rising in incidence in the US. 90% of patients in the US with liver cancer have underlying cirrhosis, thus guideline recommendations recommend surveillance in all patients with cirrhosis to facilitate early detection. Unfortunately, only 20-30% of patients are detected with early detection and are thus eligible for potentially curative treatments. There is an unmet need for reliable biomarkers for HCC to facilitate adherence to screening and for early detection. In the proposed work we will develop early detection strategies for HCC based on glycoproteomic profiles. Unique changes in glycosylation in proteins, which involve structural changes in glycan groups, have been shown to be important serum biomarkers for early cancer detection. Importantly, the subtle changes may only involve minor structures but they can be very specific in differentiating cirrhosis versus early versus late stage HCC. In addition, these changes may be specific to the etiology of liver disease. These glycan structural changes will be detected and monitored quantitatively using a mass spectrometry approach which has proven to be an accurate way to characterize even minor changes in structure which may be significant as biomarkers based on our previous mass analysis, tandem mass spectrometry measurements and databases which have been developed for glycan and glycopeptide analysis. This will be demonstrated for both glycan and glycopeptide screening from serum using novel extraction and separation methods coupled to mass spectrometry which can ultimately be used to distinguish early stage HCC from cirrhosis. The proposed work will deliver separations and mass spec methods enabling isomeric separation of glycans and glycopeptides, permitting unequivocal assignment of protein glycosylation related to disease state. We will be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. Novel software will be developed and used to assign these glycan structures. The markers will be discovered for specific etiologies of HCV-related, alcohol-related and NAFLD-related etiologies of HCC. This will be a multisite study to include all components required for a tumor biomarker lab including samples and sample preparation, separations and mass spec analysis, bioinformatics evaluation and statistical analysis. Ultimately, we will develop methods for discovery of glycan/glycopeptide markers from patient serum, the identification of potential markers and the development of new assays to provide a limited confirmation of these markers. Project Narrative: The proposed work will use new separations and mass spec methods to provide isomeric separation of glycans and glycopeptides, resulting in detailed assignment of protein glycosylation related to disease state. We expect to be able to distinguish different isomeric forms of fucosylation and sialylation which may contain important disease related markers. The markers will be discovered for specific etiologies of HCV- related, alcohol-related and NAFLD-related etiologies of HCC. Ultimately, we will develop new assays to provide a limited confirmation of these markers.",Screening of Glycan Markers in Serum for Early Detection of HCC in Different Etiologies of Disease,9893836,U01CA225753,"['Adherence', 'Alcohol-Related Hepatocellular Carcinoma', 'Alcohols', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Cancer Etiology', 'Carbon', 'Cessation of life', 'Cirrhosis', 'Complex', 'Computer software', 'Coupled', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Etiology', 'Europe', 'Evaluation', 'Frequencies', 'Glycopeptides', 'Glycoproteins', 'Guidelines', 'Hepatitis B Virus', 'Hepatitis C virus', 'Incidence', 'Isomerism', 'Japan', 'Lectin', 'Liver Cirrhosis', 'Liver diseases', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of liver', 'Mass Spectrum Analysis', 'Measurement', 'Methods', 'Minor', 'Monitor', 'Natural graphite', 'Patients', 'Pattern', 'Peptides', 'Performance', 'Polysaccharides', 'Preparation', 'Primary carcinoma of the liver cells', 'Protein Glycosylation', 'Protein Isoforms', 'Proteins', 'Proteome', 'Recommendation', 'Risk', 'Sampling', 'Screening for cancer', 'Serum', 'Serum Markers', 'Site', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Survival Rate', 'Testing', 'Time', 'Tumor Markers', 'Ultrasonography', 'Work', 'alpha-Fetoproteins', 'base', 'carbohydrate structure', 'curative treatments', 'diagnostic screening', 'early detection biomarkers', 'early onset', 'glycoproteomics', 'glycosylation', 'improved', 'non-alcoholic fatty liver disease', 'nonalcoholic steatohepatitis', 'novel', 'patient screening', 'patient stratification', 'precision medicine', 'screening', 'sialylation', 'tandem mass spectrometry', 'tumor']",NCI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,U01,2020,493563,-0.06054587849703
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9911975,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Visualization', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'osteoporosis with pathological fracture', 'patient response', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2020,276227,-0.006452803120542414
"Indiana University clinical Center for acute pancreatitis and diabetes clinical research network PROJECT SUMMARY / ABSTRACT Pancreatogenic diabetes, or type 3c diabetes (T3cDM), is a known complication of acute pancreatitis (AP). Recent data suggest that T3cDM occurs more commonly than previously recognized and exhibits a spectrum of defects including features that overlap aspects of both type 1 and type 2 diabetes. At present, the extent to which immune activation, β cell dysfunction, and insulin resistance occur following AP and the genetic, metabolic and imaging correlates of these phenotypes have not been characterized. To address these knowledge gaps, we have assembled a multidisciplinary team with expertise in pancreatitis and exocrine pathophysiology, diabetes, β cell biology, diabetes genetics, and pancreatic imaging at the Indiana University School of Medicine. The IU Clinical Center will work with other members of the Type 1 Diabetes in Acute Pancreatitis Consortium to test the hypothesis that T3cDM encompasses a heterogeneous combination of metabolic and potentially immunologic phenotypes that are determined by distinct underlying pathophysiologies. We propose the following specific aims (SA) to meet the goals of this RFA. SA #1: To perform an observational study of robustly characterized adults with AP in order to address knowledge gaps in the natural history and incidence of autoantibody-positive diabetes (AAb+), impaired glucose tolerance (IGT)/impaired fasting glucose (IFG), and diabetes occurring subsequent to AP. Enrolled participants will be longitudinally characterized with emphasis on identifying genetic, immunological, metabolic, and clinical risk factors for the development of AAb+, IGT/IFG, or T3cDM. We will use state-of-the-art immunologic phenotyping and measurements of pancreatic β cell function to define the physiologic basis for metabolic dysregulation in T3cDM after AP. In tandem, a biorepository will be developed for undertaking translational, mechanistic and biomarker investigations and ancillary studies. SA#2: The Imaging Morphology of Pancreas in Diabetic Patients following Acute Pancreatitis (IMMINENT) study aims to utilize novel quantitative magnetic resonance imaging techniques as a non-invasive biomarker to identify patients at risk for the development of post-AP T3cDM. This longitudinal study will evaluate pancreatic parenchymal morphologic and pathophysiologic changes following AP in AAb+, euglycemic, IGT and DM individuals. Imaging phenotypes will be correlated with the metabolic, genetic and immunological phenotypes established in SA#1. SA#3: To perform a nested case control study using state-of-the-art techniques to define the underlying pathophysiology of endocrine and exocrine function in the subgroup of AAb+ individuals with AP-associated metabolic dysfunction relative to those who remain normoglycemic. We will undertake detailed metabolic phenotyping to evaluate islet cell responses (i.e. β and alpha cell function) in parallel with arginine-augmented hyperglycemic clamp methodology to measure functional β cell mass, and endoscopic assessment to define the relationship between impaired exocrine and endocrine function in AAb+ T3cDM. We will utilize 25 individuals with AAb+ and IGT or T3cDM and compare findings to results in 25 normoglycemic individuals with negative autoantibodies from SA#1. PROJECT NARRATIVE While diabetes is a known complication of acute pancreatitis (AP), a full understanding of the underlying etiology of this disorder is lacking. To address this gap in knowledge, we have assembled a multidisciplinary team at Indiana University, with expertise in exocrine pancreas pathophysiology, diabetes, β cell biology, genetics, and pancreatic imaging. As part of the newly formed Type 1 Diabetes in Acute Pancreatitis Consortium, members of the Indiana University Clinical Center will collaborate with other consortium members to undertake studies focused on the comprehensive clinical, epidemiological, biological and imaging characterization of diabetes occurring after AP.",Indiana University clinical Center for acute pancreatitis and diabetes clinical research network,10127104,U01DK127382,"['3-Dimensional', 'Address', 'Adult', 'Affect', 'Alpha Cell', 'Ancillary Study', 'Arginine', 'Artificial Intelligence', 'Autoantibodies', 'Autoimmunity', 'Beta Cell', 'Biological', 'Biological Markers', 'Cell Death', 'Cell physiology', 'Cellular Stress', 'Cellular biology', 'Clinical', 'Clinical Research', 'Closure by clamp', 'Complication', 'Data', 'Defect', 'Development', 'Diabetes Mellitus', 'Diabetes autoantibodies', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Duodenum', 'Endocrine', 'Enrollment', 'Epidemiology', 'Etiology', 'Evolution', 'Exhibits', 'Exocrine pancreas', 'Exocrine pancreatic insufficiency', 'Extracellular Matrix', 'Fatty acid glycerol esters', 'Functional disorder', 'Genetic', 'Genomic DNA', 'Glucagon', 'Glucose', 'Glucose Intolerance', 'Goals', 'Health', 'Hyperglycemia', 'Image', 'Imaging Techniques', 'Immune', 'Immunologics', 'Impaired fasting glycaemia', 'Impairment', 'Incidence', 'Indiana', 'Individual', 'Infiltration', 'Insulin', 'Insulin Resistance', 'Insulin deficiency', 'Insulin-Dependent Diabetes Mellitus', 'Investigation', 'Islet Cell', 'Knowledge', 'Liquid substance', 'Longitudinal Studies', 'Magnetic Resonance', 'Magnetic Resonance Elastography', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Meta-Analysis', 'Metabolic', 'Metabolic dysfunction', 'Methodology', 'Monitor', 'Morphology', 'Motion', 'Natural History', 'Nested Case-Control Study', 'Non-Insulin-Dependent Diabetes Mellitus', 'OGTT', 'Observational Study', 'Pancreas', 'Pancreatic Function Tests', 'Pancreatitis', 'Participant', 'Patients', 'Perfusion', 'Phenotype', 'Physiological', 'Plasma', 'Population', 'Relaxation', 'Reporting', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk Marker', 'Secretin', 'Signal Transduction', 'Structure of beta Cell of islet', 'Subgroup', 'Techniques', 'Testing', 'Texture', 'Time', 'Tissues', 'Universities', 'Urine', 'Work', 'acute pancreatitis', 'biobank', 'biomarker development', 'clinical center', 'clinical phenotype', 'clinical risk', 'contrast enhanced', 'diabetes mellitus genetics', 'diabetic patient', 'endocrine pancreas development', 'extracellular', 'genetic analysis', 'glucose tolerance', 'immune activation', 'impaired glucose tolerance', 'insulin secretion', 'insulin sensitivity', 'islet', 'medical schools', 'member', 'metabolic imaging', 'metabolic phenotype', 'multidisciplinary', 'non-diabetic', 'novel', 'pancreas imaging', 'polygenic risk score', 'prospective', 'radiomics', 'repository', 'response']",NIDDK,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,U01,2020,278416,-0.0322466259792659
"Refining and Validating Borderline Personality Disorder Phenotypes Through Factor Mixture Modeling The proposed research seeks to clarify the symptomatic heterogeneity of borderline personality disorder (BPD) by examining BPD phenotypes through advanced latent variable modeling. A second, innovative aim is to validate these findings through intensive longitudinal assessment in daily life. BPD is associated with high rates of emergency room visits and costly healthcare service utilization, affecting 10-20% of psychiatric outpatients and 20-40% of psychiatric inpatients. BPD also contributes to impaired social and occupational functioning and significant suicide risk, with 1 in 10 individuals with BPD completing suicide. Recent research has aimed to enhance treatment effectiveness for BPD by identifying prototypical patterns of symptom manifestation that may suggest ideographic treatment targets. However, no research has simultaneously included: a) a sufficiently large patient sample; b) ecologically sound validation of results; and c) use of appropriate statistical techniques. The proposed project builds on this research through two aims. Aim 1: Utilize a model comparison approach to identify BPD phenotypes in a large psychiatric outpatient sample assessed via semi-structured diagnostic interviews (Study 1). Aim 2: Validate the results of Study 1 by applying phenotype classification algorithms produced in Study 1 to a smaller sample of patients who have completed 21 days of momentary surveys on symptoms and clinical outcomes (Study 2). To address Aim 1, factor mixture modeling (FMM)—a novel, flexible, and integrative latent variable modeling approach—will be compared to standard factor analysis and latent class analysis in order to evaluate the dimensional and categorical structure of BPD. We expect a single-factor, multi-class FMM will best explain heterogeneity in BPD, over and above other sources of heterogeneity (e.g., gender, comorbidity). To address Aim 2, we will use a prototype-matching approach to algorithmically assign patients in the validation sample to phenotypes identified in Aim 1 and determine their predictive validity in terms of daily clinical outcomes. Results of this project will provide empirically grounded personalized prediction tools for BPD intervention and treatment development, in line with the NIMH’s goal of “developing, testing, and refining tools and methodologies… for personalized risk and trajectory prediction and intervention.” This fellowship will allow the applicant to receive tailored consultation from experts in methodology, data analysis, and BPD theory and assessment, as well as advanced statistical training and grantsmanship courses and workshops. This training will be enhanced by the resource-rich environment and explicit support of student research and funding provided by the Pennsylvania State University, as well as the support of Dr. Kenneth Levy and his lab. This promising young researcher will gain training in computational modeling, proficiency in working with “big data,” increased understanding of conceptual and nosological models of BPD, and further skills in disseminating research findings through publication and presentation, as vital steps towards an independent research career in translational clinical science. PROJECT NARRATIVE The proposed research aims to elucidate the underlying psychopathology of borderline personality disorder (BPD), a prevalent, costly, and deadly psychiatric condition, through the identification of ecologically valid phenotypes of the disorder. Accurate identification of BPD phenotypes promises to reveal patterns of symptoms that can be targeted in treatment development, improving treatment effectiveness for this impairing disorder. This research will support scientifically and clinically useful algorithms for personalized intervention, enhancing treatment outcomes and reducing the overall burden of BPD.",Refining and Validating Borderline Personality Disorder Phenotypes Through Factor Mixture Modeling,9911299,F31MH121020,"['Address', 'Affect', 'Algorithms', 'Archives', 'Assessment tool', 'Behavior', 'Big Data', 'Biological Markers', 'Borderline Personality Disorder', 'Categories', 'Cellular Phone', 'Clinical', 'Clinical Sciences', 'Computer Models', 'Consultations', 'Data', 'Data Analyses', 'Diagnostic', 'Dimensions', 'Disease', 'Ecological momentary assessment', 'Educational workshop', 'Emergency department visit', 'Emotional', 'Ensure', 'Environment', 'Factor Analysis', 'Fellowship', 'Foundations', 'Funding', 'Gender', 'Goals', 'Health Care Costs', 'Heterogeneity', 'Impairment', 'Individual', 'Inpatients', 'Intervention', 'Interview', 'Life', 'Machine Learning', 'Mental disorders', 'Methodology', 'Modeling', 'Monitor', 'National Institute of Mental Health', 'Occupational', 'Onset of illness', 'Outcome', 'Outcome Study', 'Outpatients', 'Patients', 'Pattern', 'Pennsylvania', 'Phenotype', 'Prediction of Response to Therapy', 'Process', 'Psychopathology', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Scientist', 'Self Concept', 'Severities', 'Source', 'Structure', 'Students', 'Suicide', 'Surveys', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment outcome', 'Universities', 'Validation', 'Variant', 'Work', 'accomplished suicide', 'career', 'classification algorithm', 'comorbidity', 'cost', 'data archive', 'effective therapy', 'flexibility', 'health care service utilization', 'improved', 'innovation', 'novel', 'personalized intervention', 'personalized medicine', 'personalized predictions', 'prospective', 'prototype', 'psychologic', 'recruit', 'response', 'skills', 'social', 'sound', 'success', 'suicidal risk', 'theories', 'therapy development', 'tool']",NIMH,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,F31,2020,25358,-0.04259136194657286
"A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions Abstract. Melanoma is the third most common form of skin cancer with estimated 87,110 new cases diagnosed in the United States in the year 2017. Current routine diagnostic approaches utilize microscopic evaluation of thinly sectioned patient biopsies, but in certain cases diagnosis can be contentious even among experts. The overall goal of this multi-phase SBIR project is to develop, validate, and commercialize MelanoMap™, Frontier Diagnostics' patented assay for the diagnosis of melanoma using a matrix-assisted laser desorption/ionization imaging mass spectrometry (MALDI IMS) platform—and to have this assay available to pathologists in the U.S. as a laboratory developed test. MALDI IMS is a state-of-the-art technology that generates molecular images of tens to thousands of biomolecules from tissue sections in a single analysis. The assay uses formalin-fixed paraffin embedded (FFPE) biopsies used in routine histopathological diagnosis. The proposed assay has pathologists select regions of skin biopsies for analysis via a remote web interface. The acquired IMS data from those regions unambiguously identifies malignant melanoma or benign nevus.  Phase I of this proposal will demonstrate the feasibility of this technology platform to achieve cost-effective diagnosis of melanoma from patient skin biopsies at sample volumes acceptable for a clinical laboratory. Specific Aim 1 focuses on the development of a scalable and robust analytical protocol in both sample preparation and informatics to accurately diagnose melanoma with MALDI IMS. In specific aim 2, we will test the methodology developed in Specific Aim 1 on a cohort of melanocytic lesions with known clinical outcome and subsequently validate the classification accuracy of the proposed test  In Phase II, the protocols developed in Phase I will be integrated into a diagnostic service workflow. This phase will focus on quality control measures, client facing cloud software, clinical diagnostic reporting, and completing the analysis of a 500-patient sample set for final assay validation. Specific Aim 3 of this proposal (initial aim of Phase II) will establish and implement test tissues into standard workflows that will provide performance metrics for standard operation of a test meeting Clinical Laboratory Improvement Amendments (CLIA) standards. Protocols will be developed to monitor reagents, the reproducibility of sample preparation, and mass spectrometer performance on daily basis. Specific Aim 4 will expand software capabilities to include a secure web interface for clients ordering the test and the laboratory performing the test. The software will meet regulatory compliance, perform statistical analysis, and generate and communicate reports of the MALDI IMS analysis. Specific Aim 5 proposes to expand the sample set used in the initial assay from Specific Aim 2 to include a set of 300 patient samples from our clinical collaborators with 5 or more years follow-up data. The test will be independently validated by an additional 200 patient samples with definitive diagnoses. n/a",A Molecular Diagnostic Assay for Accurately Differentiating Melanoma from Benign Lesions,9926849,R44CA228897,"['Antigens', 'Area', 'Benign', 'Biological Assay', 'Biopsy', 'Caliber', 'Cells', 'Classification', 'Client', 'Clinical', 'Clinical Laboratory Improvement Amendments', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Services', 'Digestion', 'Early Diagnosis', 'Ensure', 'Evaluation', 'Formalin', 'Goals', 'Gold', 'Image', 'Incentives', 'Informatics', 'Laboratories', 'Legal patent', 'Lesion', 'Mass Spectrum Analysis', 'Measures', 'Metadata', 'Methodology', 'Microscopic', 'Microtomy', 'Monitor', 'Nevus', 'Outcome', 'Paraffin Embedding', 'Pathologist', 'Pathology Report', 'Patient Care', 'Patients', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Physical shape', 'Physicians', 'Preparation', 'Procedures', 'Proteins', 'Protocols documentation', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Research', 'Retrieval', 'Sampling', 'Secure', 'Security', 'Sensitivity and Specificity', 'Side', 'Skin', 'Skin Cancer', 'Small Business Innovation Research Grant', 'Software Tools', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Standardization', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Treatment Cost', 'United States', 'Validation', 'accurate diagnosis', 'analytical method', 'base', 'clinical diagnostics', 'cloud software', 'cohort', 'cost', 'cost effective', 'data acquisition', 'diagnosis standard', 'diagnostic assay', 'disease classification', 'follow-up', 'frontier', 'histopathological examination', 'instrumentation', 'interest', 'large datasets', 'machine learning algorithm', 'mass spectrometer', 'meetings', 'melanoma', 'molecular diagnostics', 'molecular imaging', 'mortality risk', 'operation', 'prototype', 'quality assurance', 'skin lesion', 'tissue preparation', 'web interface']",NCI,"FRONTIER DIAGNOSTICS, LLC",R44,2020,944492,-0.01116619087652399
"Development of a web-based platform implementing novel Predictor of Skin Sensitization for Medical Devices (PreSS/MD) PROJECT SUMMARY Medical devices have been documented to contain toxic chemicals that can leach and cause acute contact dermatitis (ACD) after repeated exposure or prolonged contact of the skin to these toxins. ACD is credited for 10-15% of all occupational illnesses and is also the second highest reported occupational hazard. Given its prevalence, ACD is also a great public health burden with combined yearly costs of up to $1 billion, which spans including medical costs, worker’s compensation and lost working time due to workplace absence. To this end, the U.S. Food and Drug Administration has mandated that all medical devices must be evaluated for possible skin sensitization using in vivo animal assays, which includes the Guinea pig maximization test (GPMT). Although GPMT tests provide valuable data on the skin sensitization effects of potential toxins, these assays are time-consuming and expensive. Moreover, the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) recently published a Strategic Roadmap, calling for the development of alternative approaches to reduce animal testing of chemical and medical agents. Thus, there is a stated need to modernize safety evaluation of medical devices to reduce animal testing and shorten the regulatory review time, which would ultimately bring safer devices to the market faster. To address this unmet need, the key objectives of our FDA Phase I SBIR project are to (i) produce rigorously validated computational models for the GPMT assay integrating data obtained in human, mouse, and in vitro assays; and (ii) integrate these models into a software product termed PreSS/MD (Predictor of Skin Sensitization for Medical Devices). Our specific aims for this study include: 1) collecting, curating, and integrating the largest publicly available dataset for GMPT; 2) creating and validating novel computational models for GMPT data; 3) developing the PreSS/MD web server to allow users to make predictions of skin sensitization potential in medical devices. We will also develop a model for mixtures, including compounds tested jointly in different concentrations, using an approach that we developed previously. Finally, we will implement novel approaches to help users of our PreSS/MD platform interpret the developed models in terms of key chemical features responsible for skin sensitization. In addition, we will employ biomedical knowledge graphs to elucidate Adverse Outcome Pathways (AOPs) for skin sensitizers. Successful execution of this Phase I project will yield in the development of PreSS/MD as a centralized resource to evaluate the skin sensitization potential for medical devices. We expect this software-as-a-service web server platform will be of great value for companies and sponsors seeking regulatory approval of medical devices. PROJECT NARRATIVE Given that medical devices have been documented to contain toxic chemicals that may lead to allergic contact dermatitis, the US Food and Drug Administration requires that all devices be evaluated for possible skin sensitization effects using in vivo assays such as the Guinea pig maximization test. In the effort to modernize skin sensitization safety evaluation methods to reduce in vivo animal testing, herein we propose to develop a software product, PreSS/-MD (Predictor of Skin Sensitization caused by Medical Devices), as an innovative and unique in silico alternative with the potential to better predict human response compared to the existing approaches for skin sensitization assessment. Successful execution of the objectives described in this project will result in a centralized web server platform to evaluate the skin sensitization potential for medical devices, which will be of significant value for companies and sponsors seeking regulatory approval of medical devices.",Development of a web-based platform implementing novel Predictor of Skin Sensitization for Medical Devices (PreSS/MD),10079701,R43ES032371,"['Acute', 'Address', 'Advanced Development', 'Allergic Contact Dermatitis', 'Animal Testing', 'Animals', 'Bayesian Method', 'Bayesian Modeling', 'Biological Assay', 'Cavia', 'Chemical Structure', 'Chemicals', 'Computer Models', 'Computer software', 'Consumption', 'Contact Dermatitis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Devices', 'Economics', 'Evaluation', 'Feedback', 'Generations', 'Human', 'Immune response', 'Instruction', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'International', 'Knowledge', 'Lead', 'Medical', 'Medical Care Costs', 'Medical Device', 'Methods', 'Modeling', 'Modernization', 'Mus', 'Occupational', 'Online Systems', 'Pathway interactions', 'Phase', 'Poison', 'Prevalence', 'Prostheses and Implants', 'Public Health', 'Publishing', 'Pythons', 'Quantitative Structure-Activity Relationship', 'Reaction', 'Reporting', 'Resources', 'Safety', 'Skin', 'Small Business Innovation Research Grant', 'Structure', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Toxin', 'United States Food and Drug Administration', 'Validation', 'Workers&apos', ' Compensation', 'Workplace', 'adverse outcome', 'chemical release', 'cost', 'experience', 'in silico', 'in vitro Assay', 'in vivo', 'innovation', 'knowledge graph', 'lymph nodes', 'machine learning algorithm', 'model development', 'novel', 'novel strategies', 'occupational hazard', 'operation', 'phase 1 study', 'response', 'skin patch', 'software as a service', 'success', 'systemic toxicity', 'tool', 'web portal', 'web server']",NIEHS,"PREDICTIVE, LLC",R43,2020,167910,-0.02160730820441509
"Multi-omic networks associated with COPD progression in TOPMed Cohorts PROJECT SUMMARY Chronic obstructive pulmonary disease (COPD) is the fourth leading cause of death in the US. Although COPD occurs predominantly in smokers, it is unknown why only a minority of smokers (~20-40%) develop chronic airflow limitation or destruction of distal airspaces (emphysema). The difference susceptibility to tobacco smoke could be explained by differences in genetics or environment, which lead to activation or repression of pathways that are important in the pathogenesis and progression of COPD. Recent advances in high throughput omics (whole genome sequencing, DNA methylation, RNA-Seq, proteomics and metabolomics) applied to NHLBI cohorts now permits a comprehensive assessment of the molecular profiles of susceptible patients. This proposal will use sparse multiple canonical correlation network analysis (SmCCNet) to integrate these existing -omics data from three NHLBI cohorts: COPDGene, Jackson Heart Study, and SPIROMICS. The three independent cohorts will allow replication of specific molecular networks which can be used to target new therapies or more precise prognostic information to individuals (i.e., precision medicine). The first two of these cohorts have large numbers of African American subjects, who are underrepresented in omics studies. We will perform population specific analyses, which will allow us to determine which molecular signatures and pathways might be specific to African Americans. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is the fourth leading cause of death in the US. To help identify why only some smokers develop COPD, this proposal will integrate recently collected extensive molecular profiles from three NHLBI cohorts (COPDGene, Jackson Heart Study, and SPIROMICS) to discover molecular networks that are important in both diagnosis and progression. In addition to non- Hispanic Whites, by focusing on African Americans, which are highly underrepresented in these types of studies, we expect that there are specific molecular network differences that may lead to specific therapies (i.e., precision medicine) based on race/ethnicity.",Multi-omic networks associated with COPD progression in TOPMed Cohorts,9998187,R01HL152735,"['African American', 'Biology', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Complex', 'Computers', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease Progression', 'Distal', 'Environment', 'Epidemiologist', 'Epigenetic Process', 'Ethnic Origin', 'Genetic', 'Genetic Markers', 'Individual', 'Investigation', 'Jackson Heart Study', 'Knowledge', 'Lead', 'Lung', 'Methods', 'Minority', 'Molecular', 'Molecular Disease', 'Molecular Profiling', 'National Heart, Lung, and Blood Institute', 'Not Hispanic or Latino', 'Pathogenesis', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Proteomics', 'Publications', 'Pulmonary Emphysema', 'Quantitative Trait Loci', 'Race', 'Repression', 'Research', 'Research Design', 'Research Personnel', 'Sample Size', 'Scanning', 'Scientist', 'Smoker', 'Spirometry', 'System', 'Technology', 'Tobacco smoke', 'Trans-Omics for Precision Medicine', 'Transcript', 'Underrepresented Minority', 'Work', 'X-Ray Computed Tomography', 'airway obstruction', 'alpha 1-Antitrypsin', 'base', 'chronic airflow obstruction', 'clinical phenotype', 'cohort', 'disease phenotype', 'genetic variant', 'genome sequencing', 'improved', 'metabolomics', 'multidisciplinary', 'multiple omics', 'new therapeutic target', 'novel', 'novel diagnostics', 'precision medicine', 'prognostic', 'protein metabolite', 'single molecule', 'soluble RAGE', 'transcriptome sequencing', 'transcriptomics', 'unsupervised learning', 'whole genome']",NHLBI,UNIVERSITY OF COLORADO DENVER,R01,2020,824479,-0.027579836250454792
"Mechanoresponsive Engrailed-1-negative fibroblasts activate Engrailed-1 to promote fibrosis in wound healing 7. Project Summary/Abstract Adult human skin heals by developing fibrotic scar tissue, which can result in devastating disfigurement, growth restriction, and permanent functional loss. Despite a plethora of clinical options, no current treatment strategies successfully prevent or reverse this fibrotic process, and scars and their sequelae cost the United States over $20 billion every year. Progress towards the development of new therapies has been significantly hindered by a lack of understanding of the specific cell populations responsible for scarring. In 2015, our group reported that Engrailed-1 (En-1) lineage-positive fibroblasts (EPFs) are responsible for the vast majority of dorsal scar production in postnatal mice. In early fetal gestation, mice heal scarlessly via skin regeneration, an ideal outcome mediated by En-1 lineage-negative fibroblasts (ENFs; the predominant fetal fibroblast). However, it has not been established if ENFs contribute to postnatal wound healing. In this proposal, we explore for the first time the postnatal conversion of ENFs to pro-fibrotic EPFs (postnatally-derived EPFs; pEPFs) within the wound environment. First, histology, immunohistochemistry, and wounding in a novel transgenic mouse model will be used to study the conversion of ENFs to pEPFs during wound healing. By examining the behavior of ENF subpopulations (derived from papillary dermis, reticular dermis, and hypodermis) in the wound environment and confirming our findings in a tamoxifen-inducible mouse model of En-1 activation, we will precisely define the ENF population that gives rise to pro-fibrotic pEPFs. Second, we will establish the specific wound environment cues that drive ENF-to-EPF transition. Given that mechanical forces are known to modulate both scar burden and fibroblast activity, we will use in vitro and in vivo models to examine the effects of mechanical environment on En-1 activation. We will further use transcriptomic and epigenomic profiling to explore the role of mechanotransduction signaling in ENF-to-EPF transition and pEPF function. Third, having established a mechanotransduction mechanism underlying En-1 activation in wound ENFs, we will inhibit mechanotransduction signaling with the goal of blocking ENF-to-EPF transition. Specifically, we will assess whether blocking mechanotransduction results in ENF-mediated wound healing with reduced fibrosis. Our ultimate translational goal is to develop therapeutics that target fibrogenic fibroblasts to promote regenerative healing. Collectively, the proposed work will significantly enhance our understanding of the key molecular and cellular determinants of cutaneous scarring, inform the development of novel anti-scarring therapies, and shed light on the cellular origin of dermal scarring fibroblasts. 8. Project Narrative Scarring is the end result of injury in adult human skin and results in an enormous financial and medical burden for our society. There are currently no effective molecular therapies that prevent scarring or its sequelae, and development of therapeutics has been hindered by lack of understanding of the precise cell populations that mediate fibrosis in wound healing. Therefore, we propose to explore the contribution of a specific fibroblast subpopulation (Engrailed-1 lineage-negative fibroblasts; ENFs) in fibrotic wound healing, in order to inform novel directions for targeted treatments that minimize scarring and promote regenerative wound healing.",Mechanoresponsive Engrailed-1-negative fibroblasts activate Engrailed-1 to promote fibrosis in wound healing,9933446,R01GM136659,"['3-Dimensional', 'Adult', 'Algorithms', 'Anatomic Surface', 'Behavior', 'Cells', 'Cellular Assay', 'Characteristics', 'Chemicals', 'Chromatin', 'Cicatrix', 'Clinical', 'Collagen', 'Connective Tissue', 'Cues', 'Cultured Cells', 'Cutaneous', 'Data', 'Dermal', 'Dermis', 'Development', 'Dipeptidyl-Peptidase IV', 'Dorsal', 'Elements', 'Engraftment', 'Environment', 'Extracellular Matrix', 'Fiber', 'Fibroblasts', 'Fibrosis', 'Fluorescence-Activated Cell Sorting', 'Focal Adhesion Kinase 1', 'Genetic Transcription', 'Goals', 'Growth', 'Hair follicle structure', 'High-Throughput Nucleotide Sequencing', 'Histologic', 'Histology', 'Hydrogels', 'Immunohistochemistry', 'In Vitro', 'Individual', 'Injury', 'Light', 'Maps', 'Measures', 'Mechanical Stress', 'Mechanics', 'Mediating', 'Medical', 'Microscopy', 'Molecular', 'Morbidity - disease rate', 'Mus', 'Outcome', 'Papillary', 'Pathway interactions', 'Population', 'Pregnancy', 'Process', 'Production', 'Protein Inhibition', 'Proteins', 'Reporting', 'Role', 'Signal Pathway', 'Signal Transduction', 'Site', 'Skin', 'Societies', 'Specimen', 'Subcutaneous Tissue', 'Surface', 'Tamoxifen', 'Time', 'Tissues', 'Transgenic Mice', 'Transposase', 'United States', 'Verteporfin', 'Visual', 'Wild Type Mouse', 'Work', 'analog', 'base', 'cost', 'digital', 'epigenomics', 'experimental study', 'fetal', 'functional loss', 'healing', 'in vivo', 'in vivo Model', 'inhibitor/antagonist', 'machine learning algorithm', 'mechanical force', 'mechanotransduction', 'mortality', 'mouse model', 'novel', 'novel therapeutics', 'postnatal', 'prevent', 'reconstruction', 'regenerative', 'response', 'single-cell RNA sequencing', 'skin regeneration', 'skin wound', 'therapeutic development', 'therapeutic target', 'tissue culture', 'tool', 'transcriptomics', 'treatment strategy', 'wound', 'wound bed', 'wound environment', 'wound healing']",NIGMS,STANFORD UNIVERSITY,R01,2020,317823,-0.03212473350551171
"Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study PROJECT SUMMARY ABSTRACT The overarching goals of this R01 proposal are to improve scientific understanding of potential mechanisms by which ophthalmic diseases lead to the risk of Alzheimer’s disease. The investigators will leverage modern ophthalmic data with state-of-the-art imaging and extensive archived clinical data from a well-characterized cohort of older adults. The investigators propose to examine the effect of structural and functional changes in retina and longitudinal severity of ophthalmic diseases on Alzheimer’s disease and related neuropathology. The proposal builds on the resources of the Adult Changes in Thought (ACT) study, a prospective longitudinal, population-based, dementia-free cohort of over 5,500 people to date established in 1994 which has detected >1,014 research quality diagnoses of Alzheimer’s disease and >1,254 dementia to date. ACT follows consenting participants to autopsy and has performed state-of-the arts autopsy on >781 decedents to date. In this extremely well-characterized cohort, the investigators found that several ophthalmic diseases (diabetic retinopathy, glaucoma, age-related macular degeneration) are significantly associated with the risk of developing Alzheimer’s disease. The investigators will use three advanced ophthalmic imaging modalities at both home and clinical research study visits: fundus photography, optical coherence tomography (OCT), and OCT angiography (OCTA), to obtain quantitative data relevant to these ophthalmic diseases. The study team will establish the distribution (Aim 1a) and 2- and 4-year evolution of ophthalmic imaging characteristics found in older adults in the community and determine associations with change in cognition (Aim 1 b, c). Additionally, magnetic resonance imaging (MRI) and MRI angiography (MRA) will be obtained in a subset of participants to investigate the contribution of small (retinal) and large (cerebral) vascular disease towards cognitive changes (Aim 1d). The study team will continue ACT study’s strong commitment for meaningful data sharing. In collaboration with the Laboratory of Neuro Imaging at University of Southern California, the study team will promulgate these ophthalmic data in addition to neuroimaging data to the research community (Aim 1e). In Aim 2, the investigators will use extensive clinical ophthalmology data captured over many decades and incorporate them in novel longitudinal models of eye disease severity. The investigators will analyze eye disease severity along with extensive neuropathology data from the ACT study, including both standard (Aim 2a) and novel quantitative (Aim 2b) neuropathology data, to further scientific understanding of neuropathological mechanisms underlying associations between eye conditions and Alzheimer’s disease risk. The brain is not amenable to direct observations during life. In contrast, the eye is an anterior extension of the central nervous system and may provide a valuable window to illuminate neurodegenerative processes in the aging brain. Proposed investigations will substantially enhance scientific understanding of the role of modern ophthalmic evaluations in delineating risk of Alzheimer's disease and other forms of neuropathology. PROJECT NARRATIVE Using a large, well-characterized, longitudinal, prospective, cohort study, the study team previously found that diabetic retinopathy, glaucoma, and age-related macular degeneration were significantly associated with Alzheimer’s disease risk. The team proposes to use three cutting edge ophthalmic imaging modalities to obtain quantitative data at both home and clinic research study visits in addition to MRI and MRA in a subset of participants to evaluate their associations with change in cognition over time (Aim 1). The team will leverage extensive ophthalmic clinical and neuropathological data already available for 781 study participants to date as well as new state-of-the-art quantitative measures of beta amyloid (Aβ1-42) and phosphorylated tau to elucidate mechanisms underlying associations between ophthalmic conditions and Alzheimer’s disease (Aim 2).",Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study,10005108,R01AG060942,"['Adult', 'Age related macular degeneration', 'Age-associated memory impairment', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease risk', 'Amyloid beta-Protein', 'Angiography', 'Anterior', 'Archives', 'Autopsy', 'Bayesian Modeling', 'Biological Markers', 'Blood Vessels', 'Brain', 'California', 'Cerebrovascular Disorders', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognition', 'Collaborations', 'Communities', 'Consent', 'Data', 'Data Collection', 'Dementia', 'Diabetic Retinopathy', 'Diagnosis', 'Disease', 'Disease model', 'Drusen', 'Elderly', 'Evaluation', 'Evolution', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Home environment', 'Image', 'Impaired cognition', 'Investigation', 'Laboratories', 'Lead', 'Life', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Microvascular Dysfunction', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Nerve Fibers', 'Neuraxis', 'Occipital lobe', 'Ophthalmology', 'Optical Coherence Tomography', 'Participant', 'Pathology', 'Perfusion', 'Persons', 'Predisposition', 'Process', 'Prospective cohort study', 'Provider', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Retina', 'Risk', 'Role', 'Scanning', 'Selection Bias', 'Series', 'Severities', 'Severity of illness', 'Structure', 'Technology', 'Testing', 'Time', 'Universities', 'Vascular Diseases', 'Visit', 'aging brain', 'area striata', 'automated algorithm', 'base', 'cognitive change', 'cohort', 'data sharing', 'deep learning', 'diagnosis quality', 'epidemiology study', 'fiber cell', 'follow-up', 'high risk', 'imaging biomarker', 'imaging modality', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuropathology', 'novel', 'paired helical filament', 'population based', 'prospective', 'public repository', 'research study', 'resilience', 'tau Proteins', 'tau-1', 'vascular contributions']",NIA,UNIVERSITY OF WASHINGTON,R01,2020,3349913,-0.08578881802790787
"A Multiwell Plate Format Microfluidic Immobilization Chip for High-Content Imaging of Whole Animals for in vivoNeurotoxicology Testing Project Summary: Neurotoxicological evaluation of new compounds intended for human use or of potential human exposure is mandated by international regulatory bodies and largely relies on lethality testing in higher-order vertebrate animals. High screening costs, long experimental times, and legislative requirements to reduce dependence on animal testing have led many industries to search for alternative technologies. In vitro toxicology testing uses isolated cells or monotypic cell culture and can only provide limited insight since these models lack biologically relevant intact multi-typic cellular network structures. While both technologies have been augmented by in silico technologies, there is still a non-trivial gap between what can be learned and translated from simple, fast, inexpensive in vitro methods versus longer, complex, and costly in vivo studies in higher order animals. Newormics’ approach to filling this gap is to enable in vivo neurotoxicological assessment in Caenorhabditis elegans, an accepted alternative invertebrate model organism, by developing neuron-specific toxicity assays, delivered via a proprietary high-density, large-scale microfluidic immobilization device for high-content, high throughput analysis. Building on advances made during Phase I and important market learnings from participation in the NIH I-Corps program, Phase II proposes several new elements of innovation to achieve our goals in 3 specific aims. In Aim 1, we will convert our first-generation microfluidic device to a high-density (384- well) vivoChip with improved microfabrication technologies, incorporate on-chip culture for transfer-less exposure and testing, and integrate automation for chip loading, imaging, and analysis. These measures will significantly increase test scale (from 80 compounds per chip to 280) and lower the consumable and labor costs per test. In Aim 2, building on our dopaminergic neurotox assay from Phase I, we will develop four neurotox assays with brightly fluorescently labeled dopaminergic, serotonergic, GABAergic, and cholinergic neurons providing the unprecedented ability to assess subtle phenotypic effects of chemicals on individual intact, functional neurons. To achieve real-time image processing, multi-parameter phenotyping, and managing the terabytes of image data generated per test, we will build a computational platform empowered by a graphic user interface. This platform will be used for image compilation, user-annotated phenotype definition and scoring, and automated report generation with appropriate statistical analysis. In Aim 3, with our industry partners, we will validate our platform and assays using reference chemicals. As more chemicals are tested, we will build a database which can be further mined. The outcome of this work will enable many industries to reduce lethal animal testing and get safer industrial and personal consumer products to market faster for economic benefit, reaching regulatory compliance for reduced animal use, and improved healthcare for neurological diseases. Narrative: The proposed work will enable the use of a small invertebrate model organism, C. elegans, for neuron-specific analysis of neurodegeneration phenotypes from high-resolution fluorescence images of individual neurons at high throughputs. The proposed imaging system and the neuron-specific assays will provide an unprecedented ability to assess developmental neurotoxicity in an intact, live, whole organism, down to a neuronal mechanism- of-action level. This project will fill the gap between in vitro and in vivo toxicology testing with an effective invertebrate model organism as alternate to vertebrate animal testing.",A Multiwell Plate Format Microfluidic Immobilization Chip for High-Content Imaging of Whole Animals for in vivoNeurotoxicology Testing,10082215,R44MH118841,"['3-Dimensional', 'Address', 'Animal Model', 'Animal Testing', 'Animals', 'Automation', 'Biological', 'Biological Assay', 'Caenorhabditis elegans', 'Cell Culture Techniques', 'Cells', 'Chemicals', 'Complement', 'Complex', 'Computer software', 'Data', 'Databases', 'Dendrites', 'Dependence', 'Development', 'Devices', 'Dopamine', 'Eating', 'Economics', 'Elements', 'Evaluation', 'Exposure to', 'Feedback', 'Future', 'Gel', 'Generations', 'Goals', 'Healthcare', 'Hour', 'Human', 'Image', 'Immobilization', 'In Vitro', 'Individual', 'Industrialization', 'Industry', 'Industry Standard', 'Innovation Corps', 'International', 'Invertebrates', 'Label', 'Learning', 'Liquid substance', 'Machine Learning', 'Manuals', 'Market Research', 'Measures', 'Methods', 'Microfabrication', 'Microfluidic Microchips', 'Microfluidics', 'Modeling', 'Nerve Degeneration', 'Nervous system structure', 'Neurons', 'Neurotransmitters', 'Organoids', 'Outcome', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Process', 'Protocol Compliance', 'Protocols documentation', 'Reporting', 'Research Activity', 'Resolution', 'Robotics', 'Serotonin', 'Services', 'Specificity', 'Speed', 'Statistical Data Interpretation', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicology', 'Translating', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebrates', 'Whole Organism', 'Work', 'base', 'cholinergic neuron', 'computational platform', 'connectome', 'consumer product', 'cost', 'cost effective', 'density', 'design', 'developmental neurotoxicity', 'developmental toxicity', 'empowered', 'experimental study', 'exposed human population', 'fluorescence imaging', 'gamma-Aminobutyric Acid', 'graphical user interface', 'high resolution imaging', 'high throughput analysis', 'image processing', 'imaging platform', 'imaging system', 'improved', 'in silico', 'in vitro Model', 'in vivo', 'in vivo Model', 'industry partner', 'innovation', 'insight', 'interest', 'nervous system disorder', 'neurotoxicity', 'neurotoxicology', 'programs', 'real-time images', 'reproductive toxicity', 'screening', 'small molecule libraries', 'success', 'terabyte', 'testing services', 'young adult']",NIMH,"NEWORMICS, LLC",R44,2020,749858,-0.02373226414064548
"Modeling and Assessment of Maternal Uteroplacental Circulation During Pregnancy ABSTRACT In this proposal I am interested in developing a computational fluid dynamic (CFD) modeling approach for volumetric assessment of human uteroplacental blood flow in vivo. The placenta is an organ that exchanges nutrients and oxygen between the maternal circulation and the growing fetus. In the United States, about 3.4% of pregnancies per year are affected by hypertensive pregnancy disorders (HPD) such as preeclampsia (PE), which have been found to carry both fetal and maternal risk. Associated placental pathologies are believed to be linked to alterations in maternal arterial remodeling. Currently, Doppler ultrasound (US) is the primary method of assessing flow to the placenta from the uterine artery (UtA). Clinical studies have shown a relationship between high UtA flow resistance and risk of adverse pregnancy outcome late in gestation, but it has not been reliable as an early gestation screening tool. In order to improve predictive technologies for reliable risk assessment assessment of HPD, more research into the relationship between vessel structure and function in the UtA is needed. I hypothesize that CFD modeling can be a useful tool for investigating possible pathophysiological mechanisms of HPD by simulating complex hemodynamics of the maternal vascular system including pressure, wall shear stress, and pulse wave velocity. I plan to set up various 1D CFD simulations to understand the hemodynamic parameters of normal versus abnormal pregnancies. Then, I will validate the simulations with 4D flow MRI acquired from an in vitro flow phantom and a cohort of normal and hypertensive pregnant women. I anticipate that the results of this investigation can advance scientific knowledge regarding the progression of early HPD phenotypes to adverse pregnancy outcomes. This CFD study can also demonstrate the extent to which patient-specific hemodynamic simulations can be reliable for future improvement of clinical management. This training experience will provide opportunities to build my expertise in cardiovascular physiology, fluid mechanics, medical imaging, data analysis, and clinical care. I will be publishing articles on my CFD/4D flow MRI findings and communicating the impact of this work in medicine at various internal and external symposia. This research will be conducted under the mentorship of award-winning experts and the University of Pennsylvania in cardiovascular MRI (Walter Witschey), fluid mechanics and computational modeling (Paris Perdikaris), maternal fetal medicine (Nadav Schwartz) and cardiovascular physiology (Victor Ferrari). PROJECT NARRATIVE The human placenta is an understudied organ believed to play a critical role in pregnancy health and the pathophysiology of hypertensive pregnancy disorders. Current technologies like ultrasound are limited in its ability to assess complex hemodynamics, and clinical studies have not yet found reliable biomarkers of adverse pregnancy outcomes. We will develop a computational fluid dynamic model of the maternal vessels delivering blood to the placenta and validate it with magnetic resonance imaging to characterize uteroplacental structure and function, providing future avenues for new predictive biomarkers of pregnancy health.",Modeling and Assessment of Maternal Uteroplacental Circulation During Pregnancy,9991192,F31HD100171,"['3-Dimensional', '4D MRI', 'Abdomen', 'Affect', 'Agreement', 'American College of Obstetricians and Gynecologists', 'Angiography', 'Arteries', 'Award', 'Bayesian Analysis', 'Biological Markers', 'Blood', 'Blood Circulation', 'Blood Vessels', 'Blood flow', 'Cardiovascular Physiology', 'Cardiovascular system', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computer Models', 'Data', 'Data Analyses', 'Development', 'Disease', 'Doppler Ultrasound', 'Early Diagnosis', 'Electric Capacitance', 'Engineering', 'Environment', 'Fetus', 'Functional disorder', 'Future', 'Geometry', 'Goals', 'Health', 'Heterogeneity', 'Human', 'In Vitro', 'Intervention', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Link', 'Liquid substance', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maternal-fetal medicine', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Navier–Stokes equations', 'Nutrient', 'Organ', 'Outcome', 'Oxygen', 'Paris, France', 'Pathologic', 'Pathology', 'Patients', 'Pennsylvania', 'Phenotype', 'Physics', 'Physiologic pulse', 'Physiological', 'Physiology', 'Placenta', 'Placenta Diseases', 'Placentation', 'Play', 'Pre-Eclampsia', 'Predictive Value', 'Pregnancy', 'Pregnancy Outcome', 'Pregnant Women', 'Property', 'Publishing', 'Radiology Specialty', 'Reporting', 'Reproducibility', 'Research', 'Resistance', 'Resolution', 'Risk', 'Risk Assessment', 'Role', 'Scientific Advances and Accomplishments', 'Scientist', 'Screening procedure', 'Structure', 'Techniques', 'Technology', 'Testing', 'Texture', 'Third Pregnancy Trimester', 'Training', 'Tube', 'Ultrasonography', 'Uncertainty', 'United States', 'Universities', 'Uteroplacental Circulation', 'Uterus', 'Variant', 'Vascular System', 'Velocimetries', 'Woman', 'Work', 'adverse pregnancy outcome', 'arterial remodeling', 'base', 'cardiovascular imaging', 'clinical care', 'cohort', 'disorder risk', 'experience', 'falls', 'fetal', 'hemodynamics', 'high risk', 'imaging approach', 'improved', 'in vivo', 'insight', 'interest', 'maternal risk', 'neural network', 'non-invasive imaging', 'phantom model', 'predictive marker', 'pregnancy disorder', 'pressure', 'screening', 'shear stress', 'simulation', 'spatiotemporal', 'symposium', 'tool']",NICHD,UNIVERSITY OF PENNSYLVANIA,F31,2020,45520,-0.022182253446900033
"Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD Overall Summary The overarching goal of the proposed center is to leverage molecular and circuit biomarkers to advance the understanding of mechanisms and personalized treatment of topiramate treatment of Alcohol Use Disorder comorbid with PTSD. We propose an integrative translational focus on alterations in excitatory and inhibitory signaling, focusing on GABA and glutamate and related circuitry, to model the neurobiology of PTSD comorbid with PTSD and the mitigating effects of topiramate. We will characterize excitatory and inhibitory molecular markers in an animal model of AUD comorbid with PTSD, utilizing genomic markers in the brain and plasma markers in rodents. In clinical trial participants we will characterize excitatory and inhibitory neuronal signaling by ascertaining plasma markers, GRIK 1 genotype and neural circuit markers utilizing TMS evoked potentials in EEG, task-based functional MRI and MR spectroscopy. This goal will be achieved through the activities of three research projects supported by two research cores, the administrative core and the Scientific Advisory Board (Figure1). In Project 1 lead by Silvia Fossati Ph.D. and Jorge Manzanares Robles Ph.D. we will study the behavioral and molecular effects of two doses of topiramate vs. vehicle in animal models of AUD alone, PTSD alone and AUD+PTSD. In Project 2 lead by Michael Bogenschutz M.D. and Joshua Lee M.D. we will study the behavioral, genetic and plasma biomarker effects of topiramate vs. placebo in 150 participants with co-occurring AUD and PTSD. In project 3 lead by Amit Etkin M.D., Ph.D. and Charles R. Marmar M.D. we will ascertain multi-modal imaging markers including task based fMRI, TMS evoked potentials in EEG and MRS. Imaging markers will be used to characterize excitatory and inhibitory circuits in Project 2 clinical trial participants with AUD+PTSD to determine predictors and mechanisms of topiramate vs. placebo treatment outcomes. Plasma biomarkers in Project 2 will be related to the same or homologous plasma biomarkers in Project 1. Circuit markers from Project 3 will be related to genomic markers in the same or homologous brain regions in Project 1. The Biofluids Biomarker Core (BBC) lead by Dr. Fossati will support collection of plasma biomarkers (GABA, glutamate, HPA axis, neuropeptides, neuroinflammatory and oxidative stress) in animals in Project 1 and clinical trial participants in Project 2. The Analytics and Biostatistics Core (ABC) lead by Eugene Laska Ph.D. and Carole Segal Ph.D. will support experimental design, formulation of hypothesis, power calculations, and data integrity, management and analysis for Project 1, 2 and 3, implementing advanced statistical models for individualized prediction of response to topiramate in Project 1 and Project 2. Overall Narrative We aim to advance personalized topiramate treatment for comorbid AUD and PTSD by focusing on network excitatory and inhibitory imbalances underlying heightened negative emotions and impairments in executive function and emotion regulation. Excitatory and inhibitory targets will be assessed with brain, plasma and circuit markers, where possible harmonizing the approach for optimizing discovery of mechanisms and prediction of topiramate treatment in animal models and in clinical trial participants. We will achieve this aim with a center driven approach to discovery with three integrative design elements and four cross-project center aims, creating translational bridges among animal model, clinical trial and imaging studies.",Leveraging biomarkers for personalized treatment of alcohol use disorder comorbid with PTSD,9996319,P01AA027057,"['Affect', 'Aftercare', 'Alcohols', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biological', 'Biological Markers', 'Biostatistics Core', 'Blood', 'Brain', 'Brain region', 'Clinical', 'Clinical Treatment', 'Clinical Trials', 'Cognition', 'Cognitive', 'Collection', 'Complement', 'Data Analytics', 'Disease model', 'Doctor of Medicine', 'Doctor of Philosophy', 'Dose', 'Electroencephalography', 'Elements', 'Emotional', 'Emotions', 'Equilibrium', 'Evoked Potentials', 'Experimental Designs', 'Formulation', 'Fostering', 'Freezing', 'Functional Magnetic Resonance Imaging', 'Gene Expression', 'Genetic', 'Genotype', 'Glutamates', 'Goals', 'Human', 'Impairment', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance Spectroscopy', 'Measures', 'Mediating', 'Mediation', 'Mission', 'Modeling', 'Molecular', 'Multimodal Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurobiology', 'Neurocognitive', 'Neurons', 'Neuropeptides', 'Outcome', 'Oxidative Stress', 'Participant', 'Patients', 'Pharmacology', 'Placebos', 'Plasma', 'Post-Traumatic Stress Disorders', 'Prediction of Response to Therapy', 'Randomized', 'Randomized Clinical Trials', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Rodent', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Stimulus', 'Structure', 'Symptoms', 'Testing', 'Transcranial magnetic stimulation', 'Translations', 'Treatment outcome', 'Work', 'alcohol abuse therapy', 'alcohol comorbidity', 'alcohol cue', 'alcohol use disorder', 'base', 'behavioral study', 'clinical imaging', 'clinical trial participant', 'clinically relevant', 'cognitive neuroscience', 'comorbidity', 'data integrity', 'data management', 'design', 'drinking', 'dual diagnosis', 'effective therapy', 'emotion regulation', 'executive function', 'gamma-Aminobutyric Acid', 'genomic biomarker', 'hypothalamic-pituitary-adrenal axis', 'imaging biomarker', 'imaging study', 'molecular marker', 'mortality', 'mouse model', 'multimodality', 'neural circuit', 'neurobiological mechanism', 'neuroinflammation', 'neurophysiology', 'neurotransmission', 'personalized medicine', 'personalized predictions', 'pre-clinical', 'precision medicine', 'predicting response', 'predictive modeling', 'protein expression', 'response', 'source localization', 'topiramate', 'treatment comparison', 'treatment effect', 'treatment response']",NIAAA,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P01,2020,1202772,-0.03493234528078444
