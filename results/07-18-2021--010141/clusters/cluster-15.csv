text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9771473,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Hazard Models', 'Health system', 'Individual', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning algorithm', 'learning strategy', 'multimodality', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'prognostic', 'repository', 'response', 'risk prediction model', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2019,251983,0.03674214677760673
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9743225,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,528283,0.061313337152203495
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9803774,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2019,631809,0.018566726392329227
"IEEE International Symposium on Biomedical Imaging Project Summary This proposal requests funds to provide travel support for graduates to attend and participate in the IEEE Inter- national Symposium on Biomedical Imaging (ISBI) 2019 conference, Venice, Italy on April 08-11, 2019. The main objective of the IEEE ISBI is to bring together researchers with interests in mathematical and computa- tional aspects of biomedical imaging, with a focus on addressing problems of significance to the development and application of imaging systems across spatial scales, from microscopy to whole-body imaging. ISBI partici- pants – on the order of 600-700 from across the world are involved in biomedical imaging research and development in academic institutions, government laboratories, or R&D departments of private companies. ISBI is co-sponsored by two IEEE societies: Signal Processing Society (SPS) and Engineering in Medicine and Biology Society (EMBS), representing academia, industry, and healthcare, and considered the world's foremost societies in biomedical engineering and imaging. SPS and EMBS publish IEEE Transactions on Medical Imag- ing, Transactions on Image Processing, Transactions on Biomedical Engineering, Transactions on Computational Imaging, and IEEE Journal of Biomedical and Health Informatics, among others. Since incep- tion in 2002, ISBI has become the leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas across imag- ing modalities and scales. Conference topics include physical, biological and statistical modeling, image formation and reconstruction, computational and statistical image analysis, visualization and image quality as- sessment, and artificial intelligence and machine learning for big image data. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of a 4-page paper. Peer reviews are handled by a 50-mem- ber editorial board (area editors) of leading experts in the community, who in turn assign papers to well- qualified reviewers. All oral and poster papers are published in IEEExplore as Proceedings of ISBI. If awarded, IEEE anticipates the primary impact of this R13 grant will be increased attendance of U.S.-based students, postdoctoral fellows, and early career faculty. By offering to cover a significant portion of attendee's travel expenses, the cost-benefit ratio for attending ISBI 2019 will be extremely favorable. Furthermore, IEEE will award travel grants based on need and scientific excellence, creating opportunities for those early career researchers who have accepted papers (of which less than 50% are accepted to ISBI) and who have limited means to travel. IEEE will be particularly supportive in providing travel awards to women, under-represented groups, and persons with disabilities. Benefits can largely be summarized as “exposure” and education. ISBI provides opportunity for student exposure to many more areas of computational imaging research than generally available in her/his home institution, and concurrently provides opportunity for students to interact with leaders in the field through tutorials, plenary, oral, and poster presentations, and individual discussions. This proposal requests funds to provide travel support for graduate to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2019 conference, to be held in Venice, Italy on April 08-11, 2019. The main objective of the IEEE international Symposium on Biomedical Imaging is to bring together researchers with interests in the mathematical and computational aspects of biomedical imaging, with a focus on addressing problems of significance to the development and application of imaging systems across spatial scales, from microscopy to whole-body imaging. The conference covers biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institute of Health.",IEEE International Symposium on Biomedical Imaging,9685477,R13EB027566,"['Academia', 'Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological Models', 'Biology', 'Biomedical Computing', 'Biomedical Engineering', 'Breeding', 'Budgets', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Costs and Benefits', 'Data', 'Development', 'Disabled Persons', 'Education', 'Engineering', 'Exposure to', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Imaging problem', 'Individual', 'Industry', 'Institution', 'International', 'Italy', 'Journals', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medical Imaging', 'Medicine', 'Methodology', 'Microscopic', 'Microscopy', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Request for Proposals', 'Research', 'Research Personnel', 'Series', 'Societies', 'Statistical Models', 'Students', 'System', 'Training', 'Transact', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Woman', 'authority', 'base', 'biocomputing', 'bioimaging', 'biomedical informatics', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'editorial', 'graduate student', 'image processing', 'imaging modality', 'imaging system', 'innovation', 'interest', 'meetings', 'member', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2019,10000,0.009105494538537632
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,0.015642063717755488
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.0036692113490967107
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,0.008806206095835371
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,0.0034601161373888855
"Training the next generation of leaders in biomedical engineering design Project Summary/Abstract The next generation of bioengineering and biomedical researchers will have unprecedented access to technologies including wireless health, big data, genetic sequencing, and machine learning approaches to enable modern diagnostic and therapeutic techniques. This presents individuals trained at the interface of technology and biomedicine with an enormous opportunity to address the world’s needs in health and medicine. In the Bioengineering Department at the University of California, Los Angeles, we aim to develop students into leaders able to seamlessly identify clinical needs that technology can address, design and validate solutions that address these needs, communicate with a variety of stakeholders to build teams invested in problem-oriented solutions, and to navigate the regulatory and commercial pathways necessary to enable their technologies to thrive. The Bioengineering Capstone Series at UCLA leverages resources available at UCLA to enable students to: 1) gain insight into clinical needs directly from clinicians and educators across the Ronald Reagan Medical Center, David Geffen School of Medicine, School of Dentistry and UCLA Health System, 2) design their solutions through mentorship from engineering professors, 3) understand the complexities of the biomedical industry with support from the UCLA Technology Development Group and members of the Department of Engineering Industry Advisory Board, 4) utilize modern technologies in wireless health and data science by collaborating with the Center of Excellence for Mobile Sensor Data- to-Knowledge (MD2K) and Mobile Health (mHealth) Institute at UCLA, and 5) work with the National Science Foundation Precise Advanced Technologies and Health Systems for Underserved Populations Engineering Research Center (NSF PATHS- UP ERC) to learn to target and communicate their technologies to maximize societal benefits. Statement of Public Health Relevance: To prepare the next generation of engineers with skills in the design of therapeutics and medical devices, this research education program will provide bioengineering students at UCLA with enhanced opportunities to engage in real world design for biomedical applications. We will guide students through the medical design process from identifying needs, creating solutions which address these needs, and communicating the significance of their contributions to the greater community, ultimately yielding a larger pool of well-trained engineers to address biomedical challenges. !",Training the next generation of leaders in biomedical engineering design,9709138,R25EB027626,"['Address', 'Applied Research', 'Area', 'Big Data', 'Biomedical Engineering', 'Biomedical Technology', 'California', 'Clinic', 'Clinical', 'Communication', 'Communities', 'Data', 'Data Science', 'Device or Instrument Development', 'Devices', 'Diagnostic', 'Educational workshop', 'Engineering', 'Event', 'Faculty', 'Fostering', 'Foundations', 'Freedom', 'Future', 'Genetic', 'Goals', 'Health', 'Health Sciences', 'Health Technology', 'Health system', 'Healthcare', 'High School Student', 'Home environment', 'Individual', 'Industry', 'Infrastructure', 'Institutes', 'Instruction', 'Intellectual Property', 'Investments', 'Laboratories', 'Learning', 'Los Angeles', 'Machine Learning', 'Medical', 'Medical Device', 'Medical center', 'Medicine', 'Mentorship', 'Modernization', 'Pathway interactions', 'Patient Monitoring', 'Physiological', 'Process', 'Recommendation', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'STEM field', 'School Dentistry', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics Education', 'Series', 'Societies', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Training', 'Translations', 'Travel', 'Underrepresented Students', 'Underserved Population', 'Universities', 'Wireless Technology', 'Work', 'base', 'career', 'clinically actionable', 'cloud based', 'commercialization', 'data to knowledge', 'deep learning', 'design', 'education research', 'engineering design', 'experience', 'insight', 'interest', 'lectures', 'mHealth', 'medical schools', 'member', 'multidisciplinary', 'next generation', 'outreach', 'product development', 'professor', 'programs', 'public health relevance', 'sensor', 'skills', 'technology development', 'undergraduate student']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R25,2019,21600,0.030198009590237176
"Transforming Analytical Learning in the Era of Big Data PROJECT SUMMARY In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio)statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field of health big data. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offers and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will primarily draw from the expertise and experience of faculty from three different departments within three different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science, Medicine, Population Health, Social and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including statistical modeling, data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high- dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and medical imaging. The diseases and conditions they study include obesity, diabetes, cardiovascular disease, cancer, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and success with the current summer program on Big Data led by this team with 164 students trained in the last 4 years (2015-2018) including 90 female students and 30 students from underrepresented minority groups. Fourteen of these participants from the last three years are currently graduate students in Michigan Biostatistics. The ongoing program has gained traction in the national landscape of summer research programs with 20% rate of admission and 80% rate of acceptance among those who are offered this opportunity. The program has consistently received very strong evaluation and our past alumni have become brand ambassadors and advocates for our program. We plan to build on the success and legacy of this program in the next three year funding cycle of this grant (2019-2021). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 45 undergraduates nationally and internationally, with 20 domestic students supported by the requested SIBS funding mechanism and others supported by supplementary institutional and foundation support. We propose to expose the trainees to diverse techniques, skills and problems in the field of health Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. They will engage in mentored research projects in three primary areas of health big data: Electronic Health Records/Medical Claims, Genomics and Imaging. Some of the projects will be defined in the area of cardiovascular precision medicine, defined by a team of highly quantitative researchers engaged in cardiovascular research that uses big data. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by U-M researchers, outside guests and a professional development workshop to prepare the students for graduate school. We propose an inter-SIBS collaboration with Dordt College summer program trainees who will attend this concluding symposium. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. We will offer multiple professional development opportunities and resources for graduate school preparation to our trainees so that they can reflect and plan beyond their senior year. All of our proposed activities are reflected through our three specific aims: Teaching, Mentoring and Dissemination. PROJECT NARRATIVE We propose a six week long undergraduate summer institute: “Transforming Analytical Learning in the Era of Big Data” to be held at the Department of Biostatistics, University of Michigan (U-M), Ann Arbor, with a group of approximately 45 undergraduate students recruited nationally and internationally, from 2019-2021. Funding is requested for 20 domestic students with supplementary funding expected to be garnered through institutional resources and private foundation support. The program builds on the success of our existing Big Data Summer Institute (BDSI) supported by a NIH BD2K Courses and Skills grant award that is ending in 2018. We plan to expose program students to diverse techniques, skills and problems in the field of Big Data and Human Health. We enhance our ongoing summer program structure in the current proposal by involving a team of researchers working at the intersection of cardiovascular research and data science with a focus on cardiovascular precision medicine where some of the new mentored research projects will be defined. We primarily focus on three genres of health Big Data arising in Electronic Health Records/Medical Claims, Genomics and Imaging. The trainees will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Computational Medicine and Bioinformatics, Statistics, Computer Science and Engineering, Information Sciences, Epidemiology and Medicine, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by (U-M) researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm and engage them in influential research related to human health.",Transforming Analytical Learning in the Era of Big Data,9733542,R25HL147207,"['Admission activity', 'Adverse drug effect', 'Advocate', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Award', 'Basic Science', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Sciences', 'Code', 'Collaborations', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Epidemiology', 'Epigenetic Process', 'Evaluation', 'Exposure to', 'Faculty', 'Female', 'Foundations', 'Funding', 'Funding Mechanisms', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Human', 'Image', 'Influentials', 'Information Sciences', 'Injury', 'International', 'Kidney Diseases', 'Learning', 'Literature', 'Machine Learning', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Medical Imaging', 'Medical Records', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Minority Groups', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Prevention', 'Privatization', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'STEM program', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Social Behavior', 'Social Sciences', 'Statistical Methods', 'Statistical Models', 'Structure', 'Student recruitment', 'Students', 'Talents', 'Techniques', 'Traction', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Woman', 'Work', 'base', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'data visualization', 'design', 'experience', 'graduate school preparation', 'graduate student', 'high dimensionality', 'instructor', 'interest', 'lectures', 'medical schools', 'member', 'metabolomics', 'nervous system disorder', 'network models', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'population health', 'posters', 'precision medicine', 'programs', 'recruit', 'signal processing', 'skills', 'statistics', 'student training', 'success', 'summer institute', 'summer program', 'summer research', 'symposium', 'tool', 'undergraduate student', 'underrepresented minority student', 'wiki']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2019,250974,0.03692044665197893
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,0.0015278674938131558
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9827788,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,33190,0.02636798195935942
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9693030,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,36510,0.02636798195935942
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9625823,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,343157,0.02636798195935942
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9855180,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2019,702655,0.023009555946335532
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9741121,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'side effect', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,281932,0.01214522661722163
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9634069,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2019,293003,0.009449756761488995
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9747967,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2019,514129,0.03658644566877163
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9734597,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'learning strategy', 'lectures', 'lens', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,249789,0.02216328129238596
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9676043,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,308000,0.013019617516742021
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,0.016344255229781775
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. • ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. • UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types • U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database • St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9853317,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Infrastructure', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'multidimensional data', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2019,855741,0.02225151932624525
"Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership ﻿    DESCRIPTION (provided by applicant): The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The aims to achieve the goals are: 1) Implement an integrated didactic program to enhance student self-efficacy with computational and informatics tool development and use for interrogating and interpreting Big Data, including a two-semester bioinformatics course in Bioinformatics, informed by the expertise of UIUC KnowEnG BD2K Center faculty with additional Special Topics courses available remotely from UIUC. 2) Develop an integrated academic year (Fisk, or partners) and summer research program at the UIUC KnowEnG BD2K Center to assure student exposure to a participation in the life cycle of a `Big Data' research problem. 3) Implement a professional skills development program that assures successful transition of undergraduate participants to a Ph.D. (or MD/PhD Program) in Big Data- reliant biomedical research. 4) Launch a faculty development program in bioinformatics that leads to embracing Big Data problems in courses in multiple disciplines for impact on all Fisk undergraduate STEM majors. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fisk R25 mentors, of value for their broader education and research training goals at UIUC and Mayo. PUBLIC HEALTH RELEVANCE: The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fis R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.  ",Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership,9729464,R25MD010396,"['Address', 'Articulation', 'Base Pairing', 'Big Data', 'Big Data Methods', 'Big Data to Knowledge', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Career Choice', 'Cellular biology', 'Chemicals', 'Collaborations', 'Communication', 'Complement', 'Computers', 'Core Facility', 'Development Plans', 'Discipline', 'Doctor of Philosophy', 'Education Projects', 'Educational Curriculum', 'Educational workshop', 'Elements', 'Exposure to', 'Faculty', 'Faculty Workshop', 'Fostering', 'Funding', 'Genomics', 'Goals', 'Grant', 'Illinois', 'Informatics', 'Journals', 'Learning', 'Life Cycle Stages', 'Literature', 'Machine Learning', 'Manuscripts', 'Mathematics', 'Mentors', 'Molecular Biology', 'Oral', 'Participant', 'Program Development', 'Proteomics', 'Reading', 'Research', 'Research Ethics', 'Research Personnel', 'Research Training', 'Role', 'STEM field', 'Science', 'Scientist', 'Self Efficacy', 'Students', 'Systems Biology', 'Teacher Professional Development', 'Technology', 'Time', 'Training', 'Training Programs', 'Training and Education', 'Underrepresented Minority', 'Universities', 'Work', 'Writing', 'base', 'career', 'computer science', 'computerized tools', 'data acquisition', 'data sharing', 'deep learning', 'experience', 'faculty research', 'individualized medicine', 'informatics\xa0tool', 'innovation', 'minority scientist', 'posters', 'programs', 'public health relevance', 'recruit', 'responsible research conduct', 'skill acquisition', 'skills', 'summer research', 'tool development', 'undergraduate research', 'undergraduate student']",NIMHD,FISK UNIVERSITY,R25,2019,216000,0.0377869370566231
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9724498,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2019,67704,0.0009912367101591157
"Knowledge Management Center for Illuminating the Druggable Genome The main goal of the Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program is to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG (“IDG List”). The long-term objective of the KMC is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins. With focus on the IDG List and human proteins, the KMC will enable support for expanded coverage for non-human proteins of therapeutic interest and other associated human health data, in order to catalyze novel biomedical discoveries. To support the overall IDG objective, and to maintain, update and improve these integrated resources, the KMC draws upon expertise from multiple knowledge domains, specifically biology, chemistry and medicine, as well as computer science, graphic design and web programming. Specifically, for the Phase 2 of the IDG KMC we propose 4 Aims:1. Create an automated workflow that captures relevant public data for the entire proteome and manual annotations for the IDG list. The KMC knowledge management system will be built around knowledge graphs, focused on five major branches of the target knowledge tree, tkt: Genotype, Phenotype, Expression, Structure & Function, and Interactions & Pathways, respectively. Aim 2: Design, develop and implement a protein knowledgebase with Data Analytics support. Our protein-centric biomedical knowledge base, TCKB (Target Central Knowledgebase) will be comprised of the data, knowledge and information container, together with its codebase and software pipelines. TCKB will be the repository for experimental, processed and computed data and reagents originating from the IDG DRGCs (Data and Resource Generation Centers). We will provide informatics and modeling support for DRGC activities. Aim 3: We will expand, improve and maintain Pharos. Particularly “knowledge packages,” support automated data summaries for Protein Dossiers, and actively seek feedback from our community. Aim 4. Outreach to scientific community. We will support a series of activities that will leverage TCKB, Pharos and other IDG resources to increase adoption of IDG work, while observing FAIR (findable, accessible, interoperable, reusable) principles for our knowledgebase, portal and pipelines. The KMC will engage in community outreach by leading tutorials and feedback sessions and dissemination of the Pharos system. To meet its goals, the KMC will coordinate all core activities in close coordination with the IDG Steering Committee and IDG Project Scientists (PS), and include members of the IDG Consortium (IDG- C), other NIH Common Fund programs, NIH Commons, as well as other initiatives. The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program plans to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG. The KMC long-term objective is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins.",Knowledge Management Center for Illuminating the Druggable Genome,9620018,U24CA224370,"['Address', 'Adoption', 'Alleles', 'Archives', 'Basic Science', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Chemistry', 'Communities', 'Community Outreach', 'Computer software', 'Coupled', 'Data', 'Data Analytics', 'Data Element', 'Data Set', 'Databases', 'Dependence', 'Disease', 'Documentation', 'Ecosystem', 'FAIR principles', 'Family', 'Feedback', 'Funding', 'Future', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genome Components', 'Genotype', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Human', 'Informatics', 'Information Resources Management', 'International', 'Internet', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Mutation', 'Non-Human Protein', 'Ontology', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Process', 'Protein Isoforms', 'Proteins', 'Proteome', 'Public Domains', 'Publications', 'Reagent', 'Resources', 'Scientist', 'Series', 'Services', 'Source', 'Structure', 'System', 'TRD@ gene cluster', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Update', 'Work', 'analytical method', 'computer science', 'data resource', 'design', 'disease classification', 'experience', 'gender difference', 'health data', 'improved', 'interest', 'knowledge base', 'member', 'novel', 'online community', 'outreach', 'programs', 'repository', 'therapeutic protein', 'tool', 'web site']",NCI,UNIVERSITY OF NEW MEXICO HEALTH SCIS CTR,U24,2019,1000000,-0.020164300521501234
"LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences PROJECT SUMMARY/ABSTRACT The overall goal of the Postbaccalaureate Research Education Program (PREP) in Biomedical Sciences (BMS) at the Louisiana State University Health Sciences Center in New Orleans (LSUHSC-NO) is to enhance the diversity of the research workforce by increasing competitiveness for acceptance and completion of PhD and MD/PhD programs by underrepresented minorities (URM). LSUHSC-NO PREP will accept 39 recent URM baccalaureate science graduates over 5 yrs for intensive research and innovative academic training experi- ences to foster success in BMS doctoral degree programs. Over 300 URM science baccalaureates are award- ed annually to URM students by universities located within 100 miles of LSUHSC-NO representing 32% of all science degrees; however, URM constitute only 10% of enrolled PhD and MD/PhD students nationwide, and 12% at LSUHSC-NO. The PREP will enhance competitiveness for acceptance, retention, and completion of BMS PhD programs by the Scholars by providing 1) intensive research experiences with committed research faculty, and 2) complementary skills development during the 1-yr training. PREP training will focus on building solid foundations in research skills by providing concentrated education in scientific critical thinking, analysis of results, statistics, and writing; inventive and personalized test-taking skills to improve GRE scores and success in graduate courses; oral and poster scientific presentations; responsible and ethical conduct in research; and writing a resume, personal statement, and selecting and applying to graduate schools. Scholars will participate in works-in-progress, journal clubs, and workshops hosted by PREP faculty, visitors, and recruitment contacts; join graduate students in skills development programs; present their research at the annual PREP poster sym- posium and local and national scientific conferences; mentor summer research fellows; and assist in communi- ty science education programs. The PREP will provide forums for the Scholars to engage in research and skills acquisition and active demonstration of knowledge. Over 60 LSUHSC-NO active research faculty, committed to the PREP, have extensive experience in mentoring high school, undergraduate, medical, predoctoral, and postdoctoral students representing URM in the BMS through NIH T32, R25, R35, and other funding mecha- nisms. There are 8 active LSUHSC-NO summer programs which provide research experiences to 75 URM fel- lows annually from which future PREP Scholars will be prescreened and recruited. Scholar recruitment efforts will extend to 4 local historically black colleges and 2 state universities to develop a community-wide mecha- nism to support the URM BMS workforce. The program will be critically evaluated using formative and summa- tive methodologies and descriptive, quantitative, and qualitative statistics to document success. The overall goal is for PREP Scholars to have an enhanced competitiveness for acceptance into rigorous graduate pro- grams with the confidence and essential research skills required to earn a PhD or MD/PhD degree, establish rewarding and successful BMS research careers, and serve as role-models for future URM students. PROJECT NARRATIVE The goal of the LSUHSC-NO PREP is to prepare individuals from backgrounds underrepresented in the biomedical sciences, who have recently completed their baccalaureate science degrees, for successful enrollment, retention, and completion of a PhD or MD-PhD training program. We aim to enhance the diversity of the biomedical research workforce by preparing PREP Scholars for the rigors and challenges of a biomedical doctoral degree program so that they will successfully obtain a PhD degree or MD/PhD degree and contribute their expertise to the biomedical scientific community.",LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences,9754847,R25GM121189,"['Academic Training', 'Anxiety', 'Award', 'Bachelor&apos', 's Degree', 'Basic Science', 'Biomedical Research', 'Centers of Research Excellence', 'Cities', 'Clinical Sciences', 'Communities', 'Critical Thinking', 'Data Analyses', 'Degree program', 'Dentistry', 'Development', 'Development Plans', 'Doctor of Philosophy', 'Doctor&apos', 's Degree', 'Education', 'Educational workshop', 'Enrollment', 'Environment', 'Ethics', 'Exercise', 'Extramural Activities', 'Faculty', 'Fostering', 'Foundations', 'Funding', 'Funding Mechanisms', 'Future', 'Goals', 'Health Sciences', 'Historically Black Colleges and Universities', 'Individual', 'Journals', 'Knowledge', 'Laboratory Research', 'Learning', 'Louisiana', 'Machine Learning', 'Manuscripts', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority Enrollment', 'NCI Scholars Program', 'Oral', 'Population', 'Postdoctoral Fellow', 'Program Development', 'Public Health', 'Research', 'Research Ethics', 'Research Project Grants', 'Resources', 'Rewards', 'Schools', 'Science', 'Scientist', 'Solid', 'Statistical Methods', 'Structure', 'Students', 'Study Skills', 'Sum', 'Technical Expertise', 'Testing', 'Time Management', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Visit', 'Work', 'Writing', 'career', 'design', 'doctoral student', 'education research', 'experience', 'faculty research', 'graduate student', 'gulf coast', 'high school', 'improved', 'innovation', 'invention', 'medical schools', 'member', 'minority communities', 'novel', 'posters', 'pre-doctoral', 'programs', 'recruit', 'role model', 'science education', 'skill acquisition', 'skills', 'skills training', 'statistics', 'success', 'summer program', 'summer research', 'symposium', 'undergraduate student', 'underrepresented minority student', 'university student']",NIGMS,LSU HEALTH SCIENCES CENTER,R25,2019,308173,0.028292018011572724
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,0.001700255032599609
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,0.01745046936411403
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,0.04266860365174229
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9762963,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Infrastructure', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2019,10000,-0.007505371265269247
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9607599,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2019,559237,0.06885414085168066
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9747930,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,258870,0.022667627307081488
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9992241,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,76935,0.022667627307081488
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9532186,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2018,259358,0.03674214677760673
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9614770,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genetic Diseases', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,538700,0.061313337152203495
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,0.007354034271973206
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,0.015642063717755488
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,0.026755170394427145
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9477110,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,548298,0.028755463491851093
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,0.0034601161373888855
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9422475,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,356646,0.02636798195935942
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,0.015171398150477347
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9635840,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2018,854309,0.023009555946335532
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9448918,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'big biomedical data', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2018,308503,0.009449756761488995
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9572992,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,297237,0.01214522661722163
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,-0.0016022292219301819
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9527186,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2018,516810,0.03658644566877163
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9453640,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2018,305500,0.013019617516742021
"Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership ﻿    DESCRIPTION (provided by applicant): The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The aims to achieve the goals are: 1) Implement an integrated didactic program to enhance student self-efficacy with computational and informatics tool development and use for interrogating and interpreting Big Data, including a two-semester bioinformatics course in Bioinformatics, informed by the expertise of UIUC KnowEnG BD2K Center faculty with additional Special Topics courses available remotely from UIUC. 2) Develop an integrated academic year (Fisk, or partners) and summer research program at the UIUC KnowEnG BD2K Center to assure student exposure to a participation in the life cycle of a `Big Data' research problem. 3) Implement a professional skills development program that assures successful transition of undergraduate participants to a Ph.D. (or MD/PhD Program) in Big Data- reliant biomedical research. 4) Launch a faculty development program in bioinformatics that leads to embracing Big Data problems in courses in multiple disciplines for impact on all Fisk undergraduate STEM majors. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fisk R25 mentors, of value for their broader education and research training goals at UIUC and Mayo. PUBLIC HEALTH RELEVANCE: The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fis R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.  ",Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership,9503633,R25MD010396,"['Address', 'Articulation', 'Base Pairing', 'Big Data', 'Big Data to Knowledge', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Career Choice', 'Cellular biology', 'Chemicals', 'Collaborations', 'Communication', 'Complement', 'Computers', 'Core Facility', 'Development Plans', 'Discipline', 'Doctor of Philosophy', 'Education Projects', 'Educational Curriculum', 'Educational workshop', 'Elements', 'Exposure to', 'Faculty', 'Faculty Workshop', 'Fostering', 'Funding', 'Genomics', 'Goals', 'Grant', 'Illinois', 'Informatics', 'Journals', 'Learning', 'Life Cycle Stages', 'Literature', 'Machine Learning', 'Manuscripts', 'Mathematics', 'Mentors', 'Molecular Biology', 'Oral', 'Participant', 'Program Development', 'Proteomics', 'Reading', 'Research', 'Research Ethics', 'Research Personnel', 'Research Training', 'Role', 'STEM field', 'Science', 'Scientist', 'Self Efficacy', 'Students', 'Systems Biology', 'Teacher Professional Development', 'Technology', 'Time', 'Training', 'Training Programs', 'Training and Education', 'Underrepresented Minority', 'Universities', 'Work', 'Writing', 'base', 'career', 'computer science', 'data acquisition', 'data sharing', 'deep learning', 'experience', 'faculty research', 'individualized medicine', 'innovation', 'minority scientist', 'posters', 'programs', 'public health relevance', 'recruit', 'responsible research conduct', 'skill acquisition', 'skills', 'summer research', 'tool', 'tool development', 'undergraduate research', 'undergraduate student']",NIMHD,FISK UNIVERSITY,R25,2018,216000,0.0377869370566231
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9534738,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'ontology development', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2018,431097,0.011601519793746923
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9545035,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2018,67704,0.0009912367101591157
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9532909,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Exposure to', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'bioinformatics resource', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'individualized medicine', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2018,365327,0.027087487864241075
"Knowledge Management Center for Illuminating the Druggable Genome The main goal of the Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program is to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG (“IDG List”). The long-term objective of the KMC is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins. With focus on the IDG List and human proteins, the KMC will enable support for expanded coverage for non-human proteins of therapeutic interest and other associated human health data, in order to catalyze novel biomedical discoveries. To support the overall IDG objective, and to maintain, update and improve these integrated resources, the KMC draws upon expertise from multiple knowledge domains, specifically biology, chemistry and medicine, as well as computer science, graphic design and web programming. Specifically, for the Phase 2 of the IDG KMC we propose 4 Aims:1. Create an automated workflow that captures relevant public data for the entire proteome and manual annotations for the IDG list. The KMC knowledge management system will be built around knowledge graphs, focused on five major branches of the target knowledge tree, tkt: Genotype, Phenotype, Expression, Structure & Function, and Interactions & Pathways, respectively. Aim 2: Design, develop and implement a protein knowledgebase with Data Analytics support. Our protein-centric biomedical knowledge base, TCKB (Target Central Knowledgebase) will be comprised of the data, knowledge and information container, together with its codebase and software pipelines. TCKB will be the repository for experimental, processed and computed data and reagents originating from the IDG DRGCs (Data and Resource Generation Centers). We will provide informatics and modeling support for DRGC activities. Aim 3: We will expand, improve and maintain Pharos. Particularly “knowledge packages,” support automated data summaries for Protein Dossiers, and actively seek feedback from our community. Aim 4. Outreach to scientific community. We will support a series of activities that will leverage TCKB, Pharos and other IDG resources to increase adoption of IDG work, while observing FAIR (findable, accessible, interoperable, reusable) principles for our knowledgebase, portal and pipelines. The KMC will engage in community outreach by leading tutorials and feedback sessions and dissemination of the Pharos system. To meet its goals, the KMC will coordinate all core activities in close coordination with the IDG Steering Committee and IDG Project Scientists (PS), and include members of the IDG Consortium (IDG- C), other NIH Common Fund programs, NIH Commons, as well as other initiatives. The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program plans to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG. The KMC long-term objective is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins.",Knowledge Management Center for Illuminating the Druggable Genome,9461406,U24CA224370,"['Address', 'Adoption', 'Alleles', 'Archives', 'Basic Science', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Chemistry', 'Communities', 'Community Outreach', 'Computer software', 'Coupled', 'Data', 'Data Analytics', 'Data Element', 'Data Set', 'Databases', 'Dependence', 'Disease', 'Documentation', 'Ecosystem', 'FAIR principles', 'Family', 'Feedback', 'Funding', 'Future', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genome Components', 'Genotype', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Human', 'Informatics', 'Information Resources Management', 'International', 'Internet', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Mutation', 'Non-Human Protein', 'Ontology', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Process', 'Protein Isoforms', 'Proteins', 'Proteome', 'Public Domains', 'Publications', 'Reagent', 'Resources', 'Scientist', 'Series', 'Services', 'Source', 'Structure', 'System', 'TRD@ gene cluster', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Update', 'Work', 'analytical method', 'computer science', 'data resource', 'design', 'disease classification', 'experience', 'gender difference', 'health data', 'improved', 'interest', 'knowledge base', 'member', 'novel', 'online community', 'outreach', 'programs', 'repository', 'therapeutic protein', 'tool', 'web site']",NCI,UNIVERSITY OF NEW MEXICO HEALTH SCIS CTR,U24,2018,1013500,-0.020164300521501234
"LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences PROJECT SUMMARY/ABSTRACT The overall goal of the Postbaccalaureate Research Education Program (PREP) in Biomedical Sciences (BMS) at the Louisiana State University Health Sciences Center in New Orleans (LSUHSC-NO) is to enhance the diversity of the research workforce by increasing competitiveness for acceptance and completion of PhD and MD/PhD programs by underrepresented minorities (URM). LSUHSC-NO PREP will accept 39 recent URM baccalaureate science graduates over 5 yrs for intensive research and innovative academic training experi- ences to foster success in BMS doctoral degree programs. Over 300 URM science baccalaureates are award- ed annually to URM students by universities located within 100 miles of LSUHSC-NO representing 32% of all science degrees; however, URM constitute only 10% of enrolled PhD and MD/PhD students nationwide, and 12% at LSUHSC-NO. The PREP will enhance competitiveness for acceptance, retention, and completion of BMS PhD programs by the Scholars by providing 1) intensive research experiences with committed research faculty, and 2) complementary skills development during the 1-yr training. PREP training will focus on building solid foundations in research skills by providing concentrated education in scientific critical thinking, analysis of results, statistics, and writing; inventive and personalized test-taking skills to improve GRE scores and success in graduate courses; oral and poster scientific presentations; responsible and ethical conduct in research; and writing a resume, personal statement, and selecting and applying to graduate schools. Scholars will participate in works-in-progress, journal clubs, and workshops hosted by PREP faculty, visitors, and recruitment contacts; join graduate students in skills development programs; present their research at the annual PREP poster sym- posium and local and national scientific conferences; mentor summer research fellows; and assist in communi- ty science education programs. The PREP will provide forums for the Scholars to engage in research and skills acquisition and active demonstration of knowledge. Over 60 LSUHSC-NO active research faculty, committed to the PREP, have extensive experience in mentoring high school, undergraduate, medical, predoctoral, and postdoctoral students representing URM in the BMS through NIH T32, R25, R35, and other funding mecha- nisms. There are 8 active LSUHSC-NO summer programs which provide research experiences to 75 URM fel- lows annually from which future PREP Scholars will be prescreened and recruited. Scholar recruitment efforts will extend to 4 local historically black colleges and 2 state universities to develop a community-wide mecha- nism to support the URM BMS workforce. The program will be critically evaluated using formative and summa- tive methodologies and descriptive, quantitative, and qualitative statistics to document success. The overall goal is for PREP Scholars to have an enhanced competitiveness for acceptance into rigorous graduate pro- grams with the confidence and essential research skills required to earn a PhD or MD/PhD degree, establish rewarding and successful BMS research careers, and serve as role-models for future URM students. PROJECT NARRATIVE The goal of the LSUHSC-NO PREP is to prepare individuals from backgrounds underrepresented in the biomedical sciences, who have recently completed their baccalaureate science degrees, for successful enrollment, retention, and completion of a PhD or MD-PhD training program. We aim to enhance the diversity of the biomedical research workforce by preparing PREP Scholars for the rigors and challenges of a biomedical doctoral degree program so that they will successfully obtain a PhD degree or MD/PhD degree and contribute their expertise to the biomedical scientific community.",LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences,9566214,R25GM121189,"['Academic Training', 'Anxiety', 'Award', 'Bachelor&apos', 's Degree', 'Basic Science', 'Biomedical Research', 'Centers of Research Excellence', 'Cities', 'Clinical Sciences', 'Communities', 'Critical Thinking', 'Data Analyses', 'Degree program', 'Dentistry', 'Development', 'Development Plans', 'Doctor of Philosophy', 'Doctor&apos', 's Degree', 'Education', 'Educational workshop', 'Enrollment', 'Environment', 'Ethics', 'Exercise', 'Extramural Activities', 'Faculty', 'Fostering', 'Foundations', 'Funding', 'Funding Mechanisms', 'Future', 'Goals', 'Health Sciences', 'Historically Black Colleges and Universities', 'Individual', 'Journals', 'Knowledge', 'Laboratory Research', 'Learning', 'Louisiana', 'Machine Learning', 'Manuscripts', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority Enrollment', 'NCI Scholars Program', 'Oral', 'Population', 'Postdoctoral Fellow', 'Program Development', 'Public Health', 'Research', 'Research Ethics', 'Research Project Grants', 'Resources', 'Rewards', 'Schools', 'Science', 'Scientist', 'Solid', 'Statistical Methods', 'Structure', 'Students', 'Study Skills', 'Sum', 'Technical Expertise', 'Testing', 'Time Management', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Visit', 'Work', 'Writing', 'career', 'design', 'doctoral student', 'education research', 'experience', 'faculty research', 'graduate student', 'gulf coast', 'high school', 'improved', 'innovation', 'invention', 'medical schools', 'member', 'minority communities', 'novel', 'posters', 'pre-doctoral', 'programs', 'recruit', 'role model', 'science education', 'skill acquisition', 'skills', 'skills training', 'statistics', 'success', 'summer program', 'summer research', 'symposium', 'undergraduate student', 'underrepresented minority student', 'university student']",NIGMS,LSU HEALTH SCIENCES CENTER,R25,2018,235732,0.028292018011572724
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9612854,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2018,10000,-0.007505371265269247
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9404042,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2018,564487,0.06885414085168066
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,0.02871350841129813
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9536962,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2018,264913,0.022667627307081488
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9392642,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2017,262150,0.03674214677760673
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM. PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9275458,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Evidence based practice', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Medicine', 'Methods', 'Modeling', 'Modernization', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2017,210198,0.005595383049048138
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,0.007354034271973206
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,0.015642063717755488
"From genomics to natural language processing: A protected environment for research computing in the health science NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Health sciences researchers are often required to manage, mine, and analyze restricted patient data (Protected Health Information, PHI) to facilitate and advance their research aims. They are often required to do this without access to central information technology expertise or resources to facilitate their research aims. These researchers are often left to their own devices to “solve” their research compute and data needs and are challenged due to lack of available resources, barriers from central IT, and/or lack of knowledge of available resources. A further challenge is that “small” data sets— data that researchers could formerly handle on office resources—have morphed and grown into the big data domain through the explosion of technical advances and significant expansion in various research directions. Examples include: genomics research, image analysis, simulation, natural language processing, and mining of EMRs. Therefore, the need exists to develop a framework for managing and processing this data securely and reliably. This S10 equipment proposal is to replace the “protected environment” (PE) prototype the University of Utah’s Center for High Performance Computing (CHPC) and Department of Biomedical Informatics built six years ago and has operated since. The PE consists of both high performance computing and virtual machine (VM) components and associated storage sufficient to manage, protect and analyze HIPAA protected health information. This environment has been very successful and has grown significantly in scope. CHPC isolated this protected environment in the secured University of Utah Downtown Data Center and setup a network protected logical partition that provided research groups specific access to individual data sets. As the environment and technology developed, CHPC added additional security features such as two-factor authentication for entry and audit/monitoring. Unfortunately, the prototype has reached the point where demand is surpassing capability and all the hardware is aged and off-warranty. To give an idea of users of the virtual machine farm component, the Biomedical Informatics Core (BMIC) REDCap (Research Electronic Data Capture) environment for data collection has over 2,500 users in 1,500 projects supporting over $25M in NIH funding at the University of Utah, including support for more than 25 active NIH R-01 grants. Moreover, the HIPAA compliant protected environment was a key factor that aided passing the recent University of Utah HIPAA audit. The “protected environment” also helped the University of Utah Health Sciences Center and the BMIC justify the NCATS Center Clinical and Translational Science award (1ULTR001067). NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Project Narrative: The proposed “Protected Environment” instrument will provide research computing and data management capabilities for health sciences researchers to properly manage, secure, and analyze HIPAA regulated protected health information. The technology will not only support a large number of clinical trials, but also enable research in Human Genetics and Natural Language Processing of electronic health records.",From genomics to natural language processing: A protected environment for research computing in the health science,9274445,S10OD021644,"['Award', 'Big Data', 'Clinical Sciences', 'Data', 'Data Collection', 'Data Set', 'Devices', 'Environment', 'Equipment', 'Explosion', 'Farming environment', 'Funding', 'Genomics', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'High Performance Computing', 'Image Analysis', 'Individual', 'Information Technology', 'Knowledge', 'Left', 'Mining', 'Monitor', 'Natural Language Processing', 'Patients', 'Research', 'Research Personnel', 'Resources', 'Secure', 'Security', 'Technology', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Utah', 'aged', 'biomedical informatics', 'computerized data processing', 'electronic data', 'prototype', 'simulation', 'virtual']",OD,UNIVERSITY OF UTAH,S10,2017,493595,-0.004813893020681021
"Transforming Analytical Learning in the Era of Big Data ﻿    DESCRIPTION (provided by applicant): In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio) statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offer and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will draw from the expertise and experience of faculty from four different departments within four different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts, and Information Science in the School of Information. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high-dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and imaging. The diseases and conditions they study include obesity, cancer, diabetes, cardiovascular disease, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and response for our pilot offering in 2015 with 153 applications for 20 positions and a yield rate of 80% from the offers we extended. We plan to build on the success of this initial offering in the next three year funding cycle of this grant (2016-2018). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a no-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 30 undergraduates nationally and expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. PUBLIC HEALTH RELEVANCE: We propose a six week long summer institute: ""Transforming Analytical Learning in the Era of Big Data"" to be held at the Department of Biostatistics, University of Michigan, Ann Arbor, with a group of approximately 30 undergraduates recruited nationally, from 2016-2018. We plan to expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Statistics, Computer Science and Engineering, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm.",Transforming Analytical Learning in the Era of Big Data,9325011,R25EB022363,"['Adverse drug effect', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Big Data', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Case Study', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Epigenetic Process', 'Faculty', 'Funding', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Image', 'Imagery', 'Information Sciences', 'Injury', 'Kidney Diseases', 'Learning', 'Literature', 'Machine Learning', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Positioning Attribute', 'Prevention', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Recruitment Activity', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Social Behavior', 'Statistical Methods', 'Students', 'Talents', 'Techniques', 'Training', 'Universities', 'Work', 'base', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'design', 'experience', 'high dimensionality', 'instructor', 'interest', 'lectures', 'member', 'metabolomics', 'nervous system disorder', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'posters', 'programs', 'public health relevance', 'response', 'signal processing', 'skills', 'statistics', 'success', 'summer institute', 'symposium', 'tool', 'undergraduate student', 'wiki']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2017,161633,0.03842087222361051
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9282279,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Custom', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Source', 'Standardization', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2017,455519,0.012478652575592426
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9442241,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,57870,0.028755463491851093
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9266490,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,552544,0.028755463491851093
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,0.0034601161373888855
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,0.015171398150477347
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community. PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.",The Big DIPA: Data Image Processing and Analysis,9295026,R25EB022366,"['Address', 'Architecture', 'Area', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Communities', 'Competence', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Education', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modality', 'Modernization', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Scientist', 'Signal Transduction', 'Site', 'Software Tools', 'Source', 'Stream', 'Systems Biology', 'Talents', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'big biomedical data', 'bioimaging', 'biological systems', 'biomedical scientist', 'computer science', 'computerized tools', 'cost', 'course implementation', 'data acquisition', 'data format', 'demographics', 'design', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging approach', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'learning materials', 'lecture notes', 'lectures', 'microscopic imaging', 'nervous system disorder', 'open source', 'outreach', 'programs', 'public health relevance', 'repository', 'skill acquisition', 'skills', 'statistics', 'theories', 'training opportunity']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2017,161997,0.031437598777923105
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9398728,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2017,546372,0.03658644566877163
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9394127,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Behavior Therapy', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Health Care Research', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health care quality', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'symptomatic improvement', 'tool']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2017,338637,0.01214522661722163
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9251828,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Modernization', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Standardization', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'interoperability', 'knowledge translation', 'learning progression', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool', 'translational impact']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,428512,0.0322390491312485
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9285168,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2017,267313,0.013019617516742021
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9486059,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2017,1408263,0.02225151932624525
"A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform     DESCRIPTION (provided by applicant): The inception of the BD2K Initiative is a testament to the foresight of NIH and our community. Clearly, the future of biomedicine rests on our collective ability to transform Big Data into intelligible scientific facts. In line with the BD2K objectives,our goal is to revolutionize how we address the universal challenge to discern meaning from unruly data. Capitalizing on our investigators' complementary strengths in computational biology and cardiovascular medicine, we will present a fusion of cutting-edge innovations that are grounded in a cardiovascular research focus, encompassing: (i) on-the-cloud data processing, (ii) crowd sourcing and text-mining data annotation, (iii) protein spatiotemporal dynamics, (iv) multi-omic integration, and (v) multiscale clinical data modeling. Drawing from our decade of experience in creating and refining bioinformatics tools, we propose to amalgamate established Big Data resources into a generalizable model for data annotation and collaborative research, through a new query system and cloud infrastructure for accessing multiple omics repositories, and through computational-supported crowdsourcing initiatives for mining the biomedical literature. We propose to interweave diverse data types for revealing biological networks that coalesce from molecular entities at multiple scales, through machine learning methods for structuring molecular data and defining relationships with drugs and diseases, and through novel algorithms for on-the-cloud integration and pathway visualization of multi-dimensional molecular data. Moreover, we propose to innovate advanced modeling tools to resolve protein dynamics and spatiotemporal molecular mechanisms, through mechanistic modeling of protein properties and 3D protein expression maps, and through Bayesian algorithms that correlate patient phenotypes, health histories, and multi-scale molecular profiles. The utility and customizability o our tools to the broader research population is clearly demonstrated using three archetypical workflows that enable annotations of large lists of genes, transcripts, proteins, or metabolites; powerful analysis of complex protein datasets acquired over time; and seamless aQoregation of diverse molecular, textual and literature data. These workflows will be rigorously validated using data from two significant clinical cohorts, the Jackson Heart Study and the Healthy Elderly Longevity (Wellderly). In parallel, a multifaceted strategy will be implemented to educate and train biomedical investigators, and to engage the public for promoting the overall BD2K initiative. We are convinced that a community-driven BD2K initiative will best realize its scientific potential and transform the research culture in a sustainable manner, exhibiting lasting success beyond the current funding period.         PUBLIC HEALTH RELEVANCE:  The challenges of biomedical Big Data are multifaceted. Biomedical investigators face daunting tasks of storing, analyzing, and distributing large-scale omics data, and aggregating all information to discern mechanistic insights. A coherent effort is required to harness disarrayed Big Data and transform them into intelligible scientific facts, whil engaging the global community via education and outreach programs. This Big Data Science Research proposal is designed to address these challenges by formulating a federated architecture of community-supported tools for enhancing data management, integration and analysis.            ",A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform,9298691,U54GM114833,"['Achievement', 'Address', 'Algorithmic Software', 'Algorithms', 'Architecture', 'Awareness', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Computational Biology', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Dimensions', 'Disease', 'Education and Outreach', 'Elderly', 'Environment', 'Exhibits', 'Face', 'Funding', 'Future', 'Gene Proteins', 'General Population', 'Generations', 'Genes', 'Goals', 'Half-Life', 'Harvest', 'Health', 'Human', 'Imagery', 'Intuition', 'Jackson Heart Study', 'Knowledge', 'Literature', 'Longevity', 'Machine Learning', 'Maps', 'Medicine', 'Mining', 'Modeling', 'Modernization', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Organ', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Population Research', 'Property', 'Protein Analysis', 'Protein Dynamics', 'Proteins', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Rest', 'Scientist', 'Structure', 'System', 'Time', 'Training', 'Training and Education', 'Transcript', 'Translating', 'United States National Institutes of Health', 'big biomedical data', 'clinical phenotype', 'clinical predictors', 'cohort', 'computerized data processing', 'computerized tools', 'crowdsourcing', 'data management', 'data mining', 'data modeling', 'data resource', 'data to knowledge', 'design', 'experience', 'improved', 'innovation', 'insight', 'interest', 'learning strategy', 'molecular scale', 'multiple omics', 'novel', 'operation', 'outreach program', 'protein complex', 'protein expression', 'protein metabolite', 'protein protein interaction', 'public health relevance', 'repository', 'spatiotemporal', 'success', 'support tools', 'text searching', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,U54,2017,168436,-0.0011246660318267488
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,0.026679083987805535
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,0.028727000296773594
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9306202,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Molecular Analysis', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,591990,0.04357892627529147
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,0.026755170394427145
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9360131,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'permissiveness', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2017,456710,0.011601519793746923
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9357752,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2017,62304,0.0009912367101591157
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9326315,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Modification', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2017,370434,0.027087487864241075
"Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership ﻿    DESCRIPTION (provided by applicant): The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The aims to achieve the goals are: 1) Implement an integrated didactic program to enhance student self-efficacy with computational and informatics tool development and use for interrogating and interpreting Big Data, including a two-semester bioinformatics course in Bioinformatics, informed by the expertise of UIUC KnowEnG BD2K Center faculty with additional Special Topics courses available remotely from UIUC. 2) Develop an integrated academic year (Fisk, or partners) and summer research program at the UIUC KnowEnG BD2K Center to assure student exposure to a participation in the life cycle of a `Big Data' research problem. 3) Implement a professional skills development program that assures successful transition of undergraduate participants to a Ph.D. (or MD/PhD Program) in Big Data- reliant biomedical research. 4) Launch a faculty development program in bioinformatics that leads to embracing Big Data problems in courses in multiple disciplines for impact on all Fisk undergraduate STEM majors. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fisk R25 mentors, of value for their broader education and research training goals at UIUC and Mayo. PUBLIC HEALTH RELEVANCE: The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fis R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.  ",Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership,9303203,R25MD010396,"['Address', 'Articulation', 'Base Pairing', 'Big Data', 'Big Data to Knowledge', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Career Choice', 'Cellular biology', 'Chemicals', 'Collaborations', 'Communication', 'Complement', 'Computers', 'Core Facility', 'Development Plans', 'Discipline', 'Doctor of Philosophy', 'Education Projects', 'Educational Curriculum', 'Educational workshop', 'Elements', 'Exposure to', 'Faculty', 'Faculty Workshop', 'Fostering', 'Funding', 'Genomics', 'Goals', 'Grant', 'Illinois', 'Informatics', 'Journals', 'Learning', 'Life Cycle Stages', 'Literature', 'Machine Learning', 'Manuscripts', 'Mathematics', 'Mentors', 'Molecular Biology', 'Oral', 'Participant', 'Program Development', 'Proteomics', 'Reading', 'Recruitment Activity', 'Research', 'Research Ethics', 'Research Personnel', 'Research Training', 'Role', 'STEM field', 'Science', 'Scientist', 'Self Efficacy', 'Students', 'Systems Biology', 'Teacher Professional Development', 'Technology', 'Time', 'Training', 'Training Programs', 'Training and Education', 'Underrepresented Minority', 'Universities', 'Work', 'Writing', 'base', 'career', 'computer science', 'data acquisition', 'data sharing', 'experience', 'faculty research', 'individualized medicine', 'innovation', 'minority scientist', 'posters', 'programs', 'public health relevance', 'responsible research conduct', 'skill acquisition', 'skills', 'summer research', 'tool', 'tool development', 'undergraduate research', 'undergraduate student']",NIMHD,FISK UNIVERSITY,R25,2017,216000,0.0377869370566231
"LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences PROJECT SUMMARY/ABSTRACT The overall goal of the Postbaccalaureate Research Education Program (PREP) in Biomedical Sciences (BMS) at the Louisiana State University Health Sciences Center in New Orleans (LSUHSC-NO) is to enhance the diversity of the research workforce by increasing competitiveness for acceptance and completion of PhD and MD/PhD programs by underrepresented minorities (URM). LSUHSC-NO PREP will accept 39 recent URM baccalaureate science graduates over 5 yrs for intensive research and innovative academic training experi- ences to foster success in BMS doctoral degree programs. Over 300 URM science baccalaureates are award- ed annually to URM students by universities located within 100 miles of LSUHSC-NO representing 32% of all science degrees; however, URM constitute only 10% of enrolled PhD and MD/PhD students nationwide, and 12% at LSUHSC-NO. The PREP will enhance competitiveness for acceptance, retention, and completion of BMS PhD programs by the Scholars by providing 1) intensive research experiences with committed research faculty, and 2) complementary skills development during the 1-yr training. PREP training will focus on building solid foundations in research skills by providing concentrated education in scientific critical thinking, analysis of results, statistics, and writing; inventive and personalized test-taking skills to improve GRE scores and success in graduate courses; oral and poster scientific presentations; responsible and ethical conduct in research; and writing a resume, personal statement, and selecting and applying to graduate schools. Scholars will participate in works-in-progress, journal clubs, and workshops hosted by PREP faculty, visitors, and recruitment contacts; join graduate students in skills development programs; present their research at the annual PREP poster sym- posium and local and national scientific conferences; mentor summer research fellows; and assist in communi- ty science education programs. The PREP will provide forums for the Scholars to engage in research and skills acquisition and active demonstration of knowledge. Over 60 LSUHSC-NO active research faculty, committed to the PREP, have extensive experience in mentoring high school, undergraduate, medical, predoctoral, and postdoctoral students representing URM in the BMS through NIH T32, R25, R35, and other funding mecha- nisms. There are 8 active LSUHSC-NO summer programs which provide research experiences to 75 URM fel- lows annually from which future PREP Scholars will be prescreened and recruited. Scholar recruitment efforts will extend to 4 local historically black colleges and 2 state universities to develop a community-wide mecha- nism to support the URM BMS workforce. The program will be critically evaluated using formative and summa- tive methodologies and descriptive, quantitative, and qualitative statistics to document success. The overall goal is for PREP Scholars to have an enhanced competitiveness for acceptance into rigorous graduate pro- grams with the confidence and essential research skills required to earn a PhD or MD/PhD degree, establish rewarding and successful BMS research careers, and serve as role-models for future URM students. PROJECT NARRATIVE The goal of the LSUHSC-NO PREP is to prepare individuals from backgrounds underrepresented in the biomedical sciences, who have recently completed their baccalaureate science degrees, for successful enrollment, retention, and completion of a PhD or MD-PhD training program. We aim to enhance the diversity of the biomedical research workforce by preparing PREP Scholars for the rigors and challenges of a biomedical doctoral degree program so that they will successfully obtain a PhD degree or MD/PhD degree and contribute their expertise to the biomedical scientific community.",LSUHSC-New Orleans Postbaccalaureate Research Education Program in Biomedical Sciences,9208463,R25GM121189,"['Academic Training', 'Anxiety', 'Award', 'Bachelor&apos', 's Degree', 'Basic Science', 'Biomedical Research', 'Centers of Research Excellence', 'Cities', 'Clinical Sciences', 'Communities', 'Critical Thinking', 'Data Analyses', 'Degree program', 'Dentistry', 'Development', 'Development Plans', 'Doctor of Philosophy', 'Doctor&apos', 's Degree', 'Education', 'Educational workshop', 'Enrollment', 'Environment', 'Ethics', 'Exercise', 'Extramural Activities', 'Faculty', 'Fostering', 'Foundations', 'Funding', 'Funding Mechanisms', 'Future', 'Goals', 'Health Sciences', 'Historically Black Colleges and Universities', 'Individual', 'Journals', 'Knowledge', 'Laboratory Research', 'Learning', 'Louisiana', 'Machine Learning', 'Manuscripts', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority Enrollment', 'NCI Scholars Program', 'Oral', 'Population', 'Postdoctoral Fellow', 'Program Development', 'Public Health', 'Recruitment Activity', 'Research', 'Research Ethics', 'Research Project Grants', 'Resources', 'Rewards', 'Schools', 'Science', 'Scientist', 'Solid', 'Statistical Methods', 'Structure', 'Students', 'Study Skills', 'Sum', 'Technical Expertise', 'Testing', 'Time Management', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Visit', 'Work', 'Writing', 'career', 'design', 'doctoral student', 'education research', 'experience', 'faculty research', 'graduate student', 'gulf coast', 'high school', 'improved', 'innovation', 'invention', 'medical schools', 'member', 'minority communities', 'novel', 'posters', 'pre-doctoral', 'programs', 'role model', 'science education', 'skill acquisition', 'skills', 'skills training', 'statistics', 'success', 'summer program', 'summer research', 'symposium', 'undergraduate student', 'underrepresented minority student', 'university student']",NIGMS,LSU HEALTH SCIENCES CENTER,R25,2017,255172,0.028292018011572724
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,0.04266860365174229
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,0.04266860365174229
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,0.035626006392899257
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9217457,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'clinical development', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2017,578512,0.06885414085168066
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,0.02871350841129813
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9302929,R25OD021880,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",OD,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2017,271803,0.022667627307081488
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM.         PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).         ",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9076888,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Methods', 'Modeling', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'abstracting', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'meetings', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'text searching', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2016,240650,0.005595383049048138
"Administrative Supplement Request for Transforming Analytical Learning in the Era of Big Data ﻿    DESCRIPTION (provided by applicant): In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio) statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offer and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will draw from the expertise and experience of faculty from four different departments within four different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts, and Information Science in the School of Information. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high-dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and imaging. The diseases and conditions they study include obesity, cancer, diabetes, cardiovascular disease, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and response for our pilot offering in 2015 with 153 applications for 20 positions and a yield rate of 80% from the offers we extended. We plan to build on the success of this initial offering in the next three year funding cycle of this grant (2016-2018). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a no-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 30 undergraduates nationally and expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. PUBLIC HEALTH RELEVANCE: We propose a six week long summer institute: ""Transforming Analytical Learning in the Era of Big Data"" to be held at the Department of Biostatistics, University of Michigan, Ann Arbor, with a group of approximately 30 undergraduates recruited nationally, from 2016-2018. We plan to expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Statistics, Computer Science and Engineering, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm.",Administrative Supplement Request for Transforming Analytical Learning in the Era of Big Data,9243811,R25EB022363,"['Administrative Supplement', 'Adverse drug effect', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Behavioral Sciences', 'Big Data', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Case Study', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Epigenetic Process', 'Faculty', 'Funding', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Health', 'Image', 'Imagery', 'Information Sciences', 'Injury', 'Kidney Diseases', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Positioning Attribute', 'Prevention', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Recruitment Activity', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Statistical Methods', 'Students', 'Talents', 'Techniques', 'Training', 'Universities', 'Work', 'base', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'design', 'experience', 'instructor', 'interest', 'lectures', 'meetings', 'member', 'metabolomics', 'nervous system disorder', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'posters', 'programs', 'response', 'signal processing', 'skills', 'statistics', 'success', 'summer institute', 'symposium', 'tool', 'undergraduate student', 'wiki']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2016,159359,0.03827735042267388
"Transforming Analytical Learning in the Era of Big Data ﻿    DESCRIPTION (provided by applicant): In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio) statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offer and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will draw from the expertise and experience of faculty from four different departments within four different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts, and Information Science in the School of Information. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high-dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and imaging. The diseases and conditions they study include obesity, cancer, diabetes, cardiovascular disease, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and response for our pilot offering in 2015 with 153 applications for 20 positions and a yield rate of 80% from the offers we extended. We plan to build on the success of this initial offering in the next three year funding cycle of this grant (2016-2018). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a no-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 30 undergraduates nationally and expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. PUBLIC HEALTH RELEVANCE: We propose a six week long summer institute: ""Transforming Analytical Learning in the Era of Big Data"" to be held at the Department of Biostatistics, University of Michigan, Ann Arbor, with a group of approximately 30 undergraduates recruited nationally, from 2016-2018. We plan to expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Statistics, Computer Science and Engineering, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm.",Transforming Analytical Learning in the Era of Big Data,9149238,R25EB022363,"['Adverse drug effect', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Behavioral Sciences', 'Big Data', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Case Study', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Epigenetic Process', 'Faculty', 'Funding', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Health', 'Image', 'Imagery', 'Information Sciences', 'Injury', 'Kidney Diseases', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Positioning Attribute', 'Prevention', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Recruitment Activity', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Statistical Methods', 'Students', 'Talents', 'Techniques', 'Training', 'Universities', 'Work', 'base', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'design', 'experience', 'instructor', 'interest', 'lectures', 'meetings', 'member', 'metabolomics', 'nervous system disorder', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'posters', 'programs', 'response', 'signal processing', 'skills', 'statistics', 'success', 'summer institute', 'symposium', 'tool', 'undergraduate student', 'wiki']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2016,160479,0.03842087222361051
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9146381,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,457075,0.012478652575592426
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9243496,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,150865,0.012478652575592426
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,0.007354034271973206
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9065611,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,548438,0.028755463491851093
"Summer Institute for Statistics of Big Data DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support. PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.",Summer Institute for Statistics of Big Data,9063061,R25EB020380,"['Academia', 'Applied Skills', 'Area', 'Big Data', 'Biology', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'data visualization', 'graduate student', 'human disease', 'improved', 'instructor', 'learning materials', 'lectures', 'massive open online courses', 'member', 'online course', 'open source', 'programs', 'skills', 'statistics', 'summer institute', 'teacher', 'teaching assistant', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2016,159605,0.041369056497787465
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,0.015171398150477347
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,0.015171398150477347
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community. PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.",The Big DIPA: Data Image Processing and Analysis,9150564,R25EB022366,"['Accounting', 'Address', 'Architecture', 'Area', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Cell physiology', 'Communities', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modality', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Scientist', 'Senior Scientist', 'Signal Transduction', 'Site', 'Skills Development', 'Software Tools', 'Source', 'Staging', 'Stream', 'Systems Biology', 'TNFRSF5 gene', 'Time', 'Training', 'United States National Institutes of Health', 'Work', 'bioimaging', 'biological systems', 'biomedical scientist', 'computer science', 'computerized tools', 'cost', 'course implementation', 'data acquisition', 'data format', 'demographics', 'design', 'education research', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'learning materials', 'lecture notes', 'lectures', 'meetings', 'microscopic imaging', 'nervous system disorder', 'open source', 'outreach', 'programs', 'repository', 'skill acquisition', 'skills', 'statistics', 'theories', 'tissue processing', 'training opportunity']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2016,161997,0.031437598777923105
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338982,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Basic Science', 'Biological', 'Biomedical Research', 'Cell physiology', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2016,1183132,0.023009555946335532
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9049511,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Health', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'knowledge translation', 'learning progression', 'learning strategy', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428512,0.0322390491312485
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338977,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'improved', 'innovation', 'inter-individual variation', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2016,1071976,0.02225151932624525
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,0.028933683414673166
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,0.028933683414673166
"A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform     DESCRIPTION (provided by applicant): The inception of the BD2K Initiative is a testament to the foresight of NIH and our community. Clearly, the future of biomedicine rests on our collective ability to transform Big Data into intelligible scientific facts. In line with the BD2K objectives,our goal is to revolutionize how we address the universal challenge to discern meaning from unruly data. Capitalizing on our investigators' complementary strengths in computational biology and cardiovascular medicine, we will present a fusion of cutting-edge innovations that are grounded in a cardiovascular research focus, encompassing: (i) on-the-cloud data processing, (ii) crowd sourcing and text-mining data annotation, (iii) protein spatiotemporal dynamics, (iv) multi-omic integration, and (v) multiscale clinical data modeling. Drawing from our decade of experience in creating and refining bioinformatics tools, we propose to amalgamate established Big Data resources into a generalizable model for data annotation and collaborative research, through a new query system and cloud infrastructure for accessing multiple omics repositories, and through computational-supported crowdsourcing initiatives for mining the biomedical literature. We propose to interweave diverse data types for revealing biological networks that coalesce from molecular entities at multiple scales, through machine learning methods for structuring molecular data and defining relationships with drugs and diseases, and through novel algorithms for on-the-cloud integration and pathway visualization of multi-dimensional molecular data. Moreover, we propose to innovate advanced modeling tools to resolve protein dynamics and spatiotemporal molecular mechanisms, through mechanistic modeling of protein properties and 3D protein expression maps, and through Bayesian algorithms that correlate patient phenotypes, health histories, and multi-scale molecular profiles. The utility and customizability o our tools to the broader research population is clearly demonstrated using three archetypical workflows that enable annotations of large lists of genes, transcripts, proteins, or metabolites; powerful analysis of complex protein datasets acquired over time; and seamless aQoregation of diverse molecular, textual and literature data. These workflows will be rigorously validated using data from two significant clinical cohorts, the Jackson Heart Study and the Healthy Elderly Longevity (Wellderly). In parallel, a multifaceted strategy will be implemented to educate and train biomedical investigators, and to engage the public for promoting the overall BD2K initiative. We are convinced that a community-driven BD2K initiative will best realize its scientific potential and transform the research culture in a sustainable manner, exhibiting lasting success beyond the current funding period.         PUBLIC HEALTH RELEVANCE:  The challenges of biomedical Big Data are multifaceted. Biomedical investigators face daunting tasks of storing, analyzing, and distributing large-scale omics data, and aggregating all information to discern mechanistic insights. A coherent effort is required to harness disarrayed Big Data and transform them into intelligible scientific facts, whil engaging the global community via education and outreach programs. This Big Data Science Research proposal is designed to address these challenges by formulating a federated architecture of community-supported tools for enhancing data management, integration and analysis.            ",A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform,9087292,U54GM114833,"['Achievement', 'Address', 'Algorithmic Software', 'Algorithms', 'Architecture', 'Awareness', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Computational Biology', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Science', 'Data Set', 'Disease', 'Education and Outreach', 'Elderly', 'Environment', 'Exhibits', 'Face', 'Funding', 'Future', 'Gene Proteins', 'General Population', 'Generations', 'Genes', 'Goals', 'Half-Life', 'Harvest', 'Health', 'Human', 'Imagery', 'Jackson Heart Study', 'Knowledge', 'Literature', 'Longevity', 'Machine Learning', 'Maps', 'Medicine', 'Mining', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Organ', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Population Research', 'Property', 'Protein Dynamics', 'Proteins', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Rest', 'Scientist', 'Structure', 'Support System', 'System', 'Time', 'Training', 'Training and Education', 'Transcript', 'Translating', 'United States National Institutes of Health', 'big biomedical data', 'clinical phenotype', 'cohort', 'computerized data processing', 'computerized tools', 'crowdsourcing', 'data management', 'data modeling', 'data to knowledge', 'design', 'experience', 'improved', 'innovation', 'insight', 'interest', 'learning strategy', 'multiple omics', 'novel', 'operation', 'outreach program', 'protein complex', 'protein expression', 'protein metabolite', 'protein protein interaction', 'public health relevance', 'repository', 'spatiotemporal', 'success', 'support tools', 'text searching', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,U54,2016,133604,-0.0011246660318267488
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,0.026755170394427145
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9113614,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,601709,0.04357892627529147
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9161167,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Maps', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'information processing', 'knowledge base', 'novel', 'personalized medicine', 'repository', 'response', 'stem', 'tool']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2016,471848,0.011601519793746923
"Big Data Coursework for Computational Medicine DESCRIPTION:  As the era of ""Big Data"" is dawning on biomedical research, multiple types of biomedical data, including phenotypic, molecular (including -omics), clinical, imaging, behavioral, and environmental data is being generated on an unprecedented scale with high volume, variety and velocity. These datasets are increasingly large and complex, challenging our current abilities for data representation, integration and analysis for improving outcomes and reducing healthcare costs. It is well-recognized that the greatest challenge to leveraging the significant potentials of Big Data is in educating and recruiting future computational and data scientists who have the background, training and experience to master fundamental opportunities in biomedical sciences. This demands interdisciplinary education and hands-on practicum training on understanding the application, analysis, limitations, and value of the Big Data. To bridge this knowledge gap for the U.S. biomedical workforce, we propose to develop a research educational program-Big Data Coursework for Computational Medicine (BDC4CM)-that will instruct students, fellows and scientists in the use of specific new methods and tools fo Big Data by providing tailored, in-depth instruction, hands-on laboratory modules, and case studies on Big Data access, integration, processing and analysis. Offered by highly interdisciplinary and experienced faculty from Mayo Clinic and the University of Minnesota, this program will provide a short- term training opportunity on Big Data methods and approaches for: 1) data and knowledge representation standards; 2) information extraction and natural language processing; 3) visualization analytics; 4) data mining and predictive modeling; 5) privacy and ethics; and 6) applications in comparative effectiveness research and population health research and improvement. Our primary educational goal is to prepare the next generation of innovators and visionaries in the emerging, multidimensional field of Big Data Science in healthcare, as well as to develop a future workforce that fulfills industry needs and increases U.S. competitiveness in healthcare technologies and applications. PUBLIC HEALTH RELEVANCE:   The postdoctoral Big Data Coursework for Computational Medicine (BDC4CM) program seeks to provide short-term education and hands-on practicum training in utilization of biomedical Big Data. BDC4CM will address a major need for the U.S. biomedical workforce to develop and enhance existing skills in application, analysis, limitations, and value of the Big Data.",Big Data Coursework for Computational Medicine,9129718,R25EB020381,"['Academic Medical Centers', 'Address', 'Advisory Committees', 'Area', 'Behavioral', 'Big Data', 'Bioethics', 'Biological Sciences', 'Biomedical Research', 'Case Study', 'Clinic', 'Clinical', 'Collection', 'Committee Members', 'Complex', 'Computational Biology', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Discipline', 'Doctor of Medicine', 'Doctor of Philosophy', 'Education', 'Effectiveness', 'Engineering', 'Ethics', 'Evaluation', 'Faculty', 'Feedback', 'Future', 'Goals', 'Grant', 'Health', 'Health Care Costs', 'Health Services Research', 'Healthcare', 'Image', 'Imagery', 'Industry', 'Informatics', 'Instruction', 'Interdisciplinary Education', 'Interview', 'Knowledge', 'Laws', 'Learning', 'Mathematics', 'Measures', 'Medicine', 'Mentorship', 'Methods', 'Minnesota', 'Molecular', 'Monitor', 'Natural Language Processing', 'Patient-Focused Outcomes', 'Peer Review', 'Performance', 'Positioning Attribute', 'Postdoctoral Fellow', 'Privacy', 'Process', 'Program Reviews', 'Public Health', 'Publications', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Students', 'Surveys', 'Technology', 'Training', 'Training Activity', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'career development', 'collaborative environment', 'comparative effectiveness', 'computer science', 'data access', 'data mining', 'education research', 'effectiveness research', 'experience', 'faculty mentor', 'improved', 'improved outcome', 'information organization', 'instrument', 'laboratory experience', 'laboratory module', 'meetings', 'multidisciplinary', 'new technology', 'next generation', 'population health', 'predictive modeling', 'programs', 'skills', 'statistics', 'student training', 'success', 'tool', 'training opportunity', 'working group']",NIBIB,WEILL MEDICAL COLL OF CORNELL UNIV,R25,2016,150010,0.05520646616330813
"Big Data Coursework for Computational Medicine DESCRIPTION:  As the era of ""Big Data"" is dawning on biomedical research, multiple types of biomedical data, including phenotypic, molecular (including -omics), clinical, imaging, behavioral, and environmental data is being generated on an unprecedented scale with high volume, variety and velocity. These datasets are increasingly large and complex, challenging our current abilities for data representation, integration and analysis for improving outcomes and reducing healthcare costs. It is well-recognized that the greatest challenge to leveraging the significant potentials of Big Data is in educating and recruiting future computational and data scientists who have the background, training and experience to master fundamental opportunities in biomedical sciences. This demands interdisciplinary education and hands-on practicum training on understanding the application, analysis, limitations, and value of the Big Data. To bridge this knowledge gap for the U.S. biomedical workforce, we propose to develop a research educational program-Big Data Coursework for Computational Medicine (BDC4CM)-that will instruct students, fellows and scientists in the use of specific new methods and tools fo Big Data by providing tailored, in-depth instruction, hands-on laboratory modules, and case studies on Big Data access, integration, processing and analysis. Offered by highly interdisciplinary and experienced faculty from Mayo Clinic and the University of Minnesota, this program will provide a short- term training opportunity on Big Data methods and approaches for: 1) data and knowledge representation standards; 2) information extraction and natural language processing; 3) visualization analytics; 4) data mining and predictive modeling; 5) privacy and ethics; and 6) applications in comparative effectiveness research and population health research and improvement. Our primary educational goal is to prepare the next generation of innovators and visionaries in the emerging, multidimensional field of Big Data Science in healthcare, as well as to develop a future workforce that fulfills industry needs and increases U.S. competitiveness in healthcare technologies and applications. PUBLIC HEALTH RELEVANCE:   The postdoctoral Big Data Coursework for Computational Medicine (BDC4CM) program seeks to provide short-term education and hands-on practicum training in utilization of biomedical Big Data. BDC4CM will address a major need for the U.S. biomedical workforce to develop and enhance existing skills in application, analysis, limitations, and value of the Big Data.",Big Data Coursework for Computational Medicine,9242970,R25EB020381,"['Academic Medical Centers', 'Address', 'Advisory Committees', 'Area', 'Behavioral', 'Big Data', 'Bioethics', 'Biological Sciences', 'Biomedical Research', 'Case Study', 'Clinic', 'Clinical', 'Collection', 'Committee Members', 'Complex', 'Computational Biology', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Discipline', 'Doctor of Medicine', 'Doctor of Philosophy', 'Education', 'Effectiveness', 'Engineering', 'Ethics', 'Evaluation', 'Faculty', 'Feedback', 'Future', 'Goals', 'Grant', 'Health', 'Health Care Costs', 'Health Services Research', 'Healthcare', 'Image', 'Imagery', 'Industry', 'Informatics', 'Instruction', 'Interdisciplinary Education', 'Interview', 'Knowledge', 'Laws', 'Learning', 'Mathematics', 'Measures', 'Medicine', 'Mentorship', 'Methods', 'Minnesota', 'Molecular', 'Monitor', 'Natural Language Processing', 'Patient-Focused Outcomes', 'Peer Review', 'Performance', 'Positioning Attribute', 'Postdoctoral Fellow', 'Privacy', 'Process', 'Program Reviews', 'Public Health', 'Publications', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Students', 'Surveys', 'Technology', 'Training', 'Training Activity', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'career development', 'collaborative environment', 'comparative effectiveness', 'computer science', 'data access', 'data mining', 'education research', 'effectiveness research', 'experience', 'faculty mentor', 'improved', 'improved outcome', 'information organization', 'instrument', 'laboratory experience', 'laboratory module', 'meetings', 'multidisciplinary', 'new technology', 'next generation', 'population health', 'predictive modeling', 'programs', 'skills', 'statistics', 'student training', 'success', 'tool', 'training opportunity', 'working group']",NIBIB,WEILL MEDICAL COLL OF CORNELL UNIV,R25,2016,68279,0.05520646616330813
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9123422,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Natural Products', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'novel therapeutics', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2016,419732,0.044783699767831704
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9195864,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphorylation Site', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Databases', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Regulation', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Staging', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'abstracting', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'kinase inhibitor', 'knowledge base', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2016,374400,0.027087487864241075
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9097763,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2016,52816,0.0011702509839642775
"Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership ﻿    DESCRIPTION (provided by applicant): The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The aims to achieve the goals are: 1) Implement an integrated didactic program to enhance student self-efficacy with computational and informatics tool development and use for interrogating and interpreting Big Data, including a two-semester bioinformatics course in Bioinformatics, informed by the expertise of UIUC KnowEnG BD2K Center faculty with additional Special Topics courses available remotely from UIUC. 2) Develop an integrated academic year (Fisk, or partners) and summer research program at the UIUC KnowEnG BD2K Center to assure student exposure to a participation in the life cycle of a `Big Data' research problem. 3) Implement a professional skills development program that assures successful transition of undergraduate participants to a Ph.D. (or MD/PhD Program) in Big Data- reliant biomedical research. 4) Launch a faculty development program in bioinformatics that leads to embracing Big Data problems in courses in multiple disciplines for impact on all Fisk undergraduate STEM majors. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fisk R25 mentors, of value for their broader education and research training goals at UIUC and Mayo. PUBLIC HEALTH RELEVANCE: The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fis R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.  ",Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership,9150655,R25MD010396,"['Address', 'Base Pairing', 'Big Data', 'Big Data to Knowledge', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Career Choice', 'Cellular biology', 'Chemicals', 'Collaborations', 'Communication', 'Complement', 'Computers', 'Core Facility', 'Data Analyses', 'Development Plans', 'Discipline', 'Diving', 'Doctor of Philosophy', 'Education Projects', 'Educational Curriculum', 'Educational workshop', 'Elements', 'Exposure to', 'Faculty', 'Faculty Workshop', 'Fostering', 'Funding', 'Genomics', 'Goals', 'Grant', 'Health', 'Illinois', 'Informatics', 'Joints', 'Journals', 'Learning', 'Life Cycle Stages', 'Literature', 'Machine Learning', 'Manuscripts', 'Mathematics', 'Mentors', 'Molecular Biology', 'Oral', 'Participant', 'Proteomics', 'Reading', 'Recruitment Activity', 'Research', 'Research Ethics', 'Research Personnel', 'Research Training', 'Role', 'STEM field', 'Science', 'Scientist', 'Self Efficacy', 'Skills Development', 'Students', 'System', 'Teacher Professional Development', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'Universities', 'Work', 'Writing', 'base', 'career', 'computer science', 'data acquisition', 'data sharing', 'education research', 'experience', 'faculty research', 'individualized medicine', 'innovation', 'minority scientist', 'posters', 'programs', 'responsible research conduct', 'skill acquisition', 'skills', 'summer research', 'tool', 'tool development', 'undergraduate research']",NIMHD,FISK UNIVERSITY,R25,2016,204941,0.0377869370566231
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,0.04266860365174229
"Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems DESCRIPTION (provided by applicant): Abstract Biomedical science, higher education, software and technology are simultaneously undergoing tectonic shifts. The amazing pace of software and technological development are driving equally amazing advances in the ability to acquire massive data sets in the biomedical sciences. These new Big Biomedical data sets come in the form of complex measurements, such as that of the brain, genome, proteome and human biome or massive databases, such as with electronic health records. Big Data issues, such as reproducibility of processing, measurement and analysis techniques, are increasingly complex, and crucial. Across all domains there is a knowledge gap of researchers to analyze and interpret these new data sets and the current higher education model cannot meet the insatiable demand for this training. We propose to make substantial progress on these issues in two domains. Specifically, we propose to use Massive Open Online Courses (MOOCs) to create two series, one in neuroimaging and one in genomics. These series will allow for flexible, student paced, low cost scalable training for tens of thousands of students. Along with these series, we propose the creation of modular Big Data biostatistical content that can be used by students as well as teachers. This effort will be parallel to work on an intelligent tutoring syste called swirl. This application proposes to use swirl to create rich, gamified learning environments for students. All of the material created from this grant will be open access and free. PUBLIC HEALTH RELEVANCE:  Project narrative: We propose two Massive Open Online Course series in neuroimaging and genomic Big Data analysis as well as the creation of modular Big Data statistics content and content creation for an intelligent tutoring system.","Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems",9061684,R25EB020378,"['Adopted', 'Amaze', 'Area', 'Attention', 'Automobile Driving', 'Big Data', 'Biological', 'Biology', 'Brain', 'Clinical Trials', 'Communities', 'Complex', 'Computer software', 'Cost Analysis', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Science', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Discipline', 'Drops', 'Education', 'Educational Curriculum', 'Educational Models', 'Educational process of instructing', 'Electronic Health Record', 'Enrollment', 'Environment', 'Generations', 'Genes', 'Genome', 'Genomics', 'Grant', 'Head', 'Health', 'Human', 'Image', 'Knowledge', 'Laboratories', 'Learning Module', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medicine', 'Modeling', 'Molecular Biology', 'Molecular Medicine', 'Multivariate Analysis', 'Persons', 'Population', 'Principal Investigator', 'Problem Sets', 'Proteome', 'Public Health', 'Public Health Nurses', 'Public Health Nursing', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Series', 'Services', 'Statistical Data Interpretation', 'Stream', 'Students', 'System', 'Systems Biology', 'Teacher Professional Development', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'Touch sensation', 'Training', 'Training Activity', 'Training Programs', 'Universities', 'Work', 'abstracting', 'big biomedical data', 'contrast enhanced', 'cost', 'course module', 'density', 'educational atmosphere', 'flexibility', 'hands-on learning', 'higher education', 'instructor', 'learning materials', 'lectures', 'massive open online courses', 'meetings', 'multidisciplinary', 'neuroimaging', 'new technology', 'novel strategies', 'open source', 'operation', 'process repeatability', 'programs', 'research study', 'skills', 'statistics', 'teacher', 'tutoring']",NIBIB,JOHNS HOPKINS UNIVERSITY,R25,2016,209444,0.03373406371803683
"The q-bio Summer School DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required. PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.",The q-bio Summer School,9002062,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Career Mobility', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Health', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Professional Competence', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Groups', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'learning materials', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2016,168780,0.021036781864360546
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8997510,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2016,510376,0.04999401770559501
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8987580,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'Thinking', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'light weight', 'next generation', 'novel', 'online community', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2016,526540,0.06119901244843551
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,0.02871350841129813
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8840267,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2015,490160,0.05632457037499085
"Transforming Analytical Learning in the Era of Big Data ﻿    DESCRIPTION (provided by applicant): In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio) statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offer and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will draw from the expertise and experience of faculty from four different departments within four different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts, and Information Science in the School of Information. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high-dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and imaging. The diseases and conditions they study include obesity, cancer, diabetes, cardiovascular disease, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and response for our pilot offering in 2015 with 153 applications for 20 positions and a yield rate of 80% from the offers we extended. We plan to build on the success of this initial offering in the next three year funding cycle of this grant (2016-2018). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a no-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 30 undergraduates nationally and expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world.         PUBLIC HEALTH RELEVANCE: We propose a six week long summer institute: ""Transforming Analytical Learning in the Era of Big Data"" to be held at the Department of Biostatistics, University of Michigan, Ann Arbor, with a group of approximately 30 undergraduates recruited nationally, from 2016-2018. We plan to expose them to diverse techniques, skills and problems in the field of Big Data. They will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Statistics, Computer Science and Engineering, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by UM researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm.            ",Transforming Analytical Learning in the Era of Big Data,9044118,R25EB022363,"['Adverse drug effect', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Behavioral Sciences', 'Big Data', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Case Study', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Epigenetic Process', 'Faculty', 'Funding', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Image', 'Imagery', 'Information Sciences', 'Injury', 'Institutes', 'Kidney Diseases', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Prevention', 'Public Health Informatics', 'Public Health Schools', 'Recruitment Activity', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Statistical Methods', 'Students', 'Talents', 'Techniques', 'Training', 'Universities', 'Work', 'base', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'design', 'experience', 'graduate student', 'instructor', 'interest', 'lectures', 'meetings', 'member', 'metabolomics', 'nervous system disorder', 'next generation', 'open source', 'personalized medicine', 'posters', 'programs', 'public health relevance', 'response', 'signal processing', 'skills', 'statistics', 'success', 'symposium', 'tool', 'undergraduate student', 'wiki']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2015,159359,0.03842087222361051
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims.         PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.            ",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,8874546,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Knowledge', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2015,478724,0.012478652575592426
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,0.007354034271973206
"Summer Institute for Statistics of Big Data DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support. PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.",Summer Institute for Statistics of Big Data,8935790,R25EB020380,"['Academia', 'Area', 'Big Data', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'biomedical scientist', 'data visualization', 'graduate student', 'human disease', 'improved', 'instructor', 'lectures', 'member', 'open source', 'programs', 'skills', 'statistics', 'teacher', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2015,159605,0.041369056497787465
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.             Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,8819017,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,548439,0.028755463491851093
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community.         PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.            ",The Big DIPA: Data Image Processing and Analysis,9044533,R25EB022366,"['Accounting', 'Address', 'Architecture', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Cell physiology', 'Communities', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Education', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Microscopy', 'Modality', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Science', 'Scientist', 'Senior Scientist', 'Signal Transduction', 'Site', 'Software Tools', 'Source', 'Staging', 'Stream', 'Systems Biology', 'TNFRSF5 gene', 'Time', 'Training', 'United States National Institutes of Health', 'Work', 'bioimaging', 'biological systems', 'biomedical scientist', 'citizen science', 'computer science', 'computerized tools', 'cost', 'data acquisition', 'data format', 'demographics', 'design', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'lecture notes', 'lectures', 'meetings', 'nervous system disorder', 'open source', 'outreach', 'programs', 'public health relevance', 'repository', 'skills', 'statistics', 'theories', 'tissue processing']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2015,161997,0.031437598777923105
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,0.015171398150477347
"Developing Cloud-based tools for Big Neural Data DESCRIPTION (provided by applicant): Big data has the potential to dramatically advance the electrophysiology biodata sciences in similar ways that it has transformed Genetics. Differences between these two areas dictate separate approaches to apply Big Data tools, and methods in order to provide successful assets to the research community. For one, neural datasets are very heterogeneous by nature. The data is difficult to interpret without knowing specifics about the data acquisition protocol, the experimental paradigm and the physiological state of the recorded subject. Many neural datasets are complemented with complex meta-data sets, which should be an integral component in any effort to integrate and share these data with other researchers. The goal of this project is to develop novel, generalizable Big Data tools to facilitate cloud-base analysis of complex multi-scale neural data. Epilepsy research will be used as a specific use case to guide the development of the tools. A cohort of established senior investigators performing epilepsy research will use and validate these tools in their laboratories. Epilepsy research is currently limited by its narrow focus on single models (animal or human) in individual centers and laboratories. Just as Genetics was revolutionized through Big Data techniques, so too can Epilepsy research be transformed through novel approaches to standardize, share, and mine data across groups of investigators. Over the past several years I have co-developed a NINDS funded cloud-based data platform, ://ieeg.org, giving me a central role in developing Big Data solutions for neural data, such as customized data sharing, large-scale cloud-based data analysis, and search and interrogation techniques for complex data and metadata. My scientific objectives for this project are: (1) to develop generalizable tools to curate, analyze, and interrogate multi-scale neural data, and (2) to create a platform that will galvanize a research community focused on sharing data, and methods to advance Big Data research in the basic and translational neurosciences. Equally important to this proposal, I present a training plan to prepare me for an academic career focused on Big Data in the neurosciences. This plan supplements my background in bioengineering and statistical modeling of neural data with broader data-science expertise in data integration and machine learning, and deeper domain knowledge of the clinical neurosciences. I have assembled a group of collaborators, basic investigators and clinician scientists, who will use the tools developed in this project to analyze and validate their data and methods. I will use the results of this project as the foundation for a R01 Grant application, in which I will expand the developed platform and tools to target other research domains (TBI, Emergency Care, Cardiac), as well as integrate other data-modalities such as Imaging, and Genomics. OMB No. 0925-0001/0002 (Rev. 08/12 Approved Through 8/31/2015) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: The goal of this proposal is to advance Big Data research in the neurosciences by developing tools and techniques to interrogate electrophysiology data sets from animal models of human neurological disorders. Development of these tools requires close collaboration between domain experts in Neuroscience, Machine Learning, Statistics and Computer Science. When developed, this platform and these tools will allow investigators to share, collaborate, annotate, standardize and analyze large, complex, multiscale data sets that are a crucial first step in advancing this field.",Developing Cloud-based tools for Big Neural Data,8935817,K01ES025436,"['Animal Model', 'Applications Grants', 'Area', 'Big Data', 'Biomedical Engineering', 'Cardiac', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational Technique', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Electrophysiology (science)', 'Emergency Care', 'Epilepsy', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Incentives', 'Individual', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Mining', 'Modality', 'National Institute of Neurological Disorders and Stroke', 'Nature', 'Neurosciences', 'Organism', 'Performance', 'Physiological', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Series', 'Solutions', 'Standardization', 'Statistical Models', 'Techniques', 'Time', 'Training', 'base', 'career', 'cloud based', 'cohort', 'comparative', 'computer science', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'improved', 'nervous system disorder', 'novel', 'novel strategies', 'relating to nervous system', 'statistics', 'tool', 'tool development', 'translational neuroscience']",NIEHS,UNIVERSITY OF PENNSYLVANIA,K01,2015,192201,0.010689883580794946
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"".         PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.                 ",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,8864679,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Solutions', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'design', 'innovation', 'interest', 'knowledge translation', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2015,445887,0.0322390491312485
"A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform     DESCRIPTION (provided by applicant): The inception of the BD2K Initiative is a testament to the foresight of NIH and our community. Clearly, the future of biomedicine rests on our collective ability to transform Big Data into intelligible scientific facts. In line with the BD2K objectives,our goal is to revolutionize how we address the universal challenge to discern meaning from unruly data. Capitalizing on our investigators' complementary strengths in computational biology and cardiovascular medicine, we will present a fusion of cutting-edge innovations that are grounded in a cardiovascular research focus, encompassing: (i) on-the-cloud data processing, (ii) crowd sourcing and text-mining data annotation, (iii) protein spatiotemporal dynamics, (iv) multi-omic integration, and (v) multiscale clinical data modeling. Drawing from our decade of experience in creating and refining bioinformatics tools, we propose to amalgamate established Big Data resources into a generalizable model for data annotation and collaborative research, through a new query system and cloud infrastructure for accessing multiple omics repositories, and through computational-supported crowdsourcing initiatives for mining the biomedical literature. We propose to interweave diverse data types for revealing biological networks that coalesce from molecular entities at multiple scales, through machine learning methods for structuring molecular data and defining relationships with drugs and diseases, and through novel algorithms for on-the-cloud integration and pathway visualization of multi-dimensional molecular data. Moreover, we propose to innovate advanced modeling tools to resolve protein dynamics and spatiotemporal molecular mechanisms, through mechanistic modeling of protein properties and 3D protein expression maps, and through Bayesian algorithms that correlate patient phenotypes, health histories, and multi-scale molecular profiles. The utility and customizability o our tools to the broader research population is clearly demonstrated using three archetypical workflows that enable annotations of large lists of genes, transcripts, proteins, or metabolites; powerful analysis of complex protein datasets acquired over time; and seamless aQoregation of diverse molecular, textual and literature data. These workflows will be rigorously validated using data from two significant clinical cohorts, the Jackson Heart Study and the Healthy Elderly Longevity (Wellderly). In parallel, a multifaceted strategy will be implemented to educate and train biomedical investigators, and to engage the public for promoting the overall BD2K initiative. We are convinced that a community-driven BD2K initiative will best realize its scientific potential and transform the research culture in a sustainable manner, exhibiting lasting success beyond the current funding period.         PUBLIC HEALTH RELEVANCE:  The challenges of biomedical Big Data are multifaceted. Biomedical investigators face daunting tasks of storing, analyzing, and distributing large-scale omics data, and aggregating all information to discern mechanistic insights. A coherent effort is required to harness disarrayed Big Data and transform them into intelligible scientific facts, whil engaging the global community via education and outreach programs. This Big Data Science Research proposal is designed to address these challenges by formulating a federated architecture of community-supported tools for enhancing data management, integration and analysis.            ",A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform,8935858,U54GM114833,"['Achievement', 'Address', 'Algorithmic Software', 'Algorithms', 'Architecture', 'Awareness', 'Big Data', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Computational Biology', 'Crowding', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Disease', 'Education and Outreach', 'Elderly', 'Environment', 'Exhibits', 'Face', 'Funding', 'Future', 'Gene Proteins', 'General Population', 'Generations', 'Genes', 'Goals', 'Half-Life', 'Harvest', 'Health', 'Human', 'Imagery', 'Jackson Heart Study', 'Knowledge', 'Literature', 'Longevity', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Organ', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Population Research', 'Property', 'Protein Dynamics', 'Proteins', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Rest', 'Science', 'Scientist', 'Structure', 'Support System', 'System', 'Time', 'Training', 'Training and Education', 'Transcript', 'Translating', 'United States National Institutes of Health', 'clinical phenotype', 'cohort', 'computerized data processing', 'computerized tools', 'data management', 'data modeling', 'design', 'experience', 'improved', 'innovation', 'insight', 'interest', 'novel', 'operation', 'outreach program', 'protein complex', 'protein expression', 'protein metabolite', 'protein protein interaction', 'public health relevance', 'repository', 'spatiotemporal', 'success', 'text searching', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,U54,2015,133604,-0.0011246660318267488
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,0.028933683414673166
"A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform     DESCRIPTION (provided by applicant): The inception of the BD2K Initiative is a testament to the foresight of NIH and our community. Clearly, the future of biomedicine rests on our collective ability to transform Big Data into intelligible scientific facts. In line with the BD2K objectives,our goal is to revolutionize how we address the universal challenge to discern meaning from unruly data. Capitalizing on our investigators' complementary strengths in computational biology and cardiovascular medicine, we will present a fusion of cutting-edge innovations that are grounded in a cardiovascular research focus, encompassing: (i) on-the-cloud data processing, (ii) crowd sourcing and text-mining data annotation, (iii) protein spatiotemporal dynamics, (iv) multi-omic integration, and (v) multiscale clinical data modeling. Drawing from our decade of experience in creating and refining bioinformatics tools, we propose to amalgamate established Big Data resources into a generalizable model for data annotation and collaborative research, through a new query system and cloud infrastructure for accessing multiple omics repositories, and through computational-supported crowdsourcing initiatives for mining the biomedical literature. We propose to interweave diverse data types for revealing biological networks that coalesce from molecular entities at multiple scales, through machine learning methods for structuring molecular data and defining relationships with drugs and diseases, and through novel algorithms for on-the-cloud integration and pathway visualization of multi-dimensional molecular data. Moreover, we propose to innovate advanced modeling tools to resolve protein dynamics and spatiotemporal molecular mechanisms, through mechanistic modeling of protein properties and 3D protein expression maps, and through Bayesian algorithms that correlate patient phenotypes, health histories, and multi-scale molecular profiles. The utility and customizability o our tools to the broader research population is clearly demonstrated using three archetypical workflows that enable annotations of large lists of genes, transcripts, proteins, or metabolites; powerful analysis of complex protein datasets acquired over time; and seamless aQoregation of diverse molecular, textual and literature data. These workflows will be rigorously validated using data from two significant clinical cohorts, the Jackson Heart Study and the Healthy Elderly Longevity (Wellderly). In parallel, a multifaceted strategy will be implemented to educate and train biomedical investigators, and to engage the public for promoting the overall BD2K initiative. We are convinced that a community-driven BD2K initiative will best realize its scientific potential and transform the research culture in a sustainable manner, exhibiting lasting success beyond the current funding period.         PUBLIC HEALTH RELEVANCE:  The challenges of biomedical Big Data are multifaceted. Biomedical investigators face daunting tasks of storing, analyzing, and distributing large-scale omics data, and aggregating all information to discern mechanistic insights. A coherent effort is required to harness disarrayed Big Data and transform them into intelligible scientific facts, whil engaging the global community via education and outreach programs. This Big Data Science Research proposal is designed to address these challenges by formulating a federated architecture of community-supported tools for enhancing data management, integration and analysis.            ",A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform,9065764,U54GM114833,"['Achievement', 'Address', 'Algorithmic Software', 'Algorithms', 'Architecture', 'Awareness', 'Big Data', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Computational Biology', 'Crowding', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Disease', 'Education and Outreach', 'Elderly', 'Environment', 'Exhibits', 'Face', 'Funding', 'Future', 'Gene Proteins', 'General Population', 'Generations', 'Genes', 'Goals', 'Half-Life', 'Harvest', 'Health', 'Human', 'Imagery', 'Jackson Heart Study', 'Knowledge', 'Literature', 'Longevity', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Organ', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Population Research', 'Property', 'Protein Dynamics', 'Proteins', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Rest', 'Science', 'Scientist', 'Structure', 'Support System', 'System', 'Time', 'Training', 'Training and Education', 'Transcript', 'Translating', 'United States National Institutes of Health', 'clinical phenotype', 'cohort', 'computerized data processing', 'computerized tools', 'data management', 'data modeling', 'design', 'experience', 'improved', 'innovation', 'insight', 'interest', 'novel', 'operation', 'outreach program', 'protein complex', 'protein expression', 'protein metabolite', 'protein protein interaction', 'public health relevance', 'repository', 'spatiotemporal', 'success', 'text searching', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,U54,2015,183520,-0.0011246660318267488
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,0.028933683414673166
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8866232,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,602189,0.04357892627529147
"Big Data Coursework for Computational Medicine DESCRIPTION:  As the era of ""Big Data"" is dawning on biomedical research, multiple types of biomedical data, including phenotypic, molecular (including -omics), clinical, imaging, behavioral, and environmental data is being generated on an unprecedented scale with high volume, variety and velocity. These datasets are increasingly large and complex, challenging our current abilities for data representation, integration and analysis for improving outcomes and reducing healthcare costs. It is well-recognized that the greatest challenge to leveraging the significant potentials of Big Data is in educating and recruiting future computational and data scientists who have the background, training and experience to master fundamental opportunities in biomedical sciences. This demands interdisciplinary education and hands-on practicum training on understanding the application, analysis, limitations, and value of the Big Data. To bridge this knowledge gap for the U.S. biomedical workforce, we propose to develop a research educational program-Big Data Coursework for Computational Medicine (BDC4CM)-that will instruct students, fellows and scientists in the use of specific new methods and tools fo Big Data by providing tailored, in-depth instruction, hands-on laboratory modules, and case studies on Big Data access, integration, processing and analysis. Offered by highly interdisciplinary and experienced faculty from Mayo Clinic and the University of Minnesota, this program will provide a short- term training opportunity on Big Data methods and approaches for: 1) data and knowledge representation standards; 2) information extraction and natural language processing; 3) visualization analytics; 4) data mining and predictive modeling; 5) privacy and ethics; and 6) applications in comparative effectiveness research and population health research and improvement. Our primary educational goal is to prepare the next generation of innovators and visionaries in the emerging, multidimensional field of Big Data Science in healthcare, as well as to develop a future workforce that fulfills industry needs and increases U.S. competitiveness in healthcare technologies and applications. PUBLIC HEALTH RELEVANCE:   The postdoctoral Big Data Coursework for Computational Medicine (BDC4CM) program seeks to provide short-term education and hands-on practicum training in utilization of biomedical Big Data. BDC4CM will address a major need for the U.S. biomedical workforce to develop and enhance existing skills in application, analysis, limitations, and value of the Big Data.",Big Data Coursework for Computational Medicine,8935791,R25EB020381,"['Academic Medical Centers', 'Address', 'Advisory Committees', 'Area', 'Behavioral', 'Big Data', 'Bioethics', 'Biological Sciences', 'Biomedical Research', 'Case Study', 'Clinic', 'Clinical', 'Collection', 'Committee Members', 'Complex', 'Computational Biology', 'Data', 'Data Reporting', 'Data Set', 'Development', 'Development Plans', 'Discipline', 'Doctor of Medicine', 'Doctor of Philosophy', 'Education', 'Effectiveness', 'Engineering', 'Ethics', 'Evaluation', 'Faculty', 'Feedback', 'Future', 'Goals', 'Grant', 'Health', 'Health Care Costs', 'Health Services Research', 'Healthcare', 'Image', 'Imagery', 'Industry', 'Informatics', 'Instruction', 'Interdisciplinary Education', 'Interview', 'Knowledge', 'Laboratories', 'Laws', 'Learning', 'Mathematics', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Minnesota', 'Molecular', 'Monitor', 'Natural Language Processing', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Positioning Attribute', 'Postdoctoral Fellow', 'Privacy', 'Process', 'Program Reviews', 'Public Health', 'Publications', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Students', 'Surveys', 'Technology', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'career', 'career development', 'collaborative environment', 'comparative effectiveness', 'computer science', 'data mining', 'effectiveness research', 'experience', 'improved', 'information organization', 'instrument', 'meetings', 'multidisciplinary', 'new technology', 'next generation', 'population health', 'predictive modeling', 'programs', 'skills', 'statistics', 'success', 'tool', 'working group']",NIBIB,MAYO CLINIC ROCHESTER,R25,2015,22575,0.05520646616330813
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9126755,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,40625,0.044783699767831704
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9117880,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,379655,0.044783699767831704
"Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership ﻿    DESCRIPTION (provided by applicant): The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The aims to achieve the goals are: 1) Implement an integrated didactic program to enhance student self-efficacy with computational and informatics tool development and use for interrogating and interpreting Big Data, including a two-semester bioinformatics course in Bioinformatics, informed by the expertise of UIUC KnowEnG BD2K Center faculty with additional Special Topics courses available remotely from UIUC. 2) Develop an integrated academic year (Fisk, or partners) and summer research program at the UIUC KnowEnG BD2K Center to assure student exposure to a participation in the life cycle of a `Big Data' research problem. 3) Implement a professional skills development program that assures successful transition of undergraduate participants to a Ph.D. (or MD/PhD Program) in Big Data- reliant biomedical research. 4) Launch a faculty development program in bioinformatics that leads to embracing Big Data problems in courses in multiple disciplines for impact on all Fisk undergraduate STEM majors. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fisk R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.   PUBLIC HEALTH RELEVANCE: The overall goal of the proposed Fisk- UIUC KnowEnG R25 program is to recruit and retain a cadre of under-represented minority scientists prepared to compete for PhD training in biomedical research with already acquired confidence in the use of Big Data. The proposed partnership with the KnowEnG BD2K Center at UIUC will permit curricular enhancements and summer research opportunities for Fisk trainees while, at the same time, reciprocally training natural scientists and mathematics majors in complementary computer and informatics sciences and providing computer science and mathematics undergraduates with essential systems, molecular and cell biology/biochemistry background at Fisk University to provide context for cutting edge genomics, proteomics, and individualized medicine research reliant on Big Data. In addition to curricular and research training program elements, Fisk students will have remote access to seminar courses to increase efficacy in communicating BD2K-based technologies and their applications. Didactic work and undergraduate research experiences will be complemented by an individualized student development plan for honing professional skills, deep understanding of the responsible conduct of research, and wrap-around mentoring to assure subsequent successful entry into competitive BD2K aligned PhD-granting programs. UIUC-hosted summer workshops for faculty will increase confidence in use of Big Data tools, leading to innovations in STEM courses that embrace Big Data, impacting all Fisk STEM undergraduates. Research collaborations between Fisk and BD2K partner faculty also will be fostered. The proposed program will increase both didactic and research experiences in Big Data for Fisk University undergraduates while preparing them for successful entry into PhD-granting programs in related disciplines at research intensive universities. Our KnowEnG partnership also will increase Fisk faculty capacity in Big Data use and foster faculty research collaborations, thus introducing Big Data into course-embedded research, impacting all Fisk University STEM Majors. Reciprocally, our KnowEnG UIUC faculty partners will enrich their holistic mentoring skills of URM trainees based on interactions with Fis R25 mentors, of value for their broader education and research training goals at UIUC and Mayo.  ",Fisk University/UIUC-Mayo KnowENG BD2K Center R25 Partnership,9049946,R25MD010396,"['Address', 'Base Pairing', 'Big Data', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Cellular biology', 'Chemicals', 'Collaborations', 'Communication', 'Complement', 'Computers', 'Core Facility', 'Development', 'Development Plans', 'Discipline', 'Diving', 'Doctor of Philosophy', 'Education', 'Education Projects', 'Educational Curriculum', 'Educational workshop', 'Elements', 'Exposure to', 'Faculty', 'Faculty Workshop', 'Fostering', 'Funding', 'Genomics', 'Goals', 'Grant', 'Health', 'Illinois', 'Informatics', 'Joints', 'Journals', 'Learning', 'Life Cycle Stages', 'Literature', 'Machine Learning', 'Manuscripts', 'Mathematics', 'Mentors', 'Minority', 'Molecular Biology', 'Oral', 'Participant', 'Pathway interactions', 'Program Development', 'Proteomics', 'Reading', 'Recruitment Activity', 'Research', 'Research Ethics', 'Research Personnel', 'Research Training', 'Role', 'STEM field', 'Science', 'Scientist', 'Self Efficacy', 'Students', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'Universities', 'Work', 'Writing', 'base', 'career', 'computer science', 'data acquisition', 'data sharing', 'experience', 'individualized medicine', 'innovation', 'posters', 'programs', 'responsible research conduct', 'skills', 'tool', 'tool development', 'undergraduate research']",NIMHD,FISK UNIVERSITY,R25,2015,168294,0.0377869370566231
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences. Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8906938,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2015,217378,0.014196075454053499
"Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems DESCRIPTION (provided by applicant): Abstract Biomedical science, higher education, software and technology are simultaneously undergoing tectonic shifts. The amazing pace of software and technological development are driving equally amazing advances in the ability to acquire massive data sets in the biomedical sciences. These new Big Biomedical data sets come in the form of complex measurements, such as that of the brain, genome, proteome and human biome or massive databases, such as with electronic health records. Big Data issues, such as reproducibility of processing, measurement and analysis techniques, are increasingly complex, and crucial. Across all domains there is a knowledge gap of researchers to analyze and interpret these new data sets and the current higher education model cannot meet the insatiable demand for this training. We propose to make substantial progress on these issues in two domains. Specifically, we propose to use Massive Open Online Courses (MOOCs) to create two series, one in neuroimaging and one in genomics. These series will allow for flexible, student paced, low cost scalable training for tens of thousands of students. Along with these series, we propose the creation of modular Big Data biostatistical content that can be used by students as well as teachers. This effort will be parallel to work on an intelligent tutoring syste called swirl. This application proposes to use swirl to create rich, gamified learning environments for students. All of the material created from this grant will be open access and free. PUBLIC HEALTH RELEVANCE:  Project narrative: We propose two Massive Open Online Course series in neuroimaging and genomic Big Data analysis as well as the creation of modular Big Data statistics content and content creation for an intelligent tutoring system.","Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems",8935788,R25EB020378,"['Adopted', 'Amaze', 'Area', 'Attention', 'Automobile Driving', 'Big Data', 'Biological', 'Biology', 'Brain', 'Clinical Trials', 'Communities', 'Complex', 'Computer software', 'Cost Analysis', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Discipline', 'Drops', 'Education', 'Educational Curriculum', 'Educational Models', 'Educational process of instructing', 'Electronic Health Record', 'Enrollment', 'Environment', 'Generations', 'Genes', 'Genome', 'Genomics', 'Grant', 'Head', 'Health', 'Human', 'Image', 'Knowledge', 'Laboratories', 'Learning', 'Learning Module', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medicine', 'Modeling', 'Molecular Biology', 'Molecular Medicine', 'Multivariate Analysis', 'Persons', 'Population', 'Principal Investigator', 'Proteome', 'Public Health', 'Public Health Nurses', 'Public Health Nursing', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Series', 'Services', 'Solutions', 'Stream', 'Students', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'Touch sensation', 'Training', 'Training Programs', 'Universities', 'Work', 'abstracting', 'contrast enhanced', 'cost', 'density', 'flexibility', 'instructor', 'lectures', 'meetings', 'multidisciplinary', 'neuroimaging', 'new technology', 'novel strategies', 'open source', 'operation', 'process repeatability', 'programs', 'research study', 'skills', 'statistics', 'teacher']",NIBIB,JOHNS HOPKINS UNIVERSITY,R25,2015,212771,0.03373406371803683
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,8898177,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2015,52816,0.0011702509839642775
"The q-bio Summer School DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required. PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.",The q-bio Summer School,8802880,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Health', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2015,169997,0.021036781864360546
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8840551,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'genetic variant', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2015,219004,-0.001190558971246754
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8803385,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2015,523965,0.04999401770559501
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8788417,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2015,526540,0.06119901244843551
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,0.014292322044549183
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,0.014292322044549183
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,0.014292322044549183
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,0.02871350841129813
"Big Data Coursework for Computational Medicine No abstract available PUBLIC HEALTH RELEVANCE:   The postdoctoral Big Data Coursework for Computational Medicine (BDC4CM) program seeks to provide short-term education and hands-on practicum training in utilization of biomedical Big Data. BDC4CM will address a major need for the U.S. biomedical workforce to develop and enhance existing skills in application, analysis, limitations, and value of the Big Data.",Big Data Coursework for Computational Medicine,9171678,R25EB020381,"['Academic Medical Centers', 'Address', 'Advisory Committees', 'Area', 'Behavioral', 'Big Data', 'Bioethics', 'Biological Sciences', 'Biomedical Research', 'Case Study', 'Clinic', 'Clinical', 'Collection', 'Committee Members', 'Complex', 'Computational Biology', 'Data', 'Data Reporting', 'Data Set', 'Development', 'Development Plans', 'Discipline', 'Doctor of Medicine', 'Doctor of Philosophy', 'Education', 'Effectiveness', 'Engineering', 'Ethics', 'Evaluation', 'Faculty', 'Feedback', 'Future', 'Goals', 'Grant', 'Health', 'Health Care Costs', 'Health Services Research', 'Healthcare', 'Image', 'Imagery', 'Industry', 'Informatics', 'Instruction', 'Interdisciplinary Education', 'Interview', 'Knowledge', 'Laboratories', 'Laws', 'Learning', 'Mathematics', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Minnesota', 'Molecular', 'Monitor', 'Natural Language Processing', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Positioning Attribute', 'Postdoctoral Fellow', 'Privacy', 'Process', 'Program Reviews', 'Public Health', 'Publications', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Students', 'Surveys', 'Technology', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'career', 'career development', 'collaborative environment', 'comparative effectiveness', 'computer science', 'data mining', 'effectiveness research', 'experience', 'improved', 'information organization', 'instrument', 'meetings', 'multidisciplinary', 'new technology', 'next generation', 'population health', 'predictive modeling', 'programs', 'skills', 'statistics', 'success', 'tool', 'working group']",NIBIB,WEILL MEDICAL COLL OF CORNELL UNIV,R25,2015,128308,0.04155577173697527
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8734439,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2014,489755,0.05632457037499085
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8669161,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2014,212994,0.005201472183045317
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8683213,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2014,221252,-0.02262447213338622
"Summer Institute for Statistics of Big Data     DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support.         PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.            ",Summer Institute for Statistics of Big Data,8829422,R25EB020380,"['Academia', 'Area', 'Big Data', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'biomedical scientist', 'graduate student', 'human disease', 'improved', 'instructor', 'lectures', 'member', 'open source', 'programs', 'public health relevance', 'skills', 'statistics', 'teacher', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2014,160523,0.041369056497787465
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,0.015171398150477347
"Developing Cloud-based tools for Big Neural Data     DESCRIPTION (provided by applicant): Big data has the potential to dramatically advance the electrophysiology biodata sciences in similar ways that it has transformed Genetics. Differences between these two areas dictate separate approaches to apply Big Data tools, and methods in order to provide successful assets to the research community. For one, neural datasets are very heterogeneous by nature. The data is difficult to interpret without knowing specifics about the data acquisition protocol, the experimental paradigm and the physiological state of the recorded subject. Many neural datasets are complemented with complex meta-data sets, which should be an integral component in any effort to integrate and share these data with other researchers. The goal of this project is to develop novel, generalizable Big Data tools to facilitate cloud-base analysis of complex multi-scale neural data. Epilepsy research will be used as a specific use case to guide the development of the tools. A cohort of established senior investigators performing epilepsy research will use and validate these tools in their laboratories. Epilepsy research is currently limited by its narrow focus on single models (animal or human) in individual centers and laboratories. Just as Genetics was revolutionized through Big Data techniques, so too can Epilepsy research be transformed through novel approaches to standardize, share, and mine data across groups of investigators. Over the past several years I have co-developed a NINDS funded cloud-based data platform, ://ieeg.org, giving me a central role in developing Big Data solutions for neural data, such as customized data sharing, large-scale cloud-based data analysis, and search and interrogation techniques for complex data and metadata. My scientific objectives for this project are: (1) to develop generalizable tools to curate, analyze, and interrogate multi-scale neural data, and (2) to create a platform that will galvanize a research community focused on sharing data, and methods to advance Big Data research in the basic and translational neurosciences. Equally important to this proposal, I present a training plan to prepare me for an academic career focused on Big Data in the neurosciences. This plan supplements my background in bioengineering and statistical modeling of neural data with broader data-science expertise in data integration and machine learning, and deeper domain knowledge of the clinical neurosciences. I have assembled a group of collaborators, basic investigators and clinician scientists, who will use the tools developed in this project to analyze and validate their data and methods. I will use the results of this project as the foundation for a R01 Grant application, in which I will expand the developed platform and tools to target other research domains (TBI, Emergency Care, Cardiac), as well as integrate other data-modalities such as Imaging, and Genomics. OMB No. 0925-0001/0002 (Rev. 08/12 Approved Through 8/31/2015) Page Continuation Format Page         PUBLIC HEALTH RELEVANCE: The goal of this proposal is to advance Big Data research in the neurosciences by developing tools and techniques to interrogate electrophysiology data sets from animal models of human neurological disorders. Development of these tools requires close collaboration between domain experts in Neuroscience, Machine Learning, Statistics and Computer Science. When developed, this platform and these tools will allow investigators to share, collaborate, annotate, standardize and analyze large, complex, multiscale data sets that are a crucial first step in advancing this field.            ",Developing Cloud-based tools for Big Neural Data,8830141,K01ES025436,"['Animal Model', 'Applications Grants', 'Area', 'Big Data', 'Biomedical Engineering', 'Cardiac', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational Technique', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Electrophysiology (science)', 'Emergency Care', 'Epilepsy', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Incentives', 'Individual', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Mining', 'Modality', 'National Institute of Neurological Disorders and Stroke', 'Nature', 'Neurosciences', 'Organism', 'Performance', 'Physiological', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Series', 'Solutions', 'Standardization', 'Statistical Models', 'Techniques', 'Time', 'Training', 'base', 'career', 'cloud based', 'cohort', 'comparative', 'computer science', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'improved', 'nervous system disorder', 'novel', 'novel strategies', 'relating to nervous system', 'statistics', 'tool', 'tool development', 'translational neuroscience']",NIEHS,UNIVERSITY OF PENNSYLVANIA,K01,2014,192201,0.010689883580794946
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.       PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8628843,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'genome annotation', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,293768,-0.017400222189621218
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,0.028933683414673166
"A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform     DESCRIPTION (provided by applicant): The inception of the BD2K Initiative is a testament to the foresight of NIH and our community. Clearly, the future of biomedicine rests on our collective ability to transform Big Data into intelligible scientific facts. In line with the BD2K objectives,our goal is to revolutionize how we address the universal challenge to discern meaning from unruly data. Capitalizing on our investigators' complementary strengths in computational biology and cardiovascular medicine, we will present a fusion of cutting-edge innovations that are grounded in a cardiovascular research focus, encompassing: (i) on-the-cloud data processing, (ii) crowd sourcing and text-mining data annotation, (iii) protein spatiotemporal dynamics, (iv) multi-omic integration, and (v) multiscale clinical data modeling. Drawing from our decade of experience in creating and refining bioinformatics tools, we propose to amalgamate established Big Data resources into a generalizable model for data annotation and collaborative research, through a new query system and cloud infrastructure for accessing multiple omics repositories, and through computational-supported crowdsourcing initiatives for mining the biomedical literature. We propose to interweave diverse data types for revealing biological networks that coalesce from molecular entities at multiple scales, through machine learning methods for structuring molecular data and defining relationships with drugs and diseases, and through novel algorithms for on-the-cloud integration and pathway visualization of multi-dimensional molecular data. Moreover, we propose to innovate advanced modeling tools to resolve protein dynamics and spatiotemporal molecular mechanisms, through mechanistic modeling of protein properties and 3D protein expression maps, and through Bayesian algorithms that correlate patient phenotypes, health histories, and multi-scale molecular profiles. The utility and customizability o our tools to the broader research population is clearly demonstrated using three archetypical workflows that enable annotations of large lists of genes, transcripts, proteins, or metabolites; powerful analysis of complex protein datasets acquired over time; and seamless aQoregation of diverse molecular, textual and literature data. These workflows will be rigorously validated using data from two significant clinical cohorts, the Jackson Heart Study and the Healthy Elderly Longevity (Wellderly). In parallel, a multifaceted strategy will be implemented to educate and train biomedical investigators, and to engage the public for promoting the overall BD2K initiative. We are convinced that a community-driven BD2K initiative will best realize its scientific potential and transform the research culture in a sustainable manner, exhibiting lasting success beyond the current funding period.         PUBLIC HEALTH RELEVANCE:  The challenges of biomedical Big Data are multifaceted. Biomedical investigators face daunting tasks of storing, analyzing, and distributing large-scale omics data, and aggregating all information to discern mechanistic insights. A coherent effort is required to harness disarrayed Big Data and transform them into intelligible scientific facts, whil engaging the global community via education and outreach programs. This Big Data Science Research proposal is designed to address these challenges by formulating a federated architecture of community-supported tools for enhancing data management, integration and analysis.            ",A Community Effort to Translate Protein Data to Knowledge: An Integrated Platform,8774362,U54GM114833,"['Achievement', 'Address', 'Algorithmic Software', 'Algorithms', 'Architecture', 'Awareness', 'Big Data', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Computational Biology', 'Crowding', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Disease', 'Education and Outreach', 'Elderly', 'Environment', 'Exhibits', 'Face', 'Funding', 'Future', 'Gene Proteins', 'General Population', 'Generations', 'Genes', 'Goals', 'Half-Life', 'Harvest', 'Health', 'Human', 'Imagery', 'Jackson Heart Study', 'Knowledge', 'Literature', 'Longevity', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Organ', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Population Research', 'Property', 'Protein Dynamics', 'Proteins', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Rest', 'Science', 'Scientist', 'Structure', 'Support System', 'System', 'Time', 'Training', 'Training and Education', 'Transcript', 'Translating', 'United States National Institutes of Health', 'clinical phenotype', 'cohort', 'computerized data processing', 'computerized tools', 'data management', 'data modeling', 'design', 'experience', 'improved', 'innovation', 'insight', 'interest', 'novel', 'operation', 'outreach program', 'protein complex', 'protein expression', 'protein metabolite', 'protein protein interaction', 'public health relevance', 'repository', 'spatiotemporal', 'success', 'text searching', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,U54,2014,28157,-0.0011246660318267488
"Developing and applying information extraction resources and technology to create     DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another.              Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8694375,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2014,746561,0.04357892627529147
"Big Data Coursework for Computational Medicine     DESCRIPTION:  As the era of ""Big Data"" is dawning on biomedical research, multiple types of biomedical data, including phenotypic, molecular (including -omics), clinical, imaging, behavioral, and environmental data is being generated on an unprecedented scale with high volume, variety and velocity. These datasets are increasingly large and complex, challenging our current abilities for data representation, integration and analysis for improving outcomes and reducing healthcare costs. It is well-recognized that the greatest challenge to leveraging the significant potentials of Big Data is in educating and recruiting future computational and data scientists who have the background, training and experience to master fundamental opportunities in biomedical sciences. This demands interdisciplinary education and hands-on practicum training on understanding the application, analysis, limitations, and value of the Big Data. To bridge this knowledge gap for the U.S. biomedical workforce, we propose to develop a research educational program-Big Data Coursework for Computational Medicine (BDC4CM)-that will instruct students, fellows and scientists in the use of specific new methods and tools fo Big Data by providing tailored, in-depth instruction, hands-on laboratory modules, and case studies on Big Data access, integration, processing and analysis. Offered by highly interdisciplinary and experienced faculty from Mayo Clinic and the University of Minnesota, this program will provide a short- term training opportunity on Big Data methods and approaches for: 1) data and knowledge representation standards; 2) information extraction and natural language processing; 3) visualization analytics; 4) data mining and predictive modeling; 5) privacy and ethics; and 6) applications in comparative effectiveness research and population health research and improvement. Our primary educational goal is to prepare the next generation of innovators and visionaries in the emerging, multidimensional field of Big Data Science in healthcare, as well as to develop a future workforce that fulfills industry needs and increases U.S. competitiveness in healthcare technologies and applications.         PUBLIC HEALTH RELEVANCE:   The postdoctoral Big Data Coursework for Computational Medicine (BDC4CM) program seeks to provide short-term education and hands-on practicum training in utilization of biomedical Big Data. BDC4CM will address a major need for the U.S. biomedical workforce to develop and enhance existing skills in application, analysis, limitations, and value of the Big Data.            ",Big Data Coursework for Computational Medicine,8827881,R25EB020381,"['Academic Medical Centers', 'Address', 'Advisory Committees', 'Area', 'Behavioral', 'Big Data', 'Bioethics', 'Biological Sciences', 'Biomedical Research', 'Case Study', 'Clinic', 'Clinical', 'Collection', 'Committee Members', 'Complex', 'Computational Biology', 'Data', 'Data Reporting', 'Data Set', 'Development', 'Development Plans', 'Discipline', 'Doctor of Medicine', 'Doctor of Philosophy', 'Education', 'Effectiveness', 'Engineering', 'Environment', 'Ethics', 'Evaluation', 'Faculty', 'Feedback', 'Future', 'Goals', 'Grant', 'Health Care Costs', 'Health Services Research', 'Healthcare', 'Image', 'Imagery', 'Industry', 'Informatics', 'Instruction', 'Interdisciplinary Education', 'Interview', 'Knowledge', 'Laboratories', 'Laws', 'Learning', 'Mathematics', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Metric', 'Minnesota', 'Molecular', 'Monitor', 'Natural Language Processing', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Positioning Attribute', 'Postdoctoral Fellow', 'Privacy', 'Process', 'Program Reviews', 'Public Health', 'Publications', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Students', 'Surveys', 'Technology', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'career', 'career development', 'comparative effectiveness', 'computer science', 'data mining', 'effectiveness research', 'experience', 'improved', 'information organization', 'instrument', 'meetings', 'multidisciplinary', 'new technology', 'next generation', 'population health', 'predictive modeling', 'programs', 'public health relevance', 'skills', 'statistics', 'success', 'tool', 'working group']",NIBIB,MAYO CLINIC ROCHESTER,R25,2014,153437,0.05520646616330813
"In silico identification of phyto-therapies     DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits.                RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.  ",In silico identification of phyto-therapies,8749705,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R01,2014,389869,0.044783699767831704
"Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems     DESCRIPTION (provided by applicant): Abstract Biomedical science, higher education, software and technology are simultaneously undergoing tectonic shifts. The amazing pace of software and technological development are driving equally amazing advances in the ability to acquire massive data sets in the biomedical sciences. These new Big Biomedical data sets come in the form of complex measurements, such as that of the brain, genome, proteome and human biome or massive databases, such as with electronic health records. Big Data issues, such as reproducibility of processing, measurement and analysis techniques, are increasingly complex, and crucial. Across all domains there is a knowledge gap of researchers to analyze and interpret these new data sets and the current higher education model cannot meet the insatiable demand for this training. We propose to make substantial progress on these issues in two domains. Specifically, we propose to use Massive Open Online Courses (MOOCs) to create two series, one in neuroimaging and one in genomics. These series will allow for flexible, student paced, low cost scalable training for tens of thousands of students. Along with these series, we propose the creation of modular Big Data biostatistical content that can be used by students as well as teachers. This effort will be parallel to work on an intelligent tutoring syste called swirl. This application proposes to use swirl to create rich, gamified learning environments for students. All of the material created from this grant will be open access and free.         PUBLIC HEALTH RELEVANCE:  Project narrative: We propose two Massive Open Online Course series in neuroimaging and genomic Big Data analysis as well as the creation of modular Big Data statistics content and content creation for an intelligent tutoring system.            ","Big Data education for the masses: MOOCs, modules, & intelligent tutoring systems",8829370,R25EB020378,"['Adopted', 'Amaze', 'Area', 'Attention', 'Automobile Driving', 'Big Data', 'Biological', 'Biology', 'Brain', 'Clinical Trials', 'Communities', 'Complex', 'Computer software', 'Cost Analysis', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Discipline', 'Drops', 'Education', 'Educational Curriculum', 'Educational Models', 'Educational process of instructing', 'Electronic Health Record', 'Enrollment', 'Environment', 'Generations', 'Genes', 'Genome', 'Genomics', 'Grant', 'Head', 'Human', 'Image', 'Knowledge', 'Laboratories', 'Learning', 'Learning Module', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medicine', 'Modeling', 'Molecular Biology', 'Molecular Medicine', 'Multivariate Analysis', 'Persons', 'Population', 'Principal Investigator', 'Proteome', 'Public Health', 'Public Health Nurses', 'Public Health Nursing', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Series', 'Services', 'Solutions', 'Stream', 'Students', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'Touch sensation', 'Training', 'Training Programs', 'Universities', 'Work', 'abstracting', 'cost', 'density', 'flexibility', 'instructor', 'lectures', 'meetings', 'multidisciplinary', 'neuroimaging', 'new technology', 'novel strategies', 'open source', 'operation', 'process repeatability', 'programs', 'public health relevance', 'research study', 'skills', 'statistics', 'teacher']",NIBIB,JOHNS HOPKINS UNIVERSITY,R25,2014,216000,0.03373406371803683
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.             Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8727093,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2014,223898,0.014196075454053499
"CSHL Computational and Comparative Genomics Course     DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions.         PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.                 ",CSHL Computational and Comparative Genomics Course,8737540,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'public health relevance', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2014,52816,0.0011702509839642775
"The q-bio Summer School     DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required.         PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.            ",The q-bio Summer School,8643269,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'public health relevance', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2014,171217,0.021036781864360546
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8642168,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2014,207764,-0.001190558971246754
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8714053,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,307471,0.040766088761212384
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8628132,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2014,525880,0.04999401770559501
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,0.014292322044549183
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8597446,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2014,533554,0.06119901244843551
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8737919,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2014,2941548,0.07603719756326674
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8474789,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2013,486484,0.05632457037499085
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8484438,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2013,202118,0.005201472183045317
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8518436,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2013,220626,-0.02262447213338622
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.       PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8447507,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'genome annotation', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,290465,-0.017400222189621218
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8733018,UH3HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH3,2013,500000,0.007482006730854419
"The q-bio Summer School     DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required.         PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.            ",The q-bio Summer School,8475276,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'public health relevance', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2013,174626,0.021036781864360546
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8599826,R01CA180777,"['Bees', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2013,214832,-0.001190558971246754
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8535824,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,291730,0.040766088761212384
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral No abstract available  Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8528719,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Chips', 'Genes', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'genome analysis', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2013,526738,-0.011396055971274882
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8504843,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2013,527736,0.04999401770559501
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,0.014292322044549183
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8438361,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2013,533554,0.06119901244843551
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541872,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2013,872488,0.07603719756326674
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.       This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8309015,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2012,179306,0.05632457037499085
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8471822,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2012,224100,0.005201472183045317
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8701603,R01GM095476,[' '],NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2012,332000,0.05632457037499085
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,-0.011425392264445822
"ISMB 2012 Conference Support for Students & Young Scientists     DESCRIPTION (provided by applicant): The 2012 Intelligent Systems for Molecular Biology (ISMB) conference in will be held in Long Beach, California, with 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a professionally organized and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to appy what they have learned as they advance their own research efforts or begin investigating new areas they were exposed to as a result of attending ISMB. The scientific program for each ISMB meeting includes parallel presentation tracks of original research papers, highlights of recently published papers, special sessions focused on emerging topics, technology demos, late breaking research and poster presentations, an art in science exhibition, tutorial workshops, special interest group meetings and a student symposium organized by and for students. For ISMB 2011, 258 original research papers were submitted and 48 selected for the Proceedings Track, while 88 previously published papers were submitted and 38 selected for the Highlights Track. In all, over 225 talks were presented during the course of the 2011 conference, and similar numbers are anticipated for 2012. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.        PUBLIC HEALTH RELEVANCE: Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.              Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.            ",ISMB 2012 Conference Support for Students & Young Scientists,8317817,R13GM101868,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Arts', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'California', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Home environment', 'Human', 'International', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Specialist', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Toy', 'Training', 'Validation', 'Work', 'biological systems', 'career', 'computer program', 'computer science', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2012,25000,0.020719769358258037
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,0.027990710123387306
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8309419,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,596909,0.028247072252975465
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.      PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.           PROJECT NARRATIVE:  TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.",The Transporter Classification Database (TCDB),8230584,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Genome', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,303730,-0.021776469008677794
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8318224,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,572436,0.04525835130765862
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8303361,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2012,516448,0.007482006730854419
"Ontology-based Information Network to Support Vaccine Research  Project Summary (Abstract):  Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.  Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8311060,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'abstracting', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,264994,0.016665589835810315
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.              Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8442618,K99LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,MAYO CLINIC ROCHESTER,K99,2012,96552,0.014196075454053499
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8326650,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,317212,0.040766088761212384
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8242742,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2012,392767,0.05870005965408737
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,0.014292322044549183
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8330927,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,1821611,0.07603719756326674
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541935,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,100000,0.07603719756326674
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.      PUBLIC HEALTH RELEVANCE: This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.          This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8106768,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2011,500273,0.05458597676218182
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence    DESCRIPTION (provided by applicant): Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.           Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8190163,K99LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2011,89802,0.0050159132710199325
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,0.027440402148245394
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.           Project Narrative The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.",Scalable Learning with Ensemble Techniques and Parallel Computing,8045486,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Biological Sciences', 'Biomedical Research', 'Classification', 'Communication', 'Communities', 'Community Financing', 'Companions', 'Complex', 'Computer software', 'Consult', 'Crowding', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Health', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'new technology', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2011,374673,0.026328107059746576
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,0.027990710123387306
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8117587,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,597135,0.028247072252975465
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.      PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.           TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8042509,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Genome', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,324495,-0.021766490504051595
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.      PUBLIC HEALTH RELEVANCE: Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.           Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8145134,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2011,540294,0.0036644782718753773
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8139258,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,513952,0.04525835130765862
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8055880,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,328942,0.0013917449219629293
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8120230,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,264994,0.0171236605083748
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,0.04222689404516109
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,8138486,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2011,266112,0.022435407215286196
"ISMB 2011 Conference Support for Students & Young Scientists    DESCRIPTION (provided by applicant): ISMB 2011 Conference Travel Support for Students and Young Scientists.  The Intelligent Systems for Molecular Biology (ISMB) conference in 2011 will be held in Vienna, Austria, as a conference of approximately 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics and/or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a well organized, well attended, and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Additionally, presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to apply what they have learned as they advance their own research efforts. The scientific program for each ISMB meeting comprises parallel presentation tracks of original research papers, highlights of recently published research, topically focused special sessions on emerging topics, technology demos, tutorial workshops, special interest group meetings and a student symposium organized by and for students. As an example, for ISMB 2010, 234 original research papers were submitted and 48 selected for the Proceedings Track; 126 published papers were submitted and 42 selected for the Highlights Track; nine proposals were submitted and four selected for presentation along with two invited for the Special Sessions Track. In all, well over 200 talks were presented during the course of the 2010 conference, and similar numbers are anticipated for 2011. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.      PUBLIC HEALTH RELEVANCE: Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.           Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.         ",ISMB 2011 Conference Support for Students & Young Scientists,8121309,R13GM097938,"['Address', 'Algorithms', 'Area', 'Austria', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Electronics', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Human', 'Industry', 'International', 'Knowledge', 'Laboratory Scientists', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Role', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Training', 'Travel', 'Validation', 'Vendor', 'Work', 'base', 'biological systems', 'career', 'computer program', 'computer science', 'computerized tools', 'cost', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'predictive modeling', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2011,20000,0.02595530814653305
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8202896,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,312599,0.040766088761212384
"Human-Centered Perceptual and Conceptual Classification of Biomedical Images    DESCRIPTION (provided by applicant): Biomedical images are ever increasing in quantity and importance yet effective computing solutions for managing images and understanding their content are lacking. Image understanding is a key limiting factor in advancing these endeavors. Major challenges remain in understanding the capabilities of the human visual system with respect to biomedical imaging and in extracting and utilizing tacit knowledge of domain experts. To meet these challenges, we propose an innovative, multidisciplinary approach which combines methods of user centered design, visual perception and computer imaging research to interact with domain experts and to elicit and use their extrinsic and intrinsic knowledge. We will use a novel contextual design approach to inspection of dermatology images to discover relationships between perceptually- relevant visual content of images and users' conceptual understanding as expressed through natural language. Analysis of users' eye movements and verbal descriptions, together with mapping to domain medical ontologies, will allow us to integrate visual data with a user-specified language model to define perceptual categories and inform image classification. This is a fundamental and challenging data to knowledge problem that has not been solved. This study will provide proof of concept of the value of eliciting tacit knowledge from domain experts through multiple perceptually relevant modes in order to integrate data and knowledge models for better image understanding and may help enact a paradigm shift in how we conceptualize and develop biomedical information systems, in general.             Project Narrative Biomedical images are ever increasing in quantity yet their usefulness for research, medicine, and teaching is limited by the design of current computing systems. Discoveries and concrete advances made in this study will contribute to solutions for effective use of digital images-a problem that is central to research and application across science, technology, and medicine. Advancements in our understanding of the design of useful and usable information systems will benefit society at large and contribute to the public health.  ",Human-Centered Perceptual and Conceptual Classification of Biomedical Images,8077991,R21LM010039,"['Algorithms', 'Categories', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Computer Systems', 'Conceptual Domain', 'Data', 'Data Set', 'Dermatologist', 'Dermatology', 'Development', 'Diagnosis', 'Educational process of instructing', 'Evaluation', 'Eye', 'Eye Movements', 'Goals', 'Human', 'Hybrids', 'Image', 'Informatics', 'Information Resources', 'Information Systems', 'Internet', 'Knowledge', 'Language', 'Learning', 'Link', 'Maps', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Multimedia', 'Natural Language Processing', 'Ontology', 'Perception', 'Phase', 'Process', 'Public Health', 'Research', 'Retrieval', 'Science', 'Semantics', 'Societies', 'Solutions', 'Specific qualifier value', 'Statistical Models', 'Structure', 'System', 'Technology', 'Training', 'Unified Medical Language System', 'Validation', 'Visual', 'Visual Perception', 'Visual system structure', 'base', 'bioimaging', 'biomedical information system', 'design', 'digital imaging', 'innovation', 'interdisciplinary approach', 'interest', 'meetings', 'natural language', 'novel', 'success', 'tool', 'user centered design', 'vector']",NLM,ROCHESTER INSTITUTE OF TECHNOLOGY,R21,2011,192348,-0.018642714877245056
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,0.014186613131777923
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8039246,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2011,526649,0.05870005965408737
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,0.01167497557478546
"COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems.  To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati,8173654,76201000029C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Health Care Costs', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data mining', 'flexibility', 'innovation', 'novel', 'novel strategies', 'rapid growth']",NLM,NORTHEASTERN UNIVERSITY,N03,2010,377968,0.03174244643434525
"COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible,8170612,76201000023C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Medical Informatics', 'Patients', 'Population', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'prototype', 'rapid growth']",NLM,"CYCORP, INC.",N03,2010,377967,0.030469964022922066
"COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat  In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat ,8172635,76201000024C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Disease', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Human', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patients', 'Population', 'Research', 'Research Personnel', 'Research Project Grants', 'Statistical Methods', 'Structure', 'System', 'Text', 'Thinking', 'biomedical scientist', 'clinical care', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,N03,2010,373073,0.026645376450066427
"COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical,8173644,76201000025C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computer Simulation', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patient Care', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'clinical application', 'evidence based guidelines', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,STANFORD UNIVERSITY,N03,2010,378000,0.031638569809301534
"COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que,8173663,76201000032C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Communities', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'rapid growth', 'social']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,N03,2010,301251,0.03159836707825316
"COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica,8173959,76201000034C,"['Caring', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'clinical decision-making', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SAMSUNG SDS AMERICA, INC.",N03,2010,377925,0.032316838791207884
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173956,76201000033C,"['Biology', 'Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computer software', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Molecular Biology', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data modeling', 'flexibility', 'innovation', 'novel strategies', 'open source', 'prototype', 'rapid growth']",NLM,UNIVERSITY OF COLORADO DENVER,N03,2010,377982,0.034762239407920024
"COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a","COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec",8173645,76201000026C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Population', 'Prevention', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'cost effectiveness', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SIMULCONSULT, INC.",N03,2010,377991,0.0332023587201899
"COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization,8173653,76201000028C,"['Age', 'Caring', 'Chronic Disease', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Complex', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Disease Management', 'Face', 'Family', 'Funding', 'Human', 'Imagery', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Methods', 'Patients', 'Population', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'Visual', 'biomedical scientist', 'design', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"RUTGERS, THE STATE UNIV OF N.J.",N03,2010,300216,0.027956574583758876
"COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De,8173961,76201000035C,"['Caring', 'Childhood', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Disease', 'Expert Systems', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Lead', 'Machine Learning', 'Medical', 'Memory', 'Mental Depression', 'Outcome', 'Patients', 'Probability', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'evidence base', 'flexibility', 'health application', 'innovation', 'novel strategies', 'rapid growth', 'tool', 'treatment planning']",NLM,SRI INTERNATIONAL,N03,2010,377997,0.031642747974317945
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,0.027440402148245394
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,8013208,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2010,376899,0.026057569068531402
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7881671,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,264640,0.018871751926247574
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,0.027990710123387306
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7908806,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,618469,0.028247072252975465
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),8110238,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,29814,0.043785750277732995
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7935408,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,515594,0.04525835130765862
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173647,76201000027C,[' '],NLM,STANFORD UNIVERSITY,N03,2010,378000,0.034762239407920024
"COMPUTATIONAL THINKING-Text Mining of clinical narratives: In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Text Mining of clinical narratives:,8173660,76201000031C,[' '],NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,N03,2010,336943,0.035972384374781376
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7784533,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,341606,0.0013917449219629293
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8056227,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,177422,0.0013917449219629293
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7935464,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2010,267671,0.0171236605083748
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,0.04222689404516109
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7929664,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2010,277200,0.022435407215286196
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,0.014186613131777923
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,0.014186613131777923
"Human-Centered Perceptual and Conceptual Classification of Biomedical Images    DESCRIPTION (provided by applicant): Biomedical images are ever increasing in quantity and importance yet effective computing solutions for managing images and understanding their content are lacking. Image understanding is a key limiting factor in advancing these endeavors. Major challenges remain in understanding the capabilities of the human visual system with respect to biomedical imaging and in extracting and utilizing tacit knowledge of domain experts. To meet these challenges, we propose an innovative, multidisciplinary approach which combines methods of user centered design, visual perception and computer imaging research to interact with domain experts and to elicit and use their extrinsic and intrinsic knowledge. We will use a novel contextual design approach to inspection of dermatology images to discover relationships between perceptually- relevant visual content of images and users' conceptual understanding as expressed through natural language. Analysis of users' eye movements and verbal descriptions, together with mapping to domain medical ontologies, will allow us to integrate visual data with a user-specified language model to define perceptual categories and inform image classification. This is a fundamental and challenging data to knowledge problem that has not been solved. This study will provide proof of concept of the value of eliciting tacit knowledge from domain experts through multiple perceptually relevant modes in order to integrate data and knowledge models for better image understanding and may help enact a paradigm shift in how we conceptualize and develop biomedical information systems, in general.             Project Narrative Biomedical images are ever increasing in quantity yet their usefulness for research, medicine, and teaching is limited by the design of current computing systems. Discoveries and concrete advances made in this study will contribute to solutions for effective use of digital images-a problem that is central to research and application across science, technology, and medicine. Advancements in our understanding of the design of useful and usable information systems will benefit society at large and contribute to the public health.  ",Human-Centered Perceptual and Conceptual Classification of Biomedical Images,7896281,R21LM010039,"['Algorithms', 'Categories', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Computer Systems', 'Conceptual Domain', 'Data', 'Data Set', 'Dermatologist', 'Dermatology', 'Development', 'Diagnosis', 'Educational process of instructing', 'Evaluation', 'Eye', 'Eye Movements', 'Goals', 'Human', 'Hybrids', 'Image', 'Informatics', 'Information Resources', 'Information Systems', 'Internet', 'Knowledge', 'Language', 'Learning', 'Link', 'Maps', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Multimedia', 'Natural Language Processing', 'Ontology', 'Perception', 'Phase', 'Process', 'Public Health', 'Research', 'Retrieval', 'Science', 'Semantics', 'Societies', 'Solutions', 'Specific qualifier value', 'Statistical Models', 'Structure', 'System', 'Technology', 'Training', 'Unified Medical Language System', 'Validation', 'Visual', 'Visual Perception', 'Visual system structure', 'base', 'bioimaging', 'biomedical information system', 'design', 'digital imaging', 'innovation', 'interdisciplinary approach', 'interest', 'meetings', 'natural language', 'novel', 'success', 'tool', 'user centered design', 'vector']",NLM,ROCHESTER INSTITUTE OF TECHNOLOGY,R21,2010,163457,-0.018642714877245056
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,8076789,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2010,956625,0.05753523721291099
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7774343,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2010,525262,0.05870005965408737
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7945368,R01HG004836,"['Anatomy', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Communities', 'Competence', 'Complex', 'Computer software', 'Consultations', 'Controlled Vocabulary', 'Dana-Farber Cancer Institute', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Fostering', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'In Vitro', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Molecular', 'National Cancer Institute', 'Nature', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'Validation', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'design', 'empowered', 'gene function', 'graphical user interface', 'information organization', 'instrument', 'interoperability', 'novel', 'open source', 'repository', 'research study', 'response', 'scale up', 'sound', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,428079,0.059408503645391394
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7928868,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2010,605009,0.04348834276787386
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,0.0054812974531576455
"National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.   n/a",National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet),8012947,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,3757192,0.042413986367816445
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,0.02220507187834409
"Towards the Building of a Comprehensive Searchable Biological Experiment Database    DESCRIPTION (provided by applicant):       The rapid growth of the biomedical literature and the expansion in disciplinary biomedical research, heralded by high-throughput genome sciences and technologies, have overwhelmed scientists who attempt to assimilate information necessary for their research. The widespread adoption of title/abstract word searches, such as highly desirable the National Library of Medicine's PubMed system, has provided the first major advance in the way bioscientists find relevant publications since the origin of Index Medicus in 1879 (Hunter and Cohen 2006). The importance of developing valid information retrieval systems for bioscientists has led to the development of information systems worldwide (e.g., Arrowsmith (Smalheiser and Swanson 1998), BioText (Hearst 2003), GeneWays (Friedman et al. 2001; Rzhetsky et al. 2004), iHOP (Hoffmann and Valencia 2005), and BioMedQA (Lee et al. 2006a), and annotated databases (e.g., SWISSPROT, OMIM (Hamosh et al. 2005) and BIND (Alfarano et al. 2005)).      However, most of information systems target only text information and fail to provide access to other important data such as images (e.g., figures). More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biological articles nearly always incorporate figures/images that are the crucial content of the biomedical literature. Our examination of biological articles in the Proceedings of the National Academy of Sciences (PNAS) revealed the occurrence of 5.2 images per article on average (Yu and Lee 2006a). Biologists need to access image data to validate research facts and to formulate or to test novel research hypotheses. It has been evaluated that textual statements reported in literature frequently are noisy (i.e., containing ""false facts"") (Krauthammer et al. 2002). Capturing images that are experimental ""evidence"" to support the textual ""fact"" will benefit bioscience information systems, databases, and bioscientists.      Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We propose the development of advanced natural language processing (NLP) tools to semantically organize images. We hypothesize that text that associated with images semantically entails the image content and natural language processing techniques can be developed to accurately associate the text to their images. Furthermore, we hypothesize that images can be semantically organized by categories specified by standard biological ontology, and that natural language processing approaches can accurately assign the ontological categories to images.      Our specific aims are:      Aim 1: To develop and evaluate NLP techniques for identifying textual statements that correspond to images in full-text articles. We will develop different approaches for two types of the associations. We will first propose rule-based and statistical approaches to identify the associated text that appears in the full-text articles. We will then develop hybrid approaches to link sentences in abstracts to images in the body of the articles.      Aim 2: To develop and evaluate NLP techniques for automatic classification of experimental results into categories (e.g., Western-Blot, PCR verification, etc) specified in the experimental protocol Protocol-Online.      We will explore the use of dictionary-based, rule-based, image classification, and machine-learning approaches for accomplishing this aim.      Aim 3: To develop and evaluate NLP techniques for automatic assignment of Gene Ontology categories to experiments, which will provide a knowledge-based organization of experiments according to biological properties (e.g., catalytic activity). We will develop statistical and machine-learning approaches for accomplishing this aim.      We found that most of the images that appear in full-text biological articles are figure images (Yu and Lee 2006a) and we therefore focus on figure images only in this proposal. The deliverable of Specific Aim 1 will be an effective user-interface BioEx from which bioscientists can access images directly from sentences in the abstracts. BioEx has the promise of improvement over the traditional single-document-per-article format that has dominated bioscience publications since the first scientific article appeared in 1665 (Gross 2002). The deliverables of Specific Aim 2 and 3 will be open-source algorithms and tools that accurately map images to categories specified by the Gene Ontology and the Protocol Online. Those algorithms and tools will enhance bioscience information retrieval, information extraction, summarization, and question answering.          n/a",Towards the Building of a Comprehensive Searchable Biological Experiment Database,7534822,R21RR024933,"['Adoption', 'Advanced Development', 'Algorithms', 'Binding', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Data', 'Databases', 'Development', 'Dictionary', 'Documentation', 'Flowcharts', 'Genes', 'Genome', 'Hybrids', 'Image', 'Index Medicus', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Natural Language Processing', 'Online Mendelian Inheritance In Man', 'Ontology', 'Principal Investigator', 'Property', 'Protocols documentation', 'PubMed', 'Publications', 'Reporting', 'Research', 'Science', 'Scientist', 'Specific qualifier value', 'SwissProt', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'United States National Academy of Sciences', 'United States National Library of Medicine', 'Western Blotting', 'abstracting', 'base', 'knowledge base', 'novel', 'open source', 'programs', 'rapid growth', 'research study', 'tool']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2009,179517,0.005227278330000622
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7872692,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,66015,0.023294518330634967
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7673720,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,142851,0.023294518330634967
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,0.027440402148245394
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7651469,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'genome database', 'improved', 'interest', 'natural language', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,273993,0.03979005938005147
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7651387,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,267801,0.018871751926247574
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,0.027990710123387306
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7906366,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,123630,-0.016649253453098527
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8151670,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,14785,-0.016649253453098527
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7662449,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,167303,-0.016649253453098527
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7840891,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,39294,-0.016649253453098527
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7668609,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,613451,0.028247072252975465
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7914681,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,116802,0.043785750277732995
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7781934,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,505564,0.04525835130765862
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7653874,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Databases', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,357875,0.0013917449219629293
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,0.02220507187834409
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,0.02220507187834409
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7735790,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2009,270375,0.0171236605083748
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,0.04222689404516109
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7693803,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2009,280000,0.022435407215286196
"Onto-BioThesaurus: ontological representation of gene/protein names for biomedica    DESCRIPTION (provided by applicant):       The long-term goal of our research is to develop resources and tools for knowledge retrieval management in the biomedical domain. As the pace of biomedical research accelerates, researchers become more and more dependent on computers to manage the explosive amount of biomedical information being published. The high quality of many databases is guaranteed by database curators who extract and synthesize information stored in literature or other databases. It is important to accurately recognize biomedical entity names in text and map the identified names to corresponding records in biomedical databases. Usually, a biomedical database provides a list of names either entered by curators or extracted from other databases. Those names could be used to retrieve records from databases or map names to database records by NLP systems. However, there are several characteristics associated with biomedical entity names, namely: synonymy (i.e., different names refer to the same database entry), ambiguity (i.e., one name is associated with different entries), and novelty (i.e., names or entities are not present in databases or knowledge bases) which make the task of retrieving database records using names and the task of associating names in text to database records very daunting. Additionally, biomedical entities can appear in text as short forms (SFs) abbreviated from their long forms (LFs). The prevalent use of SFs representing biomedical entities is another challenge faced by end users and NLP applications because of the high ambiguity of SFs.       Recently, ontology-based knowledge management is becoming increasingly popular since ontologies provide formal, machine-processable, and human-interpretable representations of the biomedical entities and their relations. We hypothesize that biomedical ontologies can be used to reduce the difficulty associated with retrieving records using names or mapping names in text to database records. Specific aims and the corresponding hypotheses are: i) develop onto-BioThesaurus by enriching BioThesaurus with gene/protein-related ontologies (Hypothesis: aligning gene/protein names to gene/protein-related ontologies can reduce the complexity associated with gene/protein names); ii) harvest synonyms for gene/protein classes and entities from online resources and text (Hypothesis: harvesting synonyms especially gene/protein SFs is critical since SFs are frequently used to represent gene/protein entities); iii) build a web user interface for gene/protein names and entries search and query through ontology-enabled onto-BioThesaurus (Hypothesis: enhancing BioThesaurus with gene/protein-related ontologies would enable us to build heuristic rules to enable machine reasoning); and iv) evaluate and distribute research methods/outcome (Hypothesis: evaluating and distributing research methods/outcome are critical to advance both basic and applied biomedical science.            The proposed research is critical for biomedical knowledge retrieval and management. It serves as one of the foundation for storing, retrieving, and extracting knowledge and information in the biomedical domain. Additionally, the proposed research will benefit biomedical researchers and general community for understanding and managing biomedical text through web interfaces and automated systems.",Onto-BioThesaurus: ontological representation of gene/protein names for biomedica,7654995,R01LM009959,"['Abbreviations', 'Biomedical Research', 'Characteristics', 'Communities', 'Computers', 'Databases', 'Expert Opinion', 'Foundations', 'Gene Proteins', 'Genes', 'Goals', 'Harvest', 'Human', 'Information Resources', 'Information Resources Management', 'Internet', 'Investigation', 'Knowledge', 'Literature', 'Manuals', 'Maps', 'Names', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Outcome', 'Peer Review', 'Process', 'Proteins', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Review Literature', 'Science', 'Services', 'System', 'Techniques', 'Terminology', 'Text', 'Thesauri', 'Time', 'acronyms', 'base', 'biomedical ontology', 'heuristics', 'knowledge base', 'tool', 'web interface', 'web site']",NLM,GEORGETOWN UNIVERSITY,R01,2009,608650,0.01340784736356197
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,0.014186613131777923
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,0.014186613131777923
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7660538,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2009,688362,0.05753523721291099
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7558468,R01HG004836,"['Anatomy', 'Architecture', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Competence', 'Complex', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Side', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'computerized data processing', 'design', 'empowered', 'graphical user interface', 'insight', 'instrument', 'open source', 'programs', 'repository', 'research study', 'response', 'scale up', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,428078,0.059408503645391394
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7565504,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Conflict (Psychology)', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2009,529858,0.05870005965408737
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7684604,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2009,639134,0.04348834276787386
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8133305,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,109860,-0.016649253453098527
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7676864,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,3464579,0.043785750277732995
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,0.0054812974531576455
"Towards the Building of a Comprehensive Searchable Biological Experiment Database    DESCRIPTION (provided by applicant):       The rapid growth of the biomedical literature and the expansion in disciplinary biomedical research, heralded by high-throughput genome sciences and technologies, have overwhelmed scientists who attempt to assimilate information necessary for their research. The widespread adoption of title/abstract word searches, such as highly desirable the National Library of Medicine's PubMed system, has provided the first major advance in the way bioscientists find relevant publications since the origin of Index Medicus in 1879 (Hunter and Cohen 2006). The importance of developing valid information retrieval systems for bioscientists has led to the development of information systems worldwide (e.g., Arrowsmith (Smalheiser and Swanson 1998), BioText (Hearst 2003), GeneWays (Friedman et al. 2001; Rzhetsky et al. 2004), iHOP (Hoffmann and Valencia 2005), and BioMedQA (Lee et al. 2006a), and annotated databases (e.g., SWISSPROT, OMIM (Hamosh et al. 2005) and BIND (Alfarano et al. 2005)).      However, most of information systems target only text information and fail to provide access to other important data such as images (e.g., figures). More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biological articles nearly always incorporate figures/images that are the crucial content of the biomedical literature. Our examination of biological articles in the Proceedings of the National Academy of Sciences (PNAS) revealed the occurrence of 5.2 images per article on average (Yu and Lee 2006a). Biologists need to access image data to validate research facts and to formulate or to test novel research hypotheses. It has been evaluated that textual statements reported in literature frequently are noisy (i.e., containing ""false facts"") (Krauthammer et al. 2002). Capturing images that are experimental ""evidence"" to support the textual ""fact"" will benefit bioscience information systems, databases, and bioscientists.      Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We propose the development of advanced natural language processing (NLP) tools to semantically organize images. We hypothesize that text that associated with images semantically entails the image content and natural language processing techniques can be developed to accurately associate the text to their images. Furthermore, we hypothesize that images can be semantically organized by categories specified by standard biological ontology, and that natural language processing approaches can accurately assign the ontological categories to images.      Our specific aims are:      Aim 1: To develop and evaluate NLP techniques for identifying textual statements that correspond to images in full-text articles. We will develop different approaches for two types of the associations. We will first propose rule-based and statistical approaches to identify the associated text that appears in the full-text articles. We will then develop hybrid approaches to link sentences in abstracts to images in the body of the articles.      Aim 2: To develop and evaluate NLP techniques for automatic classification of experimental results into categories (e.g., Western-Blot, PCR verification, etc) specified in the experimental protocol Protocol-Online.      We will explore the use of dictionary-based, rule-based, image classification, and machine-learning approaches for accomplishing this aim.      Aim 3: To develop and evaluate NLP techniques for automatic assignment of Gene Ontology categories to experiments, which will provide a knowledge-based organization of experiments according to biological properties (e.g., catalytic activity). We will develop statistical and machine-learning approaches for accomplishing this aim.      We found that most of the images that appear in full-text biological articles are figure images (Yu and Lee 2006a) and we therefore focus on figure images only in this proposal. The deliverable of Specific Aim 1 will be an effective user-interface BioEx from which bioscientists can access images directly from sentences in the abstracts. BioEx has the promise of improvement over the traditional single-document-per-article format that has dominated bioscience publications since the first scientific article appeared in 1665 (Gross 2002). The deliverables of Specific Aim 2 and 3 will be open-source algorithms and tools that accurately map images to categories specified by the Gene Ontology and the Protocol Online. Those algorithms and tools will enhance bioscience information retrieval, information extraction, summarization, and question answering.          n/a",Towards the Building of a Comprehensive Searchable Biological Experiment Database,7314689,R21RR024933,"['Adoption', 'Advanced Development', 'Algorithms', 'Binding', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Data', 'Databases', 'Development', 'Dictionary', 'Documentation', 'Flowcharts', 'Genes', 'Genome', 'Hybrids', 'Image', 'Index Medicus', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Natural Language Processing', 'Online Mendelian Inheritance In Man', 'Ontology', 'Polymerase Chain Reaction', 'Principal Investigator', 'Property', 'Protocols documentation', 'PubMed', 'Publications', 'Reporting', 'Research', 'Science', 'Scientist', 'Specific qualifier value', 'Standards of Weights and Measures', 'SwissProt', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Title', 'United States National Academy of Sciences', 'United States National Library of Medicine', 'Western Blotting', 'abstracting', 'base', 'knowledge base', 'novel', 'open source', 'programs', 'rapid growth', 'research study', 'tool']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2008,230085,0.005227278330000622
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7495148,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2008,132030,0.023294518330634967
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,0.01776143609987375
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7429768,R01LM008635,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Biological', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Condition', 'Data', 'Databases', 'Detection', 'Development', 'Discipline', 'Disease', 'Documentation', 'Educational workshop', 'Fostering', 'Foundations', 'Health Status', 'Healthcare', 'Human', 'Imagery', 'Improve Access', 'Internet', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medical Errors', 'Medical Surveillance', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Numbers', 'Ontology', 'Organism', 'Output', 'Partner in relationship', 'Patient Care', 'Performance', 'Phenotype', 'Postdoctoral Fellow', 'Process', 'Range', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Services', 'Side', 'Source', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Universities', 'Work', 'biological research', 'biomedical resource', 'clinical application', 'concept', 'high throughput technology', 'improved', 'knowledge base', 'medical specialties', 'model organisms databases', 'open source', 'research and development', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,534463,0.005231467998109847
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,0.027440402148245394
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7433144,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Class', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Computers', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Numbers', 'Performance', 'Personal Satisfaction', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Public Health', 'Randomized', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Today', 'Training', 'Voting', 'Work', 'base', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R44,2008,25548,0.026057569068531402
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7465580,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,273993,0.03979005938005147
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7468497,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,268274,0.018871751926247574
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7470146,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2008,278208,-0.016649253453098527
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7488396,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,268713,0.016228000653298498
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7474790,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Numbers', 'Ontology', 'Phenotype', 'Rate', 'Readiness', 'Representations, Knowledge (Computer)', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'concept', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,614419,0.028247072252975465
"Nation Center: Multi-Scale Study- Cellular Networks(RMI) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genomewide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 1 investigators to combine molecular interaction clues from Core 2 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions. n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7674889,U54CA121852,"['Address', 'Algorithms', 'Area', 'Automobile Driving', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA-Protein Interaction', 'Databases', 'Development', 'Dissection', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Genes', 'Genetic Engineering', 'Genomics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Literature', 'Machine Learning', 'Maps', 'Medical', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Published Comment', 'Range', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Science', 'Signal Pathway', 'Skeleton', 'Source', 'Structure', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'Work', 'base', 'computer framework', 'data mining', 'design', 'graphical user interface', 'improved', 'innovation', 'knowledge base', 'multidisciplinary', 'novel', 'response', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,113826,0.041683330627638245
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7459910,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2008,118789,0.005664736304770919
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,0.04222689404516109
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7467204,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Personal Satisfaction', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Today', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'concept', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2008,280000,0.022435407215286196
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7748401,R44GM083965,"['Learning', 'Techniques', 'parallel computing']",NIGMS,INSILICOS,R44,2008,143361,0.026057569068531402
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7502636,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2008,640921,0.04348834276787386
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7502135,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,3570137,0.043785750277732995
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,0.010271068729343788
"MUSC Minority Student Development Program    DESCRIPTION (provided by applicant):  The goals of this program are to: 1) increase the number of underrepresented minority (URM) students choosing a research career in the biomedical sciences, 2) enhance the quantity and quality of URM students enrolled in the doctoral program in the College of Graduate Studies, 3) prepare them for a successful career in biomedical sciences, 4) provide structured activities that enhance the interaction of minority and majority students and 5) increase the number of URMs in the Summer Undergraduate Research Program (SURP). To accomplish these goals a Minority Recruitment Committee will be developed to assist in recruiting URMs. An MUSC ""Science Day"" will be implemented to expose minority students to the excitement of biomedical science research. Undergraduate science advisors will nominate potential URM Ph.D. applicants who will attend ""Science Day"" with a parent and the advisor. The SURP will be expanded by five students, designated as E.E. Just Scholars. These SURP participants will interact with minority graduate students via a Big Brother/Sister program. A summer pre-matriculation course to review basic biochemical and molecular concepts will be available for all new students. All students will participate in a ""Diversity in Science"" course that will foster the interaction of minority and majority students that focuses on issues related to minorities in biomedical science. URM graduate students will serve as Graduate Student Ambassadors to HBCUs and present seminars to undergraduates. Funds will be provided for URM graduate students to present their research at a national meeting. A Mentoring Committee will be developed to 1) monitor and advise URM doctoral students through the graduate training program, 2) assist with career development and 3) organize the content of the ""Diversity in Science"" course as well as career workshops. Internal and External Advisory Committees will provide oversight for the program. These programmatic elements will increase the number of minority students participating in biomedical science research and prepare them for a successful career.         n/a",MUSC Minority Student Development Program,7383798,R25GM072643,"['5-(4-hydroxy-3-methoxyphenyl)-5-phenylhydantoin', 'Academia', 'Accounting', 'Achievement', 'Address', 'Admission activity', 'Advisory Committees', 'Affect', 'African American', 'Aging', 'Agonist', 'Alcohols', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Animals', 'Appendix', 'Applied Research', 'Area', 'Award', 'Back', 'Bacteriorhodopsins', 'Baltimore', 'Belief', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Blood Vessels', 'Bradykinin', 'Brain', 'Brothers', 'Budgets', 'Businesses', 'CXCL1 gene', 'Calcium', 'Cancer Biology', 'Cancer Center', 'Cancer Education Grant Program', 'Cardiac', 'Cardiovascular Physiology', 'Cardiovascular system', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Ceramides', 'Cessation of life', 'Chemicals', 'Chemistry', 'Chicago', 'Cities', 'Class', 'Classification', 'Cocaine', 'Collaborations', 'Commit', 'Committee Members', 'Communities', 'Complex', 'Contracts', 'Counseling', 'County', 'Cyclic AMP', 'Data', 'Daughter', 'Decision Making', 'Development', 'Development Plans', 'Disease', 'Doctor of Philosophy', 'Doctor of Public Health', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Elements', 'Endotoxins', 'Enrollment', 'Ensure', 'Environment', 'Enzymatic Biochemistry', 'Enzymes', 'Equilibrium', 'Evaluation', 'Event', 'Exhibits', 'Explosion', 'Exposure to', 'Extramural Activities', 'Faculty', 'Failure', 'Fatty Acids', 'Fees', 'Fellowship', 'Flax', 'Florida', 'Fostering', 'Foundations', 'Friends', 'Friendships', 'Funding', 'Funding Mechanisms', 'Future', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Geographic Locations', 'Goals', 'Graduate Education', 'Grant', 'Health Occupations', 'Health Professional', 'Health Services Research', 'Heart Hypertrophy', 'Heroin', 'Historically Black Colleges and Universities', 'Home Page', 'Home environment', 'Hour', 'Human', 'Hypoxia', 'Indiana', 'Individual', 'Industry', 'Information Resources', 'Institutes', 'Institution', 'Insulin Resistance', 'Insulin-Like Growth Factor Receptor', 'Insulin-Like Growth-Factor-Binding Proteins', 'Intellectual Property', 'Internal Medicine', 'International', 'Internet', 'Interview', 'Investigation', 'Journals', 'Laboratories', 'Laboratory Research', 'Lead', 'Leadership', 'Learning', 'Legal patent', 'Letters', 'Life', 'Ligands', 'Lipid Biochemistry', 'Lipids', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Marines', 'Maryland', 'Mass Spectrum Analysis', 'Mediating', 'Medical', 'Medical center', 'Meiosis', 'Membrane', 'Membrane Transport Proteins', 'Mentors', 'Metabolism', 'Methods', 'Microfluidics', 'Minority', 'Mission', 'Mitosis', 'Modality', 'Modems', 'Modification', 'Molecular', 'Molecular and Cellular Biology', 'Monitor', 'Multiple Sclerosis', 'Myelin', 'NMR Spectroscopy', 'National Cancer Institute', 'National Institute of General Medical Sciences', 'Natural regeneration', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Neurosciences', 'Neurosciences Research', 'Newsletter', 'Nucleic Acids', 'Numbers', 'Occupations', 'Oral', 'Organic Chemistry', 'Other Minority', 'Outcome', 'Oxygen', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathway Analysis', 'Pathway interactions', 'Pediatrics', 'Pennsylvania', 'Performance', 'Pharmacologic Substance', 'Pharmacology', 'Phase', 'Phospholipids', 'Phosphotransferases', 'Physics', 'Play', 'Policies', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Program Development', 'Progress Reports', 'Proteins', 'Proteomics', 'Published Comment', 'Purpose', 'Qualifying', 'Range', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Regulation', 'Reporting', 'Research', 'Research Ethics', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Residencies', 'Resolution', 'Resources', 'Restaurants', 'Rhodopsin', 'Role', 'Rotation', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Score', 'Series', 'Serotonin', 'Services', 'Shock', 'Signal Pathway', 'Signal Transduction', 'Sister', 'Site', 'Site Visit', 'Solid', 'Son', 'South Carolina', 'Sphingolipids', 'Staging', 'Stem cells', 'Strategic Planning', 'Stress', 'Structure', 'Structure-Activity Relationship', 'Students', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'Therapeutic', 'Thinking', 'Thromboxane A2', 'Time', 'Toxicology', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transportation', 'Travel', 'Underrepresented Minority', 'United States', 'United States National Institutes of Health', 'Universities', 'Vision', 'Visit', 'Vocational Guidance', 'Week', 'Wolves', 'Work', 'Writing', 'X-Ray Crystallography', 'addiction', 'aging brain', 'angiogenesis', 'anticancer research', 'base', 'body system', 'brain behavior', 'cancer therapy', 'career', 'cell growth', 'cell injury', 'cell motility', 'chemical bond', 'college', 'complement C2a', 'concept', 'cooking', 'cost', 'day', 'design', 'drug of abuse', 'expectation', 'experience', 'falls', 'fascinate', 'follow-up', 'frontier', 'gamma-Aminobutyric Acid', 'gene function', 'genome sequencing', 'high school', 'improved', 'innovation', 'insight', 'instructor', 'interest', 'journal article', 'lectures', 'lipid metabolism', 'member', 'minority health', 'myelination', 'nervous system disorder', 'novel', 'novel strategies', 'numb protein', 'patient oriented', 'posters', 'pre-clinical', 'professor', 'programs', 'protein protein interaction', 'protein structure function', 'receptor', 'receptor coupling', 'research facility', 'response', 'role model', 'science education', 'skills', 'social', 'soft drink', 'structural biology', 'success', 'symposium', 'technology development', 'theories', 'tool', 'transmission process', 'tumorigenesis', 'two-dimensional', 'tyrosine receptor', 'university student']",NIGMS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R25,2008,99916,0.023145399798624684
"MUSC Minority Student Development Program    DESCRIPTION (provided by applicant):  The goals of this program are to: 1) increase the number of underrepresented minority (URM) students choosing a research career in the biomedical sciences, 2) enhance the quantity and quality of URM students enrolled in the doctoral program in the College of Graduate Studies, 3) prepare them for a successful career in biomedical sciences, 4) provide structured activities that enhance the interaction of minority and majority students and 5) increase the number of URMs in the Summer Undergraduate Research Program (SURP). To accomplish these goals a Minority Recruitment Committee will be developed to assist in recruiting URMs. An MUSC ""Science Day"" will be implemented to expose minority students to the excitement of biomedical science research. Undergraduate science advisors will nominate potential URM Ph.D. applicants who will attend ""Science Day"" with a parent and the advisor. The SURP will be expanded by five students, designated as E.E. Just Scholars. These SURP participants will interact with minority graduate students via a Big Brother/Sister program. A summer pre-matriculation course to review basic biochemical and molecular concepts will be available for all new students. All students will participate in a ""Diversity in Science"" course that will foster the interaction of minority and majority students that focuses on issues related to minorities in biomedical science. URM graduate students will serve as Graduate Student Ambassadors to HBCUs and present seminars to undergraduates. Funds will be provided for URM graduate students to present their research at a national meeting. A Mentoring Committee will be developed to 1) monitor and advise URM doctoral students through the graduate training program, 2) assist with career development and 3) organize the content of the ""Diversity in Science"" course as well as career workshops. Internal and External Advisory Committees will provide oversight for the program. These programmatic elements will increase the number of minority students participating in biomedical science research and prepare them for a successful career.         n/a",MUSC Minority Student Development Program,7644195,R25GM072643,"['5-(4-hydroxy-3-methoxyphenyl)-5-phenylhydantoin', 'Academia', 'Accounting', 'Achievement', 'Address', 'Admission activity', 'Advisory Committees', 'Affect', 'African American', 'Aging', 'Agonist', 'Alcohols', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Animals', 'Appendix', 'Applied Research', 'Area', 'Award', 'Back', 'Bacteriorhodopsins', 'Baltimore', 'Belief', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Blood Vessels', 'Bradykinin', 'Brain', 'Brothers', 'Budgets', 'Businesses', 'CXCL1 gene', 'Calcium', 'Cancer Biology', 'Cancer Center', 'Cancer Education Grant Program', 'Cardiac', 'Cardiovascular Physiology', 'Cardiovascular system', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Ceramides', 'Cessation of life', 'Chemicals', 'Chemistry', 'Chicago', 'Cities', 'Class', 'Classification', 'Cocaine', 'Collaborations', 'Commit', 'Committee Members', 'Communities', 'Complex', 'Contracts', 'Counseling', 'County', 'Cyclic AMP', 'Data', 'Daughter', 'Decision Making', 'Development', 'Development Plans', 'Disease', 'Doctor of Philosophy', 'Doctor of Public Health', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Elements', 'Endotoxins', 'Enrollment', 'Ensure', 'Environment', 'Enzymatic Biochemistry', 'Enzymes', 'Equilibrium', 'Evaluation', 'Event', 'Exhibits', 'Explosion', 'Exposure to', 'Extramural Activities', 'Faculty', 'Failure', 'Fatty Acids', 'Fees', 'Fellowship', 'Flax', 'Florida', 'Fostering', 'Foundations', 'Friends', 'Friendships', 'Funding', 'Funding Mechanisms', 'Future', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Geographic Locations', 'Goals', 'Graduate Education', 'Grant', 'Health Occupations', 'Health Professional', 'Health Services Research', 'Heart Hypertrophy', 'Heroin', 'Historically Black Colleges and Universities', 'Home Page', 'Home environment', 'Hour', 'Human', 'Hypoxia', 'Indiana', 'Individual', 'Industry', 'Information Resources', 'Institutes', 'Institution', 'Insulin Resistance', 'Insulin-Like Growth Factor Receptor', 'Insulin-Like Growth-Factor-Binding Proteins', 'Intellectual Property', 'Internal Medicine', 'International', 'Internet', 'Interview', 'Investigation', 'Journals', 'Laboratories', 'Laboratory Research', 'Lead', 'Leadership', 'Learning', 'Legal patent', 'Letters', 'Life', 'Ligands', 'Lipid Biochemistry', 'Lipids', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Marines', 'Maryland', 'Mass Spectrum Analysis', 'Mediating', 'Medical', 'Medical center', 'Meiosis', 'Membrane', 'Membrane Transport Proteins', 'Mentors', 'Metabolism', 'Methods', 'Microfluidics', 'Minority', 'Mission', 'Mitosis', 'Modality', 'Modems', 'Modification', 'Molecular', 'Molecular and Cellular Biology', 'Monitor', 'Multiple Sclerosis', 'Myelin', 'NMR Spectroscopy', 'National Cancer Institute', 'National Institute of General Medical Sciences', 'Natural regeneration', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Neurosciences', 'Neurosciences Research', 'Newsletter', 'Nucleic Acids', 'Numbers', 'Occupations', 'Oral', 'Organic Chemistry', 'Other Minority', 'Outcome', 'Oxygen', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathway Analysis', 'Pathway interactions', 'Pediatrics', 'Pennsylvania', 'Performance', 'Pharmacologic Substance', 'Pharmacology', 'Phase', 'Phospholipids', 'Phosphotransferases', 'Physics', 'Play', 'Policies', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Program Development', 'Progress Reports', 'Proteins', 'Proteomics', 'Published Comment', 'Purpose', 'Qualifying', 'Range', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Regulation', 'Reporting', 'Research', 'Research Ethics', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Residencies', 'Resolution', 'Resources', 'Restaurants', 'Rhodopsin', 'Role', 'Rotation', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Score', 'Series', 'Serotonin', 'Services', 'Shock', 'Signal Pathway', 'Signal Transduction', 'Sister', 'Site', 'Site Visit', 'Solid', 'Son', 'South Carolina', 'Sphingolipids', 'Staging', 'Stem cells', 'Strategic Planning', 'Stress', 'Structure', 'Structure-Activity Relationship', 'Students', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'Therapeutic', 'Thinking', 'Thromboxane A2', 'Time', 'Toxicology', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transportation', 'Travel', 'Underrepresented Minority', 'United States', 'United States National Institutes of Health', 'Universities', 'Vision', 'Visit', 'Vocational Guidance', 'Week', 'Wolves', 'Work', 'Writing', 'X-Ray Crystallography', 'addiction', 'aging brain', 'angiogenesis', 'anticancer research', 'base', 'body system', 'brain behavior', 'cancer therapy', 'career', 'cell growth', 'cell injury', 'cell motility', 'chemical bond', 'college', 'complement C2a', 'concept', 'cooking', 'cost', 'day', 'design', 'drug of abuse', 'expectation', 'experience', 'falls', 'fascinate', 'follow-up', 'frontier', 'gamma-Aminobutyric Acid', 'gene function', 'genome sequencing', 'high school', 'improved', 'innovation', 'insight', 'instructor', 'interest', 'journal article', 'lectures', 'lipid metabolism', 'member', 'minority health', 'myelination', 'nervous system disorder', 'novel', 'novel strategies', 'numb protein', 'patient oriented', 'posters', 'pre-clinical', 'professor', 'programs', 'protein protein interaction', 'protein structure function', 'receptor', 'receptor coupling', 'research facility', 'response', 'role model', 'science education', 'skills', 'social', 'soft drink', 'structural biology', 'success', 'symposium', 'technology development', 'theories', 'tool', 'transmission process', 'tumorigenesis', 'two-dimensional', 'tyrosine receptor', 'university student']",NIGMS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R25,2008,95384,0.023145399798624684
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,0.010818207588782356
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7301251,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2007,130432,0.023294518330634967
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,0.01776143609987375
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,0.03577355847705756
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7257857,R01LM008635,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Biological', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Condition', 'Data', 'Databases', 'Detection', 'Development', 'Discipline', 'Disease', 'Documentation', 'Educational workshop', 'Fostering', 'Foundations', 'Health Status', 'Healthcare', 'Human', 'Imagery', 'Improve Access', 'Internet', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medical Errors', 'Medical Surveillance', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Numbers', 'Ontology', 'Organism', 'Output', 'Partner in relationship', 'Patient Care', 'Performance', 'Phenotype', 'Postdoctoral Fellow', 'Process', 'Range', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Services', 'Side', 'Source', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Universities', 'Work', 'biological research', 'biomedical resource', 'clinical application', 'concept', 'high throughput technology', 'improved', 'knowledge base', 'medical specialties', 'model organisms databases', 'open source', 'research and development', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,529014,0.005231467998109847
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7264196,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,279300,0.03979005938005147
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7299383,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2007,266852,0.018871751926247574
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7260682,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2007,291350,-0.016649253453098527
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7287359,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,350638,0.016228000653298498
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,0.02220507187834409
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7252088,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2007,276104,0.005664736304770919
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,0.0007102358082571955
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,0.014186613131777923
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7475421,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,160000,0.05753523721291099
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7248464,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,693808,0.05753523721291099
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7364235,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2007,631600,0.04348834276787386
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7286779,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2007,3638557,0.043785750277732995
"MUSC Minority Student Development Program    DESCRIPTION (provided by applicant):  The goals of this program are to: 1) increase the number of underrepresented minority (URM) students choosing a research career in the biomedical sciences, 2) enhance the quantity and quality of URM students enrolled in the doctoral program in the College of Graduate Studies, 3) prepare them for a successful career in biomedical sciences, 4) provide structured activities that enhance the interaction of minority and majority students and 5) increase the number of URMs in the Summer Undergraduate Research Program (SURP). To accomplish these goals a Minority Recruitment Committee will be developed to assist in recruiting URMs. An MUSC ""Science Day"" will be implemented to expose minority students to the excitement of biomedical science research. Undergraduate science advisors will nominate potential URM Ph.D. applicants who will attend ""Science Day"" with a parent and the advisor. The SURP will be expanded by five students, designated as E.E. Just Scholars. These SURP participants will interact with minority graduate students via a Big Brother/Sister program. A summer pre-matriculation course to review basic biochemical and molecular concepts will be available for all new students. All students will participate in a ""Diversity in Science"" course that will foster the interaction of minority and majority students that focuses on issues related to minorities in biomedical science. URM graduate students will serve as Graduate Student Ambassadors to HBCUs and present seminars to undergraduates. Funds will be provided for URM graduate students to present their research at a national meeting. A Mentoring Committee will be developed to 1) monitor and advise URM doctoral students through the graduate training program, 2) assist with career development and 3) organize the content of the ""Diversity in Science"" course as well as career workshops. Internal and External Advisory Committees will provide oversight for the program. These programmatic elements will increase the number of minority students participating in biomedical science research and prepare them for a successful career.         n/a",MUSC Minority Student Development Program,7188026,R25GM072643,"['5-(4-hydroxy-3-methoxyphenyl)-5-phenylhydantoin', 'Academia', 'Accounting', 'Achievement', 'Address', 'Admission activity', 'Advisory Committees', 'Affect', 'African American', 'Aging', 'Agonist', 'Alcohols', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Animals', 'Appendix', 'Applied Research', 'Area', 'Award', 'Back', 'Bacteriorhodopsins', 'Baltimore', 'Belief', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Blood Vessels', 'Bradykinin', 'Brain', 'Brothers', 'Budgets', 'Businesses', 'CXCL1 gene', 'Calcium', 'Cancer Biology', 'Cancer Center', 'Cancer Education Grant Program', 'Cardiac', 'Cardiovascular Physiology', 'Cardiovascular system', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Ceramides', 'Cessation of life', 'Chemicals', 'Chemistry', 'Chicago', 'Cities', 'Class', 'Classification', 'Cocaine', 'Collaborations', 'Commit', 'Committee Members', 'Communities', 'Complex', 'Contracts', 'Counseling', 'County', 'Cyclic AMP', 'Data', 'Daughter', 'Decision Making', 'Development', 'Development Plans', 'Disease', 'Doctor of Philosophy', 'Doctor of Public Health', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Elements', 'Endotoxins', 'Enrollment', 'Ensure', 'Environment', 'Enzymatic Biochemistry', 'Enzymes', 'Equilibrium', 'Evaluation', 'Event', 'Exhibits', 'Explosion', 'Exposure to', 'Extramural Activities', 'Faculty', 'Failure', 'Fatty Acids', 'Fees', 'Fellowship', 'Flax', 'Florida', 'Fostering', 'Foundations', 'Friends', 'Friendships', 'Funding', 'Funding Mechanisms', 'Future', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Geographic Locations', 'Goals', 'Graduate Education', 'Grant', 'Health Occupations', 'Health Professional', 'Health Services Research', 'Heart Hypertrophy', 'Heroin', 'Historically Black Colleges and Universities', 'Home Page', 'Home environment', 'Hour', 'Human', 'Hypoxia', 'Indiana', 'Individual', 'Industry', 'Information Resources', 'Institutes', 'Institution', 'Insulin Resistance', 'Insulin-Like Growth Factor Receptor', 'Insulin-Like Growth-Factor-Binding Proteins', 'Intellectual Property', 'Internal Medicine', 'International', 'Internet', 'Interview', 'Investigation', 'Journals', 'Laboratories', 'Laboratory Research', 'Lead', 'Leadership', 'Learning', 'Legal patent', 'Letters', 'Life', 'Ligands', 'Lipid Biochemistry', 'Lipids', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Marines', 'Maryland', 'Mass Spectrum Analysis', 'Mediating', 'Medical', 'Medical center', 'Meiosis', 'Membrane', 'Membrane Transport Proteins', 'Mentors', 'Metabolism', 'Methods', 'Microfluidics', 'Minority', 'Mission', 'Mitosis', 'Modality', 'Modems', 'Modification', 'Molecular', 'Molecular and Cellular Biology', 'Monitor', 'Multiple Sclerosis', 'Myelin', 'NMR Spectroscopy', 'National Cancer Institute', 'National Institute of General Medical Sciences', 'Natural regeneration', 'Nervous system structure', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Neurosciences', 'Neurosciences Research', 'Newsletter', 'Nucleic Acids', 'Numbers', 'Occupations', 'Oral', 'Organic Chemistry', 'Other Minority', 'Outcome', 'Oxygen', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathway Analysis', 'Pathway interactions', 'Pediatrics', 'Pennsylvania', 'Performance', 'Pharmacologic Substance', 'Pharmacology', 'Phase', 'Phospholipids', 'Phosphotransferases', 'Physics', 'Play', 'Policies', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Program Development', 'Progress Reports', 'Proteins', 'Proteomics', 'Published Comment', 'Purpose', 'Qualifying', 'Range', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Regulation', 'Reporting', 'Research', 'Research Ethics', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Residencies', 'Resolution', 'Resources', 'Restaurants', 'Rhodopsin', 'Role', 'Rotation', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Score', 'Series', 'Serotonin', 'Services', 'Shock', 'Signal Pathway', 'Signal Transduction', 'Sister', 'Site', 'Site Visit', 'Solid', 'Son', 'South Carolina', 'Sphingolipids', 'Staging', 'Stem cells', 'Strategic Planning', 'Stress', 'Structure', 'Structure-Activity Relationship', 'Students', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'Therapeutic', 'Thinking', 'Thromboxane A2', 'Time', 'Toxicology', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Transportation', 'Travel', 'Underrepresented Minority', 'United States', 'United States National Institutes of Health', 'Universities', 'Vision', 'Visit', 'Vocational Guidance', 'Week', 'Wolves', 'Work', 'Writing', 'X-Ray Crystallography', 'addiction', 'aging brain', 'angiogenesis', 'anticancer research', 'base', 'body system', 'brain behavior', 'cancer therapy', 'career', 'cell growth', 'cell injury', 'cell motility', 'chemical bond', 'college', 'complement C2a', 'concept', 'cooking', 'cost', 'day', 'design', 'drug of abuse', 'expectation', 'experience', 'falls', 'fascinate', 'follow-up', 'frontier', 'gamma-Aminobutyric Acid', 'gene function', 'genome sequencing', 'high school', 'improved', 'innovation', 'insight', 'instructor', 'interest', 'journal article', 'lectures', 'lipid metabolism', 'member', 'minority health', 'myelination', 'nervous system disorder', 'novel', 'novel strategies', 'numb protein', 'patient oriented', 'posters', 'pre-clinical', 'professor', 'programs', 'protein protein interaction', 'protein structure function', 'receptor', 'receptor coupling', 'research facility', 'response', 'role model', 'science education', 'skills', 'social', 'soft drink', 'structural biology', 'success', 'symposium', 'technology development', 'theories', 'tool', 'transmission process', 'tumorigenesis', 'two-dimensional', 'tyrosine receptor', 'university student']",NIGMS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R25,2007,217360,0.023145399798624684
"REU in Functional Genomics and Cell Biology DESCRIPTION (provided by applicant):  We propose to establish a summer program for undergraduates with non-biological sciences backgrounds that will provide cross-discipline training and experiences in combining the physical sciences backgrounds of the participants with concepts and approaches of functional genomics and cell biology to address biomedical research problems. The intended impact of our program is to encourage participating students to pursue graduate education in biomedical sciences, followed by careers in biomedical research.       Six students will be selected each year to pursue 10-week research experiences in one of eleven laboratories at the University of Cincinnati College of Medicine and Cincinnati Children's Hospital Medical Center. Instruction in functional genomics and cell biology will be provided through workshops and a series of seminars. Students will work as a group on a cross-discipline case study in wound-healing. In addition, students will receive instruction in research ethics, scientific writing, and career opportunities in biomedical sciences. At the conclusion of the summer, students will present their work at a poster session and receive evaluations. Students will be encouraged to publish their results in the Journal of the Ohio Association of Science.  To build community among these students, they will participate in a series of social and cultural events.   Formative evaluations will be used to alter and fine-tune the Program, and summative evaluations will be carried out at the conclusion of each summer and again, as the participants graduate from their undergraduate institutions, to measure the long-term impact of the Program on the participating students. n/a",REU in Functional Genomics and Cell Biology,7226706,R25GM072834,"['Academia', 'Academy', 'Accounting', 'Address', 'Adenocarcinoma Cell', 'Adhesions', 'Adhesives', 'Admission activity', 'Affect', 'African American', 'Aging', 'Algorithms', 'Androgens', 'Animals', 'Apoptosis', 'Area', 'Association of American Medical Colleges', 'Attitude', 'Authorship', 'Autoimmunity', 'Award', 'Back', 'Base Sequence', 'Basic Science', 'Belief', 'Biohazardous Substance', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Engineering', 'Biomedical Research', 'Bioterrorism', 'CCL7 gene', 'California', 'Camptothecin', 'Canada', 'Cardiac Myocytes', 'Career Choice', 'Case Study', 'Cell Adhesion', 'Cell Count', 'Cell Cycle', 'Cell Cycle Progression', 'Cell Death', 'Cell Line', 'Cell Polarity', 'Cell Proliferation', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular biology', 'Cessation of life', 'Chicago', 'Childhood', 'Cities', 'Class', 'Collagen', 'Color', 'Commit', 'Communication', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computers', 'Computing Methodologies', 'Confocal Microscopy', 'Conserved Sequence', 'Consult', 'Contact Info', 'Country', 'Cues', 'Cyclin D1', 'Cytoplasm', 'DNA', 'DNA Damage', 'DNA Fingerprinting', 'DNA Fragmentation', 'DNA biosynthesis', 'DNA-Protein Interaction', 'Data', 'Data Sources', 'Decompression Sickness', 'Dependence', 'Development', 'Developmental Biology', 'Discipline', 'Discipline of obstetrics', 'Disease', 'District of Columbia', 'Doctor of Philosophy', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Effectiveness', 'Electronic Mail', 'Electronics', 'Elements', 'Embryonic Development', 'Engineering', 'Enrollment', 'Environment', 'Environmental Health', 'Epidemiology', 'Epithelial Cells', 'Ethical Issues', 'Ethics', 'Evaluation', 'Event', 'Evolution', 'Exhibits', 'Extracellular Matrix', 'Extracellular Matrix Proteins', 'Eye', 'Eye Color', 'Faculty', 'Family', 'Fees', 'Fluorescence Microscopy', 'Forensic Medicine', 'Foxes', 'Fraud', 'Freedom', 'Functional RNA', 'Funding', 'Future', 'G1 Phase', 'Gel', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Germ Cells', 'Goals', 'Government', 'Graduate Education', 'Graph', 'Growth', 'Growth Factor', 'Guidelines', 'Guilt', 'Gynecology', 'Hand', 'Head', 'Healed', 'Heart', 'Historically Black Colleges and Universities', 'Holidays', 'Home environment', 'Homeostasis', 'Hour', 'Housing', 'Human', 'Image', 'Image Analysis', 'Implant', 'In Vitro', 'Indium', 'Individual', 'Informatics', 'Injury', 'Injury to Kidney', 'Institute of Medicine (U.S.)', 'Institution', 'Instruction', 'Integrins', 'Interdisciplinary Study', 'Internet', 'Interview', 'Investigation', 'Ireland', 'Jordan', 'Journals', 'Kentucky', 'Kidney', 'Knock-out', 'Knowledge', 'Label', 'Laboratories', 'Laboratory Research', 'Laminin', 'Lead', 'Learning', 'Letters', 'Life', 'Life Cycle Stages', 'Localized', 'Logistics', 'Lung', 'MDCK cell', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Mammalian Cell', 'Manuscripts', 'Measures', 'Mechanics', 'Mediating', 'Medical center', 'Medicine', 'Meniscus structure of joint', 'Mentors', 'Mesenchymal Stem Cells', 'Methodology', 'Methods', 'Microscopy', 'Minority', 'Minority Groups', 'Mitochondria', 'Mitochondrial DNA', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Movement', 'Mus', 'Muscle Cells', 'Muscle Fibers', 'Music', 'Myosin Light Chain Kinase', 'Natural regeneration', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurons', 'Newborn Respiratory Distress Syndrome', 'Norovirus', 'Nuclear', 'Nucleic Acid Regulatory Sequences', 'Nucleic Acids', 'Numbers', 'Object Attachment', 'Ohio', 'Online Systems', 'Operative Surgical Procedures', 'Organism', 'Orthologous Gene', 'Ownership', 'Pamphlets', 'Participant', 'Pathogenesis', 'Pathologic', 'Pattern Recognition', 'Pediatric Hospitals', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Philosophy', 'Phosphorylation', 'Plagiarism', 'Play', 'Pneumocystis', 'Pneumocystis carinii', 'Population', 'Population Theory', 'Post-Translational Protein Processing', 'Postdoctoral Fellow', 'Predisposition', 'Premature Infant', 'Preparation', 'Printing', 'Prize', 'Procedures', 'Process', 'Program Evaluation', 'Prostate Adenocarcinoma', 'Protein Dynamics', 'Proteins', 'Proteomics', 'Public Sector', 'Publications', 'Published Comment', 'Publishing', 'Pulmonary Surfactant-Associated Protein A', 'RNA Polymerase II', 'RNA Splicing', 'Race Relations', 'Radiation', 'Radioactive', 'Radioactivity', 'Railroads', 'Range', 'Rate', 'Rattus', 'Reading', 'Reagent', 'Recommendation', 'Records', 'Recruitment Activity', 'Regulation', 'Regulatory Element', 'Relative (related person)', 'Relaxation', 'Research', 'Research Activity', 'Research Ethics', 'Research Personnel', 'Research Project Grants', 'Resistance', 'Resources', 'Rewards', 'Rights', 'Risk', 'Role', 'Running', 'Safety', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Self Assessment', 'Series', 'Services', 'Side', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Site', 'Skeletal Muscle', 'Software Tools', 'Standards of Weights and Measures', 'Stimulus', 'Stress', 'Structure', 'Students', 'Suggestion', 'Surveys', 'Susceptibility Gene', 'System', 'T-Lymphocyte', 'Techniques', 'Technology', 'Telephone', 'Tendon structure', 'Tennessee', 'Testing', 'Therapeutic', 'Time', 'Tissue Engineering', 'Tissues', 'Topoisomerase-I Inhibitor', 'Touch sensation', 'Training', 'Training Activity', 'Training Programs', 'Transcript', 'Transgenic Organisms', 'Transplantation', 'Travel', 'Tubular formation', 'Tumor Suppressor Proteins', 'Underrepresented Minority', 'United States National Academy of Sciences', 'United States National Institutes of Health', 'Universities', 'Update', 'Variant', 'Viral Proteins', 'Visit', 'Week', 'Wisconsin', 'Work', 'Wound Healing', 'Writing', 'Yugoslavia', 'abstracting', 'achilles tendon', 'adult stem cell', 'austin', 'base', 'biological adaptation to stress', 'biological research', 'career', 'cell growth', 'college', 'concept', 'cost', 'day', 'dentate gyrus', 'design', 'endonuclease', 'endonuclease G', 'expectation', 'experience', 'follow-up', 'functional genomics', 'handbook', 'healing', 'high school', 'human disease', 'human subject', 'improved', 'in vivo', 'indexing', 'information gathering', 'insight', 'instructor', 'interdisciplinary approach', 'interest', 'lectures', 'lung development', 'member', 'metaplastic cell transformation', 'morris water maze', 'new technology', 'news', 'notch protein', 'novel', 'novel strategies', 'object recognition', 'physical science', 'population migration', 'posters', 'preconditioning', 'prevent', 'professor', 'programs', 'protein folding', 'protein protein interaction', 'protein structure', 'protein structure function', 'renal artery', 'repaired', 'research and development', 'research study', 'response', 'retinoblastoma tumor suppressor', 'rhoA GTP-Binding Protein', 'role model', 'satisfaction', 'science education', 'simulation', 'skills', 'social', 'stem cell therapy', 'success', 'surfactant', 'symposium', 'teacher', 'theories', 'tissue processing', 'tool', 'transcription factor', 'transmission process']",NIGMS,UNIVERSITY OF CINCINNATI,R25,2007,66454,0.026803796528556557
"AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM DESCRIPTION (provided by applicant):     The proposed training program in Informatics for Global Health will consist of several activities carried out over a five year period 1) two short courses for 30-50 participants in basic informatics to be held in Peru 2) short term skills based training in informatics in Seattle with emphasis on genomics and resource access skills building 3) long term training in Informatics at the post doctoral or Masters of Science level through the Biomedical and Health Informatics program at the School of Medicine. Six scholars will be trained in addition to the short course participants. Four scholars now engaged in research will receive focused, skills building technical training and six scholars will be recruited and enrolled for longer term (2-3 year) post doctoral or Masters of Science training at the University of Washington. It is envisioned that this training program will foster the development of the capacity of UPCH to continue its role as a leading biomedical research institution and also to create a home to health informatics research activity in Peru. n/a",AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM,7249492,D43TW007551,"['1 year old', 'AIDS prevention', 'AIDS/HIV problem', 'Academic Detailing', 'Academic Medical Centers', 'Achievement', 'Acquired Immunodeficiency Syndrome', 'Acyclovir', 'Address', 'Admission activity', 'Adopted', 'Adult', 'Advisory Committees', 'Aeromonas', 'Affect', 'Age', 'Age Reporting', 'Agreement', 'AlamarBlue', 'Algorithms', 'Alleles', 'Am 80', 'Amauta', 'American', 'Americas', 'Amino Acid Sequence', 'Aminoglycosides', 'Anal Sex', 'Anogenital venereal warts', 'Anti-Retroviral Agents', 'Antibiotic susceptibility', 'Antigens', 'Antimicrobial susceptibility', 'Antitubercular Agents', 'Appendix', 'Appointment', 'Area', 'Artificial Intelligence', 'Arts', 'Asia', 'Aspirate substance', 'Australia', 'Automation', 'Award', 'Bacillus (bacterium)', 'Back', 'Bacteria', 'Bacterial Infections', 'Bacterial Vaginosis', 'Baseline Surveys', 'Basic Science', 'Behavior', 'Behavior Control', 'Behavior Therapy', 'Behavioral Research', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Engineering', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Birth', 'Bisexual', 'Blinded', 'Blood Tests', 'Bolivia', 'Books', 'Businesses', 'CD4 Lymphocyte Count', 'Calmette-Guerin Bacillus', 'Campylobacter', 'Canada', 'Candida', 'Caring', 'Case-Control Studies', 'Cataloging', 'Catalogs', 'Categories', 'Cause of Death', 'Cell physiology', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Certification', 'Cervical', 'Characteristics', 'Chest', 'Child', 'Child health care', 'Childhood', 'Chlamydia', 'Chlamydia Infections', 'Chromosome Mapping', 'Chronic', 'Ciprofloxacin', 'Cities', 'Class', 'Classification', 'Client', 'Climate', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Clinical trial protocol document', 'Code', 'Cohort Studies', 'Collaborations', 'Collection', 'Color', 'Commit', 'Committee Membership', 'Communicable Diseases', 'Communication', 'Communities', 'Community Medicine', 'Competence', 'Complex', 'Computer Assisted', 'Computer software', 'Computerized Medical Record', 'Computers', 'Congenital Syphilis', 'Consultations', 'Contact Tracing', 'Costa Rica', 'Coughing', 'Counseling', 'Country', 'County', 'Coupled', 'Critiques', 'Cross-Sectional Studies', 'Cryptosporidium', 'DNA', 'DNA Microarray Chip', 'DNA Microarray format', 'DNA amplification', 'Daily', 'Data', 'Data Collection', 'Data Sources', 'Databases', 'Decision Analysis', 'Decision Making', 'Decision Support Systems', 'Dental Schools', 'Depth', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Diagnostic tests', 'Diarrhea', 'Dimensions', 'Discipline', 'Discipline of Nursing', 'Disease', 'Disease remission', 'Distance Education', 'Distance Learning', 'Doctor of Philosophy', 'Dose', 'Drug Formulations', 'Drug usage', 'Early Diagnosis', 'Ecology', 'Economics', 'Education', 'Educational Curriculum', 'Educational Intervention', 'Educational Status', 'Educational process of instructing', 'Effectiveness', 'Electronic Mail', 'Electronics', 'Elements', 'Emergency Situation', 'Engineering', 'English Language', 'Enrollment', 'Ensure', 'Environment', 'Environmental Health', 'Epidemiologist', 'Epidemiology', 'Ethambutol', 'Ethics', 'Etiology', 'Evaluation', 'Event', 'Evolution', 'Expert Systems', 'Exposure to', 'Face', 'Faculty', 'Familiarity', 'Family', 'Feedback', 'Fellowship', 'Fellowship Program', 'Female', 'Female Condoms', 'Fertility Rates', 'Fever', 'Figs - dietary', 'Financial Support', 'Floor', 'Focus Groups', 'Fostering', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Future', 'Gene Conversion', 'Gene Expression', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Recombination', 'Genetic screening method', 'Genome', 'Genomics', 'Genus Mycobacterium', 'Geographic Information Systems', 'Geography', 'Giardia lamblia', 'Globus Pallidus', 'Glues', 'Goals', 'Gonorrhea', 'Government', 'Grant', 'Grenada', 'Guanosine Diphosphate', 'Guidelines', 'HIV', 'HIV Infections', 'HIV Seropositivity', 'HIV prevention trials network', 'HIV vaccine', 'HIV-1', 'HIV-2 vaccine', 'Hand', 'Head', 'Health', 'Health Information System', 'Health Personnel', 'Health Policy', 'Health Professional', 'Health Science Library', 'Health Sciences', 'Health Services', 'Health Services Research', 'Health Status', 'Health care facility', 'Health education', 'Healthcare', 'Heterogeneity', 'Heterosexuals', 'High Prevalence', 'Hispanics', 'Home Page', 'Home environment', 'Hospital Administration', 'Hospital Information Systems', 'Hospitalization', 'Hospitals', 'Hour', 'Housing', 'Human', 'Human Herpesvirus 2', 'Human Papilloma Virus Vaccine', 'Human Papillomavirus', 'Human Resources', 'Human immunodeficiency virus test', 'Human papillomavirus 16', 'Hybrids', 'Hyphae', 'IL2 gene', 'Image', 'Immersion Investigative Technique', 'Immune', 'Incidence', 'Individual', 'Induration', 'Infant', 'Infection', 'Infection Control', 'Informatics', 'Information Centers', 'Information Management', 'Information Networks', 'Information Resources', 'Information Sciences', 'Information Systems', 'Institutes', 'Institution', 'Instruction', 'Interdisciplinary Study', 'Interleukin-2', 'Internal Medicine', 'International', 'International AIDS', 'International Classification of Diseases', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Intervention', 'Intervention Trial', 'Interview', 'Isoniazid resistance', 'Japan', 'Job Description', 'Journals', 'Kenya', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Laboratory Finding', 'Laboratory Research', 'Lamivudine/Zidovudine', 'Language', 'Latin America', 'Laws', 'Lead', 'Leadership', 'Learning', 'Left', 'Leishmaniasis', 'Lesion', 'Librarians', 'Libraries', 'Library Science', 'Licensing', 'Life', 'Life Expectancy', 'Light', 'Link', 'Linux', 'Literature', 'Local Area Networks', 'Location', 'London', 'Low income', 'Lung diseases', 'MEDLINE', 'Machine Learning', 'Malaria', 'Manuscripts', 'Maps', 'Marketing', 'Master of Science', 'Master&apos', 's Degree', 'Measures', 'Mediating', 'Mediation', 'Medical', 'Medical Education', 'Medical Informatics', 'Medical Libraries', 'Medical Research', 'Medical Students', 'Medical Surveillance', 'Medical Technology', 'Medical center', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Metronidazole', 'Microbiology', 'Mining', 'Minority', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Monitor', 'Montenegro', 'Mothers', 'Motivation', 'Multi-Drug Resistance', 'Multidrug-Resistant Tuberculosis', 'Mutation', 'Mycobacterium tuberculosis', 'N.I.H. Research Support', 'Names', 'Nature', 'Needs Assessment', 'Neighborhoods', 'Neisseria', 'Neonatology', 'Nested Case-Control Study', 'Nested PCR', 'Network-based', 'Nevirapine', 'Nuclear Energy', 'Nucleic Acids', 'Numbers', 'Nurses', 'Nursing Faculty', 'Nursing Schools', 'Occupational', 'Occupations', 'Online Systems', 'Ontology', 'Operating System', 'Oral', 'Outcome', 'Outcome Measure', 'Oxidation-Reduction', 'Oxidative Stress', 'Pacific Northwest', 'Pan American Health Organization', 'Paper', 'Parasites', 'Parasitic infection', 'Parasitology', 'Participant', 'Pathogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patient Education', 'Patient currently pregnant', 'Patients', 'Pattern', 'Pediatrics', 'Peer Review', 'Pelvic Examination', 'Pelvic Inflammatory Disease', 'Peptide Sequence Determination', 'Performance', 'Perinatal', 'Peripheral Blood Mononuclear Cell', 'Peroxidase', 'Peroxidases', 'Personal Satisfaction', 'Persons', 'Peru', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacologic Substance', 'Pharmacy Schools', 'Pharmacy facility', 'Phase', 'Phenotype', 'Philosophy', 'Physicians', 'Physics', 'Pilot Projects', 'Placebos', 'Placement', 'Plasma', 'Play', 'Pliability', 'Policies', 'Polymerase Chain Reaction', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Practice Management', 'Pre-Post Tests', 'Predictive Value', 'Predisposition', 'Pregnancy', 'Pregnant Women', 'Prenatal care', 'Preparation', 'Prevalence', 'Prevention', 'Prevention strategy', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Printing', 'Private Practice', 'Prize', 'Problem Solving', 'Procedures', 'Process', 'Production', 'Professional counselor', 'Program Development', 'Program Evaluation', 'Programmed Learning', 'Progress Reports', 'Prophylactic treatment', 'Proteins', 'Proteomics', 'Protocols documentation', 'Protozoa', 'Provider', 'Province', 'Psychological reinforcement', 'Public Health', 'Public Health Administration', 'Public Health Education', 'Public Health Informatics', 'Public Health Practice', 'Public Health Schools', 'Public Hospitals', 'Public Policy', 'Publications', 'Publishing', 'Pulmonary Tuberculosis', 'Purpose', 'Pyrazinamide', 'Qualifying', 'Qualitative Methods', 'Questionnaires', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Randomized', 'Randomized Controlled Clinical Trials', 'Randomized Controlled Trials', 'Range', 'Rate', 'Reaction', 'Readiness', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Recurrence', 'Regulation', 'Regulatory Element', 'Relative (related person)', 'Reporting', 'Representations, Knowledge (Computer)', 'Reproductive Tract Infections', 'Research', 'Research Activity', 'Research Design', 'Research Ethics Committees', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Research Training', 'Resistance', 'Resources', 'Restriction fragment length polymorphism', 'Retrieval', 'Review Literature', 'Rheumatology', 'Rifampin', 'Risk', 'Risk Behaviors', 'Risk Factors', 'Robotics', 'Role', 'Rotavirus', 'Route', 'Running', 'Rural', 'Rural Health', 'Rural Population', 'Safe Sex', 'Salmonella', 'Sampling', 'Scholarship', 'School Nursing', 'Schools', 'Science', 'Scientist', 'Score', 'Screening procedure', 'Second Pregnancy Trimester', 'Secure', 'Security', 'Seeds', 'Senegal', 'Sentinel', 'Sentinel Surveillance', 'Sequence Alignment', 'Sequence Analysis', 'Series', 'Seroepidemiologic Studies', 'Serological', 'Seroprevalences', 'Services', 'Sex Behavior', 'Sexual Partners', 'Sexually Transmitted Diseases', 'Side', 'Signs and Symptoms', 'Simulate', 'Singapore', 'Sister', 'Site', 'Social Development', 'Social Psychology', 'Software Engineering', 'Software Tools', 'Solid', 'Solutions', 'Source', 'Specificity', 'Specimen', 'Spectinomycin', 'Speed', 'Sputum', 'Staining method', 'Stains', 'Standards of Weights and Measures', 'Stomach', 'Streptomycin', 'Structure', 'Students', 'Supplementation', 'Support System', 'Surrogate Markers', 'Surveillance Program', 'Surveys', 'Swab', 'Switzerland', 'Symptoms', 'Syndrome', 'Syphilis', 'System', 'T-Lymphocyte', 'TAF8 gene', 'Taiwan', 'Tanzania', 'Teaching Materials', 'Techniques', 'Technology', 'Teleconferences', 'Telemedicine', 'Television', 'Test Result', 'Testing', 'Textbooks', 'Therapeutic', 'Thinking', 'Time', 'TimeLine', 'Title', 'Touch sensation', 'Trainers Training', 'Training', 'Training Activity', 'Training Programs', 'Training and Infrastructure', 'Treatment Efficacy', 'Treponema pallidum', 'Triad Acrylic Resin', 'Trichomonas Infections', 'Tropical Disease', 'Trust', 'Tuberculin', 'Tuberculosis', 'U-Series Cooperative Agreements', 'USAID', 'Ulcer', 'Underemployment', 'United States', 'United States Dept. of Health and Human Services', 'United States National Institutes of Health', 'Universities', 'Unmarried', 'Update', 'Urban Population', 'Urethritis', 'Vaccination', 'Vaccines', 'Vagina', 'Variant', 'Venezuela', 'Vertebral column', 'Vibrio', 'Videoconferences', 'Videoconferencing', 'Virulence', 'Virulence Factors', 'Vision', 'Visit', 'Visual', 'Voice', 'Walking', 'Washington', 'Week', 'Western Europe', 'Wing', 'Wolves', 'Woman', 'Work', 'Workplace', 'World Health Organization', 'Writing', 'Zidovudine', 'Zinc', 'abstracting', 'base', 'behavior change', 'biomedical informatics', 'blood filter', 'cancer therapy', 'career', 'case control', 'case-based', 'catalase', 'clinically relevant', 'cohort', 'college', 'computer center', 'computer science', 'computing resources', 'concept', 'condoms', 'contextual factors', 'coping', 'cost', 'cytokine', 'data integration', 'data management', 'data mining', 'database design', 'day', 'design', 'desire', 'digital', 'epidemiology study', 'evaluation/testing', 'expectation', 'experience', 'falls', 'fetal', 'fly', 'follow-up', 'forging', 'genital herpes', 'genome sequencing', 'global environment', 'health administration', 'health application', 'high risk behavior', 'human subject', 'image processing', 'imaging informatics', 'immunopathology', 'improved', 'informatics training', 'information gathering', 'information organization', 'innovation', 'insight', 'instrumentation', 'interest', 'international center', 'isoniazid', 'knowledge base', 'laboratory facility', 'lectures', 'macrophage', 'male', 'mathematical model', 'medical schools', 'member', 'men', 'microbial', 'mortality', 'mouse Gdi2 protein', 'mycobacterial', 'neglect', 'nevirapine resistance', 'new growth', 'next generation', 'older patient', 'older women', 'oncology', 'open source', 'outreach', 'pathogen', 'pediatric AIDS', 'pediatrician', 'peer', 'point of care', 'prenatal', 'prevent', 'professor', 'programs', 'prospective', 'prototype', 'recombinase', 'rectal', 'research and development', 'research study', 'response', 'satisfaction', 'sex', 'size', 'skills', 'skills training', 'software development', 'sound', 'statistics', 'success', 'symposium', 'syndromic surveillance', 'teacher', 'tool', 'tool development', 'trafficking', 'transcription factor', 'transmission process', 'treatment planning', 'trend', 'tumor', 'web based interface', 'wide area network', 'willingness', 'young adult']",FIC,UNIVERSITY OF WASHINGTON,D43,2007,73250,0.015599816893843271
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7075417,R01LM008635,"['artificial intelligence', 'automated medical record system', 'bioinformatics', 'biomedical automation', 'biomedical resource', 'clinical research', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data management', 'health science research', 'high throughput technology', 'human data']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,544814,0.005231467998109847
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,0.03677478815955287
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,0.017027382518902172
"Computer System for Functional Analysis of Genomic Data    DESCRIPTION (provided by applicant): In two previous stages of this project, both funded by the National Institute of General Medical Sciences and carried out successfully, we developed GeneWays, a completely automated system that efficiently distills information about molecular interactions from an astronomical number of full-text biomedical articles. The next logical stage of the project is to carry this system from the computational laboratory into a practical, useful, and even indispensable tool that researchers can use to solve complex problems currently posed in experimental medicine and biology. The central hypothesis of our work on GeneWays has been that our computational tools will generate biological predictions of a quality sufficiently high that the biomedical community will invest in serious experimental validation. Specifically, we propose the following. 1. We will improve significantly the precision and recall of the GeneWays system. 2. We will develop and implement a probabilistic belief-network formalism?a belief-graph relative of the Bayesian network formalism that allows us to place and update beliefs on both the vertices and the edges of the graph for probabilistic reasoning over the large collection of facts in the GeneWays database. We will develop and implement a coordinated collection of methods for computing and updating beliefs on individual nodes and edges of the belief graph. 3. We will develop and implement a mathematical framework for incorporating pathway information into a genetic- linkage analysis formalism in such a way that each piece of pathway knowledge includes a specified degree of confidence. 4. We will process an enormous collection of texts, such as open-access biomedical journals, PubMed abstracts, and the GeneWays corpus, and thus will build a comprehensive GeneHighWays database. We will make the GeneHighWays database easily and freely accessible to academic researchers through a web interface. We will evaluate the new version of the GeneWays system and the GeneHighWays database for the quality of data, performance of the mathematical methods, and quality of the interface.           n/a",Computer System for Functional Analysis of Genomic Data,7148274,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,303688,0.03222412598496353
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7007706,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2006,153843,0.014771458339507968
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,301738,-0.004347442901271552
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.060513367579352335
"The RPI Exploratory Center for Cheminformatics (RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics (RMI),7125575,P20HG003899,"['Internet', 'NIH Roadmap Initiative tag', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2006,377226,0.011652422207400271
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7135482,R01LM009254,"['abstracting', 'human', 'language', 'performance', 'training']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,369593,0.016228000653298498
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7126050,U54CA121852,"['NIH Roadmap Initiative tag', 'bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2006,3747227,0.043261403593741234
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,0.049658782984098745
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,6899974,R01LM008635,"['artificial intelligence', 'automated medical record system', 'bioinformatics', 'biomedical automation', 'biomedical resource', 'clinical research', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data management', 'health science research', 'high throughput technology', 'human data']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,515359,0.005231467998109847
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,0.03677478815955287
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6946761,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2005,100000,0.021215966472740315
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,0.027502321921168076
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The overall goal of this proposal is to develop an information integration architecture and associated tools to support rapid integration of data and knowledge from distributed heterogeneous data sources. The architecture aims to play a significant role in extracting coherent knowledge bases for biomedical research and improving the accuracy, completeness and quality of the extracted knowledge. Towards achieving these goals, the proposed scalable architecture includes new innovative generalized integration algorithms and tools for the generation of mediators to capture the functional behavior of data sources, semantic representation of data sources to support automated generation of integration agents, and optimization of integrated data queries. The information integration architecture keeps pace with the evolving Internet-based XML electronic data interchange, semantic web services, and web services discovery standards. Thus, leveraging the Internet technologies and standards for the purpose of providing lasting state-of-the-art solutions for information integration. In addition, the proposed architecture is inherently scalable in terms of the number of data sources that can be integrated, the number of users of the integrated system, and the range of biomedical problems that can be tackled. During phase I of the project, prototypes of the proposed integration algorithms and tools will be developed as proofs of concept and to form the foundation for evaluation and pilot testing of the proposed integration mechanisms, using private and public data sources, in terms of scalability and integration capabilities.         n/a",Information Integration of Heterogeneous Data Sources,6881960,R43RR018667,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'information system analysis', 'information systems', 'mathematics']",NCRR,"INFOTECH SOFT, INC.",R43,2005,260661,0.02190717287698218
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,0.017027382518902172
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,0.01845009639111209
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,7000653,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,SPELMAN COLLEGE,R33,2005,260030,0.017262115726805825
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7125331,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2005,152083,0.014771458339507968
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,300000,-0.004347442901271552
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.060513367579352335
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6931476,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2005,392500,0.046666416079201474
"Middle School Life Science-Education Partnership-PhaseII    DESCRIPTION (provided by applicant):    Precollege science education, particularly middle school where students first encounter courses dedicated to the sciences, has lacked the attention of the scientific and education communities. As a result, many middle level teachers are marginally prepared to teach contemporary science, the development of effective curricular materials has lagged, and schools devote inadequate resources and support to science. In the prior Phase I grant, experiment modules have been designed by scientists that address the core content of middle school Life Science. They utilize live (aquatic) organisms in the class room to stimulate inquiry into the properties of living organisms and make connections with related environmental health problems to link student learning with the world around them. Human environmental health was chosen as the biomedical context because it strongly relates to the macroscopic world in which middle school students live and provides age-appropriate learning opportunities for students as they begin to understand organisms and their underlying structures and mechanisms. During Phase II our specific objectives are 1. To infuse middle school Life Science education with attractive experiment modules that stress student inquiry. 2. To mount a comprehensive dissemination program with school systems that work with large numbers of minorities and students from families with lower socioeconomic status. 3. To utilize a 'train the trainer' method for lead teachers to achieve wide dissemination of the modules that involves close collaboration between scientists and educators and the lead teachers. 4. To provide teachers with full support to implement and sustain the modules through a science resource center. 5. To continue the development of the content of the modules. 6. To obtain a thorough formative and summative evaluation of the effectiveness of the curriculum modules and the entire mechanism for their delivery. 7. To establish a robust program to sustain the module dissemination to teachers after the completion of the grant.         n/a",Middle School Life Science-Education Partnership-PhaseII,6857683,R25RR014267,"['DVD /CD ROM', 'artificial intelligence', 'curriculum', 'education evaluation /planning', 'educational resource design /development', 'ethnic group', 'female', 'learning', 'meeting /conference /symposium', 'minority health professional', 'science education', 'secondary schools', 'social support network', 'teacher', 'university student', 'videotape /videodisc']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R25,2005,268813,0.015074473239700636
"Extracting Semantic Knowledge from Clinical Data Sources DESCRIPTION (provided by applicant):    Electronic medical record systems (EMR) contain a wealth of clinical data that is invaluable for biomedical research, but because there are no satisfactory methods to build coherent specialized knowledge bases, which represent the information in free text medical records, data mining and clinical discovery are held back. Medical Reporting Solutions, Inc. has developed advanced technology, which we propose to extend, refine, and test for constructing specialized semantic knowledge bases. These knowledge bases will encode the clinical information in medical reports, and enable automated natural language processing systems for extracting clinical knowledge.      Our research and development uses methods in corpus linguistics and sentential logic to represent the knowledge in free-text medical reports in an efficient, codeable manner. We have created tools to map sentences in a medical domain to unique codeable propositions. Our method for creating knowledge ontologies makes it easy for biomedical researchers to get semantic information at the appropriate level of detail. The knowledge base and mapping tables allow us to analyze medical reports in near real-time. One knowledge base, under development, is derived from hundreds of thousands of reports in the radiology domain, and we intend to analyze other medical domains using the methods we have pioneered.      Our phase one project plan includes further improving our knowledge editing tools, substantially enlarging our semantic knowledge base to cover over 60-70% of the radiology domain, and extensively test our knowledge representation schema against actual radiology reports. We plan to make the knowledge base freely accessible to the biomedical research community, while providing commercial services to codify free text reports found in EMRs. n/a",Extracting Semantic Knowledge from Clinical Data Sources,6988908,R43LM008974,"['automated medical record system', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'human data', 'informatics', 'information retrieval']",NLM,"LOGICAL SEMANTICS, INC.",R43,2005,100000,7.320187198891611e-05
"The RPI Exploratory Center for Cheminformatics(RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics(RMI),7032113,P20HG003899,"['Internet', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2005,375639,0.011652422207400271
"Biomedical Ontology and Tools for Database Curation DESCRIPTION (provided by applicant): This proposal describes a new tool for text data mining-a biomedical language ontology and integrated natural-language-processing methods. Our long-term goal is to provide resources for biomedical knowledge discovery from text. Our immediate goal is to provide a knowledge discovery tool for the curation of organism databases such as the Genome Database (SGD). The proposed research not only serves the research needs of the SGD community, it also helps the broader biomedical community exploit the strengths of the comparative approach to biological research. The hypothesis of this proposal is that knowledge discovery from biomedical text requires a knowledge base that integrates both genomic and linguistic information. This hypothesis is based on two observations: (a) the language of biomedicine, like all natural language, is complex in structure and morphology (the basic units of meaning) and poses problems of synonymy (several terms having the same meaning), polysemy (a term having more than one meaning), hypernymy (one term being more general than another), hyponymy (one term being more specific than another), denotation (what a term refers to in contrast to what it means), and denotation and description (different ways of referring to the same thing); and (b) important biomedical knowledge sources, such as the Gene Ontology (GO), are expressed in natural language. The specific aims of the proposed project are to: 1. Extend an existing biomedical language ontology to include genomic and linguistic data from SGD; 2. Use this ontology to discover, in full-text articles made available by SGD, information about the molecular function of yeast gene products that can be inferred from direct experimental assays; 3. Evaluate the effectiveness of the new tool and methods by comparing its results to those of the SGD curators for gene products that have GO functional annotations with evidence code IDA (Inferred from Direct Assay). n/a",Biomedical Ontology and Tools for Database Curation,6885487,R43HG003600,"['computer program /software', 'computer system design /evaluation', 'fungal genetics', 'information retrieval', 'information system analysis', 'molecular biology information system', 'yeasts']",NHGRI,"CONVERSPEECH, LLC",R43,2005,99250,0.007154335451234687
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,0.0008908214772938249
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7012104,U54CA121852,"['bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2005,3758967,0.043261403593741234
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,0.049658782984098745
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,0.03677478815955287
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6805962,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2004,100000,0.021215966472740315
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,0.017027382518902172
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,0.027502321921168076
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,0.01845009639111209
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,0.01845009639111209
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6697243,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,UNIVERSITY OF IOWA,R33,2004,252586,0.017262115726805825
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,337653,0.015534123713487884
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.060513367579352335
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6787778,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2004,392500,0.046666416079201474
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,0.049658782984098745
"BioMediator: Biologic Data Integration & Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration & Analysis System,6681249,R01HG002288,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' information retrieval', ' molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2003,100000,0.021215966472740315
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,0.027502321921168076
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,0.01845009639111209
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,0.038730947866836396
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6549357,R33EY013688,"['artificial intelligence', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' human subject', ' informatics', ' interdisciplinary collaboration', ' macular degeneration', ' mathematics', ' pathologic process', ' phenotype']",NEI,UNIVERSITY OF IOWA,R33,2003,259228,0.017262115726805825
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,327818,0.015534123713487884
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6738628,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,159000,0.015534123713487884
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6646557,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2003,237000,-0.003627548016362049
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6690235,P20GM067650,"['animal tissue', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer simulation', ' computer system design /evaluation', ' data management', ' disease /disorder etiology', ' epidemiology', ' functional /structural genomics', ' gene expression', ' human subject', ' informatics', ' interdisciplinary collaboration', ' mathematical model', ' pharmacokinetics', ' science education', ' statistics /biometry', ' technology /technique development', ' therapy', ' training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2003,392500,0.046666416079201474
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6526728,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2002,372289,0.015594381491346522
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,0.027502321921168076
"MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I   DESCRIPTION (Adapted from the applicant's abstract):  Scientists associated          with the National Institute of Environmental Health Sciences (NIEHS) Marine          and Freshwater Biomedical Sciences Center and professionals within the               University of Wisconsin-Milwaukee (UWM) School of Education will establish a         collaborative relationship with several Milwaukee metropolitan schools.  The         long range objective is to develop a significant program to enhance the              quality of the curriculum and instruction for students enrolled in middle            school Life Science.  There are two primary thrusts.  The first involves the         creation of a number of experiment modules that relate to the biomedical             content of the course and which are designed to stimulate inquiry-based              learning.  At their center is the utilization of non-mammalian organisms in          the classroom.  Enrichment is provided with interactive videotape/CD-ROM             materials that engage students with subject matter that cannot be brought into       their classrooms.  The second emphasis provides in-service teachers with a           network of support that increases their knowledge, understanding, and ability        to conduct laboratory experiments.  It also offers pre-service teachers in           training an opportunity to develop a strong foundation in life science.  The         Specific Aims are:  A) to provide teachers with a suite of fully developed           modules that emphasize hands-on, modern biomedical science; B) to surround the       modules with support materials that utilize information technology; C) to            provide a scientific community for teachers through workshops and meetings; D)       to enhance the ability of teachers to facilitate critical thinking among             students; E) to support teachers in distance learning, including interactions        with scientists, teachers, and inter-classroom communication; F) to encourage        female and minority interest and success in science; G) to initiate a model          program to enhance life science education for pre-service teachers; and H) to        provide thorough evaluation of the effectiveness of these aims.                                                                                                           n/a",MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I,6529842,R25RR014267,"['DVD /CD ROM', ' artificial intelligence', ' curriculum', ' education evaluation /planning', ' educational resource design /development', ' ethnic group', ' female', ' learning', ' meeting /conference /symposium', ' minority health professional', ' science education', ' secondary schools', ' social support network', ' teacher', ' university student', ' videotape /videodisc']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R25,2002,269200,0.03700103711008076
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,318270,0.015534123713487884
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6649647,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,232118,0.015534123713487884
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6526274,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2002,237000,-0.003627548016362049
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,0.021230474429007738
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6388359,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2001,362594,0.015594381491346522
"MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I   DESCRIPTION (Adapted from the applicant's abstract):  Scientists associated          with the National Institute of Environmental Health Sciences (NIEHS) Marine          and Freshwater Biomedical Sciences Center and professionals within the               University of Wisconsin-Milwaukee (UWM) School of Education will establish a         collaborative relationship with several Milwaukee metropolitan schools.  The         long range objective is to develop a significant program to enhance the              quality of the curriculum and instruction for students enrolled in middle            school Life Science.  There are two primary thrusts.  The first involves the         creation of a number of experiment modules that relate to the biomedical             content of the course and which are designed to stimulate inquiry-based              learning.  At their center is the utilization of non-mammalian organisms in          the classroom.  Enrichment is provided with interactive videotape/CD-ROM             materials that engage students with subject matter that cannot be brought into       their classrooms.  The second emphasis provides in-service teachers with a           network of support that increases their knowledge, understanding, and ability        to conduct laboratory experiments.  It also offers pre-service teachers in           training an opportunity to develop a strong foundation in life science.  The         Specific Aims are:  A) to provide teachers with a suite of fully developed           modules that emphasize hands-on, modern biomedical science; B) to surround the       modules with support materials that utilize information technology; C) to            provide a scientific community for teachers through workshops and meetings; D)       to enhance the ability of teachers to facilitate critical thinking among             students; E) to support teachers in distance learning, including interactions        with scientists, teachers, and inter-classroom communication; F) to encourage        female and minority interest and success in science; G) to initiate a model          program to enhance life science education for pre-service teachers; and H) to        provide thorough evaluation of the effectiveness of these aims.                                                                                                           n/a",MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I,6446869,R25RR014267,"['DVD /CD ROM', ' artificial intelligence', ' curriculum', ' education evaluation /planning', ' educational resource design /development', ' ethnic group', ' female', ' learning', ' meeting /conference /symposium', ' minority health professional', ' science education', ' secondary schools', ' social support network', ' teacher', ' university student', ' videotape /videodisc']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R25,2001,269200,0.03700103711008076
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,0.038730947866836396
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6495900,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,35096,0.015534123713487884
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,309000,0.015534123713487884
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6401728,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2001,237000,-0.003627548016362049
"KNOWLEDGE DISCOVERY IN DISTRIBUTED CARDIAC IMAGE BASES DESCRIPTION (taken from application abstract):  Heart disease continues          to be the primary cause of death in the U.S., with 25% of all deaths             related to coronary artery disease (CAD). In addition to the loss of             irreplaceable human life, there are also staggering health care costs            and losses in productivity associated with the 1.5 million myocardial            infarctions suffered in the U.S. every year.  The present competing              renewal application seeks to make a contribution toward this vital               health care problem by exploring frontier computing methods to support           and facilitate CAD assessment.  The objective of the proposed research           is to develop and evaluate a methodology to accomplish the following             specific aims:                                                                                                                                                    (1)Knowledge Discovery: To design, implement and test novel database             (DB) ""mining"" algorithms to uncover associations and inferences imbedded         in clinical DBs and which can improve diagnostic performance.                    (2)Knowledge Base Enrichment: To use the knowledge resulting from DB             mining as well as conventional knowledge-acquisition methods to create           and evaluate a robust knowledge base (KB) with which to interpret                cardiovascular SPECT imagery and other types of relevant, patient-               specific information.                                                            (3)Distributed Knowledge Discovery and Processing:  To extend both the           Knowledge-discovery and knowledge-based processing methods to                    distributed, Internet-based setting for a twofold purpose: (I) to                provide users with widespread access to the resulting KB, and (ii) to            access and mine remote multi center DBs to further improve our knowledge         regarding the assessment of CAD.                                                                                                                                  The proposed work represents pioneering research in several ways,                especially: (I) the creation of innovative algorithms to mine image DBs,         (II) the application of these algorithms to the clinical assessment of           CAD, and (III) the creation of distributed DB mining and knowledge-based         processing methods to link geographically dispersed users and clinical           DBs.  The proposed research builds on our previous work on knowledge-            guided image interpretation, and represents an interinstitutional and            interdisciplinary effort between Georgia Tech and Emory University, a            longstanding collaboration that has previously resulted in numerous              joint publications and valuable insights centering on diagnostic                 imaging, and which has also supported several academic degrees.                   n/a",KNOWLEDGE DISCOVERY IN DISTRIBUTED CARDIAC IMAGE BASES,6351635,R01LM006726,"['Internet', ' angiography', ' artificial intelligence', ' bioimaging /biomedical imaging', ' cardiovascular imaging /visualization', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer data analysis', ' coronary disorder', ' diagnosis design /evaluation', ' heart disorder diagnosis', ' human data', ' information system analysis', ' mathematical model', ' myocardium', ' perfusion', ' single photon emission computed tomography']",NLM,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2001,362213,-0.06318568657871479
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,0.021230474429007738
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6031661,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2000,354198,0.015594381491346522
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,0.011355984416412944
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,0.011355984416412944
"KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA Abstractions of time-stamped clinical data are useful for planning               therapy, for monitoring therapy, and for creating high-level summaries of        time-oriented clinical databases.  Temporal abstractions also support            explanations by an intelligent patient-record system and can be used for         representation of the goals and intentions of clinical guidelines and            protocols.                                                                                                                                                        We propose to reengineer and expand the scope of the RESUME system, a            prototype computer program that implements the knowledge-based temporal-         abstraction method, a conceptual and computational framework that we have        developed for abstraction of time-stamped clinical data into clinically          meaningful interval-based concepts. RESUME has been evaluated with highly        encouraging results in several clinical areas. We will address the               practical and theoretical issues of representation, acquisition,                 maintenance, and reuse of temporal-abstraction knowledge. Our specific           aims are defined by a four-step research plan:                                                                                                                    1. We will define formally the knowledge requirements for five                   computational modules (mechanisms) we employ, thus facilitating the              acquisition, maintenance, reuse, and sharing of the required knowledge.                                                                                           2. We will enhance, expand, and redesign five computational temporal-            abstraction mechanisms:                                                          (a) Automatic formation of meaningful contexts for interpretation of             clinical data.                                                                   (b) Classification of clinical data that have equivalent time stamps into        higher-level concepts.                                                           (c) Temporal inference (e.g., the join of certain interval-based clinical        abstractions into longer ones).                                                  (d) Interpolation between temporally disjoint clinical abstractions,             including a development of a probabilistic representation and semantics.         (e) Matching of predefined and runtime temporal patterns, given time-            stamped data and conclusions.                                                                                                                                     3. We will develop a tool for automated acquisition, from expert                 physicians, of temporal-abstraction knowledge, using techniques from the         PROTEGE-II project for designing knowledge-based systems.                                                                                                         4. We will validate and evaluate our methodology and its implementation.         (a) We will assess the value of the knowledge-acquisition tool in several        experiments.                                                                     (b) We will validate the performance of the computational mechanisms in          the domain of therapy of patients who have insulin-dependent diabetes by         collaboration with expert endocrinologists.                                      (c) We will evaluate the overall framework within EON, a project in which        researchers are implementing an integrated architecture for protocol-based       care.                                                                             n/a",KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA,6185217,R29LM006245,"['abstracting', ' artificial intelligence', ' computer assisted medical decision making', ' computer program /software', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' time resolved data']",NLM,STANFORD UNIVERSITY,R29,2000,121082,0.003111539102473237
"MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I   DESCRIPTION (Adapted from the applicant's abstract):  Scientists associated          with the National Institute of Environmental Health Sciences (NIEHS) Marine          and Freshwater Biomedical Sciences Center and professionals within the               University of Wisconsin-Milwaukee (UWM) School of Education will establish a         collaborative relationship with several Milwaukee metropolitan schools.  The         long range objective is to develop a significant program to enhance the              quality of the curriculum and instruction for students enrolled in middle            school Life Science.  There are two primary thrusts.  The first involves the         creation of a number of experiment modules that relate to the biomedical             content of the course and which are designed to stimulate inquiry-based              learning.  At their center is the utilization of non-mammalian organisms in          the classroom.  Enrichment is provided with interactive videotape/CD-ROM             materials that engage students with subject matter that cannot be brought into       their classrooms.  The second emphasis provides in-service teachers with a           network of support that increases their knowledge, understanding, and ability        to conduct laboratory experiments.  It also offers pre-service teachers in           training an opportunity to develop a strong foundation in life science.  The         Specific Aims are:  A) to provide teachers with a suite of fully developed           modules that emphasize hands-on, modern biomedical science; B) to surround the       modules with support materials that utilize information technology; C) to            provide a scientific community for teachers through workshops and meetings; D)       to enhance the ability of teachers to facilitate critical thinking among             students; E) to support teachers in distance learning, including interactions        with scientists, teachers, and inter-classroom communication; F) to encourage        female and minority interest and success in science; G) to initiate a model          program to enhance life science education for pre-service teachers; and H) to        provide thorough evaluation of the effectiveness of these aims.                                                                                                           n/a",MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I,6360111,R25RR014267,"['DVD /CD ROM', ' artificial intelligence', ' curriculum', ' education evaluation /planning', ' educational resource design /development', ' ethnic group', ' female', ' learning', ' meeting /conference /symposium', ' minority health professional', ' science education', ' secondary schools', ' social support network', ' teacher', ' university student', ' videotape /videodisc']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R25,2000,269199,0.03700103711008076
"MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I   DESCRIPTION (Adapted from the applicant's abstract):  Scientists associated          with the National Institute of Environmental Health Sciences (NIEHS) Marine          and Freshwater Biomedical Sciences Center and professionals within the               University of Wisconsin-Milwaukee (UWM) School of Education will establish a         collaborative relationship with several Milwaukee metropolitan schools.  The         long range objective is to develop a significant program to enhance the              quality of the curriculum and instruction for students enrolled in middle            school Life Science.  There are two primary thrusts.  The first involves the         creation of a number of experiment modules that relate to the biomedical             content of the course and which are designed to stimulate inquiry-based              learning.  At their center is the utilization of non-mammalian organisms in          the classroom.  Enrichment is provided with interactive videotape/CD-ROM             materials that engage students with subject matter that cannot be brought into       their classrooms.  The second emphasis provides in-service teachers with a           network of support that increases their knowledge, understanding, and ability        to conduct laboratory experiments.  It also offers pre-service teachers in           training an opportunity to develop a strong foundation in life science.  The         Specific Aims are:  A) to provide teachers with a suite of fully developed           modules that emphasize hands-on, modern biomedical science; B) to surround the       modules with support materials that utilize information technology; C) to            provide a scientific community for teachers through workshops and meetings; D)       to enhance the ability of teachers to facilitate critical thinking among             students; E) to support teachers in distance learning, including interactions        with scientists, teachers, and inter-classroom communication; F) to encourage        female and minority interest and success in science; G) to initiate a model          program to enhance life science education for pre-service teachers; and H) to        provide thorough evaluation of the effectiveness of these aims.                                                                                                           n/a",MIDDLE SCHOOL LIFE SCIENCE-EDUCATION PARTNERSHIP-PHASE I,2885219,R25RR014267,"['DVD /CD ROM', ' artificial intelligence', ' curriculum', ' education evaluation /planning', ' educational resource design /development', ' ethnic group', ' female', ' learning', ' meeting /conference /symposium', ' minority health professional', ' science education', ' secondary schools', ' social support network', ' teacher', ' university student', ' videotape /videodisc']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R25,2000,1,0.03700103711008076
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,0.038730947866836396
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,300000,0.015534123713487884
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6323962,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,186611,0.015534123713487884
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6344145,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,141192,0.015534123713487884
"KNOWLEDGE DISCOVERY IN DISTRIBUTED CARDIAC IMAGE BASES DESCRIPTION (taken from application abstract):  Heart disease continues          to be the primary cause of death in the U.S., with 25% of all deaths             related to coronary artery disease (CAD). In addition to the loss of             irreplaceable human life, there are also staggering health care costs            and losses in productivity associated with the 1.5 million myocardial            infarctions suffered in the U.S. every year.  The present competing              renewal application seeks to make a contribution toward this vital               health care problem by exploring frontier computing methods to support           and facilitate CAD assessment.  The objective of the proposed research           is to develop and evaluate a methodology to accomplish the following             specific aims:                                                                                                                                                    (1)Knowledge Discovery: To design, implement and test novel database             (DB) ""mining"" algorithms to uncover associations and inferences imbedded         in clinical DBs and which can improve diagnostic performance.                    (2)Knowledge Base Enrichment: To use the knowledge resulting from DB             mining as well as conventional knowledge-acquisition methods to create           and evaluate a robust knowledge base (KB) with which to interpret                cardiovascular SPECT imagery and other types of relevant, patient-               specific information.                                                            (3)Distributed Knowledge Discovery and Processing:  To extend both the           Knowledge-discovery and knowledge-based processing methods to                    distributed, Internet-based setting for a twofold purpose: (I) to                provide users with widespread access to the resulting KB, and (ii) to            access and mine remote multi center DBs to further improve our knowledge         regarding the assessment of CAD.                                                                                                                                  The proposed work represents pioneering research in several ways,                especially: (I) the creation of innovative algorithms to mine image DBs,         (II) the application of these algorithms to the clinical assessment of           CAD, and (III) the creation of distributed DB mining and knowledge-based         processing methods to link geographically dispersed users and clinical           DBs.  The proposed research builds on our previous work on knowledge-            guided image interpretation, and represents an interinstitutional and            interdisciplinary effort between Georgia Tech and Emory University, a            longstanding collaboration that has previously resulted in numerous              joint publications and valuable insights centering on diagnostic                 imaging, and which has also supported several academic degrees.                   n/a",KNOWLEDGE DISCOVERY IN DISTRIBUTED CARDIAC IMAGE BASES,6151289,R01LM006726,"['Internet', ' angiography', ' artificial intelligence', ' bioimaging /biomedical imaging', ' cardiovascular imaging /visualization', ' computer assisted diagnosis', ' computer assisted medical decision making', ' coronary disorder', ' diagnosis design /evaluation', ' heart disorder diagnosis', ' human data', ' information system analysis', ' mathematical model', ' myocardium', ' perfusion', ' single photon emission computed tomography']",NLM,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2000,351662,-0.06318568657871479
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,0.01863775637316856
"Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points Project Summary Biomedical data science data modeling is relevant to a plethora of informatics research activities, such as natural language processing, machine learning, artificial intelligence, and predictive analytics. As Electronic Health Record systems become more advanced and more mature, with the potential to incorporate a wide and diverse array of data from genomics to mobile health (mHealth) applications, the scope and nature of the biomedical data science questions researchers ask become broader. Concomitantly, the answers to their questions have the potential to impact the care of millions of patients—getting the answers right, proactively, is high stakes. However, in data modeling currently, there is no bioethics framework to guide the process of mapping key decision points and recording the rationale for choices made. Making data modeling decision points, as well as the reasoning behind them, explicit would have a twofold impact on improving biomedical data science by: 1. Enhancing transparency and reproducibility and maximizing the value of data science research and 2. Supporting the ability to assess decision points and rationales in terms of their most crucial ethical ramifications. Research in this area is particularly timely amid the interest in, and enthusiasm for, leveraging Big Data sources in the service of improving patient population health and the health of the general public. The National Institutes of Health (NIH) recently released a strategic plan for data science; there is no better time than now to create an initial bioethical framework to inform common data modeling decision points. The improvements in data quality that will derive from decision point mapping and bioethical review will enhance efforts to apply data models across a range of high-impact areas, from predictive analytics to support clinical decision-making to robust trending models in population health to better inform local, regional, and national health policies and resource allocation. To develop this initial bioethics framework, we will use well- established qualitative research methods (interviews, focus groups, and in-person deliberation) to map the decision points in biomedical data modeling research and document the rationales invoked to support those decisions (Aim 1 key informant interviews); assess those data science decision points and decision-making rationales for their bioethical ramifications (Aim 2 focus groups); and create an initial bioethics data modeling framework (Aim 3 deliberative meeting). This study would be the first to provide a bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe. This approach directly supports core scientific values of inclusivity, transparency, accountability, and reproducibility that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global. Project Narrative This study would be the first to develop an initial bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe—not least because poorly developed data models have the potential to impact adversely the health of individuals, groups, and communities. Currently, there is limited conversation around potential bioethics issues in data modeling, and as yet no implementable guidance on how biomedical data science modeling research activities should occur. The initial ethics framework developed by this study would provide a roadmap to ensure that data modeling decision points are documented and their ethical ramifications considered at the outset of model creation, thus supporting core scientific values of inclusivity, accountability, reproducibility, and transparency that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global.",Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points,10039527,R21HG011277,"['Accountability', 'Address', 'Area', 'Artificial Intelligence', 'Big Data', 'Bioethical Issues', 'Bioethics', 'Bioethics Consultants', 'Caring', 'Clinical', 'Communities', 'Data', 'Data Science', 'Data Scientist', 'Data Sources', 'Decision Making', 'Development', 'Electronic Health Record', 'Ensure', 'Ethical Issues', 'Ethical Review', 'Ethics', 'Focus Groups', 'Fostering', 'General Population', 'Health', 'Health Resources', 'Health system', 'Individual', 'Informatics', 'Interview', 'Machine Learning', 'Maps', 'Methods', 'Mobile Health Application', 'Modeling', 'National Health Policy', 'Natural Language Processing', 'Nature', 'Output', 'Patients', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Qualitative Research', 'Reproducibility', 'Research', 'Research Activity', 'Research Methodology', 'Research Personnel', 'Resource Allocation', 'Role', 'Services', 'Social Environment', 'Strategic Planning', 'Structure', 'System', 'Time', 'Trust', 'United States National Institutes of Health', 'Walking', 'base', 'biomedical data science', 'clinical decision support', 'clinical decision-making', 'data modeling', 'data quality', 'data tools', 'ethical legal social implication', 'genomic data', 'high standard', 'improved', 'individual patient', 'informant', 'interest', 'interoperability', 'meetings', 'model development', 'patient population', 'population health', 'programs', 'public trust', 'tool', 'trend', 'usability']",NHGRI,"HASTINGS CENTER, INC.",R21,2020,100000,0.02155284129827523
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10052028,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2020,358691,0.01544411923123548
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,0.0300948459631808
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9955351,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,517554,0.061313337152203495
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9969467,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Models', 'Computer software', 'Computers', 'Custom', 'Data', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'algorithm development', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'large datasets', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,614363,0.018566726392329227
"In-silico prediction of protein-peptide interactions. IN-SILICO PREDICTION OF PROTEIN-PEPTIDE INTERACTIONS Automated docking methods are used extensively for gaining a mechanistic understanding of the molecular interactions underpinning cellular processes. While these tools work well for small molecules they perform poorly for peptides and cannot handle Intrinsically Disordered Proteins (IDPs) which play very important roles in these processes. The goal of this project is the development of an efficient and practical peptide docking software, useful for designing therapeutic peptides and gaining insight into IDPs binding ordered proteins. The proposed software supports biomedical applications ranging from investigating chemical pathways to designing and optimizing therapeutic molecules for diseases such as cancer and metabolic disorders. Under the previous award we developed and released a new method for docking fully-flexible peptides with up to 20 standard amino acids: AutoDock CrankPep (ADCP). We showed that it outperforms current state-of-the-art docking methods. For the next award, we propose to: 1) further develop ADCP to support docking IPDs with up to 70 amino acids and improve support for therapeutic peptides containing modified amino acids and complex macrocycles; 2) develop peptide-specific scoring functions to increase docking success rates and methods for predicting the free energy of binding of peptides. This will be done by exploiting the latest advances in statistical potentials for docking, as well as applying machine-learning techniques; 3) test and validate the software on our datasets, community benchmarks, and through our collaborations with outstanding biologists working on biomedical applications spanning from designing drugs for thrombosis and influenza, to modeling IDPs interacting with globular proteins; and 4) document the software and release it under an open source license on a regular basis along with datasets we compile and update on regularly. The proposed research will occur in the context of collaborations with experimental biologists working on highly relevant biomedical projects and providing experimental feedback and validation. In addition, this project will benefit from various collaborations with experts in the fields of computational biology, applied mathematics and artificial intelligence. This docking software tool will be developed by applying best practices in software engineering and be implemented as a modular, extensible, component-based software framework for peptide docking. This docking engine will be part of the widely used AutoDock software suite. The ability to model complexes formed by proteins and fully-flexible peptides or IDPs is in high demand and will greatly extend the range of peptide-based therapeutic approaches for which automated docking can be successfully applied. It will also support gaining insights into interactions of IDPs with proteins. As such, it will impact the research of many medicinal chemists and biologist and extend the use of computational tools to a wider community of scientists, thereby supporting the advancement of biomedical research. Automated docking is a workhorse for rational drug design, however, applying these methods to peptides has remained challenging, thus impeding the designing of therapeutic peptides and the study of Intrinsically Disordered Proteins (IDP) binding to their ordered partners. During the prior funding period, we made substantial progress toward peptide docking, resulting in a new docking engine: AutoDock CrankPep, which outperforms state-of-the-art docking methods for linear and cyclic peptides with up to 20 standard amino acids. We propose to further develop AutoDock CrankPep to support docking of therapeutic peptides with modified amino acids as well as IDPs with up to 70 amino acids, creating a practical docking tool for peptides that will impact the research of many computational and medicinal chemists and biologist, contribute to our understanding of biological processes, and significantly advance biomedical research.",In-silico prediction of protein-peptide interactions.,10116950,R01GM096888,"['Amino Acids', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'Benchmarking', 'Binding', 'Binding Proteins', 'Biological', 'Biological Availability', 'Biological Process', 'Biomedical Research', 'Cell physiology', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Cyclic Peptides', 'Data Set', 'Development', 'Disease', 'Docking', 'Documentation', 'Drug Design', 'Educational workshop', 'Feedback', 'Free Energy', 'Funding', 'Goals', 'Half-Life', 'Influenza', 'Insulin', 'Libraries', 'Licensing', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Mediating', 'Metabolic Diseases', 'Methods', 'Modeling', 'Modernization', 'Mutate', 'Pathway interactions', 'Peptides', 'Performance', 'Peripheral', 'Permeability', 'Pharmaceutical Preparations', 'Play', 'Process', 'Production', 'Property', 'Proteins', 'Renaissance', 'Research', 'Role', 'Scientist', 'Signal Pathway', 'Software Engineering', 'Software Framework', 'Software Tools', 'Specificity', 'Structure', 'Study models', 'Techniques', 'Testing', 'Therapeutic', 'Thrombosis', 'Toxic effect', 'Training', 'Update', 'Validation', 'Work', 'base', 'combinatorial', 'computerized tools', 'computing resources', 'design', 'flexibility', 'globular protein', 'graphical user interface', 'improved', 'improved functioning', 'in silico', 'insight', 'interest', 'interoperability', 'novel', 'open source', 'peptide drug', 'predictive tools', 'programs', 'protein protein interaction', 'receptor', 'screening', 'small molecule', 'success', 'symposium', 'therapeutic target', 'tool', 'translational study', 'virtual screening']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2020,399375,-0.027772358543498903
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.0036692113490967107
"Scientific Questions: A New Target for Biomedical NLP Project Summary  Natural language processing (NLP) technology is now widespread (e.g. Google Translate) and has several important applications in biomedical research. We propose a new target for NLP: extraction of scientific questions stated in publications. A system that automatically captures and organizes scientific questions from across the biomedical literature could have a wide range of significant impacts, as attested to in our diverse collection of support letters from researchers, journal editors, educators and scientific foundations. Prior work focused on making binary (or probabilistic) assessments of whether a text is hedged or uncertain, with the goal of downgrading such statements in information extraction tasks—not computationally capturing what the uncertainty is about. In contrast, we propose an ambitious plan to identify, represent, integrate and reason about the content of scientific questions, and to demonstrate how this approach can be used to address two important new use cases in biomedical research: contextualizing experimental results and enhancing literature awareness. Contextualizing results is the task of linking elements of genome-scale results to open questions across all of biomedical research. Literature awareness is the ability to understand important characteristics of large, dynamic collections of research publications as a whole. We propose to produce rich computational representations of the dynamic evolution of research questions, and to prototype textual and visual interfaces to help students and researchers explore and develop a detailed understanding of key open scientific questions in any area of biomedical research. Project Narrative The scientific literature is full of statements of important unsolved questions. By using artificial intelligence systems to identify and categorize these questions, the proposed work would help other researchers discover when their findings might address an important question in another scientific area. This work would also make it easier for students, journal editors, conference organizers and others understand where science is headed by tracking the evolution of scientific questions.",Scientific Questions: A New Target for Biomedical NLP,10069773,R01LM013400,"['Address', 'Area', 'Artificial Intelligence', 'Awareness', 'Biomedical Research', 'Characteristics', 'Collection', 'Computerized Patient Records', 'Cues', 'Data', 'Elements', 'Environment', 'Evolution', 'Expert Systems', 'Foundations', 'Genes', 'Goals', 'Gold', 'Information Retrieval', 'Journals', 'Letters', 'Link', 'Literature', 'Manuals', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Phenotype', 'Proteomics', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Signal Transduction', 'Source', 'Students', 'System', 'Taxonomy', 'Technology', 'Text', 'Time', 'Translating', 'Uncertainty', 'Update', 'Visual', 'Work', 'design', 'dynamical evolution', 'experimental study', 'genome wide association study', 'genome-wide', 'graduate student', 'high throughput screening', 'innovation', 'journal article', 'news', 'novel', 'pharmacovigilance', 'prototype', 'symposium', 'text searching', 'tool', 'transcriptome sequencing', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,462393,0.027590151207438383
"National Resource for Network Biology (NRNB) OVERALL - PROJECT SUMMARY The mission of the National Resource for Network Biology (NRNB) is to advance the science of biological networks by creating leading-edge bioinformatic methods, software tools and infrastructure, and by engaging the scientific community in a portfolio of collaboration and training opportunities. Much of biomedical research is dependent on knowledge of biological networks of multiple types and scales, including molecular interactions among genes, proteins, metabolites and drugs; cell communication systems; relationships among genotypes and biological and clinical phenotypes; and patient and social networks. NRNB-supported platforms like Cytoscape are among the most widely used software tools in biology, with tens of thousands of active users, enabling researchers to apply network concepts and data to understand biological systems and how they are reprogrammed in disease.  NRNB’s three Technology Research and Development projects introduce innovative concepts with the potential to transform network biology, transitioning it from a static to a dynamic science (TR&D 1); from flat network diagrams to multi-scale hierarchies of biological structure and function (TR&D 2); and from descriptive interaction maps to predictive and interpretable machine learning models (TR&D 3). In previous funding periods our technology projects have produced novel and highly cited approaches, including network-based biomarkers for stratification of disease, data-driven gene ontologies assembled completely from network data, and deep learning models of cell structure and function built using biological networks as a scaffold.  During the next period of support, we introduce dynamic regulatory networks formulated from single-cell transcriptomics and advanced proteomics data (TR&D 1); substantially improved methodology for the study of hierarchical structure and pleiotropy in biological networks (TR&D 2); and procedures for using networks to seed machine learning models of drug response that are both mechanistically interpretable and transferable across biomedical contexts (TR&D 3). These efforts are developed and applied in close collaboration with outside investigators from 19 Driving Biomedical Projects who specialize in experimental generation of network data, disease biology (cancer, neuropsychiatric disorders, diabetes), single-cell developmental biology, and clinical trials. TR&Ds are also bolstered by 7 Technology Partnerships in which NRNB scientists coordinate technology development with leading resource-development groups in gene function prediction, mathematics and algorithm development, and biomedical databases. Beyond these driving collaborations, we continually support a broader portfolio of transient (non-driving) research collaborations; organize and lead international meetings including the popular Network Biology track of the Intelligent Systems for Molecular Biology conference; and deliver a rich set of training opportunities and network analysis protocols. OVERALL - PROJECT NARRATIVE We are all familiar with some of the components of biological systems – DNA, proteins, cells, organs, individuals – but understanding biological systems involves more than just cataloging its component parts. It is critical to understand the many interactions of these parts within systems, and how these systems give rise to biological functions and responses and determine states of health and disease. The National Resource for Network Biology provides the scientific community with a broad platform of computational tools for the study of biological networks and for incorporating network knowledge in biomedical research.",National Resource for Network Biology (NRNB),9937486,P41GM103504,"['Area', 'Automobile Driving', 'Beds', 'Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Biomedical Technology', 'Cataloging', 'Catalogs', 'Cell Communication', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Conceptions', 'DNA', 'Data', 'Data Set', 'Databases', 'Developmental Cell Biology', 'Diabetes Mellitus', 'Disease', 'Disease stratification', 'Drug Modelings', 'Ecosystem', 'Educational workshop', 'Event', 'Expert Systems', 'Feedback', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic Risk', 'Genomics', 'Genotype', 'Goals', 'Health', 'Individual', 'Infrastructure', 'International', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mentors', 'Methodological Studies', 'Methods', 'Mission', 'Modeling', 'Molecular Biology', 'National Institute of General Medical Sciences', 'Network-based', 'Ontology', 'Organ', 'Pathway Analysis', 'Patients', 'Pharmaceutical Preparations', 'Phase Transition', 'Phenotype', 'Positioning Attribute', 'Procedures', 'Proteins', 'Proteomics', 'Protocols documentation', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Running', 'Science', 'Scientist', 'Seeds', 'Services', 'Social Network', 'Software Tools', 'Structure', 'Students', 'System', 'Technology', 'Testing', 'Tissues', 'Training', 'Visual', 'Visualization', 'Work', 'algorithm development', 'biological systems', 'clinical phenotype', 'cloud storage', 'computational platform', 'computerized tools', 'deep learning', 'gene function', 'genomics cloud', 'improved', 'innovation', 'interoperability', 'lens', 'mathematical algorithm', 'meetings', 'method development', 'multi-scale modeling', 'neuropsychiatric disorder', 'next generation', 'novel', 'pleiotropism', 'prediction algorithm', 'programs', 'protein metabolite', 'response', 'scaffold', 'single cell analysis', 'software infrastructure', 'symposium', 'technology development', 'technology research and development', 'tool', 'training opportunity', 'transcriptome', 'transcriptomics']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",P41,2020,1995376,0.008218574208985408
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,0.015642063717755488
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,0.021678784204112955
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,-0.0016022292219301819
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,0.008806206095835371
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,0.0034601161373888855
"Transforming Analytical Learning in the Era of Big Data PROJECT SUMMARY In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio)statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field of health big data. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offers and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will primarily draw from the expertise and experience of faculty from three different departments within three different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science, Medicine, Population Health, Social and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including statistical modeling, data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high- dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and medical imaging. The diseases and conditions they study include obesity, diabetes, cardiovascular disease, cancer, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and success with the current summer program on Big Data led by this team with 164 students trained in the last 4 years (2015-2018) including 90 female students and 30 students from underrepresented minority groups. Fourteen of these participants from the last three years are currently graduate students in Michigan Biostatistics. The ongoing program has gained traction in the national landscape of summer research programs with 20% rate of admission and 80% rate of acceptance among those who are offered this opportunity. The program has consistently received very strong evaluation and our past alumni have become brand ambassadors and advocates for our program. We plan to build on the success and legacy of this program in the next three year funding cycle of this grant (2019-2021). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 45 undergraduates nationally and internationally, with 20 domestic students supported by the requested SIBS funding mechanism and others supported by supplementary institutional and foundation support. We propose to expose the trainees to diverse techniques, skills and problems in the field of health Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. They will engage in mentored research projects in three primary areas of health big data: Electronic Health Records/Medical Claims, Genomics and Imaging. Some of the projects will be defined in the area of cardiovascular precision medicine, defined by a team of highly quantitative researchers engaged in cardiovascular research that uses big data. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by U-M researchers, outside guests and a professional development workshop to prepare the students for graduate school. We propose an inter-SIBS collaboration with Dordt College summer program trainees who will attend this concluding symposium. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. We will offer multiple professional development opportunities and resources for graduate school preparation to our trainees so that they can reflect and plan beyond their senior year. All of our proposed activities are reflected through our three specific aims: Teaching, Mentoring and Dissemination. PROJECT NARRATIVE We propose a six week long undergraduate summer institute: “Transforming Analytical Learning in the Era of Big Data” to be held at the Department of Biostatistics, University of Michigan (U-M), Ann Arbor, with a group of approximately 45 undergraduate students recruited nationally and internationally, from 2019-2021. Funding is requested for 20 domestic students with supplementary funding expected to be garnered through institutional resources and private foundation support. The program builds on the success of our existing Big Data Summer Institute (BDSI) supported by a NIH BD2K Courses and Skills grant award that is ending in 2018. We plan to expose program students to diverse techniques, skills and problems in the field of Big Data and Human Health. We enhance our ongoing summer program structure in the current proposal by involving a team of researchers working at the intersection of cardiovascular research and data science with a focus on cardiovascular precision medicine where some of the new mentored research projects will be defined. We primarily focus on three genres of health Big Data arising in Electronic Health Records/Medical Claims, Genomics and Imaging. The trainees will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Computational Medicine and Bioinformatics, Statistics, Computer Science and Engineering, Information Sciences, Epidemiology and Medicine, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by (U-M) researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm and engage them in influential research related to human health.",Transforming Analytical Learning in the Era of Big Data,9888408,R25HL147207,"['Admission activity', 'Adverse drug effect', 'Advocate', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Award', 'Basic Science', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Sciences', 'Code', 'Collaborations', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Epidemiology', 'Epigenetic Process', 'Evaluation', 'Exposure to', 'Faculty', 'Female', 'Foundations', 'Funding', 'Funding Mechanisms', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Human', 'Image', 'Influentials', 'Information Sciences', 'Injury', 'International', 'Kidney Diseases', 'Learning', 'Literature', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Medical Imaging', 'Medical Records', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Minority Groups', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Prevention', 'Privatization', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'STEM program', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Social Behavior', 'Social Sciences', 'Statistical Methods', 'Statistical Models', 'Structure', 'Student recruitment', 'Students', 'Talents', 'Techniques', 'Traction', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Woman', 'Work', 'base', 'big-data science', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'data visualization', 'design', 'experience', 'graduate school preparation', 'graduate student', 'heterogenous data', 'high dimensionality', 'instructor', 'interest', 'lectures', 'medical schools', 'member', 'metabolomics', 'nervous system disorder', 'network models', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'population health', 'posters', 'precision medicine', 'programs', 'recruit', 'signal processing', 'skills', 'statistical and machine learning', 'statistics', 'student training', 'success', 'summer institute', 'summer program', 'summer research', 'symposium', 'tool', 'undergraduate student', 'underrepresented minority student', 'wiki']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2020,250974,0.03692044665197893
"Training the next generation of leaders in biomedical engineering design Project Summary/Abstract The next generation of bioengineering and biomedical researchers will have unprecedented access to technologies including wireless health, big data, genetic sequencing, and machine learning approaches to enable modern diagnostic and therapeutic techniques. This presents individuals trained at the interface of technology and biomedicine with an enormous opportunity to address the world’s needs in health and medicine. In the Bioengineering Department at the University of California, Los Angeles, we aim to develop students into leaders able to seamlessly identify clinical needs that technology can address, design and validate solutions that address these needs, communicate with a variety of stakeholders to build teams invested in problem-oriented solutions, and to navigate the regulatory and commercial pathways necessary to enable their technologies to thrive. The Bioengineering Capstone Series at UCLA leverages resources available at UCLA to enable students to: 1) gain insight into clinical needs directly from clinicians and educators across the Ronald Reagan Medical Center, David Geffen School of Medicine, School of Dentistry and UCLA Health System, 2) design their solutions through mentorship from engineering professors, 3) understand the complexities of the biomedical industry with support from the UCLA Technology Development Group and members of the Department of Engineering Industry Advisory Board, 4) utilize modern technologies in wireless health and data science by collaborating with the Center of Excellence for Mobile Sensor Data- to-Knowledge (MD2K) and Mobile Health (mHealth) Institute at UCLA, and 5) work with the National Science Foundation Precise Advanced Technologies and Health Systems for Underserved Populations Engineering Research Center (NSF PATHS- UP ERC) to learn to target and communicate their technologies to maximize societal benefits. Statement of Public Health Relevance: To prepare the next generation of engineers with skills in the design of therapeutics and medical devices, this research education program will provide bioengineering students at UCLA with enhanced opportunities to engage in real world design for biomedical applications. We will guide students through the medical design process from identifying needs, creating solutions which address these needs, and communicating the significance of their contributions to the greater community, ultimately yielding a larger pool of well-trained engineers to address biomedical challenges. !",Training the next generation of leaders in biomedical engineering design,9951056,R25EB027626,"['Address', 'Applied Research', 'Area', 'Big Data', 'Biomedical Engineering', 'Biomedical Technology', 'California', 'Clinic', 'Clinical', 'Communication', 'Communities', 'Data', 'Data Science', 'Device or Instrument Development', 'Devices', 'Diagnostic', 'Educational workshop', 'Engineering', 'Event', 'Faculty', 'Fostering', 'Foundations', 'Freedom', 'Future', 'Genetic', 'Goals', 'Health', 'Health Sciences', 'Health Technology', 'Health system', 'Healthcare', 'High School Student', 'Home environment', 'Individual', 'Industry', 'Infrastructure', 'Institutes', 'Instruction', 'Intellectual Property', 'Investments', 'Laboratories', 'Learning', 'Los Angeles', 'Machine Learning', 'Medical', 'Medical Device', 'Medical center', 'Medicine', 'Mentorship', 'Modernization', 'Pathway interactions', 'Patient Monitoring', 'Physiological', 'Process', 'Recommendation', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'STEM field', 'School Dentistry', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics Education', 'Series', 'Societies', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Training', 'Translations', 'Travel', 'Underrepresented Students', 'Underserved Population', 'Universities', 'Wireless Technology', 'Work', 'base', 'career', 'clinically actionable', 'cloud based', 'commercialization', 'data to knowledge', 'deep learning', 'design', 'education research', 'engineering design', 'experience', 'insight', 'interest', 'lectures', 'mHealth', 'medical schools', 'member', 'multidisciplinary', 'next generation', 'outreach', 'product development', 'professor', 'programs', 'public health relevance', 'sensor', 'skills', 'technology development', 'undergraduate student']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R25,2020,21600,0.030198009590237176
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9900790,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'feature selection', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'machine learning method', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistical and machine learning', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2020,293003,0.009449756761488995
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.006596222339465385
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,9851602,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2020,382108,0.01639493855711967
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9888421,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'clinical trial analysis', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'lectures', 'lens', 'machine learning method', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistical and machine learning', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,249789,0.02216328129238596
"Center for Modeling Complex Interactions Biomedical problems are innately complex, and their solutions require input from many fields. Many centers focus on a single disease or organ system. By contrast, the Center for Modeling Complex Interactions focuses on an approach that can address many biomedical problems: team-based, interdisciplinary research centered around modeling. Our goal is to support and facilitate biomedical discovery by integrating modeling into interdisciplinary research. Modeling improves research at all stages—hypothesis formulation, experimental design, analysis, and interpretation. It provides a unifying language by which exchange of ideas can highlight commonalities and uncover unforeseen connections between problems. Formalization of ideas into this unifying language also improves rigor and reproducibility. We define modeling broadly to include everything from deterministic and stochastic mathematical approaches, to physical and computational models of three- dimensional objects, to agent-based and machine learning approaches where exact solutions are not possible. We seek to support modelers by increasing their numbers, and by giving them opportunities to play on interdisciplinary teams. We seek to support empiricists by giving them access to relevant modeling expertise, and by creating a community and a culture to facilitate interdisciplinary research. In Phase I, the Center for Modeling Complex Interactions created the intellectual, cultural, and physical environment to promote team- based, interdisciplinary research. In Phase II, we will build on that foundation by maintaining a strong interdisciplinary culture to foster collaboration among people who might otherwise never connect, and by adding additional faculty to expand our modeling expertise. We have four Aims: 1) Support faculty to carry out model-based, interdisciplinary biomedical research and increase their competitiveness for external funding. Research in the Center is carried out in the context of Working Groups—zero-barrier, interdisciplinary, goal- focused teams that meet regularly to get work done. Supported research includes three Research Projects, Pilot Projects, Modeling Access Grants, and ad hoc teams. Our comprehensive plan for proposal preparation improves grantsmanship, and our staff assists with submission and grant management. 2) Increase University of Idaho’s faculty participation in biomedical research. We will add six new faculty as a commitment to this Phase II COBRE and attract broader participation from across the University. 3) Extend the reach of the Modeling Core into new areas of modeling to capitalize on emerging opportunities. The Modeling Core accelerates interdisciplinary research by placing Core Fellows at the hub of the research community. We have added new Core Initiatives in machine learning and geospatial modeling to stimulate research in these areas with high potential for future growth. 4) Establish a path to long-term sustainability under the umbrella of the Institute for Modeling Collaboration & Innovation. The major hurdle for sustainability is to maintain a robust Modeling Core. We have developed a business plan that calls for us to diversify our funding sources to include institutional, state, and private support to supplement federal grants. Human health is determined by interactions of complex biological systems at multiple scales, from the ecological to the biophysical; these are layered with spatial and temporal variation. To decipher these systems requires predictive modeling, coupled with strong empirical work, to be guided by and to feed the models. The Center for Modeling Complex Interactions generates model-based biomedical research and connects people who might otherwise never interact, which enhances the strong interdisciplinary culture of the University of Idaho.",Center for Modeling Complex Interactions,10026000,P20GM104420,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Biophysics', 'Businesses', 'Centers of Research Excellence', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Models', 'Coupled', 'Data', 'Development', 'Disease', 'Ensure', 'Experimental Designs', 'Faculty', 'Feedback', 'Formulation', 'Fostering', 'Foundations', 'Funding', 'Funding Agency', 'Future', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Holly', 'Home environment', 'Human', 'Human Resources', 'Idaho', 'Incubators', 'Individual', 'Infrastructure', 'Institutes', 'Interdisciplinary Study', 'Language', 'Lead', 'Machine Learning', 'Modeling', 'Outcome', 'Phase', 'Physical environment', 'Pilot Projects', 'Play', 'Population', 'Postdoctoral Fellow', 'Preparation', 'Privatization', 'Progress Reports', 'Property', 'Reproducibility', 'Research', 'Research Institute', 'Research Project Grants', 'Research Support', 'Schedule', 'Scientist', 'Structure', 'Students', 'System', 'Testing', 'Time', 'Training', 'Uncertainty', 'Universities', 'Ursidae Family', 'Work', 'base', 'biological systems', 'body system', 'complex biological systems', 'experience', 'faculty support', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'mathematical methods', 'member', 'next generation', 'novel strategies', 'physical model', 'predictive modeling', 'spatial temporal variation', 'success', 'three-dimensional modeling', 'undergraduate student', 'working group']",NIGMS,UNIVERSITY OF IDAHO,P20,2020,2172986,0.025253180202884768
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9856493,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'data exchange', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual machine', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,339098,0.02636798195935942
"IEEE International Symposium on Biomedical Imaging (ISBI) 2020 Project Summary This R13 application will provide travel support for competitively selected U.S.-based students, postdoctoral fellows, and junior faculty to present their work at the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 to be held April 4-8, 2020 at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa. ISBI is a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biological and biomedical imaging, across all scales of observation (from microscopic to whole-body imaging) and is sponsored by both the IEEE Signal Processing Society (SPS) and the IEEE Engineering in Medicine and Biology Society (EMBS). It attracts approximately 600-700 attendees each year involved in biomedical imaging research and development from academic institutions, government laboratories, and private companies. Since its inception in 2002, ISBI has become a leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of four-page papers which are peer-reviewed much like journal articles. In addition to oral and poster presentations of peer-reviewed papers in multiple tracks during the main conference, ISBI 2020 will also include special student events, tutorials, a “clinical day” emphasizing multi-disciplinary presentations/collaborations, plenary talks, workshops, and onsite grand challenges. Recipients of the travel awards will be competitively selected based on need and scientific excellence. U.S.- based students, postdoctoral fellows, and junior faculty with accepted papers will be eligible to apply. We will be particularly supportive in providing travel awards to women and under-represented groups to help increase the diversity of the attendees. We anticipate that the travel awards will provide sufficient funding to the awardees to make the cost-benefit ratio for attendance extremely favorable. Through their attendance at ISBI, the awardees will benefit through their exposure to the simultaneous breadth and depth of topics offered at ISBI (presented by a mixture of leaders in the field as well as those early in their careers), their experience of presenting their work at an international conference, and their interactions and discussions with other attendees and leaders in the field. The conference as a whole will also benefit by not only enabling the high-quality work of the attendees to be presented but by also enabling an increased attendance of (and discussions/ideas/interactions with) U.S.-based students, postdoctoral fellows, and early career faculty with diverse backgrounds. Project Narrative This application requests funds to provide travel support for students, postdoctoral fellows, and/or early- career faculty to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 conference, to be held at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa, April 4-8, 2020. The conference covers many of the mathematical and computational aspects of biological and biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institutes of Health.",IEEE International Symposium on Biomedical Imaging (ISBI) 2020,9914410,R13EB029304,"['Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological', 'Biological Models', 'Biology', 'Biomedical Computing', 'Breeding', 'Budgets', 'Cities', 'Clinical', 'Collaborations', 'Complement', 'Computational algorithm', 'Computer Models', 'Costs and Benefits', 'Data', 'Educational workshop', 'Engineering', 'Event', 'Exposure to', 'Faculty', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Human', 'Image', 'Image Analysis', 'Imaging problem', 'Institution', 'International', 'Iowa', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Microscopic', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Recommendation', 'Request for Applications', 'Research', 'Research Personnel', 'Societies', 'Statistical Models', 'Students', 'Training', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'Visualization', 'Women&apos', 's Group', 'Work', 'authority', 'base', 'bioimaging', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'experience', 'graduate student', 'imaging modality', 'innovation', 'interest', 'journal article', 'mathematical algorithm', 'meetings', 'member', 'multidisciplinary', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2020,5000,0.02227114550584334
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,0.02959261700391659
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9979969,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Estrogen receptor positive', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'data standards', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'public repository', 'repository', 'structured data', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2020,511367,0.03658644566877163
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9894759,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Information Retrieval', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Medicine', 'Methods', 'Mining', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'machine learning method', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'structured data', 'tool', 'unstructured data']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,269500,0.013019617516742021
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,0.019204864566373114
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9848600,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data quality', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'large datasets', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2020,559088,0.06885414085168066
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9984446,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,253654,0.022667627307081488
"Knowledge Management Center for Illuminating the Druggable Genome The main goal of the Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program is to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG (“IDG List”). The long-term objective of the KMC is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins. With focus on the IDG List and human proteins, the KMC will enable support for expanded coverage for non-human proteins of therapeutic interest and other associated human health data, in order to catalyze novel biomedical discoveries. To support the overall IDG objective, and to maintain, update and improve these integrated resources, the KMC draws upon expertise from multiple knowledge domains, specifically biology, chemistry and medicine, as well as computer science, graphic design and web programming. Specifically, for the Phase 2 of the IDG KMC we propose 4 Aims:1. Create an automated workflow that captures relevant public data for the entire proteome and manual annotations for the IDG list. The KMC knowledge management system will be built around knowledge graphs, focused on five major branches of the target knowledge tree, tkt: Genotype, Phenotype, Expression, Structure & Function, and Interactions & Pathways, respectively. Aim 2: Design, develop and implement a protein knowledgebase with Data Analytics support. Our protein-centric biomedical knowledge base, TCKB (Target Central Knowledgebase) will be comprised of the data, knowledge and information container, together with its codebase and software pipelines. TCKB will be the repository for experimental, processed and computed data and reagents originating from the IDG DRGCs (Data and Resource Generation Centers). We will provide informatics and modeling support for DRGC activities. Aim 3: We will expand, improve and maintain Pharos. Particularly “knowledge packages,” support automated data summaries for Protein Dossiers, and actively seek feedback from our community. Aim 4. Outreach to scientific community. We will support a series of activities that will leverage TCKB, Pharos and other IDG resources to increase adoption of IDG work, while observing FAIR (findable, accessible, interoperable, reusable) principles for our knowledgebase, portal and pipelines. The KMC will engage in community outreach by leading tutorials and feedback sessions and dissemination of the Pharos system. To meet its goals, the KMC will coordinate all core activities in close coordination with the IDG Steering Committee and IDG Project Scientists (PS), and include members of the IDG Consortium (IDG- C), other NIH Common Fund programs, NIH Commons, as well as other initiatives. The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program plans to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG. The KMC long-term objective is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins.",Knowledge Management Center for Illuminating the Druggable Genome,9859214,U24CA224370,"['Address', 'Adoption', 'Alleles', 'Archives', 'Basic Science', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Chemistry', 'Communities', 'Community Outreach', 'Computer software', 'Coupled', 'Data', 'Data Analytics', 'Data Element', 'Data Set', 'Databases', 'Dependence', 'Disease', 'Documentation', 'FAIR principles', 'Family', 'Feedback', 'Funding', 'Future', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genome Components', 'Genotype', 'Goals', 'Grant', 'Healthcare', 'Human', 'Informatics', 'Information Resources Management', 'International', 'Internet', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Mutation', 'Non-Human Protein', 'Ontology', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Process', 'Protein Isoforms', 'Proteins', 'Proteome', 'Public Domains', 'Publications', 'Reagent', 'Resources', 'Scientist', 'Series', 'Services', 'Source', 'Structure', 'System', 'TRD@ gene cluster', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Update', 'Work', 'analytical method', 'computer science', 'data ecosystem', 'data resource', 'design', 'disease classification', 'experience', 'gender difference', 'genome resource', 'health data', 'improved', 'interest', 'knowledge base', 'knowledge graph', 'member', 'novel', 'online community', 'outreach', 'programs', 'public repository', 'repository', 'therapeutic protein', 'tool', 'web site']",NCI,UNIVERSITY OF NEW MEXICO HEALTH SCIS CTR,U24,2020,1034327,-0.020164300521501234
"Computational Methods for Enhancing Privacy in Biomedical Data Sharing Project Summary Data sharing is essential to modern biomedical data science. Access to a large amount of genomic and clinical data can help us better understand human genetics and its impact on health and disease. However, the sensitive nature of biomedical information presents a key bottleneck in data sharing and collection efforts, limiting the utility of these data for science. The goal of this project is to leverage cutting-edge advances in cryptography and information theory to develop innovative computational frameworks for privacy-preserving sharing and analysis of biomedical data. We will draw upon our recent success in developing secure pipelines for collaborative biomedical analyses to address the imminent need to share sensitive data securely and at scale.  Practical adoption of existing privacy-preserving techniques in biomedicine has thus far been largely limited due to two major pitfalls, which this project overcomes with novel technical advances. First, emerging cryptographic data sharing frameworks, which promise to enable collaborative analysis pipelines that securely combine data across multiple institutions with theoretical privacy guarantees, are too costly to support complex and large-scale computations required in biomedical analyses. In this project, we will build upon recent advances in cryptography (e.g., secure distributed computation, pseudorandom correlation, zero-knowledge proofs) to significantly enhance the scalability and security of cryptographic biomedical data sharing pipelines. Second, existing approaches that locally transform data to protect sensitive information before sharing (e.g. de-identification techniques) either offer insufficient levels of protection or require excessive perturbation in order to ensure privacy. We will draw upon recent tools from information theory to develop effective local privacy protection methods that achieve superior utility-privacy tradeoffs on a range of biomedical data including genomes, transcriptomes, and medical images by directly exploiting the latent correlation structure of the data.  To promote the use of our privacy techniques, we will create production-grade software of our tools and publicly release them. We will also actively participate in international standard-setting organizations in genomics, e.g. GA4GH and ICDA, to incorporate our insights into community guidelines for biomedical privacy. Successful completion of these aims will result in computational methods and software tools that open the door to secure sharing and analysis of massive sets of sensitive genomic and clinical data. Our long-term goal is to broadly enable data sharing and collaboration efforts in biomedicine, thus empowering researchers to better understand the molecular basis of human health and to drive translation of new biological insights to the clinic. Project Narrative Rapidly-growing volume of biomedical datasets around the world promises to enable unprecedented insights into human health and disease. However, increasing concerns for individual privacy severely limited the extent of data sharing in the field. This project draws upon cutting-edge tools from cryptography and information theory to develop effective privacy- preserving methods for collecting, sharing, and analyzing sensitive biomedical data to empower advances in genomics and medicine.",Computational Methods for Enhancing Privacy in Biomedical Data Sharing,10017554,DP5OD029574,"['Address', 'Adoption', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Clinic', 'Clinical Data', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Electronic Health Record', 'Engineering', 'Enhancement Technology', 'Ensure', 'Foundations', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Genetics', 'Image', 'Individual', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'International', 'Knowledge', 'Letters', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Mathematics', 'Medical Genetics', 'Medical Imaging', 'Mentorship', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Pattern', 'Pharmacology', 'Policies', 'Polynomial Models', 'Preservation Technique', 'Privacy', 'Privatization', 'Production', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Science', 'Secure', 'Security', 'Software Tools', 'Structure', 'Techniques', 'Technology', 'Translations', 'Vision', 'Work', 'analysis pipeline', 'base', 'biomedical data science', 'computer framework', 'computing resources', 'cost', 'cryptography', 'data sharing', 'design', 'empowered', 'experience', 'experimental study', 'genetic analysis', 'genome wide association study', 'genomic data', 'infancy', 'innovation', 'insight', 'novel', 'privacy preservation', 'privacy protection', 'software development', 'statistics', 'structured data', 'success', 'task analysis', 'tool', 'transcriptome', 'transcriptomics', 'web server']",OD,"BROAD INSTITUTE, INC.",DP5,2020,392799,0.03059672729542793
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,0.012590202734167942
"Privacy-preserving genomic medicine at scale 1 Project Summary  2  3 High-throughput sequencing, biomedical imaging, and electronic health record technologies are 4 generating health-related datasets of unprecedented scale. Integrative analysis of these  5 resources promises to reveal new biology and drive personal and precision medicine. Yet, the  6 sensitive nature of these data often requires that they be kept in isolated silos, limiting their 7 usefulness to science. The goal of this project is to develop innovative privacy-preserving  8 algorithms to enable data sharing and drive genomic medicine. Crucially, we will draw upon our  9 past success in secure genome analysis and algorithmic expertise in computational biology to 10 address the imminent need to perform complex integrative analyses securely and at scale. 11 Current privacy-preserving tools are prohibitively too costly to perform the complex 12 calculations required in genomic analysis. We previously leveraged the highly structured nature 13 of biological data and novel optimization strategies to implement efficient pipelines for secure 14 genome-wide association studies (GWAS) and drug interaction predictions which scaled to 15 millions of samples. In this project, we will further exploit the unique properties of biomedical data 16 to: (i) develop secure integrative analysis methods for genomic medicine; (ii) develop an easy-to- 17 use programming environment with advanced automated optimizations to facilitate the adoption 18 of privacy-preserving analyses; and (iii) promote the use of our privacy techniques to gain novel 19 biological insights through large-scale collaborative genetic studies of multi-ethnic cohorts. 20 With co-I’s Amarasinghe (MIT) and Cho (Broad Institute), we aim to apply these tools to 21 realize the first multi-institution, multi-national secure genetic studies with our partners at the 22 Swiss Personalized Health Network, UK Biobank, Finnish FinnGen, All of Us, NIH NCBI, Broad 23 and Barcelona Supercomputing Center (Letters of Support). We will also use our privacy- 24 preserving approaches to study genomic origins of polygenic traits for disease as well as 25 neuroimaging and other clinical phenotypes. We will continue to actively integrate our methods 26 into community standards (MPEG-G, GA4GH). 27 Successful completion of these aims will result in computational methods and open-source, 28 easy-to-use, production-grade implementations that open the door to secure integration and 29 analysis of massive sets of sensitive genomic and clinical data. With input from our collaborations, 30 we will build these tools and apply them to better understand the molecular causes of human 31 health and its translation to the clinic. Project Narrative Combining genomic and health-related data from millions of patients will empower the development of clinically relevant measures of human health and disease risks. However, this task requires securely sharing sensitive data at an immense scale beyond what existing cryptographic platforms can achieve. Here we develop novel computational methods to enable biomedical data integration, analysis, and interpretation in a privacy-preserving and highly scalable manner.",Privacy-preserving genomic medicine at scale,9998648,R01HG010959,"['Address', 'Adoption', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biological', 'Biology', 'Clinic', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Data Analyses', 'Data Pooling', 'Data Security', 'Data Set', 'Disease', 'Drug Interactions', 'Electronic Health Record', 'Engineering', 'Environment', 'Genetic', 'Genetic study', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Institutes', 'Institution', 'Knowledge', 'Letters', 'Machine Learning', 'Mainstreaming', 'Measures', 'Medical Imaging', 'Medical Records', 'Medicine', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Patients', 'Performance', 'Pharmacology', 'Polygenic Traits', 'Privacy', 'Process', 'Production', 'Property', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Science', 'Secure', 'Security', 'Software Engineering', 'Software Tools', 'Standardization', 'Stream', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Time', 'Translations', 'United States National Institutes of Health', 'Work', 'analysis pipeline', 'base', 'biobank', 'bioimaging', 'clinical development', 'clinical phenotype', 'clinically relevant', 'cohort', 'computer framework', 'cost', 'cryptography', 'data analysis pipeline', 'data integration', 'data sharing', 'data warehouse', 'disorder risk', 'epidemiology study', 'experimental study', 'genome analysis', 'genome wide association study', 'genomic data', 'health data', 'innovation', 'insight', 'monomethoxypolyethylene glycol', 'neuroimaging', 'novel', 'open source', 'polygenic risk score', 'precision medicine', 'preservation', 'privacy preservation', 'statistics', 'success', 'task analysis', 'theories', 'tool']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,636185,-0.006308062115869778
