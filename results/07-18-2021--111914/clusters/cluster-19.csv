text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,9887588,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2019,286435,-0.004165920361746578
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9753261,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Techniques', 'Time', 'Variant', 'Work', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria mosquito', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'supervised learning', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,UNIVERSITY OF OREGON,R01,2019,295000,0.04104566991861767
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ï»¿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9610628,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Infrastructure', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multi-task learning', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2019,224899,0.04324558442720681
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9783881,U01EB029373,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIBIB,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2019,611358,0.014594872812987358
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9666926,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2019,242725,0.016202346085873363
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9774249,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2019,332952,0.005603469615580228
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales â precisely the scales at which these processes occur â microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9699440,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'machine learning algorithm', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2019,186101,0.03636994205746542
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to ânavigateâ across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or âpositionâ on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9988039,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,49496,-0.008936112242926827
"Computational modeling of spatial genome organization and gene regulation PROJECT SUMMARY/ABSTRACT The three-dimensional (3D) organization of the genome plays an essential role in genome stability, gene regulation, and many diseases, including cancer. The recent development of high-throughput chromatin conformation capture (Hi-C) and its variants provide an unprecedented opportunity to investigate higher-order chromatin organization. Despite the rapidly accumulating resources for investigating 3D genome organization, our understanding of the regulatory mechanisms and functions of the genome organization remain largely incomplete. Hi-C analyses and 3D genome research are still in their early stage and face several challenges. First, high-resolution chromatin contact maps require extremely deep sequencing and hence have been achieved only for a few cell lines. Second, it is computationally challenging to complement 3D genome structure with one-dimensional (1D) genomic and epigenomic features. Third, recent studies have just begun to infer associations between chromatin interactions and genetic variants and to identify potential target genes of those variants at the genome-wide scale. Given these challenges and my unique multi-disciplinary training, my long-term research goal is to develop innovative computational and statistical methods to uncover the interplay between 3D genome structure and function. Speciï¬cally, in the next ï¬ve years, I will i) develop computational approaches to enhance the resolution of existing Hi-C data and investigate ï¬ne-scale 3D genome architecture as well as its spatiotemporal dynamics and ii) build scalable and interpretable machine learning models that leverage 1D epigenomic data to predict cell type-speciï¬c 3D chromatin interactions and gene expression and elucidate the function of 3D genome organization in gene regulation and human diseases. The completion of the proposed work will deepen our knowledge of 3D genome architecture as well as its functions in gene regulation and disease. PROJECT NARRATIVE The overarching mission of my research is to understand the interplay between genome architecture and gene regulation. Recent development of high-throughput chromatin conformation capture techniques has allowed us to look beyond the nucleotide sequence of DNA and investigate the principles of higher-order chromatin organization. I will develop innovative computational and statistical strategies to investigate 3D genome organization at an unprecedented scale, thereby elucidating the impacts of genome organization on gene regulation and disease.",Computational modeling of spatial genome organization and gene regulation,9798957,R35GM133678,"['3-Dimensional', 'Architecture', 'Base Sequence', 'Cell Line', 'Chromatin', 'Complement 3d', 'Computer Simulation', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Development', 'Dimensions', 'Disease', 'Face', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome Stability', 'Genomics', 'Goals', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mission', 'Modeling', 'Play', 'Research', 'Resolution', 'Resources', 'Role', 'Statistical Methods', 'Structure', 'Techniques', 'Training', 'Variant', 'Work', 'cell type', 'chromosome conformation capture', 'deep sequencing', 'epigenomics', 'genetic variant', 'genome-wide', 'human disease', 'innovation', 'multidisciplinary', 'spatiotemporal']",NIGMS,UNIVERSITY OF CALIFORNIA RIVERSIDE,R35,2019,369984,-0.022702312139532943
"Development of New Genome Editing Agents Using RNA Modifying Enzymes Komor â Project Summary/Abstract - âDevelopment of New Genome Editing Agents Using RNA Modifying Enzymesâ  While targeted genome editing, the introduction of a specific modification in genomic DNA, has the potential to allow researchers to study and better understand mechanisms of human genetic diseases, traditional genome editing methods (including CRISPR-Cas9) that rely on the initial introduction of double stranded DNA breaks (DSB) suffer from modest genome editing efficiencies as well as unwanted gene alterations (indels), particularly when attempting to correct point mutations. Recently, a class of genome editing agents called single base editors was developed that does not involve DSBs, but rather uses a dCas9-tethered single-stranded DNA (ssDNA) modifying enzyme to directly chemically modify target nucleobases within a ~5 nucleotide window determined by the protospacer. Two classes of editors have been developed that use cytosine and adenine deamination chemistries to catalyze the conversion of Câ¢G base pairs to Tâ¢A (CBEs), and Aâ¢T base pairs to Gâ¢C (ABEs), respectively. Here we propose the development and characterization of new base editors capable of facilitating new point mutations using methylation chemistry. We have use a bioinformatic approach to identify RNA modifying enzymes that have the potential to be repurposed into new base editors, and have rationally designed mutant libraries to use with directed evolution to convert these enzymes into base editors (Aim 1). Concurrently, we are developing a machine learning program that utilizes existing ssDNA modifying enzymes to identify putative mutations that will expand the substrate scope of the identified methyltransferases to ssDNA (Aim 2). Mutations identified from both strategies will then be tested and characterized for base editing in multiple orthogonal systems (Aim 3). The successful completion of the proposed work will represent a significant addition to existing base editing technologies, and will enable researchers to cleanly and efficiently install two additional types of point mutations into the genome of living cells, allowing researchers to quickly and effectively general model systems for the study of human genetic diseases. Komor â Project Narrative - âDevelopment of New Genome Editing Agents Using RNA Modifying Enzymesâ Base editing enables high efficiency genomic point mutation introduction in a variety of cell types and has the potential to allow researchers to better study human genetic diseases. We propose transformative improvements to current base editing technologies that will expand the types of point mutations that can be introduced by base editors. The tools developed here will enable researchers to cleanly and efficiently install additional types of point mutations into the genome of living cells for the study and potential treatment of human genetic diseases.",Development of New Genome Editing Agents Using RNA Modifying Enzymes,9876634,R21GM135736,"['Adenine', 'Adoption', 'Algorithms', 'Base Pairing', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'CRISPR/Cas technology', 'Case Study', 'Cell Line', 'Cells', 'Chemicals', 'Chemistry', 'Communities', 'Cytosine', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'Deamination', 'Development', 'Directed Molecular Evolution', 'Engineering', 'Enzymes', 'Escherichia coli', 'Evolution', 'Gene Mutation', 'Generations', 'Genetic Diseases', 'Genome', 'Genomic DNA', 'Genomics', 'Human', 'Human Genetics', 'In Vitro', 'Individual', 'Inosine', 'Lesion', 'Libraries', 'Link', 'Machine Learning', 'Mammalian Cell', 'Measures', 'Mediating', 'Methods', 'Methylation', 'Methyltransferase', 'Modification', 'Mutation', 'Nucleotides', 'Pathogenicity', 'Point Mutation', 'Program Development', 'Proteins', 'Purines', 'Pyrimidine', 'RNA', 'Research Personnel', 'Single-Stranded DNA', 'Site', 'Specificity', 'System', 'Technology', 'Testing', 'Transfer RNA', 'Uracil', 'Variant', 'Work', 'adenosine deaminase', 'base', 'cell type', 'combat', 'design', 'genome editing', 'insertion/deletion mutation', 'machine learning algorithm', 'molecular dynamics', 'mutant', 'novel', 'nucleobase', 'preference', 'programs', 'tool', 'transition mutation', 'transversion mutation']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2019,205479,-0.0044228109116183365
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9766330,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'competitive environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,468098,0.011461316423508798
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades â such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases â have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9658531,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2019,479215,0.008846057967383093
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9967363,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,102272,-0.015035370210694152
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9623355,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,PENNSYLVANIA STATE UNIV HERSHEY MED CTR,R01,2019,284020,-0.015035370210694152
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called âsmORFsâ. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human âsecretomeâ and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9751141,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2019,1186500,0.02600734396315248
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nationâs leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanfordâs long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9789914,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2019,1500000,0.0062366022730935815
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9626416,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Infrastructure', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2019,2000000,0.02618742958890524
"Computational evaluation of the causal role of somatic mutations in human aging Project Abstract Although genome instability has long been considered as one of the major causal factors of aging, little is known about the actual number of genome alterations per cell and their effects on aging organisms, most notably humans. In the research proposed here I will take a single cell approach to identify the most common types of somatic mutations, i.e., base substitutions, small INDELS, copy number variation, genome structural variation and retrotranspositions, in human B lymphocytes as a function of age. The overarching goal is then to estimate functional effects of these DNA mutations accumulated during human aging in this particular cell type, which will also serve as a model for studying somatic mutations and their consequences in other cell types. This could never be tested before, because it was never possible to analyze random somatic mutations in a tissue by sequencing bulk DNA from that tissue (mutations are low- abundant), I will achieve this goal by utilizing a new, single-cell, whole genome sequencing (SCWGS) protocol that we developed. In this project I will focus on human B lymphocytes from individuals varying in age from about 30 to over 100 years and determine the genome-wide frequency and location of the different types of mutations in multiple cells from each individual (Aim 1). Preliminary results already show a significant increase of both base substitution mutations and CNVs with age, with a substantial number of these mutations in B cell genomic regions that are potentially functional. Hence, in Aim 2 I will predict the actual functional effects of these potentially functional, age-related mutations using machine learning approaches and integrative network analysis. Finally, in Aim 3 I will empirically test these predictions as to whether the mutation loads observed affect B cell's ability of response to stimulus. Hence, to test the long-standing hypothesis of genome instability as a causal factor in aging ,I will determine age-related mutations in single cells at four levels: (1) number of mutations, mutation spectra and genome distribution in individual cells; (2) potential functional effects of individual mutations, i.e., non-synonymous mutations in exons and mutations in gene regulatory regions; (3) mutations collectively affecting the gene regulatory network; and (4) relationship between mutation load and B cell activation status. In summary, the results of the proposed project will, for the first time uncover possible direct functional effects of somatic mutations on cellular function. Project Narrative Genome instability is considered as one of the major factors of aging and age-related diseases. This research aims to study somatic DNA mutations in normal blood cells (B lymphocytes) of humans of different ages and evaluate the functional effect of these mutations. It will dramatically improve the knowledge of DNA mutations in aging and deepen the understanding of genome instability as a basic aging mechanism in human.",Computational evaluation of the causal role of somatic mutations in human aging,9785353,K99AG056656,"['3&apos', ' Untranslated Regions', '5&apos', ' Untranslated Regions', 'Affect', 'Age', 'Aging', 'B-Cell Activation', 'B-Lymphocytes', 'Binding Sites', 'Blood Cells', 'CRISPR/Cas technology', 'Cancer Etiology', 'Cell physiology', 'Cells', 'Centenarian', 'Code', 'Collecting Cell', 'Copy Number Polymorphism', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Replication Damage', 'DNA Sequence Alteration', 'DNA Transposable Elements', 'DNA amplification', 'Data', 'Defect', 'Deoxyribonuclease I', 'Disease', 'Elderly', 'Enhancers', 'Evaluation', 'Exons', 'Frequencies', 'Functional disorder', 'Genes', 'Genome', 'Genomic Instability', 'Genomic Segment', 'Goals', 'Human', 'Hypersensitivity', 'Immunization', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Location', 'Locus Control Region', 'Machine Learning', 'Mentors', 'Methods', 'Mutation', 'Mutation Analysis', 'Mutation Spectra', 'Nucleic Acid Regulatory Sequences', 'Nucleotides', 'Organism', 'Pathway Analysis', 'Process', 'Proteins', 'Protocols documentation', 'RNA', 'RNA amplification', 'Regulator Genes', 'Research', 'Retrotransposition', 'Role', 'Site', 'Software Tools', 'Somatic Cell', 'Somatic Mutation', 'Source', 'Stimulus', 'Structure', 'Study models', 'Testing', 'Time', 'Tissues', 'Variant', 'age related', 'base', 'cell type', 'crosslink', 'dietary restriction', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'nonsynonymous mutation', 'promoter', 'repair enzyme', 'repaired', 'response', 'single cell sequencing', 'single cell technology', 'theories', 'transcription factor', 'whole genome']",NIA,ALBERT EINSTEIN COLLEGE OF MEDICINE,K99,2019,135945,-0.011726858059804356
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled âComputational and Comparative Genomicsâ, to be held in the Fall of 2017 â 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9724498,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2019,67704,0.02760946245233096
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9693291,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2019,397125,0.023354274815228435
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIHâs mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9765970,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Simulation', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Imagery', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2019,433604,0.017055000467057574
"A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions PROJECT SUMMARY Cancer genomes typically harbor a substantial number of somatic mutations. Relatively few driver mutations actually alter the function of proteins in tumor cells, whereas most mutations are considered to be functionally neutral passenger mutations. Over the past decade, the search for cancer driver mutations has focused on coding regions and several mutational significance algorithms have been developed for coding mutations. The contribution of mutations in noncoding regulatory regions to tumor formation largely remains unknown and current mutational significance algorithms are not designed to detect driver mutations in noncoding regions, due to biological differences between coding and noncoding mutations. The emerging availability of large whole- genome sequencing datasets (e.g. PCAWG and HMF datasets) creates an ample opportunity to develop new mutational significance algorithms that are particularly designed for the interpretation of noncoding regions. Recently, we have developed a new statistical approach that identifies driver mutations in coding regions based on the nucleotide context. Critically, consideration of the nucleotide context around mutations does not require prior knowledge for functional consequences associated with these mutations. Hence, we hypothesize that generalizing our nucleotide context model to noncoding regions will uncover novel noncoding driver mutations that cannot be detected using the mutational significance approaches currently available. For this purpose, we will develop a statistical framework that incorporates the biological differences between coding and noncoding mutations and that is specifically designed to detect driver mutations in noncoding regions. Specifically, we will consider the context-dependent distribution of passenger mutations, modeling of the background mutation rate, accurately partition the background mutation rate, model the sequence composition of the reference genome, and account for coverage fluctuation. We will then combine these statistical components by computing an independent product of their underlying probabilities. We will derive a significance p-value using a Monte-Carlo simulation approach, and use FDR for multiple hypothesis test correction. This strategy will allow us to accurately estimate the significance of somatic mutations in noncoding genomic regions. We will next apply this statistical framework to whole-genome sequencing data of 5,523 tumor patients, thereby deriving a comprehensive list of candidate driver mutations in noncoding regions. Finally, we will investigate whether noncoding mutations are overrepresented in transcription factor binding sites, regulate gene expression levels, induce alternative splicing, or affect epigenomic states. Upon the completion of this project, we will have developed and applied a statistical framework for discovery of significant somatic mutations in noncoding regions, and defined the mutational landscape of the non-coding cancer genome. All aspects of the methods developed and applied in this project will be made open source and developed in an online platform. PROJECT NARRATIVE While coding cancer driver mutations have been characterized in detail over the past decade, the contribution of noncoding mutations to tumor formation remains - apart from few examples (e.g. mutations in TERT promoters) - largely unknown. Recently, large-scale whole-genome sequencing datasets have been made available, but a major bottleneck for the biological and clinical interpretation of these cancer whole-genome cohorts is the lack of statistical models that identify driver mutations in noncoding regions. We developed a new statistical approach that characterizes driver mutations based on their surrounding nucleotide context in coding regions, and herein we propose a concrete plan to generalize our computational model to noncoding regions, apply our model to aggregated whole-genome sequencing data of 5,523 tumor patients (PCAWG, HMF datasets), and define the noncoding driver and passenger mutational landscape for biological discovery and focused clinical application.",A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions,9825986,R21CA242861,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Attention', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Clinical', 'Code', 'Communities', 'Computational Biology', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Gene Expression', 'Gene Expression Regulation', 'Genomic Segment', 'Immunotherapy', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Methods', 'Microsatellite Instability', 'Modeling', 'Monte Carlo Method', 'Mutation', 'Nucleic Acid Regulatory Sequences', 'Nucleotides', 'Outcome', 'Patients', 'Pattern', 'Play', 'Positioning Attribute', 'Probability', 'Process', 'Role', 'Somatic Mutation', 'Statistical Models', 'Stratification', 'Testing', 'The Cancer Genome Atlas', 'Untranslated RNA', 'actionable mutation', 'base', 'cancer genome', 'cancer immunotherapy', 'checkpoint therapy', 'clinical application', 'clinical effect', 'cohort', 'design', 'epigenomics', 'exome sequencing', 'genome sequencing', 'genome-wide', 'immune checkpoint blockade', 'immunogenicity', 'malignant breast neoplasm', 'melanoma', 'mutant', 'neoantigens', 'neoplastic cell', 'novel', 'open source', 'predicting response', 'promoter', 'protein function', 'reference genome', 'response', 'targeted treatment', 'transcription factor', 'tumor', 'whole genome']",NCI,DANA-FARBER CANCER INST,R21,2019,232291,0.0034265393561965004
"PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A) PROJECT SUMMARY  Since its inception 40 years ago, in vitro fertilization (IVF) has resulted in the birth of more than 1 million babies in the United States, and has revolutionized the field of reproductive medicine. Unfortunately, the success rate of IVF is still exceedingly low, especially for women >40 years old, with only 15.5% of implanted embryos resulting in pregnancy. This is partly due to the cytological method used for pre-implantation screening, which cannot detect the most common genetic defect during IVF, aneuploidy (i.e. chromosomal copy-number variation). Aneuploidy is linked to higher rates of miscarriage, and occurs more often in women >40 years of age; thus, aneuploidy has been a frequent target for genetic screening to improve IVF outcomes.  Pre-implantation genetic testing for aneuploidy (PGT-A) refers to a variety of techniques aimed at detecting changes in chromosomal copy number, with the goal of identifying high-quality euploid embryos for implantation. Recent advances in next-generation sequencing (NGS) technologies have made it possible to screen embryos at higher levels of precision, and across a wider range of genetic defects, including mosaicism, triploidy and single nucleotide polymorphisms (SNPs). Despite these remarkable advances, there are still significant challenges with PGT-A sequencing. Indeed, the most commonly implemented software for PGT-A (i.e. BlueFuseÂ® ) are bundled with specific sequencing platforms (i.e. VeriSeqÂ®), and are only designed to test for aneuploidy. Furthermore, existing pipelines are not user-friendly or customizable, which is a serious obstacle prohibiting the use of NGS by clinicians / embryologists. A more accessible bioinformatics platform is desperately needed that will bridge the gap between PGT-A sequencing and IVF outcomes.  Basepairâ¢ is an innovator in efficient, user-friendly, web-based NGS analysis systems, with fully automated ChIP-, RNA-, ATAC-, and DNA-Seq bioinformatics pipelines available online. Here, Basepair will deliver PiNDAâ¢, the first fully integrated software solution for comprehensive PGT-A analysis. In Aim 1, we will develop modules to test for specific chromosomal abnormalities, including mosaicism and triploidy, and validate each model with training data derived from somatic cell lines with known chromosomal aberrations. In Aim 2, we will integrate our modules into the PiNDA software system, creating a user-friendly, web-based interface that will perform full data analysis (raw data to full summary report) in <15 minutes, with no manual input required. Final data will be accessible via Basepairâs online portal, facilitating rapid data transfer from embryologists to physicians, and supporting the integration of NGS tests in IVF. Our innovative bioinformatics platform will accelerate NGS analysis for IVF, improving rates of pregnancy and advancing research in the success of IVF procedures. PROJECT NARRATIVE  In vitro fertilization (IVF) methods have begun to leverage next-generation sequencing technologies for pre-implantation genetic testing of aneuploidy (PGT-A), expanding the array of chromosomal abnormalities that can be accurately detected. However, the vast majority of software can only distinguish one type of genetic defect (i.e. aneuploidy), are difficult to use, and are tied to distinct sequencing platforms, limiting the clinical utility of resulting analyses. Basepairâ¢ Inc. is a pioneer in user-friendly, web-based bioinformatics pipelines, providing comprehensive services for a wide range of sequencing projects. Here, Basepair will develop an inclusive suite of software for PGT-A, compatible with sequencing data from multiple platforms. This product will be of high value to the field and will help bridge the gap between advances in DNA sequencing and IVF technology.",PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A),9846492,R43HD100280,"['ATAC-seq', 'Age-Years', 'Algorithms', 'Aneuploid Cells', 'Aneuploidy', 'Bioinformatics', 'Biopsy', 'Birth', 'Cell Line', 'Cell division', 'Centers for Disease Control and Prevention (U.S.)', 'ChIP-seq', 'Chromosome abnormality', 'Clinical', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'Culture Media', 'Cytology', 'DNA sequencing', 'Data', 'Data Analyses', 'Embryo', 'Feedback', 'Fertility Agents', 'Fertilization in Vitro', 'Genetic Screening', 'Goals', 'Harvest', 'Implant', 'Letters', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Morphology', 'Mosaicism', 'Mutation', 'Online Systems', 'Outcome', 'Phase', 'Physicians', 'Polymorphism Analysis', 'Pregnancy', 'Pregnancy Rate', 'Preimplantation Diagnosis', 'Procedures', 'Reporting', 'Reproductive Medicine', 'Research', 'Role', 'Sampling', 'Services', 'Single Nucleotide Polymorphism', 'Somatic Cell', 'Specificity', 'Spontaneous abortion', 'Summary Reports', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Training', 'Triploidy', 'United States', 'Uterus', 'Woman', 'analysis pipeline', 'aneuploidy analysis', 'cell free DNA', 'design', 'early embryonic stage', 'egg', 'implantation', 'improved', 'innovation', 'natural Blastocyst Implantation', 'next generation sequencing', 'phase 1 study', 'preimplantation', 'screening', 'sequencing platform', 'software development', 'software systems', 'sperm cell', 'success', 'transcriptome sequencing', 'user-friendly', 'web based interface']",NICHD,"BASEPAIR, INC.",R43,2019,298717,-0.01280372171444731
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,0.003695010997727641
"A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions Project summary There is a fundamental gap in our understanding of how mutations are preferentially targeted to the variable (V) regions of the Immunoglobulin (Ig) loci during somatic hypermutation (SHM). The persistence of this gap has limited our understanding of the mutagenic mechanisms involving activation-induced deaminase (AID) in the immune response and in the role of AID in mis-targeting mutations leading to B-cell lymphomas and other cancers. The long-term goal of the proposed research is to understand the global targeting of mutations in immunity that are required to protect us from infections. As high-throughput data from human antibody immune responses became available, it provided us with new opportunities to generate hypotheses to explain the underlying mechanisms of SHM. We now propose to generate further hypotheses using computational models applied to additional databases and to validate these hypotheses using cellular and animal experiments. Our objective is to understand what directs SHM across the many human Ig heavy chain V-regions. Our central hypothesis is that the V-region SHM process is highly dependent on a DNA sequence signature(s) that drives mutations in a largely deterministic fashion. This hypothesis is supported by our preliminary results using human in vivo data from a few human V region genes and has begun to be validated using independent databases and experiments in human B cell lines. The rationale is that evaluations of computational data based upon biological mechanisms, together with appropriate biological experiments, will reveal the key differences between IGHV regions (IGHV 3-23, 4-34, 1-18, 1-02, etc.) that lead to the dominance of each of those V regions in the responses to medically important antigens. Our hypothesis will be tested by pursuing two specific aims: 1) identify the extent to which a DNA signature determines the mutation process in four individual human IGHV genes that are important in disease responses; 2) examine the relationship between AID hotspots and PolÎ· hotspots across all the other human V region genes, thus rigorously defining a mutation targeting signature. Both aims will also entail studying human V region genes and modifications of them in human cell lines and in mice expressing a human V region to further confirm the signature and identify molecular mechanisms in vivo. Our approach is innovative because the computational models we are proposing will be mechanistically motivated focusing on the interaction between AID and PolÎ· hotspots, thus testing molecular mechanisms as opposed to classic statistical models using whole V region sequences that ignore the underlying biology. In addition, to focus on mechanisms we will leverage new high-throughput data from human V regions that have not undergone antigen selection. Our results will be highly relevant to human IgV repertoire analyses from immune responses that are currently hard to interpret and will help future vaccine and therapeutic antibody development, as well as help to understand mutations in human malignancies where AID plays a key role. Project narrative The proposed research is relevant to public health because understanding the targeting of AID-mediated mutations across many human heavy chain V-regions will make it possible to develop vaccines that will lead more rapidly to better and more broadly protective antibodies to infectious agents and reveal the risk factors in the development of B-cell malignancies and gastric and other solid tumors where AID is implicated.",A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions,9653955,R01AI132507,"['Affect', 'Alleles', 'Animal Experiments', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Autoimmunity', 'B lymphoid malignancy', 'B-Cell Development', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Biological', 'Biology', 'Cell Line', 'Chromatin', 'Complementarity Determining Regions', 'Computer Analysis', 'Computer Simulation', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Enzyme Activation', 'Evaluation', 'Event', 'Evolution', 'Family', 'Feedback', 'Frequencies', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Gene-Modified', 'Genes', 'Goals', 'HIV', 'Heavy-Chain Immunoglobulins', 'Human', 'Human Cell Line', 'Immune response', 'Immunity', 'Immunoglobulin Somatic Hypermutation', 'Immunoglobulins', 'Individual', 'Infection', 'Infectious Agent', 'Influenza', 'Influenza Hemagglutinin', 'Knock-in', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medical', 'Mismatch Repair', 'Molecular', 'Mus', 'Mutate', 'Mutation', 'Outcome', 'Pattern', 'Play', 'Polymerase', 'Process', 'Public Health', 'Research', 'Risk Factors', 'Role', 'Site', 'Solid Neoplasm', 'Statistical Models', 'Stomach', 'Structure of germinal center of lymph node', 'Techniques', 'Testing', 'Therapeutic antibodies', 'Time', 'Transgenic Mice', 'Vaccines', 'Validation', 'Variant', 'activation-induced cytidine deaminase', 'base', 'chromatin modification', 'density', 'experimental study', 'genetic variant', 'human data', 'in vivo', 'in vivo Model', 'innovation', 'neutralizing antibody', 'recruit', 'repair enzyme', 'response', 'spatial relationship', 'vaccine response']",NIAID,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2019,587448,0.009218605178767611
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to ânavigateâ across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",9919259,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Work', 'base', 'cell type', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,600000,-0.008895884696722192
"Genome Based Influenza Vaccine Strain Selection using Machine Learning No abstract available PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection using Machine Learning,10044945,R01AI116744,[' '],NIAID,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2019,147704,0.028007086277540387
"Computational approaches for identifying epigenomic contexts of somatic mutations ABSTRACT During normal development, aging, and diseases such as cancer, DNA damage due to endogenous and external factors, and repair defects result in accumulation of different types of somatic mutations including single nucleotide substitutions, small InDels, copy number alterations, translocations, and ploidy changes. While a vast majority of somatic mutations in the genome are not disease drivers, their patterns of genetic changes and associated context can provide insights into past exposure to mutagens, mechanisms of DNA damage and repair defects, and extent of genomic instability, which are important for understanding disease etiology, minimizing hazardous environmental exposure, and also predicting efficacy of emerging treatment strategies such as immunotherapy. A number of mutation signatures have been identified based on local sequence contexts to address this need. But, mechanisms of DNA damage and repair preferences depend on both local sequence and epigenomic contexts, and it remains to be understood whether epigenomic contexts of emerging mutation signatures can provide critical, complementary etiological insights at a genome-wide scale, which are not apparent from sequence contexts alone. This is of fundamental importance, because (i) etiology of many of the emerging mutation signatures is currently unknown, (ii) DNA damage response and repair depends on tissue contexts, and defects in core DNA repair genes often result in cancer development in tissue-specific manner, and (iii) differences in the extent of DNA damage and repair between stem and differentiated cells within the same tissues have consequences for aging and disease incidence rates. Built logically on our previous works, we propose to develop computational approaches to determine the impact of epigenomic contexts on the patterns of somatic mutations within and across tissue types, and validate computational predictions using targeted experiments. In Aim-1, we will develop an epigenomic context preference map for emerging mutation signatures. In Aim-2, we will determine the basis of tissue-dependent differences in mutation profiles attributed to DNA repair defects. In Aim-3, we will predict the extent of cell lineage-dependent patterns of mutation accumulation from the mutational landscape of terminal cells. I am currently an early stage investigator, and the proposal is aligned with my long-term goal to identify fundamental principles of mutability and evolvability of somatic genomes. Our project will deliver novel resources and knowledge for addressing questions regarding genomic integrity during development and aging, and diseases such as cancer. ! PUBLIC HEALTH RELEVANCE: The proposed project will use computational biology approaches to determine epigenomic context preference for somatic mutations, and use that to infer tissue-dependent changes in mutation patterns. Our results will provide fundamental insights into aspects of genome maintenance, which is important for advancing our understanding of cancer etiology, reducing exposure to mutagenic factors, and also predicting efficacy of emerging treatment strategies. !",Computational approaches for identifying epigenomic contexts of somatic mutations,9737246,R01GM129066,"['Address', 'Affect', 'Aging', 'Biometry', 'Blood', 'Cancer Etiology', 'Cancer Relapse', 'Cell Differentiation process', 'Cell Line', 'Cell Lineage', 'Cells', 'Chromatin', 'Clinical', 'Computational Biology', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'Data', 'Defect', 'Development', 'Disease', 'Doctor of Philosophy', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Evolution', 'Exposure to', 'Genome', 'Genomic DNA', 'Genomic Instability', 'Genomics', 'Goals', 'Immunotherapy', 'Incidence', 'Knowledge', 'Least-Squares Analysis', 'Location', 'Maintenance', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutagenesis', 'Mutagens', 'Mutation', 'Nuclear', 'Nucleotides', 'Pathway interactions', 'Pattern', 'Ploidies', 'Point Mutation', 'Process', 'Publishing', 'Radiation Tolerance', 'Research Personnel', 'Resources', 'Role', 'Somatic Mutation', 'Source', 'Stem cells', 'Tissues', 'Work', 'base', 'cancer genomics', 'computer framework', 'epigenomics', 'experimental study', 'genome integrity', 'genome-wide', 'human tissue', 'improved', 'insertion/deletion mutation', 'insight', 'markov model', 'medical schools', 'novel', 'preference', 'public health relevance', 'random forest', 'repaired', 'response', 'stem', 'transcriptomics', 'treatment strategy']",NIGMS,RBHS -CANCER INSTITUTE OF NEW JERSEY,R01,2019,336177,-0.0038148125842501542
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9729028,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2019,1002519,0.0224594939587468
"The Enzymatic Reader Project Summary At this point in time, it is generally understood and agreed upon that single-molecule sequencing (SMS) is the future of genomics, transcriptomics, epigenomics, and epitranscriptomics due to its significant advantages over other technologies and methods. However, in order for these advantages to be fully realized, and for SMS to become the âgold standardâ sequencing approach, significant issues and hurdles must be solved and overcome. During this program, Electronic BioSciences, Inc. (EBS) aims to demonstrate a completely new and enabling SMS method that will possess the ability to directly and correctly identify individual nucleotides, including chemically modified nucleotides. During this project, we will both demonstrate the ability of this entirely new sequencing approach to sequence DNA with high accuracy (directly comparing the obtained accuracy, throughput, error mechanisms and associated rates to other SMS approaches) and correctly identify (and sequence) 5-methylcytosine (5mC) and its derivatives, at the single molecule level. At the conclusion of this Phase I project, we will have successfully demonstrated an entirely new and dramatically improved SMS approach, and reduced the associated risks involved with its full future commercial developments. There is a current need within the field of next generation sequencing (NGS) or so called third generation sequencing (TGS) for new, enabling instrumentation that is capable of high-accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats. The entirely new SMS methodology that will be developed during this project will overcome known hurdles and limitations of currently available NGS, TGS, and SMS technologies, resulting in technology that is cost-efficient, highly accurate, easy to setup and utilize, capable of de novo sequencing and modified base calling, and yields highly simplistic data for easy analysis and post possessing. Through significant advancements made during this program, this resulting technology will revolutionize the use of the genome and epigenome, radically change standard R&D and clinical practices, and greatly advance clinical diagnostics, prognostics, and therapeutic decision making. Project Narrative The novel single-molecule sequencing (SMS) technology developed during this project will enable high- accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats via a cost-efficient and easy-to-use methodology. The impact of these advances in SMS will eventually enable wide-scale, routine clinical care and diagnostics toward advanced precision medicine, not just R&D. The performance and accessibility of such technology will transform the understanding and application of genomics and epigenomics, the associated clinical practices, that ability to provide precision clinical diagnostics, prognostics, and therapeutic decision making for improved public healthcare and wellbeing.",The Enzymatic Reader,9677956,R43HG010427,"['Biological', 'Biological Sciences', 'Caliber', 'Chemicals', 'Chemistry', 'Church', 'Complex', 'DNA Primers', 'DNA Sequence', 'DNA polymerase A', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disadvantaged', 'Drops', 'Electrodes', 'Enzymes', 'Evaluation', 'Future', 'Genome', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Ions', 'Label', 'Length', 'Lipid Bilayers', 'Logistics', 'Methodology', 'Methods', 'Motor', 'Movement', 'Noise', 'Nucleotides', 'Performance', 'Personal Satisfaction', 'Phase', 'Polymerase', 'Polymers', 'Preparation', 'Process', 'Proteins', 'RNA', 'Reader', 'Reading Frames', 'Reproducibility', 'Risk', 'Sampling', 'Side', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Software Tools', 'Speed', 'Stretching', 'System', 'Technology', 'Therapeutic', 'Third Generation Sequencing', 'Time', 'base', 'clinical care', 'clinical diagnostics', 'clinical practice', 'cost', 'cost efficient', 'electric field', 'epigenome', 'epigenomics', 'epitranscriptomics', 'improved', 'instrumentation', 'machine learning algorithm', 'nanopore', 'next generation sequencing', 'novel', 'precision medicine', 'prevent', 'prognostic', 'programs', 'research and development', 'single molecule', 'solid state', 'transcriptomics']",NHGRI,"ELECTRONIC BIOSCIENCES, INC.",R43,2019,247611,0.024978925292570772
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",9649188,U24DK112331,"['ATAC-seq', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Cost efficiency', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Infrastructure', 'Institutes', 'Knowledge', 'Lead', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'analysis pipeline', 'base', 'bisulfite sequencing', 'data resource', 'epigenomics', 'exercise intervention', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'individual response', 'insight', 'machine learning algorithm', 'medical schools', 'methylome', 'mortality risk', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2019,2593647,0.016358811889451846
"Novel Statistical methods for DNA Sequencing Data, and applications to Autism. Summary One of the major problems in human genetics is understanding the genetic causes underlying complex phenotypes, including neuropsychiatric traits such as autism spectrum disorders and schizophrenia. Despite tremendous work over the past few decades, the underlying biological mechanisms are poorly understood in most cases. Recent advances in high-throughput, massively parallel genomic technologies have revolutionized the field of human genetics and promise to lead to important scientific advances. Despite this progress in data generation, it remains very challenging to analyze and interpret these data. The main focus of this proposal is the development of powerful statistical methods for the integration of whole-genome sequencing data with rich functional genomics data with the goal to improve the discovery of genes involved in autism spectrum disorders. We propose to integrate data from many different sources, including epigenetic data from projects such as ENCODE, Roadmap, and PsychENCODE, eQTL data from the GTEx, PsychENCODE and CommonMind consortia, data from large scale databases of genetic variation such as ExAC and gnomAD, in order to predict functional effects of genetic variants in non-coding genetic regions in a tissue and cell type specific manner, and generate functional maps across large number of tissues and cell types in the human body that we can then use to identify novel associations with autism in whole-genome sequencing studies. The proposed functional predictions and functional maps will be broadly available in the popular ANNOVAR database. We further propose to use these functional predictions in the analysis of almost 20,000 whole genomes from three large whole genome sequencing studies for autism. We believe that the proposed research is very timely and has the potential to substantially improve the analysis of non-coding genetic variation, and hence provide new insights into the biological mechanisms underlying risk to autism, and more broadly to other neuropsychiatric diseases. Narrative Autism Spectrum Disorders are common diseases with major impact on public health. Although coding variation has been extensively studied for its role in affecting risk to autism, the analysis of non-coding variation poses tremendous challenges. The proposed statistical methods and their applications to nearly 20,000 whole genomes from three large autism whole genome sequencing studies will improve our understanding of the biological mechanisms involved in autism with important implications for disease treatment strategies.","Novel Statistical methods for DNA Sequencing Data, and applications to Autism.",9735436,R01MH095797,"['Affect', 'Anterior', 'Biochemical', 'Biological', 'Biological Assay', 'Brain region', 'Chromatin', 'Code', 'Collection', 'Complex', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Encyclopedia of DNA Elements', 'Epigenetic Process', 'Generations', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human body', 'Individual', 'International', 'Lead', 'Maps', 'Measures', 'Methods', 'Molecular', 'Phenotype', 'Prefrontal Cortex', 'Public Health', 'Research', 'Risk', 'Role', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Source', 'Statistical Methods', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Variant', 'Work', 'autism spectrum disorder', 'cell type', 'cingulate cortex', 'data integration', 'design', 'epigenomics', 'exome', 'frontal lobe', 'functional genomics', 'gene discovery', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic data', 'histone modification', 'improved', 'insight', 'large-scale database', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'software development', 'supervised learning', 'tool', 'trait', 'treatment strategy', 'whole genome']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,454366,-0.01270589159881854
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9637410,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Consumption', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computational suite', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2019,387500,-0.04309703353558224
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ï»¿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9552183,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'deep learning', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2018,297725,0.005072572080522888
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ï»¿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9406205,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2018,372603,0.04324558442720681
"Population genomics of adaptation Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",Population genomics of adaptation,9554999,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Supervision', 'Techniques', 'Time', 'Variant', 'Work', 'deep learning', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",R01,2018,18944,0.04104566991861767
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell- based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off- target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9678132,U01HL145793,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety testing', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NHLBI,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2018,651251,0.014594872812987358
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9617314,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2018,249000,0.016202346085873363
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9547454,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2018,332952,0.007139689570725162
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to ânavigateâ across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or âpositionâ on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9687220,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Research Infrastructure', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2018,330000,-0.008936112242926827
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales â precisely the scales at which these processes occur â microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9507167,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2018,242837,0.03636994205746542
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades â such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases â have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9486266,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2018,479215,0.008846057967383093
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nationâs leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanfordâs long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9593406,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2018,1420000,0.0062366022730935815
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9531422,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,468098,0.011461316423508798
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called âsmORFsâ. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human âsecretomeâ and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9562959,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2018,1186500,0.02600734396315248
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9420662,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2018,2000000,0.02618742958890524
"Computational evaluation of the causal role of somatic mutations in human aging Project Abstract Although genome instability has long been considered as one of the major causal factors of aging, little is known about the actual number of genome alterations per cell and their effects on aging organisms, most notably humans. In the research proposed here I will take a single cell approach to identify the most common types of somatic mutations, i.e., base substitutions, small INDELS, copy number variation, genome structural variation and retrotranspositions, in human B lymphocytes as a function of age. The overarching goal is then to estimate functional effects of these DNA mutations accumulated during human aging in this particular cell type, which will also serve as a model for studying somatic mutations and their consequences in other cell types. This could never be tested before, because it was never possible to analyze random somatic mutations in a tissue by sequencing bulk DNA from that tissue (mutations are low- abundant), I will achieve this goal by utilizing a new, single-cell, whole genome sequencing (SCWGS) protocol that we developed. In this project I will focus on human B lymphocytes from individuals varying in age from about 30 to over 100 years and determine the genome-wide frequency and location of the different types of mutations in multiple cells from each individual (Aim 1). Preliminary results already show a significant increase of both base substitution mutations and CNVs with age, with a substantial number of these mutations in B cell genomic regions that are potentially functional. Hence, in Aim 2 I will predict the actual functional effects of these potentially functional, age-related mutations using machine learning approaches and integrative network analysis. Finally, in Aim 3 I will empirically test these predictions as to whether the mutation loads observed affect B cell's ability of response to stimulus. Hence, to test the long-standing hypothesis of genome instability as a causal factor in aging ,I will determine age-related mutations in single cells at four levels: (1) number of mutations, mutation spectra and genome distribution in individual cells; (2) potential functional effects of individual mutations, i.e., non-synonymous mutations in exons and mutations in gene regulatory regions; (3) mutations collectively affecting the gene regulatory network; and (4) relationship between mutation load and B cell activation status. In summary, the results of the proposed project will, for the first time uncover possible direct functional effects of somatic mutations on cellular function. Project Narrative Genome instability is considered as one of the major factors of aging and age-related diseases. This research aims to study somatic DNA mutations in normal blood cells (B lymphocytes) of humans of different ages and evaluate the functional effect of these mutations. It will dramatically improve the knowledge of DNA mutations in aging and deepen the understanding of genome instability as a basic aging mechanism in human.",Computational evaluation of the causal role of somatic mutations in human aging,9527264,K99AG056656,"['3&apos', ' Untranslated Regions', '5&apos', ' Untranslated Regions', 'Affect', 'Age', 'Aging', 'B-Cell Activation', 'B-Lymphocytes', 'Binding Sites', 'Blood Cells', 'CRISPR/Cas technology', 'Cancer Etiology', 'Cell Count', 'Cell physiology', 'Cells', 'Centenarian', 'Code', 'Collecting Cell', 'Copy Number Polymorphism', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Replication Damage', 'DNA Sequence Alteration', 'DNA Transposable Elements', 'DNA amplification', 'Data', 'Defect', 'Deoxyribonuclease I', 'Disease', 'Elderly', 'Enhancers', 'Evaluation', 'Exons', 'Frequencies', 'Functional disorder', 'Genes', 'Genome', 'Genomic Instability', 'Genomic Segment', 'Goals', 'Human', 'Hypersensitivity', 'Immunization', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Location', 'Locus Control Region', 'Machine Learning', 'Mentors', 'Methods', 'Mutation', 'Mutation Analysis', 'Mutation Spectra', 'Nucleic Acid Regulatory Sequences', 'Nucleotides', 'Organism', 'Pathway Analysis', 'Process', 'Proteins', 'Protocols documentation', 'RNA', 'RNA amplification', 'Regulator Genes', 'Research', 'Retrotransposition', 'Role', 'Site', 'Software Tools', 'Somatic Cell', 'Somatic Mutation', 'Source', 'Stimulus', 'Study models', 'Testing', 'Time', 'Tissues', 'Variant', 'age related', 'base', 'cell type', 'crosslink', 'dietary restriction', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'promoter', 'repair enzyme', 'repaired', 'response', 'single cell sequencing', 'single cell technology', 'theories', 'transcription factor', 'whole genome']",NIA,"ALBERT EINSTEIN COLLEGE OF MEDICINE, INC",K99,2018,40760,-0.011726858059804356
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled âComputational and Comparative Genomicsâ, to be held in the Fall of 2017 â 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9545035,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2018,67704,0.02760946245233096
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",9394010,U24DK112331,"['ATAC-seq', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Cost efficiency', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Institutes', 'Intervention', 'Knowledge', 'Lead', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Research Infrastructure', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'base', 'bisulfite sequencing', 'data resource', 'epigenomics', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'individual response', 'insight', 'medical schools', 'methylome', 'mortality', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2018,1639153,0.016358811889451846
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9498252,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'cancer therapy', 'computerized data processing', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",R01,2018,158992,0.023354274815228435
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9425925,R01HG009906,"['Address', 'Biological Neural Networks', 'CRISPR/Cas technology', 'Cell Count', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Gene Targeting', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'forest', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,PENNSYLVANIA STATE UNIV HERSHEY MED CTR,R01,2018,383250,-0.015035370210694152
"A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions Project summary There is a fundamental gap in our understanding of how mutations are preferentially targeted to the variable (V) regions of the Immunoglobulin (Ig) loci during somatic hypermutation (SHM). The persistence of this gap has limited our understanding of the mutagenic mechanisms involving activation-induced deaminase (AID) in the immune response and in the role of AID in mis-targeting mutations leading to B-cell lymphomas and other cancers. The long-term goal of the proposed research is to understand the global targeting of mutations in immunity that are required to protect us from infections. As high-throughput data from human antibody immune responses became available, it provided us with new opportunities to generate hypotheses to explain the underlying mechanisms of SHM. We now propose to generate further hypotheses using computational models applied to additional databases and to validate these hypotheses using cellular and animal experiments. Our objective is to understand what directs SHM across the many human Ig heavy chain V-regions. Our central hypothesis is that the V-region SHM process is highly dependent on a DNA sequence signature(s) that drives mutations in a largely deterministic fashion. This hypothesis is supported by our preliminary results using human in vivo data from a few human V region genes and has begun to be validated using independent databases and experiments in human B cell lines. The rationale is that evaluations of computational data based upon biological mechanisms, together with appropriate biological experiments, will reveal the key differences between IGHV regions (IGHV 3-23, 4-34, 1-18, 1-02, etc.) that lead to the dominance of each of those V regions in the responses to medically important antigens. Our hypothesis will be tested by pursuing two specific aims: 1) identify the extent to which a DNA signature determines the mutation process in four individual human IGHV genes that are important in disease responses; 2) examine the relationship between AID hotspots and PolÎ· hotspots across all the other human V region genes, thus rigorously defining a mutation targeting signature. Both aims will also entail studying human V region genes and modifications of them in human cell lines and in mice expressing a human V region to further confirm the signature and identify molecular mechanisms in vivo. Our approach is innovative because the computational models we are proposing will be mechanistically motivated focusing on the interaction between AID and PolÎ· hotspots, thus testing molecular mechanisms as opposed to classic statistical models using whole V region sequences that ignore the underlying biology. In addition, to focus on mechanisms we will leverage new high-throughput data from human V regions that have not undergone antigen selection. Our results will be highly relevant to human IgV repertoire analyses from immune responses that are currently hard to interpret and will help future vaccine and therapeutic antibody development, as well as help to understand mutations in human malignancies where AID plays a key role. Project narrative The proposed research is relevant to public health because understanding the targeting of AID-mediated mutations across many human heavy chain V-regions will make it possible to develop vaccines that will lead more rapidly to better and more broadly protective antibodies to infectious agents and reveal the risk factors in the development of B-cell malignancies and gastric and other solid tumors where AID is implicated.",A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions,9518337,R01AI132507,"['Affect', 'Alleles', 'Animal Experiments', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Autoimmunity', 'B lymphoid malignancy', 'B-Cell Development', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Biological', 'Biology', 'Cell Line', 'Chromatin', 'Complementarity Determining Regions', 'Computer Analysis', 'Computer Simulation', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Enzyme Activation', 'Evaluation', 'Event', 'Evolution', 'Family', 'Feedback', 'Frequencies', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Gene-Modified', 'Genes', 'Goals', 'HIV', 'Heavy-Chain Immunoglobulins', 'Human', 'Human Cell Line', 'Immune response', 'Immunity', 'Immunoglobulin Somatic Hypermutation', 'Immunoglobulins', 'Individual', 'Infection', 'Infectious Agent', 'Influenza', 'Influenza Hemagglutinin', 'Knock-in', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medical', 'Mismatch Repair', 'Molecular', 'Mus', 'Mutate', 'Mutation', 'Outcome', 'Pattern', 'Play', 'Polymerase', 'Process', 'Public Health', 'Research', 'Risk Factors', 'Role', 'Site', 'Solid Neoplasm', 'Statistical Models', 'Stomach', 'Structure of germinal center of lymph node', 'Techniques', 'Testing', 'Therapeutic antibodies', 'Time', 'Transgenic Mice', 'Vaccines', 'Validation', 'Variant', 'activation-induced cytidine deaminase', 'base', 'chromatin modification', 'density', 'experimental study', 'genetic variant', 'human data', 'in vivo', 'in vivo Model', 'innovation', 'neutralizing antibody', 'recruit', 'repair enzyme', 'response', 'spatial relationship', 'vaccine response']",NIAID,"ALBERT EINSTEIN COLLEGE OF MEDICINE, INC",R01,2018,537909,0.009218605178767611
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9416160,R00HG008171,"['Advisory Committees', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Code', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'cancer drug resistance', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'targeted nucleases', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2018,242325,0.031734481735584116
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9548692,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2018,1002519,0.0224594939587468
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9815897,R01GM117241,[' '],NIGMS,UNIVERSITY OF OREGON,R01,2018,276973,0.04104566991861767
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9420630,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2018,387500,-0.04309703353558224
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ï»¿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9205487,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2017,372603,0.04324558442720681
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ï»¿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9328097,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2017,298655,0.005072572080522888
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9333402,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2017,37785,0.016202346085873363
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9382936,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Architecture', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2017,225087,0.007139689570725162
"Population genomics of adaptation Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",Population genomics of adaptation,9383198,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Biological Neural Networks', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependency', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Learning', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Recurrence', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Supervision', 'Techniques', 'Time', 'Variant', 'Work', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria transmission', 'markov model', 'novel', 'resistance allele', 'response', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",R01,2017,295480,0.04104566991861767
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called âsmORFsâ. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human âsecretomeâ and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9344966,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Molecular Profiling', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2017,1186500,0.02600734396315248
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9353854,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Molecular Models', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,438098,0.011461316423508798
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9248178,U24HG009446,"['ATAC-seq', 'Alleles', 'Alpha Cell', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2017,2000000,0.02618742958890524
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,0.03063924569652135
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,0.035527385825775255
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,9528959,U41HG007497,"['Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Computational algorithm', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Phenotype', 'Population', 'Process', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'experimental study', 'genetic analysis', 'genetic variant', 'improved', 'integration site', 'method development', 'novel', 'tool']",NHGRI,JACKSON LABORATORY,U41,2017,1986604,-0.006465567274785604
"Genome analysis based on the integration of DNA sequence and shape DESCRIPTION (provided by applicant): Current techniques for genome analysis are mainly based on the one-dimensional DNA sequence, comprised of the letters A, C, G, and T. However, proteins recognize DNA as a three-dimensional (3D) object. Nuances in DNA shape at single nucleotide resolution play a crucial role in the binding specificity of transcription facors (TFs), including those involved in embryonic development and human cancer. This project involves the development of a battery of tools for genome analysis, through the integration of information derived from the DNA sequence and the 3D structure of DNA, or ""DNA shape"". The basis for these novel tools is a high- throughput (HT) method for the prediction of multiple features of local DNA shape at the genomic scale. Data will be made available to the community in the UCSC Genome Browser track format through a web server interface. These tools will enable users to analyze the shape of any number or length of DNA sequences, including whole genomes and the effect of DNA methylation. HT shape predictions will be validated based on X-ray crystallography, NMR spectroscopy, and hydroxyl radical cleavage data. Predictions will be combined with ORChID, an ENCODE project that infers DNA minor groove geometry from hydroxyl radical cleavage experiments. The HT method will be used to study how paralogous TFs select different target sites in vivo despite sharing core-binding motifs or having similar binding properties in vitro. To study this question, we will investigate the effect of flanking sequences on multiple structural features of TF binding sites (TFBSs). The initial focus of this study will be homeodomains and basic helix-loop-helix (bHLH) TFs. Other protein families will later be included and used to construct a comprehensive TFBS database that provides shape features for binding motifs derived from JASPAR and other motif databases. Structural effects of single nucleotide polymorphisms (SNPs) will also be analyzed. Some SNPs are associated with deleterious functions, whereas others have no apparent effect. The HT shape prediction method will be used to predict the function of SNPs in non-coding regions based on DNA shape. We will correlate quantitative effects of SNPs on DNA structure with expression quantitative trait loci (eQTLs) and genome-wide association study (GWAS) signals, to develop a predictive tool for the functional effect of SNPs. The HT shape prediction approach will be used to design DNA sequences with different AT/GC contents but similar shapes. The relative contributions of sequence and shape to binding will be tested with analytic models including multiple linear regression (MLR) and support vector regression (SVR). For systems in which the integration of sequence and shape proves advantageous, novel motif finding tools will be developed based on an extended alphabet that combines sequence with informative structural features, selected by machine learning and feature selection approaches. Sequence+shape motifs will be tested by motif scanning, compared to sequence-only motifs, and integrated into the MEME Suite. The goal of this sequence-shape integration is to increase the accuracy of finding in vivo TFBSs in the genome. PUBLIC HEALTH RELEVANCE: Protein-DNA recognition is a critical yet poorly understood component of gene regulation. This proposal will connect the fields of DNA sequence and structure analysis, which so far have been developed in parallel but largely disconnected from each other. Integration of the one-dimensional DNA sequence at a genome-wide scale with the three-dimensional DNA structure at atomic resolution will lead to the development of novel genome analysis tools and will advance our understanding of genome function, leading to fundamentally new insights into the mechanisms of gene regulation and its impact on human disease.",Genome analysis based on the integration of DNA sequence and shape,9203633,R01GM106056,"['Affect', 'Affinity', 'Algorithms', 'BHLH Protein', 'Base Pairing', 'Base Sequence', 'Benchmarking', 'Binding', 'Binding Proteins', 'Binding Sites', 'Biological Process', 'ChIP-on-chip', 'ChIP-seq', 'Characteristics', 'Communities', 'Computational algorithm', 'DNA', 'DNA Databases', 'DNA Integration', 'DNA Structure', 'DNase-I Footprinting', 'Data', 'Data Analyses', 'Databases', 'Deoxyribonuclease I', 'Development', 'Dimensions', 'Drosophila genus', 'Embryonic Development', 'Family', 'Gaussian model', 'Gene Components', 'Gene Expression Regulation', 'Genetic Transcription', 'Genome', 'Genome Scan', 'Genomics', 'Geometry', 'Goals', 'Guanine + Cytosine Composition', 'Helix-Turn-Helix Motifs', 'Human', 'Hybrids', 'Hydroxyl Radical', 'In Vitro', 'Internet', 'Length', 'Letters', 'Linear Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Methylation', 'Minor Groove', 'Modeling', 'Molecular Biology', 'Molecular Conformation', 'NMR Spectroscopy', 'Nucleotides', 'Pilot Projects', 'Play', 'Process', 'Property', 'Protein Family', 'Proteins', 'Publishing', 'Quantitative Trait Loci', 'Resolution', 'Role', 'Scanning', 'Sequence Analysis', 'Shapes', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Site', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Validation', 'Variant', 'Width', 'X-Ray Crystallography', 'Yeasts', 'base', 'design', 'experimental study', 'flexibility', 'genetic evolution', 'genome analysis', 'genome browser', 'genome wide association study', 'genome-wide', 'homeodomain', 'human disease', 'in vivo', 'insight', 'member', 'novel', 'novel strategies', 'predictive tools', 'public health relevance', 'three dimensional structure', 'tool', 'transcription factor', 'vector', 'whole genome']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,309913,0.0011265110456884224
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled âComputational and Comparative Genomicsâ, to be held in the Fall of 2017 â 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9357752,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2017,62304,0.02760946245233096
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",9246108,U24DK112331,"['ATAC-seq', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Institutes', 'Intervention', 'Knowledge', 'Lead', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Research Infrastructure', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'base', 'bisulfite sequencing', 'cost', 'data resource', 'epigenomics', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'insight', 'medical schools', 'methylome', 'mortality', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2017,93230,0.016358811889451846
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",9508669,U24DK112331,"['ATAC-seq', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Institutes', 'Intervention', 'Knowledge', 'Lead', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Research Infrastructure', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'base', 'bisulfite sequencing', 'cost', 'data resource', 'epigenomics', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'insight', 'medical schools', 'methylome', 'mortality', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2017,162828,0.016358811889451846
"NHGRI PAGE Coordinating Center DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE. PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.",NHGRI PAGE Coordinating Center,9461800,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiology', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomic Segment', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Productivity', 'Protocols documentation', 'Publications', 'Recruitment Activity', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost efficient', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'genetic variant', 'genomic epidemiology', 'genomic variation', 'improved', 'instrument', 'meetings', 'next generation', 'programs', 'public health relevance', 'rare variant', 'software development', 'study population', 'success', 'symposium', 'tool', 'web site', 'whole genome', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2017,710189,-0.010406279513785506
"Genomics-based prediction of antibiotic failure in S. aureus infections ï»¿    DESCRIPTION (provided by applicant)    The Gram positive bacterium Staphylococcus aureus is both an asymptomatic human colonizer and a pathogen that can cause infections in multiple tissue sites, including blood, skin and soft tissue, bone, and internal organs. Methicillin resistant Staphylococcus aureus (MRSA) is a common cause of death by hospital infections (HA-MRSA) and is now also a common community acquired infection (CA-MRSA). Vancomycin (a glycopeptide antibiotic) is the most commonly prescribed drug to treat MRSA infections. High-level resistance (minimal inhibitory concentration (MIC) â¥16 Î¼g/ml) to vancomycin encoded by the mobile vanA gene is rare due to a fitness burden on S. aureus. However, it is more common to encounter strains with mutations conferring intermediate resistance to vancomycin arising from selection during the course of antibiotic therapy. The genetic basis of these vancomycin intermediate S. aureus (VISA) and heterogeneous resistant (hVISA) (MIC 2-8 Î¼g/ml) strains involves a large number of different genomic mutations that result in cell wall thickening through changes in cellular signaling and regulation. Routine phenotypic testing in clinical labs probably underestimates the incidence of VISA and hVISA. Due to the fact that mutations in several genes have been linked with VISA, genetic-based detection of intermediate vancomycin resistance has not been developed for routine clinical microbiological use. In our preliminary work, we created an extensive catalog of sequenced clinical and laboratory-selected VISA as well as databases of SNPs and genetic variation in thousands of public S. aureus genomes. In this work we plan to extend these studies toward development of a sequence-based testing protocol that could be used for large numbers of clinical strains. In Specific Aim 1 we plan to extend our knowledge of the mutations that cause VISA by sequencing a panel of 300 novel mutants strains spontaneously selected from 40 S. aureus parent genotypes. We estimate, based on the results of the preliminary data, that this number of strains will be sufficient identify mutations found in 95% of VISA strains. These data will be used for creation of a comprehensive VISA detection assay based on whole genome data with an accuracy of at least 95%. In Specific Aim 2 we will use the information learned from Aim 1 to create a multiplex PCR sequence test for VISA, VRSA and other resistance determinants of S. aureus based on the commercially available Fluidigm platform. We will ultimately aim to have an assay that can be used to monitor systemic MRSA infections, such as bacteremia, to detect development of VISA in its early stages in clinical specimens from the patient. The test will also be able to detect other S. aureus resistance phenotypes and call the genotype of the strain. PUBLIC HEALTH RELEVANCE    Vancomycin is an antibiotic commonly used to treat methicillin resistant Staphylococcus aureus (MRSA) infections in chronically ill patients. The efficacy of this relatively cheap and well-tolerated therapy is compromised by mutations in the genome of the bacterium. In this project we propose to develop a genetic test based on a library of MRSA genome sequences with known antibiotic susceptibility level that identifies bacteria with diminished resistance to vancomycin.",Genomics-based prediction of antibiotic failure in S. aureus infections,9241329,R21AI121860,"['Address', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Bacteremia', 'Bacteria', 'Base Sequence', 'Biological Assay', 'Blood', 'Catalogs', 'Cause of Death', 'Cell Wall', 'Chronically Ill', 'Clinical', 'Clinical Microbiology', 'Community-Acquired Infections', 'Complex', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Evolution', 'Failure', 'Frequencies', 'Genes', 'Genetic', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Genotype', 'Glycopeptide Antibiotics', 'Goals', 'Gram-Positive Bacteria', 'Healthcare', 'Human', 'Incidence', 'Infection', 'Intermediate resistance', 'Knowledge', 'Laboratories', 'Lead', 'Libraries', 'Link', 'Machine Learning', 'Methicillin', 'Modeling', 'Monitor', 'Mutation', 'Nosocomial Infections', 'Organ', 'Parents', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Protocols documentation', 'Regulation', 'Resistance', 'Signal Transduction', 'Site', 'Skin Tissue', 'Specimen', 'Staphylococcus aureus', 'Testing', 'Tissues', 'Training', 'Treatment Failure', 'University Hospitals', 'Vancomycin', 'Vancomycin Resistance', 'Vancomycin-resistant S. aureus', 'Variant', 'Virulence', 'Work', 'base', 'bone', 'design', 'economic cost', 'fitness', 'genetic predictors', 'genetic variant', 'interest', 'methicillin resistant Staphylococcus aureus', 'mortality', 'mutant', 'novel', 'pathogen', 'phenotypic data', 'pleiotropism', 'public health relevance', 'soft tissue', 'tool', 'whole genome']",NIAID,EMORY UNIVERSITY,R21,2017,195000,-0.007508232418288978
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9258454,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2017,244439,0.031734481735584116
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9228843,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2017,1002725,0.0224594939587468
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ï»¿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8994718,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Health', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2016,370329,0.04324558442720681
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ï»¿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9145232,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Health', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'stem', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2016,303092,0.005072572080522888
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,9015770,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'genomic data', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2016,71329,0.003128648162337334
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9180486,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'abstracting', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2016,83411,0.016202346085873363
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9172062,R01HG009188,"['Affect', 'Base Pairing', 'Binding', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'DNA', 'DNA Sequence', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Health Care Research', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Marketing', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modification', 'Molecular Models', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Process', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Staging', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'research study', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2016,584552,0.011461316423508798
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,9268117,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost effectiveness', 'cost efficient', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2016,1378926,0.059616427831910235
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ï»¿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices. PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,9109642,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Health', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'therapeutic biomarker', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2016,271743,0.013355438151919974
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,0.036283902764977435
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,0.036283902764977435
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,9103177,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'learning strategy', 'meetings', 'population based', 'programs', 'whole genome']",NHGRI,STANFORD UNIVERSITY,R01,2016,300000,0.021225486916621742
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,9208745,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'genetic variant', 'genome sequencing', 'improved', 'integration site', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2016,2868077,-0.006465567274785604
"Genome analysis based on the integration of DNA sequence and shape DESCRIPTION (provided by applicant): Current techniques for genome analysis are mainly based on the one-dimensional DNA sequence, comprised of the letters A, C, G, and T. However, proteins recognize DNA as a three-dimensional (3D) object. Nuances in DNA shape at single nucleotide resolution play a crucial role in the binding specificity of transcription facors (TFs), including those involved in embryonic development and human cancer. This project involves the development of a battery of tools for genome analysis, through the integration of information derived from the DNA sequence and the 3D structure of DNA, or ""DNA shape"". The basis for these novel tools is a high- throughput (HT) method for the prediction of multiple features of local DNA shape at the genomic scale. Data will be made available to the community in the UCSC Genome Browser track format through a web server interface. These tools will enable users to analyze the shape of any number or length of DNA sequences, including whole genomes and the effect of DNA methylation. HT shape predictions will be validated based on X-ray crystallography, NMR spectroscopy, and hydroxyl radical cleavage data. Predictions will be combined with ORChID, an ENCODE project that infers DNA minor groove geometry from hydroxyl radical cleavage experiments. The HT method will be used to study how paralogous TFs select different target sites in vivo despite sharing core-binding motifs or having similar binding properties in vitro. To study this question, we will investigate the effect of flanking sequences on multiple structural features of TF binding sites (TFBSs). The initial focus of this study will be homeodomains and basic helix-loop-helix (bHLH) TFs. Other protein families will later be included and used to construct a comprehensive TFBS database that provides shape features for binding motifs derived from JASPAR and other motif databases. Structural effects of single nucleotide polymorphisms (SNPs) will also be analyzed. Some SNPs are associated with deleterious functions, whereas others have no apparent effect. The HT shape prediction method will be used to predict the function of SNPs in non-coding regions based on DNA shape. We will correlate quantitative effects of SNPs on DNA structure with expression quantitative trait loci (eQTLs) and genome-wide association study (GWAS) signals, to develop a predictive tool for the functional effect of SNPs. The HT shape prediction approach will be used to design DNA sequences with different AT/GC contents but similar shapes. The relative contributions of sequence and shape to binding will be tested with analytic models including multiple linear regression (MLR) and support vector regression (SVR). For systems in which the integration of sequence and shape proves advantageous, novel motif finding tools will be developed based on an extended alphabet that combines sequence with informative structural features, selected by machine learning and feature selection approaches. Sequence+shape motifs will be tested by motif scanning, compared to sequence-only motifs, and integrated into the MEME Suite. The goal of this sequence-shape integration is to increase the accuracy of finding in vivo TFBSs in the genome. PUBLIC HEALTH RELEVANCE: Protein-DNA recognition is a critical yet poorly understood component of gene regulation. This proposal will connect the fields of DNA sequence and structure analysis, which so far have been developed in parallel but largely disconnected from each other. Integration of the one-dimensional DNA sequence at a genome-wide scale with the three-dimensional DNA structure at atomic resolution will lead to the development of novel genome analysis tools and will advance our understanding of genome function, leading to fundamentally new insights into the mechanisms of gene regulation and its impact on human disease.",Genome analysis based on the integration of DNA sequence and shape,8998963,R01GM106056,"['Affect', 'Affinity', 'Algorithms', 'BHLH Protein', 'Base Pairing', 'Base Sequence', 'Benchmarking', 'Binding', 'Binding Proteins', 'Binding Sites', 'Biological Process', 'ChIP-on-chip', 'ChIP-seq', 'Characteristics', 'Communities', 'Computational algorithm', 'DNA', 'DNA Binding', 'DNA Databases', 'DNA Methylation', 'DNA Sequence', 'DNA Structure', 'DNA-Binding Proteins', 'DNase-I Footprinting', 'Data', 'Data Analyses', 'Databases', 'Deoxyribonuclease I', 'Development', 'Drosophila genus', 'Embryonic Development', 'Family', 'Gene Expression Regulation', 'Genetic Transcription', 'Genome', 'Genome Scan', 'Genomics', 'Geometry', 'Goals', 'Guanine + Cytosine Composition', 'Health', 'Helix-Turn-Helix Motifs', 'Human', 'Hybrids', 'Hydroxyl Radical', 'In Vitro', 'Internet', 'Lead', 'Length', 'Letters', 'Linear Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Methylation', 'Mining', 'Minor Groove', 'Modeling', 'Molecular Biology', 'NMR Spectroscopy', 'Nucleotides', 'Pilot Projects', 'Play', 'Process', 'Property', 'Protein Family', 'Proteins', 'Publishing', 'Quantitative Trait Loci', 'Resolution', 'Role', 'Scanning', 'Sequence Analysis', 'Shapes', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Site', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Validation', 'Variant', 'Width', 'X-Ray Crystallography', 'Yeasts', 'base', 'design', 'flexibility', 'genetic evolution', 'genome analysis', 'genome browser', 'genome wide association study', 'genome-wide', 'homeodomain', 'human disease', 'in vivo', 'insight', 'member', 'novel', 'novel strategies', 'predictive tools', 'research study', 'three dimensional structure', 'tool', 'transcription factor', 'vector', 'whole genome']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2016,306949,0.0011265110456884224
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9097763,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2016,52816,0.027839869044004394
"Genomics-based prediction of antibiotic failure in S. aureus infections ï»¿    DESCRIPTION (provided by applicant)    The Gram positive bacterium Staphylococcus aureus is both an asymptomatic human colonizer and a pathogen that can cause infections in multiple tissue sites, including blood, skin and soft tissue, bone, and internal organs. Methicillin resistant Staphylococcus aureus (MRSA) is a common cause of death by hospital infections (HA-MRSA) and is now also a common community acquired infection (CA-MRSA). Vancomycin (a glycopeptide antibiotic) is the most commonly prescribed drug to treat MRSA infections. High-level resistance (minimal inhibitory concentration (MIC) â¥16 Î¼g/ml) to vancomycin encoded by the mobile vanA gene is rare due to a fitness burden on S. aureus. However, it is more common to encounter strains with mutations conferring intermediate resistance to vancomycin arising from selection during the course of antibiotic therapy. The genetic basis of these vancomycin intermediate S. aureus (VISA) and heterogeneous resistant (hVISA) (MIC 2-8 Î¼g/ml) strains involves a large number of different genomic mutations that result in cell wall thickening through changes in cellular signaling and regulation. Routine phenotypic testing in clinical labs probably underestimates the incidence of VISA and hVISA. Due to the fact that mutations in several genes have been linked with VISA, genetic-based detection of intermediate vancomycin resistance has not been developed for routine clinical microbiological use. In our preliminary work, we created an extensive catalog of sequenced clinical and laboratory-selected VISA as well as databases of SNPs and genetic variation in thousands of public S. aureus genomes. In this work we plan to extend these studies toward development of a sequence-based testing protocol that could be used for large numbers of clinical strains. In Specific Aim 1 we plan to extend our knowledge of the mutations that cause VISA by sequencing a panel of 300 novel mutants strains spontaneously selected from 40 S. aureus parent genotypes. We estimate, based on the results of the preliminary data, that this number of strains will be sufficient identify mutations found in 95% of VISA strains. These data will be used for creation of a comprehensive VISA detection assay based on whole genome data with an accuracy of at least 95%. In Specific Aim 2 we will use the information learned from Aim 1 to create a multiplex PCR sequence test for VISA, VRSA and other resistance determinants of S. aureus based on the commercially available Fluidigm platform. We will ultimately aim to have an assay that can be used to monitor systemic MRSA infections, such as bacteremia, to detect development of VISA in its early stages in clinical specimens from the patient. The test will also be able to detect other S. aureus resistance phenotypes and call the genotype of the strain.             PUBLIC HEALTH RELEVANCE    Vancomycin is an antibiotic commonly used to treat methicillin resistant Staphylococcus aureus (MRSA) infections in chronically ill patients. The efficacy of this relatively cheap and well-tolerated therapy is compromised by mutations in the genome of the bacterium. In this project we propose to develop a genetic test based on a library of MRSA genome sequences with known antibiotic susceptibility level that identifies bacteria with diminished resistance to vancomycin.            ",Genomics-based prediction of antibiotic failure in S. aureus infections,9017369,R21AI121860,"['Address', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Bacteremia', 'Bacteria', 'Base Sequence', 'Biological Assay', 'Blood', 'Cataloging', 'Catalogs', 'Cause of Death', 'Cell Wall', 'Chronically Ill', 'Clinical', 'Community-Acquired Infections', 'Complex', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Drug Prescriptions', 'Evolution', 'Failure', 'Frequencies', 'Genes', 'Genetic', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Genotype', 'Glycopeptide Antibiotics', 'Goals', 'Gram-Positive Bacteria', 'Healthcare', 'Human', 'Incidence', 'Infection', 'Intermediate resistance', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Libraries', 'Link', 'Machine Learning', 'Methicillin', 'Modeling', 'Monitor', 'Mutation', 'Nosocomial Infections', 'Organ', 'Parents', 'Patients', 'Phenotype', 'Protocols documentation', 'Regulation', 'Resistance', 'Signal Transduction', 'Site', 'Skin Tissue', 'Specimen', 'Staging', 'Staphylococcus aureus', 'Testing', 'Tissues', 'Training', 'Treatment Failure', 'University Hospitals', 'Vancomycin', 'Vancomycin Resistance', 'Vancomycin-resistant S. aureus', 'Variant', 'Virulence', 'Work', 'base', 'bone', 'clinical sequencing', 'design', 'economic cost', 'fitness', 'genetic predictors', 'genetic variant', 'genome sequencing', 'interest', 'methicillin resistant Staphylococcus aureus', 'mortality', 'mutant', 'novel', 'pathogen', 'pleiotropism', 'public health relevance', 'soft tissue', 'tool', 'whole genome']",NIAID,EMORY UNIVERSITY,R21,2016,234000,-0.007508232418288978
"NHGRI PAGE Coordinating Center DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE. PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.",NHGRI PAGE Coordinating Center,9065945,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Cataloging', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'European', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Methodology', 'Methods', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Productivity', 'Protocols documentation', 'Publications', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost efficient', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'genetic variant', 'genomic variation', 'improved', 'instrument', 'meetings', 'next generation', 'next generation sequencing', 'programs', 'public health relevance', 'rare variant', 'software development', 'study population', 'success', 'symposium', 'tool', 'web site', 'whole genome', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2016,728166,-0.010406279513785506
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,8974432,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",K99,2016,25020,0.031734481735584116
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9242250,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2016,246031,0.031734481735584116
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ï»¿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control.         PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.                ",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8859887,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2015,381704,0.04324558442720681
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ï»¿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale.         PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.                ",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,8887722,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'stem', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2015,303504,0.005072572080522888
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8825472,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis', 'transcriptome sequencing']",NCI,BROWN UNIVERSITY,R01,2015,71329,0.003128648162337334
"Informatics Tools for High-Throughput Sequences Data Analysis DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK. The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.",Informatics Tools for High-Throughput Sequences Data Analysis,8788050,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2015,967608,0.04724545801938801
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8889700,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2015,2005492,0.059616427831910235
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ï»¿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices.         PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.            ",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,8984813,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'public health relevance', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2015,273816,0.013355438151919974
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,0.036283902764977435
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,0.036283902764977435
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,8930750,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs']",NHGRI,STANFORD UNIVERSITY,R01,2015,292499,0.021225486916621742
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8920443,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'genetic variant', 'genome sequencing', 'improved', 'integration site', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2015,2663757,-0.006465567274785604
"Human-Specific Gain and Loss of Function DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university. PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.",Human-Specific Gain and Loss of Function,8796200,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomic Segment', 'Genomic approach', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2015,54194,0.038874859806177116
"Genome analysis based on the integration of DNA sequence and shape DESCRIPTION (provided by applicant): Current techniques for genome analysis are mainly based on the one-dimensional DNA sequence, comprised of the letters A, C, G, and T. However, proteins recognize DNA as a three-dimensional (3D) object. Nuances in DNA shape at single nucleotide resolution play a crucial role in the binding specificity of transcription facors (TFs), including those involved in embryonic development and human cancer. This project involves the development of a battery of tools for genome analysis, through the integration of information derived from the DNA sequence and the 3D structure of DNA, or ""DNA shape"". The basis for these novel tools is a high- throughput (HT) method for the prediction of multiple features of local DNA shape at the genomic scale. Data will be made available to the community in the UCSC Genome Browser track format through a web server interface. These tools will enable users to analyze the shape of any number or length of DNA sequences, including whole genomes and the effect of DNA methylation. HT shape predictions will be validated based on X-ray crystallography, NMR spectroscopy, and hydroxyl radical cleavage data. Predictions will be combined with ORChID, an ENCODE project that infers DNA minor groove geometry from hydroxyl radical cleavage experiments. The HT method will be used to study how paralogous TFs select different target sites in vivo despite sharing core-binding motifs or having similar binding properties in vitro. To study this question, we will investigate the effect of flanking sequences on multiple structural features of TF binding sites (TFBSs). The initial focus of this study will be homeodomains and basic helix-loop-helix (bHLH) TFs. Other protein families will later be included and used to construct a comprehensive TFBS database that provides shape features for binding motifs derived from JASPAR and other motif databases. Structural effects of single nucleotide polymorphisms (SNPs) will also be analyzed. Some SNPs are associated with deleterious functions, whereas others have no apparent effect. The HT shape prediction method will be used to predict the function of SNPs in non-coding regions based on DNA shape. We will correlate quantitative effects of SNPs on DNA structure with expression quantitative trait loci (eQTLs) and genome-wide association study (GWAS) signals, to develop a predictive tool for the functional effect of SNPs. The HT shape prediction approach will be used to design DNA sequences with different AT/GC contents but similar shapes. The relative contributions of sequence and shape to binding will be tested with analytic models including multiple linear regression (MLR) and support vector regression (SVR). For systems in which the integration of sequence and shape proves advantageous, novel motif finding tools will be developed based on an extended alphabet that combines sequence with informative structural features, selected by machine learning and feature selection approaches. Sequence+shape motifs will be tested by motif scanning, compared to sequence-only motifs, and integrated into the MEME Suite. The goal of this sequence-shape integration is to increase the accuracy of finding in vivo TFBSs in the genome. PUBLIC HEALTH RELEVANCE: Protein-DNA recognition is a critical yet poorly understood component of gene regulation. This proposal will connect the fields of DNA sequence and structure analysis, which so far have been developed in parallel but largely disconnected from each other. Integration of the one-dimensional DNA sequence at a genome-wide scale with the three-dimensional DNA structure at atomic resolution will lead to the development of novel genome analysis tools and will advance our understanding of genome function, leading to fundamentally new insights into the mechanisms of gene regulation and its impact on human disease.",Genome analysis based on the integration of DNA sequence and shape,8795204,R01GM106056,"['Affect', 'Affinity', 'Algorithms', 'BHLH Protein', 'Base Pairing', 'Base Sequence', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological Process', 'ChIP-on-chip', 'ChIP-seq', 'Characteristics', 'Communities', 'Computational algorithm', 'DNA', 'DNA Binding', 'DNA Databases', 'DNA Methylation', 'DNA Sequence', 'DNA Structure', 'DNA-Binding Proteins', 'DNase-I Footprinting', 'Data', 'Data Analyses', 'Databases', 'Deoxyribonuclease I', 'Development', 'Drosophila genus', 'Embryonic Development', 'Family', 'Gene Expression Regulation', 'Genetic Transcription', 'Genome', 'Genome Scan', 'Genomics', 'Geometry', 'Goals', 'Guanine + Cytosine Composition', 'Health', 'Helix-Turn-Helix Motifs', 'Human', 'Hybrids', 'Hydroxyl Radical', 'In Vitro', 'Internet', 'Lead', 'Length', 'Letters', 'Linear Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Methylation', 'Mining', 'Minor Groove', 'Modeling', 'Molecular Biology', 'NMR Spectroscopy', 'Nucleotides', 'Pilot Projects', 'Play', 'Process', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Publishing', 'Quantitative Trait Loci', 'Relative (related person)', 'Resolution', 'Role', 'Scanning', 'Sequence Analysis', 'Shapes', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Site', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Validation', 'Variant', 'Width', 'X-Ray Crystallography', 'Yeasts', 'base', 'design', 'flexibility', 'genetic evolution', 'genome analysis', 'genome wide association study', 'genome-wide', 'homeodomain', 'human disease', 'in vivo', 'insight', 'member', 'novel', 'novel strategies', 'research study', 'three dimensional structure', 'tool', 'transcription factor', 'vector']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2015,304719,0.0011265110456884224
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,8898177,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2015,52816,0.027839869044004394
"NHGRI PAGE Coordinating Center DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE. PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.",NHGRI PAGE Coordinating Center,8849936,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Cataloging', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'European', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Methodology', 'Methods', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Population Study', 'Productivity', 'Protocols documentation', 'Publications', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'genetic variant', 'genomic variation', 'improved', 'instrument', 'meetings', 'next generation', 'next generation sequencing', 'programs', 'public health relevance', 'rare variant', 'software development', 'success', 'symposium', 'tool', 'web site', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2015,681355,-0.010406279513785506
"NHGRI PAGE Coordinating Center DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE. PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.",NHGRI PAGE Coordinating Center,9121301,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Cataloging', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'European', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Methodology', 'Methods', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Population Study', 'Productivity', 'Protocols documentation', 'Publications', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'genetic variant', 'genomic variation', 'improved', 'instrument', 'meetings', 'next generation', 'next generation sequencing', 'programs', 'public health relevance', 'rare variant', 'software development', 'success', 'symposium', 'tool', 'web site', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2015,124339,-0.010406279513785506
"Genome engineering tools for functional screening of non-coding elements     DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root).         PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.                ",Genome engineering tools for functional screening of non-coding elements,8804084,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Relative (related person)', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",K99,2015,99937,0.031734481735584116
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8840551,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'genetic variant', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2015,219004,0.030840894072916996
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,0.03777324361874283
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,0.03777324361874283
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,0.03777324361874283
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8683213,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2014,221252,0.04911292257665478
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8685211,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2014,69189,0.004684290358007594
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8601147,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2014,989800,0.04724545801938801
"Gene Prediction by Markov Models and Complementary Methods DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad. NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8909702,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'pyrosequencing', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2014,100000,0.02432260998204609
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8725717,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2014,2015775,0.059616427831910235
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,0.036283902764977435
"Statistical and computational analysis in whole genome sequencing studies.     DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges.         PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.                ",Statistical and computational analysis in whole genome sequencing studies.,8750827,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs', 'public health relevance']",NHGRI,STANFORD UNIVERSITY,R01,2014,300000,0.021225486916621742
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8737934,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Detection', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Site', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'design', 'genetic variant', 'genome sequencing', 'improved', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2014,2684060,-0.006465567274785604
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8635216,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2014,51530,0.038874859806177116
"Genome analysis based on the integration of DNA sequence and shape  Title: Genome analysis based on the integration of DNA sequence and shape PI: Rohs, Remo (USC); Co-I: Noble, William Stafford (UW); Co-I: Tullius, Thomas D. (BU) PROJECT SUMMARY Current techniques for genome analysis are mainly based on the one-dimensional DNA sequence, comprised of the letters A, C, G, and T. However, proteins recognize DNA as a three-dimensional (3D) object. Nuances in DNA shape at single nucleotide resolution play a crucial role in the binding specificity of transcription factors (TFs), including those involved in embryonic development and human cancer. This project involves the development of a battery of tools for genome analysis, through the integration of information derived from the DNA sequence and the 3D structure of DNA, or ""DNA shape"". The basis for these novel tools is a high- throughput (HT) method for the prediction of multiple features of local DNA shape at the genomic scale. Data will be made available to the community in the UCSC Genome Browser track format through a web server interface. These tools will enable users to analyze the shape of any number or length of DNA sequences, including whole genomes and the effect of DNA methylation. HT shape predictions will be validated based on X-ray crystallography, NMR spectroscopy, and hydroxyl radical cleavage data. Predictions will be combined with ORChID, an ENCODE project that infers DNA minor groove geometry from hydroxyl radical cleavage experiments. The HT method will be used to study how paralogous TFs select different target sites in vivo despite sharing core-binding motifs or having similar binding properties in vitro. To study this question, we will investigate the effect of flanking sequences on multiple structural features of TF binding sites (TFBSs). The initial focus of this study will be homeodomains and basic helix-loop-helix (bHLH) TFs. Other protein families will later be included and used to construct a comprehensive TFBS database that provides shape features for binding motifs derived from JASPAR and other motif databases. Structural effects of single nucleotide polymorphisms (SNPs) will also be analyzed. Some SNPs are associated with deleterious functions, whereas others have no apparent effect. The HT shape prediction method will be used to predict the function of SNPs in non-coding regions based on DNA shape. We will correlate quantitative effects of SNPs on DNA structure with expression quantitative trait loci (eQTLs) and genome-wide association study (GWAS) signals, to develop a predictive tool for the functional effect of SNPs. The HT shape prediction approach will be used to design DNA sequences with different AT/GC contents but similar shapes. The relative contributions of sequence and shape to binding will be tested with analytic models including multiple linear regression (MLR) and support vector regression (SVR). For systems in which the integration of sequence and shape proves advantageous, novel motif finding tools will be developed based on an extended alphabet that combines sequence with informative structural features, selected by machine learning and feature selection approaches. Sequence+shape motifs will be tested by motif scanning, compared to sequence-only motifs, and integrated into the MEME Suite. The goal of this sequence-shape integration is to increase the accuracy of finding in vivo TFBSs in the genome. PUBLIC HEALTH RELEVANCE: Protein-DNA recognition is a critical yet poorly understood component of gene regulation. This proposal will connect the fields of DNA sequence and structure analysis, which so far have been developed in parallel but largely disconnected from each other. Integration of the one-dimensional DNA sequence at a genome-wide scale with the three-dimensional DNA structure at atomic resolution will lead to the development of novel genome analysis tools and will advance our understanding of genome function, leading to fundamentally new insights into the mechanisms of gene regulation and its impact on human disease.            ",Genome analysis based on the integration of DNA sequence and shape,8632246,R01GM106056,"['Affect', 'Affinity', 'Algorithms', 'BHLH Protein', 'Base Pairing', 'Base Sequence', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological Process', 'ChIP-on-chip', 'ChIP-seq', 'Characteristics', 'Communities', 'Computational algorithm', 'DNA', 'DNA Binding', 'DNA Databases', 'DNA Methylation', 'DNA Sequence', 'DNA Structure', 'DNA-Binding Proteins', 'DNase-I Footprinting', 'Data', 'Data Analyses', 'Databases', 'Deoxyribonuclease I', 'Development', 'Drosophila genus', 'Embryonic Development', 'Family', 'Functional RNA', 'Gene Expression Regulation', 'Genetic Transcription', 'Genome', 'Genome Scan', 'Genomics', 'Geometry', 'Goals', 'Guanine + Cytosine Composition', 'Helix-Turn-Helix Motifs', 'Human', 'Hybrids', 'Hydroxyl Radical', 'In Vitro', 'Internet', 'Lead', 'Length', 'Letters', 'Linear Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Methylation', 'Mining', 'Minor Groove', 'Modeling', 'Molecular Biology', 'NMR Spectroscopy', 'Nucleotides', 'Pilot Projects', 'Play', 'Process', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Publishing', 'Quantitative Trait Loci', 'Relative (related person)', 'Resolution', 'Role', 'Scanning', 'Sequence Analysis', 'Shapes', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Site', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Variant', 'Width', 'X-Ray Crystallography', 'Yeasts', 'base', 'design', 'flexibility', 'genetic evolution', 'genome analysis', 'genome wide association study', 'genome-wide', 'homeodomain', 'human disease', 'in vivo', 'insight', 'member', 'novel', 'novel strategies', 'public health relevance', 'research study', 'three dimensional structure', 'tool', 'transcription factor', 'vector']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2014,334303,0.0021801079274086444
"CSHL Computational and Comparative Genomics Course     DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions.         PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.                 ",CSHL Computational and Comparative Genomics Course,8737540,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'public health relevance', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2014,52816,0.027839869044004394
"NHGRI PAGE Coordinating Center     DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE.         PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.                ",NHGRI PAGE Coordinating Center,8728994,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Cataloging', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'European', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Methodology', 'Methods', 'Metric', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Population Study', 'Productivity', 'Protocols documentation', 'Publications', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'improved', 'instrument', 'meetings', 'next generation', 'next generation sequencing', 'programs', 'public health relevance', 'rare variant', 'software development', 'success', 'symposium', 'tool', 'web site', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2014,690507,-0.010406279513785506
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8642168,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2014,207764,0.030840894072916996
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,0.03777324361874283
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8699810,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic DNA', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,641093,0.06433858250698606
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8518436,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2013,220626,0.04911292257665478
"Analytical Approaches to Massive Data Computation with Applications to Genomics     DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.              n/a",Analytical Approaches to Massive Data Computation with Applications to Genomics,8599823,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics']",NCI,BROWN UNIVERSITY,R01,2013,71329,0.004766450568378225
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8416349,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2013,964551,0.04724545801938801
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8722983,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,115680,0.059616427831910235
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8548395,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,1871839,0.059616427831910235
"Second generation sequencing and analysis for cancer    DESCRIPTION (provided by applicant): Kevin Squire is a PhD in Electrical Engineering and former Computer Science professor with a strong interest in bioinformatics and human genetics. To this end, he joined Dr. Stanley F. Nelson's laboratory as a Postdoctoral Fellow in Human Genetics at UCLA one year ago, in order to retrain in bioinformatics and sequencing. Kevin's background in machine learning gives him a good foundation for a career in bioinformatics. What he needs, and what obtaining this grant will give him, is a good background in basic biology, biochemistry, and genetics, in order to better understand the biological processes behind the data he is working with. Kevin explorations in genetics have inspired him to make his career in this field. He hopes to gain a much better understanding of biology in order to ask and answer research questions relevant to the biology of cancer using second (and later) generation sequencing and through the use and development of relevant tools and analysis. As part of the research development plan, Kevin will complete a didactic coursework component in the first 2 years, to fill in the gaps in biology and bioinformatics in his background. During and after that, his research will focus on giving him a better understanding the genetics of cancer, attempting to answer relevant research questions from high throughput genomic sequencing data, through the use and creation of sequence analysis tools and other bioinformatics tools. Through his coursework and research, Kevin will attain the necessary skills to become an independent researcher in bioinformatics and genetics.       PUBLIC HEALTH RELEVANCE: Cancer is a genetic disease, caused by mutations in DNA and other genetic changes which affect how cells work. The work in this proposal uses and enhances new sequencing technology to help accurately determine exactly what changes are occurring in tumor DNA and RNA, which in turn affect protein production and the health of the cell. While much of the work is broadly applicable, the research will be evaluated in glioblastoma, the most common and most deadly form of brain cancer, and will help determine the best course of treatment for patients with this disease.         ",Second generation sequencing and analysis for cancer,8471121,K25GM097097,"['Address', 'Affect', 'Aftercare', 'Algorithms', 'Alternative Splicing', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Biological Process', 'Biology', 'Cancer Biology', 'Cells', 'Collaborations', 'Complex', 'DNA', 'DNA Sequence Rearrangement', 'Data', 'Data Analyses', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Electrical Engineering', 'Exons', 'Fellowship', 'Foundations', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Glioblastoma', 'Glioma', 'Grant', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Laboratories', 'Lasso', 'Lead', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Measurement', 'Mentors', 'Methodology', 'Methods', 'Methylation', 'Modeling', 'Molecular', 'Mutation', 'Normal tissue morphology', 'Nucleotides', 'Other Genetics', 'Patients', 'Postdoctoral Fellow', 'Prevention strategy', 'Process', 'Production', 'Protein Isoforms', 'Proteins', 'RNA', 'RNA Sequences', 'Radiation therapy', 'Recording of previous events', 'Recurrence', 'Recurrent tumor', 'Relapse', 'Research', 'Research Personnel', 'Salvage Therapy', 'Sampling', 'Sequence Analysis', 'Spliced Genes', 'Techniques', 'Technology', 'Training Programs', 'Tumor Cell Biology', 'Tumor Tissue', 'Variant', 'Work', 'anticancer research', 'base', 'bevacizumab', 'cancer genetics', 'cancer genomics', 'career', 'computer science', 'disease-causing mutation', 'exome', 'exome sequencing', 'expectation', 'experience', 'genetic selection', 'improved', 'insertion/deletion mutation', 'insight', 'interest', 'methylome', 'next generation sequencing', 'professor', 'programs', 'public health relevance', 'research and development', 'resistance mechanism', 'skills', 'temozolomide', 'tool', 'tumor']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,K25,2013,123483,-0.02081795610922435
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8589933,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Detection', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Site', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'design', 'genetic variant', 'genome sequencing', 'improved', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2013,2766009,-0.006465567274785604
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8457179,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2013,47114,0.038874859806177116
"NHGRI PAGE Coordinating Center     DESCRIPTION (provided by applicant): NHGRI developed the Population Architecture Using Genomics and Epidemiology (PAGE) research program to identify and characterize genomic variants in non-European populations. To support the complexities of such an ambitious effort, we have convened a strong team of statistical, population, and molecular geneticists, computer and information scientists, biostatisticians, and project management staff with many years of related experience to serve as a Coordinating Center (CC). Specifically, the CC will serve as a centralized resource to facilitate and support the activities of the program and Study Investigators focused on characterization of causal variants by: (1) coordinating phenotype harmonization efforts, including mapping phenotype variables across studies and to the PhenX measures; (2) synthesizing individual-level data into centralized datasets to facilitate sharing of data within and outside of PAGE; (3) utilizing state-of-the-art computer and information science support and scientific workflows that will facilitate analyses, ancestry deconvolution, genotype calling and imputation, SNP annotation, and data synthesis; (4) rapidly disseminating all study data via dbGaP and/or the PAGE website or other applicable databases; and (5) serving as a centralized resource to facilitate, support, and manage program activities and logistics as requested by the Steering Committee or Project Office and as needed for successful coordination of the program. Coordination of the program will be done in a spirit of collaboration using creative and flexible approaches, while providing leadership in statistical genetic methodologies and approaches to project management. The ultimate goal of our CC is to facilitate the identification and characterization of genotype-phenotype associations, especially as relevant to non-European populations, thereby accelerating our understanding of ancestral differences in the genetic and environmental causes of common diseases. Critical to achieving this mission is the deployment of powerful methods for ancestry deconvolution, multi- and trans-ethnic mapping, and imputation. Building upon our success as the PAGE I CC, we have added additional investigators with expertise in these areas and consortium experience with next-generation sequence analysis of both whole-genome and exome data. Our collaborative team is ideally staffed to meet the challenges of the new round of PAGE.         PUBLIC HEALTH RELEVANCE: The PAGE study focuses on analysis of existing large samples of primarily non- European ancestry to broaden our understanding of the ethnic differences in the genetic basis of complex disease. The PAGE coordinating center supports the functions of this study.                ",NHGRI PAGE Coordinating Center,8573120,U01HG007419,"['African American', 'Architecture', 'Area', 'Biological Assay', 'Cataloging', 'Catalogs', 'Collaborations', 'Communication', 'Complex', 'Computers', 'Custom', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Documentation', 'Eligibility Determination', 'Ensure', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'European', 'Funding', 'Future', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Group Meetings', 'Hispanics', 'Individual', 'Information Sciences', 'Informed Consent', 'Internet', 'Latino', 'Leadership', 'Letters', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Methodology', 'Methods', 'Metric', 'Mining', 'Mission', 'Molecular', 'Monitor', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'Phase', 'Phenotype', 'Population', 'Population Study', 'Productivity', 'Protocols documentation', 'Publications', 'Reporting', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Site', 'Source', 'Technology', 'Time', 'Translational Research', 'Update', 'Variant', 'Voice', 'Work', 'base', 'computer science', 'cost', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'disease phenotype', 'epidemiology study', 'ethnic difference', 'exome', 'exome sequencing', 'experience', 'flexibility', 'formycin triphosphate', 'genetic analysis', 'genetic epidemiology', 'improved', 'instrument', 'meetings', 'next generation', 'next generation sequencing', 'programs', 'public health relevance', 'software development', 'success', 'symposium', 'tool', 'web site', 'wiki', 'working group']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",U01,2013,720000,-0.010406279513785506
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8599826,R01CA180777,"['Bees', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2013,214832,0.030840894072916996
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,0.03777324361874283
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8537965,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,624741,0.06433858250698606
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,0.04422961812203502
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.      PUBLIC HEALTH RELEVANCE: The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.              The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8237596,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Delivery Systems', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2012,1010000,0.0448127508107319
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8494858,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2012,371054,0.058183627043252646
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.           Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8320160,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'metagenome', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2012,204974,0.02965645173506191
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8521766,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,75000,0.02432260998204609
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8266525,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,577121,0.02432260998204609
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8402447,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Set', 'Development', 'Disease', 'Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2012,2460045,0.059616427831910235
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.         The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8331495,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2012,103535,0.003173433929607051
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8228154,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'ChIP-seq', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,323572,-0.0013860766109394915
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8310258,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,367520,0.014303686447727552
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8537085,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,139853,0.014303686447727552
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,0.03777324361874283
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.        PUBLIC HEALTH RELEVANCE: Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.              Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8373752,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,654177,0.0616272265841228
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8107695,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2011,108418,0.058183627043252646
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,8134360,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait', 'treatment strategy']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2011,342569,0.024848449341410478
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8053866,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2011,577264,0.02432260998204609
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.      PUBLIC HEALTH RELEVANCE:    Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.                 Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8192895,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2011,180669,0.029378214541879763
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8035949,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,321670,-0.0013860766109394915
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.       PUBLIC HEALTH RELEVANCE: The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.              The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8164533,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2011,102709,0.0125904547832231
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8150462,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2011,407746,0.014303686447727552
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8321717,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,342218,-0.0035370592877964086
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8320051,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,2615448,-0.0035370592877964086
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,8071964,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,BOISE STATE UNIVERSITY,R01,2011,431250,-0.004855155993885261
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,8321269,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,BOISE STATE UNIVERSITY,R01,2011,797678,-0.004855155993885261
"Statistical Methods for Correlated and High-Dimensional Biomedical Data We propose in the renewal of this MERIT award application to continue developing advanced statistical and computational methods for analysis of correlated and high-dimensional data, which arise frequently in health science research, especially in cancer research. Correlated data are often observed in observational studies and clinical trials, such as longitudinal studies and familial studies. High-dimensional data have emerged rapidly in recent years due to the advance of high-throughput 'omics technologies, e.g, in Genome- Wide Association Studies (GWAS), and genome-wide epigenetic (DNA methylation) studies. Massive next generation sequencing data are soon available. There is an urgent need to develop advanced stati stical and computational methods for analyzing such high throughput 'omics data in observational studies and clinical trials. We propose to develop statistical and computational methods for analysis of (1) genome-wide association studies; (2) sequencing data for studying rare variant effects; (3) genome-wide DNA methylation studies; (4) gene-gene and gene-environment interactions. We will develop methods for both case-control studies and cohort studies, such as longitudinal studies and survival studies. We will study the theoretical properties of the proposed methods and evaluate the finite s ample performance using simulation studies. We will develop efficient numerical algorithms and user-friendly statistical software, and disseminate these tools to health sciences researchers. In collaboration with biomedical investigators, we will apply the proposed models methods to data from several genome-wide epidemiological studies in cancer and other chronic diseases. RELEVANCE (See instructions):  Development of new statistical methods for analysis of correlated and high-dimensional data will provide  powerful analytic tools to advance 'omics research in observational studies and clinical trials and to help  understand the roles of genes, gene products, and the environment in causing human diseases.",Statistical Methods for Correlated and High-Dimensional Biomedical Data,8117858,R37CA076404,"['Advanced Development', 'Aging', 'Algorithms', 'Award', 'Biomedical Research', 'Case-Control Studies', 'Childhood', 'Chronic Disease', 'Clinical Treatment', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Computer software', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Development', 'Dimensions', 'Disease', 'Environment', 'Epidemiologic Studies', 'Epigenetic Process', 'Etiology', 'Explosion', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health Sciences', 'Human Genome', 'Instruction', 'Intervention', 'Investigation', 'Knowledge', 'Longitudinal Studies', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measures', 'Messenger RNA', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Prevention strategy', 'Property', 'Proteins', 'Public Health Schools', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Single Nucleotide Polymorphism', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Technology', 'Testing', 'Theoretical Studies', 'Variant', 'anticancer research', 'cancer risk', 'case control', 'computer science', 'computerized data processing', 'epigenomics', 'gene environment interaction', 'gene interaction', 'genome wide association study', 'genome-wide', 'health science research', 'high throughput analysis', 'human disease', 'malignant breast neoplasm', 'new technology', 'next generation', 'novel', 'programs', 'simulation', 'tool', 'user-friendly']",NCI,HARVARD SCHOOL OF PUBLIC HEALTH,R37,2011,308344,-0.0015121441629931493
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,8327885,RC2HG005639,"['Animal Model', 'Animals', 'Beryllium', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Messenger RNA', 'Methodology', 'Methods', 'MicroRNAs', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2011,1296550,-0.021305903200833316
"Enhance human ENCODE by function comparisons to mouse    DESCRIPTION (provided by applicant): Our goal is to discover and use relationships between mouse and human regulatory genomes to advance the ENCODE Project in its effort to map all functional elements in the human genome. Our comparative approach aims to uncover principles and solve problems that are proving difficult by studying the human genome alone. ENCODE is vigorously mapping hundreds of function-associated biochemical markers in selected cell lines, resulting already in tens of millions of reproducible biochemical features. Some observed protein:DNA interactions find and refine known transcriptional enhancers, promoters, silencers, together with associated chromatin structure, as was anticipated. But substantial questions arise as to how many of the myriad biochemical events are functional, what those functions are, which gene or genes are meaningful targets, etc. To highlight and sort functionally important biochemical marks from others, we will systematically identify the molecular events retained by both mouse and human since they diverged. We will then analyze how conservation of biochemical features relates to conservation of DNA sequence and conservation of regulated gene expression. By using the mouse, we can leverage decades of molecular genetics and manipulated mouse genomes that do not exist in any other mammal. In Aim 1 we execute genome-wide assays for biochemical signatures of functional DNA sequences in a few specific mouse cell types. By using well-studied mouse lines and cell states, we can interpret results in light of previously validated elements and in light of ENCODE human results. We will use ENCODE standards for high throughput, sequence-based assays to determine gene expression, DNase hypersensitive sites, histone modifications and selected transcription factor occupancy in seven mouse cell types. The eight selected features are the most informative ones for function, and thus most useful for comparison with human data. In Aim 2, we apply a genome-wide implementation of chromosome conformation capture to map the interactions between transcription factor binding sites and their responsive genes in two cell types. These results will be compared to those from an ENCODE developmental project. Comparative analysis in Aim 3 will insure that the impact of the data we produce will go beyond the individual mouse cell systems per se. To do this we have organized a collaboration of investigators at multiple institutions, in which each group is expert in one or more critical aspects. Our data, made public and accessible via ENCODE, will fuel and accelerate many future studies after the 2-yr stimulus both in and beyond ENCODE. This responds to NHGRI request for applications on ""Enhancement of the value of the human ENCODE Project by conducting a parallel effort on the mouse genome."" The proposed work will improve the maps of biologically functional DNA sequences in humans, which in turn will help explain how variants in human genome sequences could be associated with human diseases, leading to candidates for novel avenues for effective therapy and prevention.      PUBLIC HEALTH RELEVANCE:  Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.          Project Narrative for ""Enhance human ENCODE by functional comparisons to mouse"" Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.",Enhance human ENCODE by function comparisons to mouse,8321719,RC2HG005573,"['Adopted', 'BFU-E', 'Base Sequence', 'Binding', 'Binding Sites', 'Biochemical', 'Biochemical Markers', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cell model', 'Cells', 'Chromatin Structure', 'Chromosomes', 'Classification', 'Collaborations', 'Complement', 'Complex', 'Correlative Study', 'DNA', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Deoxyribonucleases', 'Development', 'Developmental Biology', 'Diabetes Mellitus', 'Disease', 'Disease susceptibility', 'Elements', 'Enhancers', 'Erythroblasts', 'Erythropoiesis', 'Event', 'Evolution', 'Frequencies', 'Future', 'GATA1 gene', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Goals', 'Harvest', 'Heart Diseases', 'Human', 'Human Cell Line', 'Human Genome', 'Human Genome Project', 'Indium', 'Individual', 'Institution', 'Investigation', 'Lead', 'Light', 'Location', 'Lymphocyte', 'Lymphoid', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Megakaryocytopoieses', 'Molecular', 'Molecular Conformation', 'Molecular Evolution', 'Molecular Genetics', 'Mouse Cell Line', 'Mus', 'Muscle Cells', 'Myelogenous', 'National Human Genome Research Institute', 'Nuclear', 'Persons', 'Phase', 'Phylogenetic Analysis', 'Predisposition', 'Prevention', 'Problem Solving', 'Process', 'Quality Control', 'RNA', 'Request for Applications', 'Research', 'Research Personnel', 'Sequence Analysis', 'Site', 'Sorting - Cell Movement', 'Staging', 'Stimulus', 'System', 'Technology', 'Time Study', 'Transcript', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Variant', 'Work', 'base', 'cell type', 'comparative', 'data mining', 'effective therapy', 'embryonic stem cell', 'genome sequencing', 'genome-wide', 'high standard', 'histone modification', 'human GATA1 protein', 'human data', 'human disease', 'improved', 'insight', 'mouse genome', 'novel', 'pathogen', 'promoter', 'public health relevance', 'response', 'restoration', 'transcription factor']",NHGRI,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,RC2,2011,749992,-0.02986189983876759
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,0.027316623148367004
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,0.03830063443960731
"Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform    DESCRIPTION (provided by applicant): DNAnexus proposes to develop a complete solution for the identification and stratification of personal genetic variation from ultra-high-throughput sequencing projects. The solution will be implemented as a Web 2.0 service and online browsing tool that will integrate public data sources such as the 1000 genomes project, comparative information, and the ENCODE II project data. Users will be able to browse and stratify the identified variation in the context of these genomic annotations, and according to the likely functional impact. In Phase I of our project, we will develop a basic browser for displaying sequence reads that are mapped to a reference genome with our state-of-the-art read mapper. The browser will support viewing mate paired reads as well as display of the variation between these reads and the reference genome. It will facilitate the algorithmic development that we will perform during Phase II, and it will be the foundation for the more sophisticated variation browser also proposed in Phase II. In Phase II, we will develop algorithms for detecting genomic variation, and a state-of-the-art browser for viewing variation in the context of existing genome annotations, functional genomic and comparative genomic data. Our algorithms for detecting variation will support all major types of genomic variation, including SNPs, microindels, larger insertions and deletions, duplications, copy number variations, inversions, and translocations. Our algorithms will be based on state-of-the-art statistical and machine learning methodology for human genome resequencing. The DNAnexus browser will have two components: a list browser that displays variation as a list filtered and stratified by criteria that a user chooses, and a powerful GUI whose navigation capabilities are inspired by modern online tools such as Google Maps.      PUBLIC HEALTH RELEVANCE: DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.           Project Narrative DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.",Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform,7909096,R43HG005794,"['Algorithms', 'Architecture', 'Arts', 'Code', 'Copy Number Polymorphism', 'DNA Resequencing', 'Data', 'Data Display', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Environment', 'Foundations', 'Genes', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genetics', 'Human Genome', 'Imagery', 'Individual', 'Internet', 'Machine Learning', 'Maps', 'Methodology', 'Partner in relationship', 'Phase', 'Point Mutation', 'Reading', 'Services', 'Solutions', 'Statistical Methods', 'Stratification', 'Technology', 'Variant', 'base', 'comparative', 'comparative genomics', 'cost', 'flexibility', 'functional genomics', 'genome sequencing', 'graphical user interface', 'insertion/deletion mutation', 'public health relevance', 'tool']",NHGRI,"DNANEXUS, INC.",R43,2010,74477,0.019502755083374568
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7913074,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,1248287,0.058183627043252646
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8121894,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,300000,0.058183627043252646
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8144973,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,113520,0.058183627043252646
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8147585,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,151816,0.058183627043252646
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7902231,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2010,346884,0.024848449341410478
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7809669,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2010,573248,0.02432260998204609
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7795846,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,326175,-0.0013860766109394915
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8020799,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2010,408559,0.014303686447727552
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,7802061,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'public health relevance', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2010,435435,-0.004855155993885261
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7860712,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2010,297123,-0.004600786633304887
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,7943875,RC2HG005639,"['Animal Model', 'Animals', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulation', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2010,1316360,-0.021305903200833316
"Enhance human ENCODE by function comparisons to mouse    DESCRIPTION (provided by applicant): Our goal is to discover and use relationships between mouse and human regulatory genomes to advance the ENCODE Project in its effort to map all functional elements in the human genome. Our comparative approach aims to uncover principles and solve problems that are proving difficult by studying the human genome alone. ENCODE is vigorously mapping hundreds of function-associated biochemical markers in selected cell lines, resulting already in tens of millions of reproducible biochemical features. Some observed protein:DNA interactions find and refine known transcriptional enhancers, promoters, silencers, together with associated chromatin structure, as was anticipated. But substantial questions arise as to how many of the myriad biochemical events are functional, what those functions are, which gene or genes are meaningful targets, etc. To highlight and sort functionally important biochemical marks from others, we will systematically identify the molecular events retained by both mouse and human since they diverged. We will then analyze how conservation of biochemical features relates to conservation of DNA sequence and conservation of regulated gene expression. By using the mouse, we can leverage decades of molecular genetics and manipulated mouse genomes that do not exist in any other mammal. In Aim 1 we execute genome-wide assays for biochemical signatures of functional DNA sequences in a few specific mouse cell types. By using well-studied mouse lines and cell states, we can interpret results in light of previously validated elements and in light of ENCODE human results. We will use ENCODE standards for high throughput, sequence-based assays to determine gene expression, DNase hypersensitive sites, histone modifications and selected transcription factor occupancy in seven mouse cell types. The eight selected features are the most informative ones for function, and thus most useful for comparison with human data. In Aim 2, we apply a genome-wide implementation of chromosome conformation capture to map the interactions between transcription factor binding sites and their responsive genes in two cell types. These results will be compared to those from an ENCODE developmental project. Comparative analysis in Aim 3 will insure that the impact of the data we produce will go beyond the individual mouse cell systems per se. To do this we have organized a collaboration of investigators at multiple institutions, in which each group is expert in one or more critical aspects. Our data, made public and accessible via ENCODE, will fuel and accelerate many future studies after the 2-yr stimulus both in and beyond ENCODE. This responds to NHGRI request for applications on ""Enhancement of the value of the human ENCODE Project by conducting a parallel effort on the mouse genome."" The proposed work will improve the maps of biologically functional DNA sequences in humans, which in turn will help explain how variants in human genome sequences could be associated with human diseases, leading to candidates for novel avenues for effective therapy and prevention.      PUBLIC HEALTH RELEVANCE:  Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.           Project Narrative for ""Enhance human ENCODE by functional comparisons to mouse"" Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.",Enhance human ENCODE by function comparisons to mouse,7940960,RC2HG005573,"['Adopted', 'BFU-E', 'Base Sequence', 'Binding', 'Binding Sites', 'Biochemical', 'Biochemical Markers', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cell model', 'Cells', 'Chromatin Structure', 'Chromosomes', 'Classification', 'Collaborations', 'Complement', 'Complex', 'Correlative Study', 'DNA', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Deoxyribonucleases', 'Development', 'Developmental Biology', 'Diabetes Mellitus', 'Disease', 'Disease susceptibility', 'Elements', 'Enhancers', 'Erythroblasts', 'Erythropoiesis', 'Event', 'Evolution', 'Frequencies', 'Future', 'GATA1 gene', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Goals', 'Harvest', 'Heart Diseases', 'Human', 'Human Cell Line', 'Human Genome', 'Human Genome Project', 'Indium', 'Individual', 'Institution', 'Investigation', 'Lead', 'Light', 'Location', 'Lymphocyte', 'Lymphoid', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Molecular', 'Molecular Conformation', 'Molecular Evolution', 'Molecular Genetics', 'Mouse Cell Line', 'Mus', 'Muscle Cells', 'Myelogenous', 'National Human Genome Research Institute', 'Nuclear', 'Persons', 'Phase', 'Phylogenetic Analysis', 'Predisposition', 'Prevention', 'Problem Solving', 'Process', 'Quality Control', 'Request for Applications', 'Research', 'Research Personnel', 'Sequence Analysis', 'Site', 'Sorting - Cell Movement', 'Staging', 'Stimulus', 'System', 'Technology', 'Time Study', 'Transcript', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Variant', 'Work', 'base', 'cell type', 'comparative', 'data mining', 'effective therapy', 'embryonic stem cell', 'genome sequencing', 'genome-wide', 'high standard', 'histone modification', 'human GATA1 protein', 'human data', 'human disease', 'improved', 'insight', 'mouse genome', 'novel', 'pathogen', 'promoter', 'public health relevance', 'response', 'restoration', 'transcription factor']",NHGRI,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,RC2,2010,749997,-0.02986189983876759
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,0.027316623148367004
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,0.027316623148367004
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7622614,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2009,1224323,0.058183627043252646
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7681225,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2009,351164,0.024848449341410478
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7656528,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2009,560000,0.02432260998204609
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7635337,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2009,330660,-0.0013860766109394915
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,7583730,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'public health relevance', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2009,450000,-0.004855155993885261
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7788875,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Ontology', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'empowered', 'genome wide association study', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'histone modification', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'public health relevance', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2009,142123,0.007664828891693777
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,0.013677855668336352
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7642231,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2009,922959,-0.004600786633304887
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,7854989,RC2HG005639,"['Animal Model', 'Animals', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Classification', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulation', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2009,1473507,-0.021305903200833316
"Enhance human ENCODE by function comparisons to mouse    DESCRIPTION (provided by applicant): Our goal is to discover and use relationships between mouse and human regulatory genomes to advance the ENCODE Project in its effort to map all functional elements in the human genome. Our comparative approach aims to uncover principles and solve problems that are proving difficult by studying the human genome alone. ENCODE is vigorously mapping hundreds of function-associated biochemical markers in selected cell lines, resulting already in tens of millions of reproducible biochemical features. Some observed protein:DNA interactions find and refine known transcriptional enhancers, promoters, silencers, together with associated chromatin structure, as was anticipated. But substantial questions arise as to how many of the myriad biochemical events are functional, what those functions are, which gene or genes are meaningful targets, etc. To highlight and sort functionally important biochemical marks from others, we will systematically identify the molecular events retained by both mouse and human since they diverged. We will then analyze how conservation of biochemical features relates to conservation of DNA sequence and conservation of regulated gene expression. By using the mouse, we can leverage decades of molecular genetics and manipulated mouse genomes that do not exist in any other mammal. In Aim 1 we execute genome-wide assays for biochemical signatures of functional DNA sequences in a few specific mouse cell types. By using well-studied mouse lines and cell states, we can interpret results in light of previously validated elements and in light of ENCODE human results. We will use ENCODE standards for high throughput, sequence-based assays to determine gene expression, DNase hypersensitive sites, histone modifications and selected transcription factor occupancy in seven mouse cell types. The eight selected features are the most informative ones for function, and thus most useful for comparison with human data. In Aim 2, we apply a genome-wide implementation of chromosome conformation capture to map the interactions between transcription factor binding sites and their responsive genes in two cell types. These results will be compared to those from an ENCODE developmental project. Comparative analysis in Aim 3 will insure that the impact of the data we produce will go beyond the individual mouse cell systems per se. To do this we have organized a collaboration of investigators at multiple institutions, in which each group is expert in one or more critical aspects. Our data, made public and accessible via ENCODE, will fuel and accelerate many future studies after the 2-yr stimulus both in and beyond ENCODE. This responds to NHGRI request for applications on ""Enhancement of the value of the human ENCODE Project by conducting a parallel effort on the mouse genome."" The proposed work will improve the maps of biologically functional DNA sequences in humans, which in turn will help explain how variants in human genome sequences could be associated with human diseases, leading to candidates for novel avenues for effective therapy and prevention.      PUBLIC HEALTH RELEVANCE:  Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.           Project Narrative for ""Enhance human ENCODE by functional comparisons to mouse"" Every person differs in his or her response to pathogens and in the likelihood that they will suffer from complex diseases such as cancer, heart disease or diabetes. Individual susceptibility to disease is determined in part by genetics, and we can map with high precision the locations of DNA variants associated with disease susceptibility. In order to understand how these variants contribute to disease susceptibility, we need to identify the biological functions of all DNA sequences; the proposed work will help us map these functional DNA sequences.",Enhance human ENCODE by function comparisons to mouse,7852369,RC2HG005573,"['Adopted', 'BFU-E', 'Base Sequence', 'Binding', 'Binding Sites', 'Biochemical', 'Biochemical Markers', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cell model', 'Cells', 'Chromatin Structure', 'Chromosomes', 'Classification', 'Collaborations', 'Complement', 'Complex', 'Correlative Study', 'DNA', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Deoxyribonucleases', 'Development', 'Developmental Biology', 'Diabetes Mellitus', 'Disease', 'Disease susceptibility', 'Elements', 'Enhancers', 'Erythroblasts', 'Erythropoiesis', 'Event', 'Evolution', 'Frequencies', 'Future', 'GATA1 gene', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Goals', 'Harvest', 'Heart Diseases', 'Human', 'Human Cell Line', 'Human Genome', 'Human Genome Project', 'Indium', 'Individual', 'Institution', 'Investigation', 'Lead', 'Light', 'Location', 'Lymphocyte', 'Lymphoid', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Molecular', 'Molecular Conformation', 'Molecular Evolution', 'Molecular Genetics', 'Mouse Cell Line', 'Mus', 'Muscle Cells', 'Myelogenous', 'National Human Genome Research Institute', 'Nuclear', 'Persons', 'Phase', 'Phylogenetic Analysis', 'Predisposition', 'Prevention', 'Problem Solving', 'Process', 'Quality Control', 'Request for Applications', 'Research', 'Research Personnel', 'Sequence Analysis', 'Site', 'Sorting - Cell Movement', 'Staging', 'Stimulus', 'System', 'Technology', 'Time Study', 'Transcript', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Variant', 'Work', 'base', 'cell type', 'comparative', 'data mining', 'effective therapy', 'embryonic stem cell', 'genome sequencing', 'genome-wide', 'high standard', 'histone modification', 'human GATA1 protein', 'human data', 'human disease', 'improved', 'insight', 'mouse genome', 'novel', 'pathogen', 'promoter', 'public health relevance', 'response', 'restoration', 'transcription factor']",NHGRI,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,RC2,2009,750003,-0.02986189983876759
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,0.027316623148367004
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,0.027316623148367004
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7499147,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Depth', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Numbers', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'foot', 'insight', 'member', 'novel', 'quality assurance', 'scale up', 'size', 'symposium', 'theories', 'tool']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2008,1200000,0.058183627043252646
"Exon recognition during constitutive pre-mRNA splicing    DESCRIPTION (provided by applicant): The splicing together of exons during the transcription of most eukaryotic pre-mRNA molecules is a fundamental step in the transfer of information from DNA to protein. However, how the splice sites are recognized to initiate this fast and accurate process in not at all clear, since false sequences that resemble real sites outnumber the latter by 1 or 2 orders of magnitude yet are not used. We are searching for the additional information that specifies splice site recognition during pre-mRNA splicing in mammalian cells. In previous work we have used computational analysis to discover that: 1) that exon flanks 50 nt beyond the splice site consensuses can greatly influence splicing and contain sequences that stand out from the background; and 2) there are a large number of specific 8 mers that can act either as exonic splicing enhancers or splicing silencers. We will characterize both of these types of sequences, defining their base requirements, extent, position dependence, and specificity. With regard to the last, we will develop a novel exhaustive survey method that uses massively parallel solid state sequencing to identify every possible 8-mer that can be functional in a particular context, and we will compare this profile in different intronic, exonic and cellular contexts for constitutive and alternatively spliced exons. The flanking sequences fall into 2 distinct classes according to their G+C contents; we will test the idea that the rules governing exon definition differ for the two classes. To simplify the discovery of rules that govern exon definition, we will create designer exons made up of synthetic 8-mer modules we have identified as enhancers, silencers or neutral sequences. The numbers and spacing of these modules will reveal relationships that are hidden in the complexity of natural sequences. Finally, we will continue to use computation, including new machine learning algorithms, to better define intronic elements and to discover interactions between two or more features of a local sequence that define a splice site. We will also develop a Web-based exon-finding program that integrates all the information we have accumulated. Pre-mRNA splicing goes awry in a large proportion or human genetic disease cases. An understanding of the rules that govern the recognition of splice sites should help in the design of therapeutic intervention strategies to reverse such ill effects.                           n/a",Exon recognition during constitutive pre-mRNA splicing,7469442,R01GM072740,"['Algorithms', 'Alternative Splicing', 'Class', 'Computer Analysis', 'Consensus', 'DNA', 'Dependence', 'Elements', 'Enhancers', 'Exons', 'Genetic Transcription', 'Guanine + Cytosine Composition', 'Hereditary Disease', 'Heterogeneous Nuclear RNA', 'Human', 'Intervention', 'Introns', 'Machine Learning', 'Mammalian Cell', 'Numbers', 'Online Systems', 'Positioning Attribute', 'Process', 'Proteins', 'RNA Splicing', 'Role', 'Signal Transduction', 'Site', 'Specific qualifier value', 'Specificity', 'Structure', 'Survey Methodology', 'Surveys', 'Testing', 'Therapeutic Intervention', 'Work', 'base', 'design', 'falls', 'genetic selection', 'mRNA Precursor', 'novel', 'prevent', 'programs', 'size', 'solid state']",NIGMS,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2008,291756,0.0002493620909199249
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7522602,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Class', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pan troglodytes', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Rate', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2008,369424,0.024848449341410478
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7477279,R01LM005652,"['Active Sites', 'Algorithms', 'Appendix', 'Area', 'Basic Science', 'Binding', 'Binding Sites', 'Biochemical', 'Biological', 'Biology', 'Catalysis', 'Communities', 'Computer software', 'Crystallography', 'Data', 'Data Collection', 'Databases', 'Development', 'Elements', 'Engineering', 'Evaluation', 'Failure', 'Growth', 'Informatics', 'Internet', 'Libraries', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Neighborhoods', 'Online Systems', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'Proteomics', 'Resolution', 'Retrieval', 'Seeds', 'Site', 'Speed', 'Statistical Methods', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'Work', 'base', 'design', 'follow-up', 'functional genomics', 'genome sequencing', 'high throughput technology', 'improved', 'indexing', 'macromolecule', 'novel', 'open source', 'programs', 'size', 'structural biology', 'structural genomics', 'success', 'technology development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vector']",NLM,STANFORD UNIVERSITY,R01,2008,338976,0.0009688887742562629
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,0.013677855668336352
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7538527,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'GDF15 gene', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Histones', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Modification', 'Numbers', 'Ontology', 'PLAB Protein', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'RNA', 'Range', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2008,157474,0.007664828891693777
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,-0.0012096611921993176
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7282065,R01LM005652,"['Active Sites', 'Algorithms', 'Appendix', 'Area', 'Basic Science', 'Binding', 'Binding Sites', 'Biochemical', 'Biological', 'Biology', 'Catalysis', 'Communities', 'Computer software', 'Crystallography', 'Data', 'Data Collection', 'Databases', 'Development', 'Elements', 'Engineering', 'Evaluation', 'Failure', 'Growth', 'Informatics', 'Internet', 'Libraries', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Neighborhoods', 'Online Systems', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'Proteomics', 'Resolution', 'Retrieval', 'Seeds', 'Site', 'Speed', 'Statistical Methods', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'Work', 'base', 'design', 'follow-up', 'functional genomics', 'genome sequencing', 'high throughput technology', 'improved', 'indexing', 'macromolecule', 'novel', 'open source', 'programs', 'size', 'structural biology', 'structural genomics', 'success', 'technology development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vector']",NLM,STANFORD UNIVERSITY,R01,2007,355458,0.0009688887742562629
"A Comprehensive catalog of human DNasel hypersensitive sites    DESCRIPTION (provided by applicant):   The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNaseI hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNaseI hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5).           n/a",A Comprehensive catalog of human DNasel hypersensitive sites,7410206,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Class', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonuclease I', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Histones', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Modification', 'Molecular', 'Noise', 'Numbers', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Preparation', 'Production', 'Public Domains', 'Range', 'Rate', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Score', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Standards of Weights and Measures', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'density', 'design', 'digital', 'experience', 'functional genomics', 'high throughput screening', 'human tissue', 'in vivo', 'insight', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2007,3114596,-0.00296441902697059
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,0.013677855668336352
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,0.027316623148367004
"Exon recognition during constitutive pre-mRNA splicing    DESCRIPTION (provided by applicant): The splicing together of exons during the transcription of most eukaryotic pre-mRNA molecules is a fundamental step in the transfer of information from DNA to protein. However, how the splice sites are recognized to initiate this fast and accurate process in not at all clear, since false sequences that resemble real sites outnumber the latter by 1 or 2 orders of magnitude yet are not used. We are searching for the additional information that specifies splice site recognition during pre-mRNA splicing in mammalian cells. In previous work we have used computational analysis to discover that: 1) that exon flanks 50 nt beyond the splice site consensuses can greatly influence splicing and contain sequences that stand out from the background; and 2) there are a large number of specific 8 mers that can act either as exonic splicing enhancers or splicing silencers. We will characterize both of these types of sequences, defining their base requirements, extent, position dependence, and specificity. With regard to the last, we will develop a novel exhaustive survey method that uses massively parallel solid state sequencing to identify every possible 8-mer that can be functional in a particular context, and we will compare this profile in different intronic, exonic and cellular contexts for constitutive and alternatively spliced exons. The flanking sequences fall into 2 distinct classes according to their G+C contents; we will test the idea that the rules governing exon definition differ for the two classes. To simplify the discovery of rules that govern exon definition, we will create designer exons made up of synthetic 8-mer modules we have identified as enhancers, silencers or neutral sequences. The numbers and spacing of these modules will reveal relationships that are hidden in the complexity of natural sequences. Finally, we will continue to use computation, including new machine learning algorithms, to better define intronic elements and to discover interactions between two or more features of a local sequence that define a splice site. We will also develop a Web-based exon-finding program that integrates all the information we have accumulated. Pre-mRNA splicing goes awry in a large proportion or human genetic disease cases. An understanding of the rules that govern the recognition of splice sites should help in the design of therapeutic intervention strategies to reverse such ill effects.                           n/a",Exon recognition during constitutive pre-mRNA splicing,7261326,R01GM072740,[' '],NIGMS,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2007,290853,0.0002493620909199249
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,7120160,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2006,414036,0.02152635003569585
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7117852,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2006,326259,0.0009688887742562629
"Exon recognition during constitutive pre-mRNA splicing    DESCRIPTION (provided by applicant): The splicing together of exons during the transcription of most eukaryotic pre-mRNA molecules is a fundamental step in the transfer of information from DNA to protein. However, how the splice sites are recognized to initiate this fast and accurate process in not at all clear, since false sequences that resemble real sites outnumber the latter by 1 or 2 orders of magnitude yet are not used. We are searching for the additional information that specifies splice site recognition during pre-mRNA splicing in mammalian cells. In previous work we have used computational analysis to discover that: 1) that exon flanks 50 nt beyond the splice site consensuses can greatly influence splicing and contain sequences that stand out from the background; and 2) there are a large number of specific 8 mers that can act either as exonic splicing enhancers or splicing silencers. We will characterize both of these types of sequences, defining their base requirements, extent, position dependence, and specificity. With regard to the last, we will develop a novel exhaustive survey method that uses massively parallel solid state sequencing to identify every possible 8-mer that can be functional in a particular context, and we will compare this profile in different intronic, exonic and cellular contexts for constitutive and alternatively spliced exons. The flanking sequences fall into 2 distinct classes according to their G+C contents; we will test the idea that the rules governing exon definition differ for the two classes. To simplify the discovery of rules that govern exon definition, we will create designer exons made up of synthetic 8-mer modules we have identified as enhancers, silencers or neutral sequences. The numbers and spacing of these modules will reveal relationships that are hidden in the complexity of natural sequences. Finally, we will continue to use computation, including new machine learning algorithms, to better define intronic elements and to discover interactions between two or more features of a local sequence that define a splice site. We will also develop a Web-based exon-finding program that integrates all the information we have accumulated. Pre-mRNA splicing goes awry in a large proportion or human genetic disease cases. An understanding of the rules that govern the recognition of splice sites should help in the design of therapeutic intervention strategies to reverse such ill effects.                           n/a",Exon recognition during constitutive pre-mRNA splicing,7105514,R01GM072740,"['Internet', 'RNA splicing', 'binding sites', 'computational biology', 'computer system design /evaluation', 'genetic regulation', 'messenger RNA', 'molecular biology information system', 'molecular shape', 'nucleic acid sequence', 'polymerase chain reaction', 'serial analysis of gene expression', 'statistics /biometry']",NIGMS,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2006,298611,0.0002493620909199249
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,0.02387947288373474
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6908174,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2005,375000,0.0033932974500853956
"BioHDF - Open Binary File Standards for Bioinformatics DESCRIPTION (provided by applicant):  Geospiza Inc. and the National Center for Supercomputing Applications (NCSA) are creating a standards based software framework around NCSA's Heirarchical Data Format (HDF5). The envisioned framework will integrate algorithms important in DNA and protein sequence analysis to create scalable high throughput software systems which will be accessed using new graphical user interfaces (GUIs) to provide researchers with new views of their data to finish sequencing projects in large-scale genome sequencing, microbial genome sequencing, viral epidemiology, polymorphism detection, phylogenetic analysis, multi-locus sequence typing, confirmatory sequencing, and EST analysis.    In our vision, algorithms will be either integrated into the system to directly read and write from HDF5 project files, or they will communicate with project files via filter programs that produce standardized XML formatted data. Through this model, a scalable solution will support different applications of DNA sequencing, fulfilling the many needs and requirements expressed by the medical research community now and into the future. As the first step in this process we will, define requirements for editing and versioning data in DNA sequencing, research and propose data models for the computational phases of DNA sequencing and annotating DNA sequence data using existing standards, create a prototype application for DNA sequencing based SNP discovery, and engage the bioinformatics community for BioHDF adoption.       In the past ten years the cost of sequencing DNA has dropped over 1000 fold and the amount of raw sequence data, entering our national repositories is doubling every 12 months. DNA sequencing is fundamental to biological research activities such as genomics, systems biology, and clinical medicine. Proposals are being sought to decrease sequencing costs by two orders of magnitude through technology refinements with an ultimate vision of developing technology to sequence human genome equivalents for $1000 each. The amount of data that will be produced through these endeavors is unimaginable. However, the $1,000 genome will not advance medical research unless we integrate all phases of the DNA sequencing process and treat the creation, management, finishing, analysis, and sharing of the data as common goals. n/a",BioHDF - Open Binary File Standards for Bioinformatics,6992995,R41HG003792,"['DNA', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'functional /structural genomics', 'genetic mapping', 'genetic polymorphism', 'mathematics', 'molecular biology information system', 'nucleic acid sequence', 'single nucleotide polymorphism', 'virus genetics']",NHGRI,"GEOSPIZA, INC.",R41,2005,142775,0.03658846571975786
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6952028,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2005,412000,0.02152635003569585
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,6942980,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2005,324673,0.0009688887742562629
"Exon recognition during constitutive pre-mRNA splicing    DESCRIPTION (provided by applicant): The splicing together of exons during the transcription of most eukaryotic pre-mRNA molecules is a fundamental step in the transfer of information from DNA to protein. However, how the splice sites are recognized to initiate this fast and accurate process in not at all clear, since false sequences that resemble real sites outnumber the latter by 1 or 2 orders of magnitude yet are not used. We are searching for the additional information that specifies splice site recognition during pre-mRNA splicing in mammalian cells. In previous work we have used computational analysis to discover that: 1) that exon flanks 50 nt beyond the splice site consensuses can greatly influence splicing and contain sequences that stand out from the background; and 2) there are a large number of specific 8 mers that can act either as exonic splicing enhancers or splicing silencers. We will characterize both of these types of sequences, defining their base requirements, extent, position dependence, and specificity. With regard to the last, we will develop a novel exhaustive survey method that uses massively parallel solid state sequencing to identify every possible 8-mer that can be functional in a particular context, and we will compare this profile in different intronic, exonic and cellular contexts for constitutive and alternatively spliced exons. The flanking sequences fall into 2 distinct classes according to their G+C contents; we will test the idea that the rules governing exon definition differ for the two classes. To simplify the discovery of rules that govern exon definition, we will create designer exons made up of synthetic 8-mer modules we have identified as enhancers, silencers or neutral sequences. The numbers and spacing of these modules will reveal relationships that are hidden in the complexity of natural sequences. Finally, we will continue to use computation, including new machine learning algorithms, to better define intronic elements and to discover interactions between two or more features of a local sequence that define a splice site. We will also develop a Web-based exon-finding program that integrates all the information we have accumulated. Pre-mRNA splicing goes awry in a large proportion or human genetic disease cases. An understanding of the rules that govern the recognition of splice sites should help in the design of therapeutic intervention strategies to reverse such ill effects.                           n/a",Exon recognition during constitutive pre-mRNA splicing,6966860,R01GM072740,"['Internet', 'RNA splicing', 'binding sites', 'computational biology', 'computer system design /evaluation', 'genetic regulation', 'messenger RNA', 'molecular biology information system', 'molecular shape', 'nucleic acid sequence', 'polymerase chain reaction', 'serial analysis of gene expression', 'statistics /biometry']",NIGMS,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2005,304845,0.0002493620909199249
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6912979,R44HG002244,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data management', 'genotype', 'informatics', 'mathematical model', 'nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2004,191986,0.048600220067441594
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6788945,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2004,375000,0.0033932974500853956
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6737944,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2004,400000,0.02152635003569585
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,6825272,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2004,396367,0.0009688887742562629
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6622259,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2003,560392,0.048600220067441594
"Development of Bioinformatic Tools for Virtual Cloning    DESCRIPTION (provided by applicant): The ability to delineate (at least in theory) all the proteins encoded in the human genome and all of those encoded by the genomes of major human parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease.  However, the vast increase in biological knowledge that has resulted from the last decade of genomic DNA sequencing has led us to a a crisis in bioinformatics.  This crisis is two-fold: analysis of data and planning of experiments.  Most of the scientific resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology.  All modern experimental molecular biology (and, increasingly, structural biology) depends upon the availability of plasmid clones to address specific scientific questions.  Although software facilitating DNA manipulations exists, few programs advise users of optimal strategies and none automate the process of clone generation. Genomics initiatives identify proteins at the genome level and demand the generation of hundreds of expression clones for recombinant protein production in exogenous hosts such as E. coli.  Establishment of libraries of expression clones requires automation and optimization as well as effective means of data storage, archiving, annotation and query.  To address these needs, as well as to facilitate routine DNA manipulations in virtually any molecular biology laboratory, we propose (1) to test and build a task centered virtual cloning expert system that serves as a knowledge base for DNA manipulations, and (2) to test and build an information automaton for the construction of expression clone libraries in support of structural genomics initiatives and other high throughput experiments.         n/a",Development of Bioinformatic Tools for Virtual Cloning,6583437,R43GM067279,"['artificial intelligence', ' biomedical automation', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' expression cloning', ' gene expression', ' genetic library', ' genetic manipulation', ' informatics', ' molecular biology information system', ' transfection /expression vector']",NIGMS,"VIRMATICS, LLC",R43,2003,100000,0.01893563214729457
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6444292,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2002,531259,0.048600220067441594
"GENE PREDICTION: MARKOV MODELS AND COMPLEMENTARY METHODS   DESCRIPTION (Adapted from the Investigator's Abstract): The goal of the project                                               is to build more accurate and powerful DNA sequence interpretation algorithms        utilizing the positive experience and ideas of previously proven GeneMark and        GenMark.hmm methods.                                                                                                                                                      We plan to improve the quality of gene finding in prokaryotic genomes in terms       of reliable and accurate prediction of gene starts and detection of frameshift                                                   sequencing errors. We also plan to develop a machine-learning iterative              procedure for deriving all necessary models for precise gene                         prediction/annotation from totally anonymous prokaryotic sequences.                                                                                                       For eukaryotic species, we will improve the accuracy of the ab initio method         GeneMark.hmm by building more accurate models for splice sites and                   initiation/termination sites, and we will address the problem of accurately          finding intergenic regions with polyadenilation sites and promoters.                                                                                                      On the basis of GeneMark.hmm, we plan to develop an integrated gene finding          approach by ""projecting"" pieces of diverse extrinsic evidence into DNA level,        the translating them into DNA patterns and combining these patterns with             statistical patterns of DNA coding and non-coding sequence within a generalized      HMM model. The most intriguing sources of this additional information are            evolutionary conserved regions in DNA sequences of closely related species,          functional motifs in protein sequences and protein sequence patterns reflecting      three dimensional structural motifs.                                                                                                                                      All these newly developed methods, as well as several others mentioned in the        proposal, will deal with anonymous DNA for which interpretation is increasingly      needed in the post-genomic era.                                                                                                                                           n/a",GENE PREDICTION: MARKOV MODELS AND COMPLEMENTARY METHODS,6536458,R01HG000783,"['DNA', ' RNA splicing', ' computer assisted sequence analysis', ' computer program /software', ' computer system design /evaluation', ' eukaryote', ' frameshift mutation', ' genetic mapping', ' introns', ' mathematical model', ' model design /development', ' nucleic acid sequence', ' protein sequence', ' statistics /biometry']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2002,318645,-0.011791179457195624
"Functional Genomics Software   DESCRIPTION (Applicant's abstract): A substantial commercial potential exists        for software tools that allow a biomedical research scientist to use genomic         data to form experimentally testable hypotheses. These will be used to exploit       genomic sequence data to understand the aetiology of disease, to improve             diagnostic tools, and to develop more effective therapies. The Master Catalog,       a commercial product developed jointly by EraGen Biosciences and the Benner          laboratory at the University of Florida, provides a convenient framework for         implementing heuristics that do this. The Master Catalog is a naturally              organized database that contains evolutionary trees, multiple sequence               alignments, and reconstructed evolutionary intermediates for all of the              proteins in the GenBank database. The Benner laboratory has developed and            anecdotally tested heuristics that date events in the molecular history,             provide evidence for and against functional recruitment within a protein             family, detect distant homologs, associate individual residues important for         functional changes with a crystal structure, find metabolic and regulatory           pathways, and correlate events in the molecular record with the history of life      on Earth. This Phase I proposal seeks to validate a set of these heuristics          more broadly to determine their suitability for database-wide application. In        Phase II, we will implement these within the Master Catalog, and launch a            commercial bioinformatics product to support functional analysis of genomic          databases.                                                                           PROPOSED COMMERCIAL APPLICATION:  In its present version, the Master Catalog is a successful commercial product within a  niche: ""best in class"" of bioinformatics databases.  Adding a validated set of heuristics  for extracting functional information from genome databases will make it the software  of choice for most functional genomics work, and be a central tool in the pharmaceutical/  biotechnology industries.  Academic versions and student versions will find markets in most  universities.                                                                                      n/a",Functional Genomics Software,6337786,R41HG002331,"['artificial intelligence', ' biochemical evolution', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' functional /structural genomics', ' informatics', ' molecular biology information system', ' nucleic acid sequence']",NHGRI,"ERAGEN BIOSCIENCES, INC.",R41,2001,96855,0.0001359166813941507
"GENE PREDICTION: MARKOV MODELS AND COMPLEMENTARY METHODS   DESCRIPTION (Adapted from the Investigator's Abstract): The goal of the project                                               is to build more accurate and powerful DNA sequence interpretation algorithms        utilizing the positive experience and ideas of previously proven GeneMark and        GenMark.hmm methods.                                                                                                                                                      We plan to improve the quality of gene finding in prokaryotic genomes in terms       of reliable and accurate prediction of gene starts and detection of frameshift                                                   sequencing errors. We also plan to develop a machine-learning iterative              procedure for deriving all necessary models for precise gene                         prediction/annotation from totally anonymous prokaryotic sequences.                                                                                                       For eukaryotic species, we will improve the accuracy of the ab initio method         GeneMark.hmm by building more accurate models for splice sites and                   initiation/termination sites, and we will address the problem of accurately          finding intergenic regions with polyadenilation sites and promoters.                                                                                                      On the basis of GeneMark.hmm, we plan to develop an integrated gene finding          approach by ""projecting"" pieces of diverse extrinsic evidence into DNA level,        the translating them into DNA patterns and combining these patterns with             statistical patterns of DNA coding and non-coding sequence within a generalized      HMM model. The most intriguing sources of this additional information are            evolutionary conserved regions in DNA sequences of closely related species,          functional motifs in protein sequences and protein sequence patterns reflecting      three dimensional structural motifs.                                                                                                                                      All these newly developed methods, as well as several others mentioned in the        proposal, will deal with anonymous DNA for which interpretation is increasingly      needed in the post-genomic era.                                                                                                                                           n/a",GENE PREDICTION: MARKOV MODELS AND COMPLEMENTARY METHODS,6388304,R01HG000783,"['DNA', ' RNA splicing', ' computer assisted sequence analysis', ' computer program /software', ' computer system design /evaluation', ' eukaryote', ' frameshift mutation', ' genetic mapping', ' introns', ' mathematical model', ' model design /development', ' nucleic acid sequence', ' protein sequence', ' statistics /biometry']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2001,318645,-0.011791179457195624
"GENE PREDICTION--MARKOV MODELS AND COMPLEMENTARY METHODS   DESCRIPTION (Adapted from the Investigator's Abstract): The goal of the project                                               is to build more accurate and powerful DNA sequence interpretation algorithms        utilizing the positive experience and ideas of previously proven GeneMark and        GenMark.hmm methods.                                                                                                                                                      We plan to improve the quality of gene finding in prokaryotic genomes in terms       of reliable and accurate prediction of gene starts and detection of frameshift                                                   sequencing errors. We also plan to develop a machine-learning iterative              procedure for deriving all necessary models for precise gene                         prediction/annotation from totally anonymous prokaryotic sequences.                                                                                                       For eukaryotic species, we will improve the accuracy of the ab initio method         GeneMark.hmm by building more accurate models for splice sites and                   initiation/termination sites, and we will address the problem of accurately          finding intergenic regions with polyadenilation sites and promoters.                                                                                                      On the basis of GeneMark.hmm, we plan to develop an integrated gene finding          approach by ""projecting"" pieces of diverse extrinsic evidence into DNA level,        the translating them into DNA patterns and combining these patterns with             statistical patterns of DNA coding and non-coding sequence within a generalized      HMM model. The most intriguing sources of this additional information are            evolutionary conserved regions in DNA sequences of closely related species,          functional motifs in protein sequences and protein sequence patterns reflecting      three dimensional structural motifs.                                                                                                                                      All these newly developed methods, as well as several others mentioned in the        proposal, will deal with anonymous DNA for which interpretation is increasingly      needed in the post-genomic era.                                                                                                                                           n/a",GENE PREDICTION--MARKOV MODELS AND COMPLEMENTARY METHODS,6286238,R01HG000783,"['DNA', ' RNA splicing', ' computer assisted sequence analysis', ' computer program /software', ' computer system design /evaluation', ' eukaryote', ' frameshift mutation', ' genetic mapping', ' introns', ' mathematical model', ' model design /development', ' nucleic acid sequence', ' protein sequence', ' statistics /biometry']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2000,414664,-0.011791179457195624
"COMPUTER BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION The goal of the proposed research is the analysis of biological sequence         data to address the molecular mechanisms of evolution and the origin(s)          of all viruses and related genetic elements. Phylogenetic trees will             provide a framework for the mapping of cell and tissue tropism,                  pathogenicity and virulence, modes of transmission and geographical              distributions, and many other higher order characteristics of viruses.           The specific aims of proposed analytical studies are: 1) determining             functionally equivalent networks and frequency of exchange among and             between retroid elements, and their potential cellular homologues,               including new studies on 300 retroviral env proteins; 2) inferring               functionally important regions of all proteins of paramyxo-, rhabdo- and         filoviruses, (with privileged access to new Ebola sequences), and Borna          Disease virus, (including potential BDV sequences from schizophrenic             patients); and 3) the analysis of the dUTPase gene, as a model system,           to address issues relevant to the structure, function and evolution of           duplicated sequences, and potential horizontal transfer among and between        host and viral genomes. The specific aims of the technical studies are:          1) evaluation of stochastic production model approaches for generation           of multiple alignments, detection of recombination, and calculation of           evolutionary distances; and 2) development and testing of new and                existing methods for historical reconstruction of functionally equivalent        networks.                                                                                                                                                         RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,        animal and plant viral diseases world wide. The heterogeneous nature of          RNA populations makes it difficult to develop effective, anti-viral              agents. The sequence database is now large enough to conduct comparative         studies on natural variants versus chemotheraputically induced mutants           for several retroviral proteins. This model study will provide new               information on the nature of selected mutations which will be useful in          future anti-viral drug development.                                                                                                                               Computational analysis of primary sequence data is an area of intense            interest in biology, mathematics, statistics and systems science. In the         last few years new approaches to problem solving and classification, such        as machine learning, neural networks, genetic algorithms, and stochastic         production models or, ""intelligent systems"" as they are referred to              collectively, have become available. Unfortunately most biologists are           unaware of these developments. Application of these methods to real data         remains unexplored. The proposed studies will go a long way in rectifying        this gap in technological utilization. These studies will continue to            define important evolutionary relationships and events, provide                  biologically informative sequence relationships for bench-marking new            software, and contribute new information relevant to the structure and           function of viral proteins suggesting new directions in laboratory               experimentation. Strategies and techniques developed for the analysis of         highly divergent genomes can also be applied to the study of the wealth          of sequence information generated under the auspices of the Human Genome         Project.                                                                          n/a",COMPUTER BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION,6163865,R01AI028309,"['DNA replication', ' Mononegavirales', ' RNA biosynthesis', ' biochemical evolution', ' computer assisted sequence analysis', ' computer program /software', ' nucleic acid sequence', ' virus genetics', ' virus protein']",NIAID,MONTANA STATE UNIVERSITY (BOZEMAN),R01,2000,225156,-0.009888273827606141
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,10020995,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2020,287504,-0.004165920361746578
"Development of a joint machine learning/de novo assembly system for resolving viral quasispecies PROJECT SUMMARY Viral hepatitis from hepatitis B (HBV) establishes chronic infections in >250M people worldwide; chronicity is on the rise, and approximately one-third of the worldâs population (2 billion) has serologic evidence of exposure. HBV coinfection with HCV and HIV is a hidden consequence of the substance use disorder epidemic. Viral populations have extremely high sequence diversity and rapidly evolve, which explains the vaccine failure rates and viral resistance to existing therapies and makes discovering lasting therapies extremely challenging. Next Generation Sequencing (NGS) is the method of choice to assess the intra-host virus population, termed a âquasispeciesâ. While a large set of short DNA sequencing reads are acquired that represent the virions in the quasispecies, computational technologies are limited in their analysis capabilities, resulting in particularly low resolution of complex HBV genomic structures. Another challenge is assembling NGS reads representing short fragment of the host genome into full strains (haplotypes) without knowledge of their true occurrence in the samples. To meet these challenges, GATACA is developing pathogen-specific bioinformatics software, GAT-ML (GATACA Assembly Tool â machine learning [ML]) to support treatment discovery and improve infection control. Its specifically designed algorithm utilizes novel ML methodologies adapted and modified for assisting genome assembly that will allow GAT-ML to reconstruct complete viral haplotypes and populations by learning the âlanguageâ of the sequences. Tailored initially for HBV samples, GAT and its new ML system will be integrated for feasibility testing in this Phase I with the following Specific Aims: 1. Specific Aim 1. Build a joint learning system. Train and test natural language processing (NLP) methods on HBV genetic variation. 2. Specific Aim 2. Implement and test the machine learning methods in GAT (GAT-ML). We anticipate a working tool for characterizing HBV haplotypes, validated with multi-sourced datasets, and extensive testing and benchmarking of offline and integrated methods. The proposed project will develop and increase the capabilities of our novel computational tool, GAT, to help researchers identify the full spectrum of genetic features of a viral populationâsuch as emergence and persistence of resistance or baseline polymorphisms regardless of their frequenciesâand translate these findings to the development of new or improved antiviral drugs and other applications requiring high analytic sensitivity. GAT will particularly benefit researchers working in preclinical stages of drug development who require rapid, sensitive, and reliable results to inform decisions about which targets to advance to clinical trial testing.",Development of a joint machine learning/de novo assembly system for resolving viral quasispecies,10011686,R43AI152894,"['Adoption', 'Algorithm Design', 'Algorithms', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Chronic', 'Chronic Hepatitis', 'Classification', 'Clinical Trials', 'Complex', 'Computer software', 'DNA Structure', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Epidemic', 'Failure', 'Frequencies', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'HIV', 'HIV/HCV', 'Haplotypes', 'Healthcare', 'Hepatitis B', 'Hepatitis B Virus', 'Infection Control', 'Joints', 'Knowledge', 'Language', 'Language Development', 'Learning', 'Link', 'Liver diseases', 'Machine Learning', 'Metagenomics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Natural Language Processing', 'Outcome', 'Pattern', 'Performance', 'Phase', 'Population', 'Population Analysis', 'Privatization', 'Research Personnel', 'Resistance', 'Resolution', 'Sampling', 'Semantics', 'Serological', 'Serotyping', 'Source', 'Speed', 'Substance Use Disorder', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translating', 'Trust', 'Vaccines', 'Validation', 'Variant', 'Viral', 'Viral hepatitis', 'Virion', 'Virus', 'base', 'chronic infection', 'co-infection', 'commercialization', 'computerized tools', 'contig', 'design', 'drug development', 'improved', 'insertion/deletion mutation', 'machine learning algorithm', 'machine learning method', 'multiple data sources', 'neural network', 'next generation sequencing', 'novel', 'pathogen', 'pre-clinical', 'structural genomics', 'syntax', 'tool', 'viral resistance']",NIAID,"GATACA, LLC",R43,2020,267225,-0.0001043275173063116
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9957109,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Techniques', 'Time', 'Variant', 'Work', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'machine learning method', 'malaria infection', 'malaria mosquito', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'supervised learning', 'support vector machine', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,UNIVERSITY OF OREGON,R01,2020,295000,0.04104566991861767
"Center for Critical Assessment of Genome Interpretation Genomic data hold the promise of revolutionizing our understanding and treatment of human disease. Multiple barriers stand between the acquisition of the data and realizing these and other benefits. Rapid accumulation of genomic data far exceeds our capacity to reliably interpret genomic variation. New developments in artificial intelligence and machine learning, combined with increased computing power and domain knowledge, provide hope for the deployment of enhanced computational tools in both basic research and clinical practice. Use of these methods critically depends upon reliable characterization of their performance.  The Center for Critical Assessment of Genome Interpretation (C-CAGI) will address these needs, through objective evaluation of the state of the art in relating human genetic variation and health. CAGI has had five editions since 2010 with 50 challenges posed to the community taken on by hundreds of predictors, leading to scores of publications about prediction methods and their assessment. We propose for C-CAGI to continue to advance the field of variant interpretation through the following Specific Aims: 1. Develop community experiments to evaluate the quality of computational methods for interpreting genomic variation data. C-CAGI will conduct community experiments in which participants make bona fide blinded predictions of disease related phenotypes on the basis of genomic data. We will engage a diverse predictor community to spur innovation. The CAGI Ethics Forum will vet studies to ensure that privacy and sharing maintain the highest standards and will educate the community. 2. Assess the quality of current computational methods for interpreting genomic variation data; highlight innovations and progress at interactive conferences. Predictions will be evaluated by independent assessors, who will be supported by new assessment approaches from C-CAGI. Results will be presented at CAGI experiment conferences with deep technical engagement, which will be interleaved with reflective CAGIÃ¢ï¢ meetings that create an environment for a comprehensive evaluation of the field, facilitating identification of major bottlenecks and problems faced by the current genome interpretation approaches. 3. Broadly disseminate the results and conclusions from the CAGI experiments and analysis. C-CAGI will outreach to the broader scientific and clinical community through its publications, and the creation of a calibrated reference integrated into the most common workflows for ready adoption. CAGI will also be represented at international meetings with presentations and workshops. 4. Operate effectively and responsively. C-CAGI will operate efficiently as it closely interacts with hundreds of participants. CAGI will build upon a robust information infrastructure that securely facilitates data dissemination, prediction submission, and assessment. Genomic variation is responsible for numerous rare diseases, for propensity for many common traits and diseases, for drug response, and is a key characteristic of cancer evolution. At present, our ability to characterize genetic differences far exceeds our capacity to interpret them either for basic research understanding or for clinical application. The Center for Critical Assessment of Genome Interpretation, operating on robust ethical foundations, will provide an evaluation of the current state of the art and help promote progress in understanding the impact of genomic variation.",Center for Critical Assessment of Genome Interpretation,9937546,U24HG007346,"['Address', 'Adoption', 'Affect', 'Amino Acid Sequence', 'Artificial Intelligence', 'Basic Science', 'Blinded', 'Characteristics', 'Clinical', 'Communities', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Ensure', 'Environment', 'Ethics', 'Evaluation', 'Evolution', 'Foundations', 'Genetic', 'Genetic Variation', 'Genome', 'Health', 'Human Genetics', 'Infrastructure', 'International', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Nucleotides', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Privacy', 'Provider', 'Publications', 'RNA Splicing', 'Rare Diseases', 'Secure', 'Structure', 'Trust', 'Variant', 'Work', 'base', 'clinical application', 'clinical practice', 'computerized tools', 'data acquisition', 'data dissemination', 'exome', 'experimental study', 'genetic information', 'genetic variant', 'genomic data', 'genomic variation', 'high standard', 'human disease', 'innovation', 'meetings', 'multiple omics', 'operation', 'outreach', 'response', 'symposium', 'trait', 'whole genome']",NHGRI,UNIVERSITY OF CALIFORNIA BERKELEY,U24,2020,314933,0.014354012026042353
"Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders The purpose of this training and research application is to study the functional impact of mobile element insertions (MEIs) in neurological disorders (NDs) using new developments in deep learning techniques. MEIs are transposable DNA fragments that are able to insert throughout the human genome. There are at least 124 independent MEIs associated with human diseases. Approximately 20% of these diseases represent a spectrum of NDs, yet the overall contribute of MEIs to the etiology of NDs has not been systematically estimated. To address this, we will (1) characterize functional MEIs in GTEx cohorts in healthy individuals; (2) build a comprehensive functional map of MEIs to determine tissue-specific and brain-specific impact; and (3) impute transcriptional changes on various NDs where whole-genome sequencing (WGS) data will be generated. The proposed application will also develop an extensive research program for Dr. Dadi Gao, a computational biologist and statistical geneticist who has trained in functional genomic studies of alternative splicing in neurodegenerative disorders and therapeutic targeting of a splicing defect that causes a severe neurodevelopmental disorder. He has developed novel methods to investigate regulation of the transcriptome and to facilitate analyses in drug development. He now seeks to expand his expertise by applying statistical and deep learning models on large cohorts of sequencing data from controls and cases with NDs from post-mortem tissues, then impute functional consequences of MEIs from WGS in large-scale disease cohorts. The training plan consists of two years of mentored research to learn new skills in genome analysis, MEI characterization, and advanced deep learning techniques, followed by three years of shaping an independent laboratory. The research plan is developed to comprehensively explore functional variation in the genome by decomposing transcriptomic changes against MEIs. Dr. Michael Talkowski at Massachusetts General Hospital, Harvard, and the Broad Institute will serve as the primary mentor, while Dr. Manolis Kellis at MIT and the MIT Computational Biology Group, and the Broad Institute will serve as a co-mentor and close collaborator. These mentors are recognized experts in genomic structural variants, functional genomics, the genetics of neurological disorders, and computational modeling to establish functional elements in the human genome. In addition, a team of independent investigators from basic and translational research will provide Dr. Gao with comprehensive feedback to keep both his science and career development on track. The highly collaborative environment in CGM, MGH, Harvard Medical School, the Broad Institute and the University of Michigan Medical School will prepare Dr. Gao for his transition to an independent investigator. This outstanding mentorship team and training program will facilitate the career development of Dr. Gao as he seeks to redefine the functional maps of MEIs in the human genome and to impute their impact in large-scale neurological disorders. Mobile element insertions (MEIs) represent a largely undefined component of the genetic architecture of neurological disorders, as a number of MEIs have been associated with alternative splicing in these disorders but large-scale genome-wide functional characterization has not been systematically performed across tissues. This program study will functionally characterize the impact of MEIs on alternative splicing from whole-genome sequencing and transcriptome sequencing in large cohorts using new developments in deep learning models. These results will enhance our understanding of the etiological role and pathogenic mechanisms associated with MEIs in neuronal development and human neurological disorders.",Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders,10041366,K99NS118109,"['Address', 'Algorithms', 'Alternative Splicing', 'Alzheimer&apos', 's Disease', 'Autopsy', 'Basic Science', 'Biology', 'Blood', 'Brain', 'Brain Diseases', 'CRISPR/Cas technology', 'Cells', 'Chromosome Pairing', 'Cohort Studies', 'Computational Biology', 'Computer Analysis', 'Computer Models', 'DNA', 'DNA Insertion Elements', 'Data', 'Data Set', 'Defect', 'Detection', 'Development', 'Disease', 'Disease model', 'Dorsal', 'Dystonia', 'Elements', 'Etiology', 'Event', 'Evolution', 'Excision Repair', 'Familial Dysautonomia', 'Feedback', 'Fellowship', 'Filipino', 'General Hospitals', 'Generations', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Haplotypes', 'Human', 'Human Genome', 'Individual', 'Institutes', 'International', 'Introns', 'Laboratories', 'Lateral', 'Lead', 'Learning', 'Linear Regressions', 'Link', 'Machine Learning', 'Maps', 'Massachusetts', 'Measures', 'Mentors', 'Mentorship', 'Methods', 'Michigan', 'Mind', 'Minisatellite Repeats', 'Modeling', 'Molecular', 'Mosaicism', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuromuscular Diseases', 'Neurons', 'Outcome', 'Parkinsonian Disorders', 'Pathogenicity', 'Pattern', 'Peripheral', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Prefrontal Cortex', 'Process', 'Property', 'RNA Splicing', 'Regulation', 'Research', 'Research Personnel', 'Retroelements', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Shapes', 'Short Interspersed Nucleotide Elements', 'Source', 'Specificity', 'Structure', 'TAF1 gene', 'Techniques', 'Therapeutic Trials', 'Tissue-Specific Splicing', 'Tissues', 'Training', 'Training Programs', 'Transcription Alteration', 'Translational Research', 'Universities', 'Untranslated RNA', 'Variant', 'Work', 'brain tissue', 'career development', 'cohort', 'collaborative environment', 'convolutional neural network', 'deep learning', 'drug development', 'functional genomics', 'functional outcomes', 'gene function', 'genetic architecture', 'genome analysis', 'genome editing', 'genome sequencing', 'genome-wide', 'human disease', 'in silico', 'insight', 'medical schools', 'mind control', 'nervous system disorder', 'neuron development', 'novel', 'programs', 'response', 'skills', 'statistical learning', 'structural genomics', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,K99,2020,91226,-0.009749740721327057
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,10016298,U01AI157189,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Bar Codes', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'machine learning method', 'next generation', 'novel', 'novel therapeutics', 'off-target mutation', 'off-target site', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIAID,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2020,610710,0.014594872812987358
"A priori adaptive evolution predictions for antibiotic resistance through genome-wide network analyses and machine learning SUMMARY Adaptive evolution (AE) is both a âforce of goodâ as it can help to optimize biological processes in industry, but it is also a âforce of frustrationâ when infectious diseases exploit AE to escape the host immune system or become resistant to drugs. It has long been assumed close to impossible to make predictions on AE due to the presumed predominating influences of random forces and events. However, the observation that evolutionary repeatability across traits and species is far more common than previously thought, suggests that AE, with the right data and approach, may become (partially) predictable. Indeed, we found through experiments with the bacterial pathogen Streptococcus pneumoniae on its response to antibiotics and the emergence of antimicrobial resistance, that in order to make AE predictable a detailed understanding of at least two aspects of the bacterial system are required: 1.) the genetic constraints of the system (i.e. the architecture of the organismal network); and 2.) where and how in the system stress is experienced and processed. We showed that by mapping out ~25% of the bacterium's network, determining phenotypic and transcriptional antibiotic responses, applying network analyses to capture and quantify the responses in a network context, and exploiting experimental evolution to pin-point adaptive mutations in the genome it becomes possible, by means of machine learning, to uncover hidden patterns in the data that make AE predictions feasible. This means that the network in interaction with the environment shapes the adaptive landscape, it limits available solutions and makes some solutions more likely than others, thereby driving repeatability and enabling predictability. In this proposal we build on these exciting developments with the goal to map out the constraints of S. pneumoniae's entire network and develop a machine learning model that can forecast adaptive evolution a priori, and on a genome-wide scale. To accomplish this, we combine in aim 1 parts of Tn-Seq, dTn-Seq and Drop-Seq to finalize a new tool Tn-Seq^2 (Tn-Seq squared) that is able to map genetic-interactions in high-throughput and genome-wide. We use Tn-Seq^2 to reconstruct the first genome-wide genetic interaction network for S. pneumoniae in the presence of 20 antibiotics. In aim 2 we create 85 HA-tagged Transcription factor induction (TFI) strains and: a) Determine with ChIP-Seq the DNA-binding sites for all 85 TFs in S. pneumoniae; b) By overexpressing each TFI strain followed by RNA- Seq we determine each TFs regulatory signature; c) Use a Transcriptional Regulator Induced Phenotype screen in the presence of 20 antibiotics to untangle environment specific links between genetic and transcriptional perturbations and their phenotypic outcomes. Lastly, in aim 3, we train and test a variety of machine learning approaches to design an optimal model that predicts which genes in the genome are most likely to adapt in the presence of a specific antibiotic. The development of this predictive AE model, will not only be useful in predicting the emergence of antibiotic resistance, but the strategy should be valuable for most any biological field for which adaptive changes are important, ranging from biological engineering to cancer. NARRATIVE Adaptive evolution (AE) is the driving force behind the emergence of antibiotic resistance and if it were possible to predict AE before it happens, it could help in preventing resistance. Here we use cutting-edge existing and newly designed genomics tools and analytical approaches to develop a machine learning model that can predict AE a priori, and on a genome-wide scale.",A priori adaptive evolution predictions for antibiotic resistance through genome-wide network analyses and machine learning,10049219,R01AI148470,"['Achievement', 'Affect', 'Antibiotic Resistance', 'Antibiotics', 'Architecture', 'Automobile Driving', 'Bacteria', 'Binding Sites', 'Biological', 'Biological Process', 'Biomass', 'ChIP-seq', 'Communicable Diseases', 'Complex', 'DNA Binding', 'Data', 'Development', 'Drug resistance', 'Engineering', 'Ensure', 'Environment', 'Escherichia coli', 'Event', 'Evolution', 'Exposure to', 'Fermentation', 'Frustration', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Immune system', 'Immunotherapeutic agent', 'Industry', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Microfluidics', 'Modeling', 'Mutation', 'Organism', 'Outcome', 'Pathway Analysis', 'Pattern', 'Phenotype', 'Photosynthesis', 'Planet Earth', 'Process', 'Resistance', 'Shapes', 'Streptococcus pneumoniae', 'Stress', 'System', 'Testing', 'Time', 'Training', 'Yeasts', 'design', 'driving force', 'droplet sequencing', 'emerging antibiotic resistance', 'emerging antimicrobial resistance', 'experience', 'experimental study', 'genetic architecture', 'genome-wide', 'genomic tools', 'network architecture', 'novel', 'overexpression', 'pathogenic bacteria', 'predictive modeling', 'prevent', 'process optimization', 'programs', 'response', 'tool', 'trait', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transposon sequencing']",NIAID,BOSTON COLLEGE,R01,2020,391250,-0.008927244726902584
"Scalable detection and interpretation of structural variation in human genomes PROJECT SUMMARY Structural variation (SV), is a diverse class of genome variation that includes copy number variants (CNVs) such as deletions and duplications, as well as balanced rearrangements, such as inversions and reciprocal translocations. A typical human genome harbors >4,000 SVs larger than 300bp and their large size increases the potential to delete or duplicate genes, disrupt chromatin structure, and alter expression. Despite their prevalence and potential for phenotypic consequence, SVs remain notoriously difficult to detect and genotype with high accuracy. Much of this difficulty is driven by the fact DNA sequence alignment âsignalsâ indicating SVs are far more complex than for single-nucleotide and insertion deletion variants. Unlike SNP alignments that vary only in allele state, alignments supporting SVs vary in state (supports an alternate structure or not) alignment location, and type. Consequently, the accuracy of SV discovery is much lower than that of SNPs and INDELs. Furthermore, SV pipelines scale poorly and are difficult to run. These challenges are a barrier for single genome analysis and studies of families must invest substantial effort into eliminating a sea of false positives. These problems become exponentially more acute for large-scale sequencing efforts such as TOPmed, the Centers for Common Disease Genetics, and the All of Us program. Software efficiency is key to scalability for such projects. However, of equal importance is comprehensive, accurate discovery.  Building upon more than a decade of software development experience and analyzing SV in diverse disease contexts, we have invested significant effort into understanding the causes of the insufficient accuracy for SV discovery. These efforts, together with our research and development experience in this area, give us unique insight into improving the accuracy and scalability of SV discovery. Our goal is to narrow the accuracy gap between SNP/INDEL variation and structural variation discovery. These developments will empower studies of human genomes in diverse contexts and will therefore have broad impact. Our goals are to: 1. Develop a deep learning model to correct systematic variation in sequence depth. This new machine  learning model will correct systematic biases in DNA sequence depth and dramatically improve the  discovery of deletions and duplications. 2. Improve the speed, scalability, and accuracy of SV detection and genotyping. Using new algorithms,  we will bring the accuracy of SV detection much closer to that of SNP and INDEL discovery and allow  accurate SV discovery to be deployed at scale. 3. Create a map of genomic constraint for SV from population-scale genome analysis. We will deploy  our new methods to detect and genotype structural variation among tens of thousands of human genomes.  The resulting SV map will empower the creation of a model of genomic constraint for SV and enable new  software to predict deleterious SVs, especially in the noncoding genome. PROJECT NARRATIVE Single-nucleotide DNA changes paint an incomplete picture of a humanâs genome. A more complete picture must include a genome's structural variation (SV), an important class of genome variation that includes copy number variants (CNVs) such as deletions and duplications. However, existing methods have poor accuracy. As the genetics community transitions to large-scale genome sequencing studies, there is an acute need for improved SV discovery methods. This proposal introduces a series of algorithmic and software innovations that will empower SV discovery, genotyping, and interpretation in large-scale human disease studies.",Scalable detection and interpretation of structural variation in human genomes,9973582,R01HG010757,"['Acute', 'Affect', 'Algorithmic Software', 'Algorithms', 'All of Us Research Program', 'Alleles', 'Area', 'Automobile Driving', 'Biological Assay', 'Chromatin Structure', 'Chromosome Structures', 'Clip', 'Cloud Computing', 'Code', 'Communities', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'DNA', 'DNA Sequence', 'Data', 'Data Reporting', 'Detection', 'Development', 'Disease', 'Environment', 'Error Sources', 'Exhibits', 'Family Study', 'Funding', 'Future', 'Gene Duplication', 'Gene Expression', 'Gene Fusion', 'Gene Structure', 'Genetic', 'Genetic Diseases', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genome', 'Individual', 'Laboratories', 'Large-Scale Sequencing', 'Location', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Noise', 'Nucleotides', 'Paint', 'Pathogenicity', 'Performance', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Process', 'Reciprocal Translocation', 'Research', 'Running', 'Sampling', 'Sea', 'Sensitivity and Specificity', 'Sequence Alignment', 'Series', 'Signal Transduction', 'Software Tools', 'Source', 'Speed', 'Structure', 'Systematic Bias', 'Techniques', 'Technology', 'Training', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'algorithm development', 'base', 'convolutional neural network', 'deep learning', 'developmental disease', 'dosage', 'exome', 'experience', 'genome analysis', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'large datasets', 'method development', 'nanopore', 'novel', 'prevent', 'research and development', 'software development', 'success', 'tool', 'variant detection', 'whole genome']",NHGRI,UNIVERSITY OF UTAH,R01,2020,692048,0.018700811140882224
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9868315,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'de novo mutation', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'machine learning method', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2020,237600,0.016202346085873363
"Conceptualizing Actionability in Clinical Genomic Screening Project Summary/Abstract. Clinical genomic sequencing (CGS) produces large amounts of data, much of which is hard to characterize or may have a negligible influence on health. The concept of actionability is commonly used to help separate information that may be useful from information that is likely irrelevant for patients. Actionability directs attention to whether genomic information warrants action and reflects its initial development as a strategy to augment diagnosis and treatment in sick patients. As CGS expands towards healthy populations in primary care settings, actionability is still widely embraced despite little consensus regarding its definition and use. Because this ambiguity could become an obstacle to the successful implementation of clinical genomic sequencing in healthy populations, greater clarity about this concept is necessary. The proposed research will fulfill this need by characterizing the emergence and varied meanings of actionability in clinical genomics, focusing on clinical genomics' transition into primary care settings. By identifying underlying values and assumptions related to actionability, this research will push beyond definitional disputes and provide a deeper framework for assessing how genetic information is valued. The specific aims are: 1. Identify and characterize, through in-depth interviews, how genomics experts and primary care providers conceptualize what makes genomic information actionable for healthy populations. 2. Identify and characterize, through a natural language processing (NLP) analysis of published literature, how the concept of actionability emerged, spread, and is used throughout clinical genomics. 3. Convene a workshop with genomics experts, primary care providers, and ELSI scholars to produce a white paper on actionability and the ethical, effective integration of CGS into primary care, guided by the results from Aims 1 and 2. This K99/R00 Pathway to Independence Award includes a highly-structured, mentored training program that will support the candidate's goal to become an independent, mixed-methods ELSI investigator focused on assessing the value of genomic information. To achieve this career goal, the candidate will: 1. Receive training in genetic and genomic science to facilitate collaboration with genomics care teams and make scientifically accurate policy recommendations 2. Build new methodological skills in biomedical informatics and natural language processing to conduct generalizable research 3. Publish and engage with scientific and medical audiences to have a more direct impact on future guidelines and policies. 4. Develop a collaborative and interdisciplinary research network. This training will include coursework, guided readings, network building, and sustained mentorship by a highly-qualified team of faculty with expertise in ELSI research, bioethics, clinical genomics, biomedical informatics, and the history and sociology of medicine. This training will prepare the candidate to transition to an independent ELSI investigator focused on ethical issues related to the actionability of genomic health information â an ELSI research priority in Genetic and Genomic Healthcare. Project Narrative. This K99/R00 Pathway to Independence Award will prepare the candidate to become an independent, mixed-methods ELSI researcher pursing a research program on ethical issues related to the actionability of genomic information. The study examines the values and assumptions underlying conceptualizations of the actionability of genomic information for healthy populations. Results of the study will contribute to the ethical and effective implementation of genomic sequencing into care for healthy populations.",Conceptualizing Actionability in Clinical Genomic Screening,10054993,K99HG010905,"['American', 'Award', 'Bioethics', 'Caring', 'Clinical', 'Collaborations', 'Consensus', 'Data', 'Development', 'Diagnosis', 'Disease', 'Disputes', 'Educational workshop', 'Ethical Issues', 'Ethics', 'Faculty', 'Future', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Healthcare', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Laboratories', 'Level of Evidence', 'Literature', 'Medical', 'Medical Genetics', 'Medical Sociology', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Natural Language Processing', 'Outcome', 'Paper', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Penetrance', 'Policies', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Provider', 'Publishing', 'Qualitative Methods', 'Reading', 'Recommendation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Priority', 'Risk', 'Science', 'Severities', 'Structure', 'Surveys', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Variant', 'biomedical informatics', 'care providers', 'career', 'clinical implementation', 'directed attention', 'genetic information', 'genome sciences', 'health management', 'innovation', 'lifestyle intervention', 'medical schools', 'prevent', 'primary care setting', 'programs', 'screening', 'skills']",NHGRI,UNIVERSITY OF PENNSYLVANIA,K99,2020,103353,-0.007823826065183845
"Identification of Transposable Element Insertions in the Kids First Data Project Summary Insertion of transposable elements (TEs, sometimes referred to as âjumping genesâ) into the human genome can be pathogenic. Our aim in this project is to use sophisticated computational approaches to characterize TE insertions in the whole-genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program and identify any insertional mutations that may disrupt gene function. The large scale of the Kids First program provides an unprecedented opportunity to investigate the role of TE insertions in childhood cancers and structural birth defects, as well as to create a resource of reference TE maps that will be important for all other TE studies. We will first modify our existing algorithm called xTEA for the trio design of the Kids First studies and increase the accuracy and efficiency of the algorithm. Then, we will apply it to the thousands of trios that have been profiled in the Kids First program, using a pipeline optimized for the cloud environment. The resulting set of TE insertions (especially L1, Alu, SVA, and HERV insertions) will be curated with all relevant features and be made into a database for the community. We will also apply machine learning methods to improve the calls once a sufficient amount of training data have been obtained. To investigate the potential pathogenicity of the mutation, we will first focus on insertions within genes, but we will also explore those in regulatory elements inferred from epigenetic profiling data. PROJECT NARRATIVE Transposable elements, or âjumping genesâ, are genetic elements that can alter the DNA of an individual. We aim to utilize a computational method to identify such elements in the genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program. Our analysis will identify transposable elements that may be causal for a disease phenotype.",Identification of Transposable Element Insertions in the Kids First Data,9957262,R03CA249364,"['Algorithms', 'Communities', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Insertion Elements', 'DNA Transposable Elements', 'Data', 'Data Set', 'Databases', 'Disease', 'Elements', 'Endogenous Retroviruses', 'Environment', 'Gene Frequency', 'Genes', 'Genetic Diseases', 'Genome', 'Genomics', 'Genotype', 'Human Genome', 'Individual', 'Inherited', 'Insertion Mutation', 'Jumping Genes', 'Length', 'Location', 'Machine Learning', 'Malignant Childhood Neoplasm', 'Malignant Neoplasms', 'Maps', 'Mendelian disorder', 'Methods', 'Modeling', 'Mutation', 'Neurons', 'Output', 'Parents', 'Paste substance', 'Pathogenicity', 'Pediatric Research', 'Play', 'Population', 'Regulatory Element', 'Reporting', 'Resources', 'Retrotransposon', 'Role', 'Sampling', 'Sensitivity and Specificity', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Speed', 'Structural Congenital Anomalies', 'Training', 'base', 'cloud based', 'cohort', 'design', 'disease phenotype', 'epigenetic profiling', 'gene function', 'genetic element', 'genome sequencing', 'improved', 'machine learning method', 'proband', 'programs', 'transcriptome sequencing', 'whole genome']",NCI,HARVARD MEDICAL SCHOOL,R03,2020,169041,-0.00891398979251696
"Advancing evolutionary genetic inference in humans and other taxa Project Summary/Abstract Background: A major challenge in evolutionary genomics is to characterize the forces shaping present-day patterns of genetic variation. For instance, the extent and manner in which natural selection affects genetic diversity remains highly controversial. Researchers have largely addressed this problem by developing statistical tests or summaries of genome sequence variation that provide insights into the evolutionary forces at play. However, because such approaches typically rely on a single univariate summary of the data, valuable discriminatory information present in the original dataset is lost. A more fruitful strategy would thus be to use multidimensional summaries of genomic data (e.g. a large vector of summary statistics) or even the totality of the input data (e.g. a matrix-representation of a sequence alignment) to make more accurate inferences. An even more powerful approach is to utilize data sets in which the same population is sampled at multiple time points, allowing one to observe evolutionary dynamics in action. Although such genomic time-series data are becoming more prevalent, the development of appropriate computational methodologies has lagged behind the proliferation of such data. Proposal: The Schrider Lab seeks to develop and apply powerful machine learning methods for evolutionary inference. Our work over the next five years will yield powerful software tools leveraging novel representations of genomic datasets, including time-series data. These efforts will dramatically improve researchers' ability to make accurate evolutionary inferences from both population genomic and phylogenetic data. Indeed, preliminary results demonstrate that our methods vastly outperform current approaches in evolutionary genetics. More importantly, we will use these tools to answer pressing evolutionary questions. In particular, our use of time-series data will reveal loci responsible for recent adaptation with much greater confidence than currently possible. Our efforts will help to resolve the controversy over the role of adaptation in shaping patterns of diversity across the human genome. This research has important implications for public health as well, as genes underlying recent adaptations are enriched for disease-associations. Moreover, we are constructing a time-series dataset in the mosquito vector species Aedes aegypti and Aedes albopictus. We will interrogate these data for evidence of recent and ongoing adaptationâthis work will reveal loci responsible for the evolution of resistance to insecticides and other control efforts. Encouraging preliminary data also suggest that our work in phylogenetics will substantially improve inferential power in this important research area. More broadly, the success of the novel approaches described in this proposal has the potential to transform the methodological landscape of evolutionary genomic data analysis. Project Narrative The work proposed here seeks to develop and apply powerful machine-learning based software tools for evolutionary genetic inference in humans, mosquito vectors, and other species. Such efforts have important health implications, as they can identify genes involved in adaptation, which in humans are often associated with disease and in mosquitos are often associated with resistance to insecticides and other control efforts.",Advancing evolutionary genetic inference in humans and other taxa,10028474,R35GM138286,"['Address', 'Aedes', 'Affect', 'Area', 'Computing Methodologies', 'Culicidae', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Health', 'Human', 'Human Genome', 'Insecticides', 'Machine Learning', 'Methodology', 'Methods', 'Natural Selections', 'Pattern', 'Phylogenetic Analysis', 'Play', 'Population', 'Public Health', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Sampling', 'Sequence Alignment', 'Series', 'Shapes', 'Software Tools', 'Testing', 'Time', 'Variant', 'Work', 'base', 'genomic data', 'improved', 'insight', 'machine learning method', 'novel', 'novel strategies', 'statistics', 'success', 'time use', 'tool', 'vector', 'vector mosquito']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R35,2020,382894,0.007028207639759122
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nationâs leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanfordâs long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,10124880,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,50000,0.0062366022730935815
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nationâs leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanfordâs long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9980967,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,1100000,0.0062366022730935815
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nationâs leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanfordâs long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,0.007630767655414745
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9840492,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Models', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'Visualization', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'genomic locus', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,395000,-0.015035370210694152
"Computational modeling of spatial genome organization and gene regulation PROJECT SUMMARY/ABSTRACT The three-dimensional (3D) organization of the genome plays an essential role in genome stability, gene regulation, and many diseases, including cancer. The recent development of high-throughput chromatin conformation capture (Hi-C) and its variants provide an unprecedented opportunity to investigate higher-order chromatin organization. Despite the rapidly accumulating resources for investigating 3D genome organization, our understanding of the regulatory mechanisms and functions of the genome organization remain largely incomplete. Hi-C analyses and 3D genome research are still in their early stage and face several challenges. First, high-resolution chromatin contact maps require extremely deep sequencing and hence have been achieved only for a few cell lines. Second, it is computationally challenging to complement 3D genome structure with one-dimensional (1D) genomic and epigenomic features. Third, recent studies have just begun to infer associations between chromatin interactions and genetic variants and to identify potential target genes of those variants at the genome-wide scale. Given these challenges and my unique multi-disciplinary training, my long-term research goal is to develop innovative computational and statistical methods to uncover the interplay between 3D genome structure and function. Speciï¬cally, in the next ï¬ve years, I will i) develop computational approaches to enhance the resolution of existing Hi-C data and investigate ï¬ne-scale 3D genome architecture as well as its spatiotemporal dynamics and ii) build scalable and interpretable machine learning models that leverage 1D epigenomic data to predict cell type-speciï¬c 3D chromatin interactions and gene expression and elucidate the function of 3D genome organization in gene regulation and human diseases. The completion of the proposed work will deepen our knowledge of 3D genome architecture as well as its functions in gene regulation and disease. PROJECT NARRATIVE The overarching mission of my research is to understand the interplay between genome architecture and gene regulation. Recent development of high-throughput chromatin conformation capture techniques has allowed us to look beyond the nucleotide sequence of DNA and investigate the principles of higher-order chromatin organization. I will develop innovative computational and statistical strategies to investigate 3D genome organization at an unprecedented scale, thereby elucidating the impacts of genome organization on gene regulation and disease.",Computational modeling of spatial genome organization and gene regulation,9999005,R35GM133678,"['3-Dimensional', 'Architecture', 'Base Sequence', 'Cell Line', 'Chromatin', 'Complement 3d', 'Computer Models', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Development', 'Dimensions', 'Disease', 'Face', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome Stability', 'Genomics', 'Goals', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mission', 'Modeling', 'Play', 'Research', 'Resolution', 'Resources', 'Role', 'Statistical Methods', 'Structure', 'Techniques', 'Training', 'Variant', 'Work', 'cell type', 'chromosome conformation capture', 'deep sequencing', 'epigenomics', 'genetic variant', 'genome-wide', 'human disease', 'innovation', 'multidisciplinary', 'spatiotemporal']",NIGMS,UNIVERSITY OF CALIFORNIA RIVERSIDE,R35,2020,378309,-0.022702312139532943
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9990809,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'statistical and machine learning', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2020,332952,0.005603469615580228
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9870944,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2020,397125,0.023354274815228435
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called âsmORFsâ. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human âsecretomeâ and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9928344,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2020,1186500,0.02600734396315248
"Advanced algorithms to infer and analyze 3D genome structures Project Summary For the past decade, the population-cell Hi-C technique has significantly improved our ability to discover genome-wide DNA proximities. However, because population Hi-C is based on a pool of cells, it will not help us reveal each single cell's 3D genome structure or understand cell-to-cell variability in terms of 3D genome structure and gene regulation. It is also difficult to achieve a high resolution, such as 1 Kbp, with population Hi- C; therefore, when finding and analyzing the spatial interactions for the promoter or enhancer regions typically associated with biologically-important regulatory elements, population Hi-C data's resolution is too low to be useful. Moreover, while we know that the CTCF-cohesin complex plays a key role in the formation of genome 3D structures, the question is whether long non-coding RNAs (lncRNAs) are involved in the process since lncRNAs have been found to recruit proteins needed for chromatin remodeling, and our preliminary research has found that lncRNA LINC00346 directly interacts with CTCF. Finally, while members of the bioinformatics community, including the PI, have developed many algorithms to reconstruct 3D genome structures based on population Hi-C data, important questions still must be answered regarding how 3D genome structures are involved in gene regulation and whether there are relationships between 3D genome structures and genetic and epigenetic features. The PI proposes to conduct leading research to overcome these challenges and address these questions. During the next five years, the PI will develop algorithms to reconstruct the 3D whole- genome structures for single cells and analyze cell-to-cell variabilities in terms of 3D genome structure and gene regulation. The PI will develop a deep learning algorithm to enhance the resolution of population Hi-C data to that of Capture Hi-C data (1 Kbp) so that we can make good use of the large amount of Hi-C data accumulated in the past decade. An online database will be built to allow the community to access both population and single-cell 3D genome structures in an integrated way. The PI will work with a cancer biologist to discover any lncRNAs that function as a scaffold to fine-tune the CTCF-cohesin protein complex, as well as two neuron scientists to develop a more complete understanding of gene regulation while considering 3D genome and other genetic and epigenetic features. Given the PI's track record and productivity, having three computational goals and two collaborative goals is not only feasible but computationally and biologically rewarding. In five years, once the proposed studies are accomplished, the PI should have established a uniquely independent place in the field of 3D genome, maintaining leading positions in inferring single-cell 3D genome structures, enhancing Hi-C data resolution, and building 3D genome databases, while establishing similar positions in reconstructing high-resolution 3D genome structures, finding lncRNAs' roles in the formation of genome structures, and understanding how 3D genome structures are involved in gene regulation. Project Narrative The three-dimensional (3D) structure of genome is critically important for gene regulation; and the abnormal 3D genome structures are often associated with diseases such as cancer. This proposal aims to reconstruct and analyze the 3D genome structures of thousands of individual cells, study how 3D genome structures participate in gene regulation, determine whether long noncoding RNAs (lncRNAs) are involved in the formation of genome structures, and build an online knowledge base integrating 3D genome structures with other biological information.",Advanced algorithms to infer and analyze 3D genome structures,10027542,R35GM137974,"['3-Dimensional', 'Address', 'Algorithms', 'Bioinformatics', 'Biological', 'Cells', 'Communities', 'Complex', 'DNA', 'Data', 'Databases', 'Disease', 'Enhancers', 'Epigenetic Process', 'Gene Expression Regulation', 'Genetic', 'Genome', 'Goals', 'Individual', 'Malignant Neoplasms', 'Neurons', 'Other Genetics', 'Play', 'Population', 'Positioning Attribute', 'Process', 'Productivity', 'Proteins', 'Regulatory Element', 'Research', 'Resolution', 'Rewards', 'Role', 'Scientist', 'Structure', 'Techniques', 'Untranslated RNA', 'Work', 'base', 'chromatin remodeling', 'cohesin', 'deep learning algorithm', 'genome database', 'genome-wide', 'improved', 'knowledge base', 'member', 'promoter', 'protein complex', 'recruit', 'scaffold', 'three dimensional structure', 'whole genome']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R35,2020,354694,-0.009077856694225067
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades â such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases â have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9876220,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2020,479215,0.008846057967383093
"A database for high-resolution chromatin contact maps and human genetic variants Abstract After the completion of the Human Genome Project, several landmarking consortia have accumulated large amounts of genomic data towards understanding the functions of human genome. The ENCODE project has annotated genome-wide regulatory elements. The Roadmap Epigenomic project has characterized tissue-speciï¬c variation in epigenetic state. The NIH Common Fund GTEx project has delineated tissue-speciï¬c gene expression and transcription regulation. The NIH Common Fund 4D Nucleome (4DN) project has revealed dynamic 3D chromatin organization in many cell and tissue types. Each of the aforementioned consortia has generated thousands or even tens of thousands of datasets, and provided different insights regarding human genome at an unprecedent scale and depth. However, the datasets generated from these consortia are isolated in terms of cell types and tissue types covered, how the data are stored, and the resolution of the genomic data. These gaps bring realistic data analysis challenges to biomedical researchers when they use these public datasets jointly in their research â they need to go through different data portals with heterogeneous processing pipelines, different data formats, and unmatched resolutions. We aim to develop the most cutting-edge deep learning approaches to impute high-resolution chromatin contact maps, and integrate the high-resolution chromatin contact maps with transcriptional data available from GTEx project and epigenomic data from ENCODE/Roadmap. We plan to share the integrated data on a public web server with a multi-panel interactive visualization genome browser. The integrated data will provide an important resource for understanding of tissue-speciï¬c genetic variation in the light of the spatial organization of these genomic and epigenomic elements and their functional implications. Project Narrative The goal of this project is to develop novel computational methods to integrate 4DN datasets with GTEx datasets and ENCODE/Roadmap datasets. The integrated datasets will be critical resource to unveil the mechanisms of the genetic variants identiï¬ed in genome-wide association studies. The new knowledge gained here could help us understand the genetic basis of many human diseases.",A database for high-resolution chromatin contact maps and human genetic variants,10109293,R03OD030599,"['3-Dimensional', 'Address', 'Area', 'Base Pairing', 'Cells', 'Chromatin', 'Chromatin Structure', 'Communities', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Elements', 'Epigenetic Process', 'Funding', 'Gene Expression Regulation', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Internet', 'Knowledge', 'Maps', 'Molecular', 'Nucleosomes', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Technology', 'Tissue-Specific Gene Expression', 'Tissues', 'Transcriptional Regulation', 'United States National Institutes of Health', 'Variant', 'Visualization', 'Work', 'cell type', 'data format', 'data integration', 'data portal', 'data resource', 'data visualization', 'deep learning', 'epigenomics', 'expectation', 'genetic variant', 'genome annotation', 'genome browser', 'genome sciences', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'insight', 'interest', 'next generation sequencing', 'novel', 'open data', 'web server']",OD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2020,265495,-0.020608425486297995
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9858390,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data infrastructure', 'data integration', 'data standards', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'large scale data', 'member', 'mouse genome', 'multiple data types', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2020,2000000,0.02618742958890524
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to ânavigateâ across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an extensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spatial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high-resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",10148333,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Visualization', 'Work', 'base', 'cell type', 'data exploration', 'data standards', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2020,1000000,-0.009292047559976636
"Computational approaches for identifying epigenomic contexts of somatic mutations ABSTRACT During normal development, aging, and diseases such as cancer, DNA damage due to endogenous and external factors, and repair defects result in accumulation of different types of somatic mutations including single nucleotide substitutions, small InDels, copy number alterations, translocations, and ploidy changes. While a vast majority of somatic mutations in the genome are not disease drivers, their patterns of genetic changes and associated context can provide insights into past exposure to mutagens, mechanisms of DNA damage and repair defects, and extent of genomic instability, which are important for understanding disease etiology, minimizing hazardous environmental exposure, and also predicting efficacy of emerging treatment strategies such as immunotherapy. A number of mutation signatures have been identified based on local sequence contexts to address this need. But, mechanisms of DNA damage and repair preferences depend on both local sequence and epigenomic contexts, and it remains to be understood whether epigenomic contexts of emerging mutation signatures can provide critical, complementary etiological insights at a genome-wide scale, which are not apparent from sequence contexts alone. This is of fundamental importance, because (i) etiology of many of the emerging mutation signatures is currently unknown, (ii) DNA damage response and repair depends on tissue contexts, and defects in core DNA repair genes often result in cancer development in tissue-specific manner, and (iii) differences in the extent of DNA damage and repair between stem and differentiated cells within the same tissues have consequences for aging and disease incidence rates. Built logically on our previous works, we propose to develop computational approaches to determine the impact of epigenomic contexts on the patterns of somatic mutations within and across tissue types, and validate computational predictions using targeted experiments. In Aim-1, we will develop an epigenomic context preference map for emerging mutation signatures. In Aim-2, we will determine the basis of tissue-dependent differences in mutation profiles attributed to DNA repair defects. In Aim-3, we will predict the extent of cell lineage-dependent patterns of mutation accumulation from the mutational landscape of terminal cells. I am currently an early stage investigator, and the proposal is aligned with my long-term goal to identify fundamental principles of mutability and evolvability of somatic genomes. Our project will deliver novel resources and knowledge for addressing questions regarding genomic integrity during development and aging, and diseases such as cancer. ! PUBLIC HEALTH RELEVANCE: The proposed project will use computational biology approaches to determine epigenomic context preference for somatic mutations, and use that to infer tissue-dependent changes in mutation patterns. Our results will provide fundamental insights into aspects of genome maintenance, which is important for advancing our understanding of cancer etiology, reducing exposure to mutagenic factors, and also predicting efficacy of emerging treatment strategies. !",Computational approaches for identifying epigenomic contexts of somatic mutations,9902467,R01GM129066,"['Address', 'Affect', 'Aging', 'Biometry', 'Blood', 'Cancer Etiology', 'Cancer Relapse', 'Cell Differentiation process', 'Cell Line', 'Cell Lineage', 'Cells', 'Chromatin', 'Clinical', 'Computational Biology', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'Data', 'Defect', 'Development', 'Disease', 'Doctor of Philosophy', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Evolution', 'Exposure to', 'Genome', 'Genomic DNA', 'Genomic Instability', 'Genomics', 'Goals', 'Immunotherapy', 'Incidence', 'Knowledge', 'Least-Squares Analysis', 'Location', 'Maintenance', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutagenesis', 'Mutagens', 'Mutation', 'Nuclear', 'Nucleotides', 'Pathway interactions', 'Pattern', 'Ploidies', 'Point Mutation', 'Process', 'Publishing', 'Radiation Tolerance', 'Research Personnel', 'Resources', 'Role', 'Somatic Mutation', 'Source', 'Tissues', 'Work', 'base', 'cancer genomics', 'computer framework', 'epigenomics', 'experimental study', 'genome integrity', 'genome-wide', 'human tissue', 'improved', 'insertion/deletion mutation', 'insight', 'markov model', 'medical schools', 'novel', 'preference', 'public health relevance', 'random forest', 'repaired', 'response', 'stem', 'stem cells', 'tissue stem cells', 'transcriptomics', 'treatment strategy']",NIGMS,RBHS -CANCER INSTITUTE OF NEW JERSEY,R01,2020,324350,-0.0038148125842501542
"A Proteogenomic Search Engine for Direct Mass Spectrometric Identification of Variant Proteins Using Genomic Data Project Summary / Abstract Mass spectrometry-based proteomics, used in conjunction with genomics, has been called proteogenomics. Recent exponential increases in variant identification by next-generation sequencing (NGS) is redefining the concept of the human genome/proteome. Our project is the commercialization of a first-to-market proteomic database search engine for mass spectrometry capable of directly reading NGS data for the identification of mutilations from individual samples or from curated resources. Such an offering has the potential to bring together these two fields, enabling validation of mutations at the protein-level. Mutated proteins have been shown to make ideal targets for drug therapies and diagnostics in cancer. Our software will provide an intuitive user experience, approachable by scientists who may not be expert both proteomic and genomic data analysis. Since the search engine is guided by prior knowledge, performance exceeds current practice. The software will come complete with a full array of post-processing validation, and visualization tools. Project Narrative The detection of protein variants, which differ from those predicted from the reference human genome sequence, can make ideal candidates for the development of targeted treatments and diagnostics for many clinical conditions such as cancer. This project proposes the development a first-of-its-kind âproteogenomicsâ search engine for the identification of protein variants by mass spectrometry, making direct use of genomic data and ever-growing public and private databases of genetic variation.",A Proteogenomic Search Engine for Direct Mass Spectrometric Identification of Variant Proteins Using Genomic Data,10082114,R44CA217432,"['Agreement', 'Cancer Vaccines', 'Clinical', 'Communities', 'Complement', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Databases', 'Deposition', 'Detection', 'Development', 'Diagnostic', 'Environment', 'Future', 'Gefitinib', 'Genetic Databases', 'Genetic Diseases', 'Genetic Variation', 'Genomics', 'Goals', 'Guidelines', 'Heterozygote', 'Human Genome', 'Individual', 'Informatics', 'Intuition', 'Ions', 'Isomerism', 'Knowledge', 'Licensing', 'Malignant Neoplasms', 'Marketing', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Modification', 'Monitor', 'Mutate', 'Mutation', 'Mutation Detection', 'Pathway interactions', 'Peptides', 'Performance', 'Phase', 'Post Translational Modification Analysis', 'Privatization', 'Probability', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Publications', 'Readability', 'Reading', 'Research', 'Resolution', 'Resources', 'Risk', 'Running', 'Sales', 'Sampling', 'Scientist', 'Services', 'Small Business Innovation Research Grant', 'Testing', 'Transcript', 'Validation', 'Variant', 'Visualization', 'Visualization software', 'Work', 'base', 'commercialization', 'cost', 'experience', 'genomic data', 'graphical user interface', 'human reference genome', 'improved', 'next generation sequencing', 'open source', 'protein expression', 'proteogenomics', 'prototype', 'repository', 'scaffold', 'search engine', 'support vector machine', 'targeted treatment', 'tool', 'transcriptome sequencing']",NCI,"SPECTRAGEN INFORMATICS, LLC",R44,2020,529294,-0.02231208669020144
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIHâs mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9959498,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Models', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'Visualization', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'genomic locus', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2020,360268,0.017055000467057574
"A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions PROJECT SUMMARY Cancer genomes typically harbor a substantial number of somatic mutations. Relatively few driver mutations actually alter the function of proteins in tumor cells, whereas most mutations are considered to be functionally neutral passenger mutations. Over the past decade, the search for cancer driver mutations has focused on coding regions and several mutational significance algorithms have been developed for coding mutations. The contribution of mutations in noncoding regulatory regions to tumor formation largely remains unknown and current mutational significance algorithms are not designed to detect driver mutations in noncoding regions, due to biological differences between coding and noncoding mutations. The emerging availability of large whole- genome sequencing datasets (e.g. PCAWG and HMF datasets) creates an ample opportunity to develop new mutational significance algorithms that are particularly designed for the interpretation of noncoding regions. Recently, we have developed a new statistical approach that identifies driver mutations in coding regions based on the nucleotide context. Critically, consideration of the nucleotide context around mutations does not require prior knowledge for functional consequences associated with these mutations. Hence, we hypothesize that generalizing our nucleotide context model to noncoding regions will uncover novel noncoding driver mutations that cannot be detected using the mutational significance approaches currently available. For this purpose, we will develop a statistical framework that incorporates the biological differences between coding and noncoding mutations and that is specifically designed to detect driver mutations in noncoding regions. Specifically, we will consider the context-dependent distribution of passenger mutations, modeling of the background mutation rate, accurately partition the background mutation rate, model the sequence composition of the reference genome, and account for coverage fluctuation. We will then combine these statistical components by computing an independent product of their underlying probabilities. We will derive a significance p-value using a Monte-Carlo simulation approach, and use FDR for multiple hypothesis test correction. This strategy will allow us to accurately estimate the significance of somatic mutations in noncoding genomic regions. We will next apply this statistical framework to whole-genome sequencing data of 5,523 tumor patients, thereby deriving a comprehensive list of candidate driver mutations in noncoding regions. Finally, we will investigate whether noncoding mutations are overrepresented in transcription factor binding sites, regulate gene expression levels, induce alternative splicing, or affect epigenomic states. Upon the completion of this project, we will have developed and applied a statistical framework for discovery of significant somatic mutations in noncoding regions, and defined the mutational landscape of the non-coding cancer genome. All aspects of the methods developed and applied in this project will be made open source and developed in an online platform. PROJECT NARRATIVE While coding cancer driver mutations have been characterized in detail over the past decade, the contribution of noncoding mutations to tumor formation remains - apart from few examples (e.g. mutations in TERT promoters) - largely unknown. Recently, large-scale whole-genome sequencing datasets have been made available, but a major bottleneck for the biological and clinical interpretation of these cancer whole-genome cohorts is the lack of statistical models that identify driver mutations in noncoding regions. We developed a new statistical approach that characterizes driver mutations based on their surrounding nucleotide context in coding regions, and herein we propose a concrete plan to generalize our computational model to noncoding regions, apply our model to aggregated whole-genome sequencing data of 5,523 tumor patients (PCAWG, HMF datasets), and define the noncoding driver and passenger mutational landscape for biological discovery and focused clinical application.",A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions,10260680,R21CA242861,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Attention', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Clinical', 'Code', 'Communities', 'Computational Biology', 'Computer Models', 'Data', 'Data Set', 'Development', 'Gene Expression', 'Gene Expression Regulation', 'Genomic Segment', 'Immunotherapy', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Methods', 'Microsatellite Instability', 'Modeling', 'Monte Carlo Method', 'Mutation', 'Nucleic Acid Regulatory Sequences', 'Nucleotides', 'Outcome', 'Patients', 'Pattern', 'Play', 'Positioning Attribute', 'Probability', 'Process', 'Role', 'Somatic Mutation', 'Statistical Models', 'Stratification', 'Testing', 'The Cancer Genome Atlas', 'Untranslated RNA', 'base', 'cancer genome', 'cancer immunotherapy', 'checkpoint therapy', 'clinical application', 'clinical effect', 'cohort', 'design', 'driver mutation', 'epigenomics', 'exome sequencing', 'genome sequencing', 'genome-wide', 'immune checkpoint blockade', 'immunogenicity', 'large datasets', 'malignant breast neoplasm', 'melanoma', 'mutant', 'neoantigens', 'neoplastic cell', 'novel', 'open source', 'predicting response', 'promoter', 'protein function', 'reference genome', 'response', 'targeted treatment', 'transcription factor', 'tumor', 'whole genome']",NCI,DANA-FARBER CANCER INST,R21,2020,177000,0.0034265393561965004
"A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions PROJECT SUMMARY Cancer genomes typically harbor a substantial number of somatic mutations. Relatively few driver mutations actually alter the function of proteins in tumor cells, whereas most mutations are considered to be functionally neutral passenger mutations. Over the past decade, the search for cancer driver mutations has focused on coding regions and several mutational significance algorithms have been developed for coding mutations. The contribution of mutations in noncoding regulatory regions to tumor formation largely remains unknown and current mutational significance algorithms are not designed to detect driver mutations in noncoding regions, due to biological differences between coding and noncoding mutations. The emerging availability of large whole- genome sequencing datasets (e.g. PCAWG and HMF datasets) creates an ample opportunity to develop new mutational significance algorithms that are particularly designed for the interpretation of noncoding regions. Recently, we have developed a new statistical approach that identifies driver mutations in coding regions based on the nucleotide context. Critically, consideration of the nucleotide context around mutations does not require prior knowledge for functional consequences associated with these mutations. Hence, we hypothesize that generalizing our nucleotide context model to noncoding regions will uncover novel noncoding driver mutations that cannot be detected using the mutational significance approaches currently available. For this purpose, we will develop a statistical framework that incorporates the biological differences between coding and noncoding mutations and that is specifically designed to detect driver mutations in noncoding regions. Specifically, we will consider the context-dependent distribution of passenger mutations, modeling of the background mutation rate, accurately partition the background mutation rate, model the sequence composition of the reference genome, and account for coverage fluctuation. We will then combine these statistical components by computing an independent product of their underlying probabilities. We will derive a significance p-value using a Monte-Carlo simulation approach, and use FDR for multiple hypothesis test correction. This strategy will allow us to accurately estimate the significance of somatic mutations in noncoding genomic regions. We will next apply this statistical framework to whole-genome sequencing data of 5,523 tumor patients, thereby deriving a comprehensive list of candidate driver mutations in noncoding regions. Finally, we will investigate whether noncoding mutations are overrepresented in transcription factor binding sites, regulate gene expression levels, induce alternative splicing, or affect epigenomic states. Upon the completion of this project, we will have developed and applied a statistical framework for discovery of significant somatic mutations in noncoding regions, and defined the mutational landscape of the non-coding cancer genome. All aspects of the methods developed and applied in this project will be made open source and developed in an online platform. PROJECT NARRATIVE While coding cancer driver mutations have been characterized in detail over the past decade, the contribution of noncoding mutations to tumor formation remains - apart from few examples (e.g. mutations in TERT promoters) - largely unknown. Recently, large-scale whole-genome sequencing datasets have been made available, but a major bottleneck for the biological and clinical interpretation of these cancer whole-genome cohorts is the lack of statistical models that identify driver mutations in noncoding regions. We developed a new statistical approach that characterizes driver mutations based on their surrounding nucleotide context in coding regions, and herein we propose a concrete plan to generalize our computational model to noncoding regions, apply our model to aggregated whole-genome sequencing data of 5,523 tumor patients (PCAWG, HMF datasets), and define the noncoding driver and passenger mutational landscape for biological discovery and focused clinical application.",A statistical framework to systematically characterize cancer driver mutations in noncoding genomic regions,9957082,R21CA242861,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Attention', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Clinical', 'Code', 'Communities', 'Computational Biology', 'Computer Models', 'Data', 'Data Set', 'Development', 'Gene Expression', 'Gene Expression Regulation', 'Genomic Segment', 'Immunotherapy', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Methods', 'Microsatellite Instability', 'Modeling', 'Monte Carlo Method', 'Mutation', 'Nucleic Acid Regulatory Sequences', 'Nucleotides', 'Outcome', 'Patients', 'Pattern', 'Play', 'Positioning Attribute', 'Probability', 'Process', 'Role', 'Somatic Mutation', 'Statistical Models', 'Stratification', 'Testing', 'The Cancer Genome Atlas', 'Untranslated RNA', 'base', 'cancer genome', 'cancer immunotherapy', 'checkpoint therapy', 'clinical application', 'clinical effect', 'cohort', 'design', 'driver mutation', 'epigenomics', 'exome sequencing', 'genome sequencing', 'genome-wide', 'immune checkpoint blockade', 'immunogenicity', 'large datasets', 'malignant breast neoplasm', 'melanoma', 'mutant', 'neoantigens', 'neoplastic cell', 'novel', 'open source', 'predicting response', 'promoter', 'protein function', 'reference genome', 'response', 'targeted treatment', 'transcription factor', 'tumor', 'whole genome']",NCI,DANA-FARBER CANCER INST,R21,2020,193576,0.0034265393561965004
"PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site Project Summary Physical activity (PA) prevents or ameliorates a large number of diseases, and inactivity is the 4th leading global mortality risk factor. The molecular mechanisms responsible for the diverse benefits of PA are not well understood. The Molecular Transducers of Physical Activity Consortium (MoTrPAC) is being formed to advance knowledge in this area. We propose to establish PAGES, a Physical Activity Genomics, Epigenomics/transcriptomics Site as an integral component of the MoTrPAC. PAGES will conduct comprehensive analyses of the rat and human PA intervention MoTrPAC samples, contribute these data to public databases, help identify candidate molecular transducers of PA and elucidate new PA response mechanisms, and help develop predictive models of the individual response to PA. PAGES assay sites at Icahn School of Medicine at Mount Sinai, New York Genome Center and Broad Institute provide the infrastructure, expertise and experience to support this large scale, comprehensive analysis of molecular changes associated with PA. PAGES aims are to 1. Work with the MoTrPAC Steering Committee in Year 1 to finalize plans and protocols; 2. Perform assays and analyses to help Identify candidate molecular transducers of the response to PA in rat models and the pathways responsible for model differences, including high-depth RNA-seq and Whole Genome Bisulfite Sequencing (WGBS), supplemented by additional assay types such as ChIP-seq, ATAC-seq based on initial results; 3. Perform comprehensive assays and analyses of the human MoTrPAC clinical study tissue samples, including RNA-seq, WGBS, H3K27ac ChIP-seq, ATAC-seq and whole genome sequencing. 4. Collaborate with the MoTrPAC to analyze data from PAGES and other MoTrPAC analysis sites to identify candidate PA transducers and molecular mechanisms, and to develop predictive models of PA capacity and response to training. The success of PAGES and the MoTrPAC program will transform insight into the molecular networks that transduce PA into health, create an unparalleled comprehensive public PA data resource, and can provide the foundation for profound advances in the prevention and treatment of many major human diseases. Project Narrative While physical activity prevents or improves a large number of diseases, the chemical changes that occur in the body and lead to better health are not well known. As a part of a consortium of physical activity research programs working together, we will use cutting-edge approaches to comprehensively study the changes in genes and gene products caused by physical activity. This study has the potential to lead to advances in the prevention and treatment of many diseases.","PAGES: Physical Activity Genomics, Epigenomics/transcriptomics Site",9840897,U24DK112331,"['ATAC-seq', 'Area', 'Bioinformatics', 'Biological Assay', 'Budgets', 'ChIP-seq', 'Chemicals', 'Chromatin', 'Clinical Research', 'Collaborations', 'Cost efficiency', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Disease', 'Elements', 'Foundations', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Infrastructure', 'Institutes', 'Knowledge', 'Lead', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Analysis', 'New York', 'Ontology', 'Pathway interactions', 'Physical activity', 'Pilot Projects', 'Prevention', 'Production', 'Protocols documentation', 'Rat Strains', 'Rattus', 'Research Activity', 'Risk Factors', 'Sampling', 'Scientist', 'Site', 'Tissue Sample', 'Tissues', 'Training', 'Training Activity', 'Transducers', 'Universities', 'Validation', 'Work', 'analysis pipeline', 'base', 'bisulfite sequencing', 'data exchange', 'data resource', 'epigenomics', 'exercise intervention', 'experience', 'fitness', 'gene product', 'genome sequencing', 'high throughput analysis', 'human data', 'human disease', 'improved', 'individual response', 'insight', 'machine learning algorithm', 'medical schools', 'methylome', 'mortality risk', 'predictive modeling', 'prevent', 'programs', 'response', 'sedentary', 'success', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'web page', 'web portal', 'whole genome']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2020,4401725,0.016358811889451846
"Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation Project Summary/Abstract: With the surge of large genomics data, there is an immense increase in the breadth and depth of different omics datasets and an increasing importance in the topic of privacy of individuals in genomic data science. Detailed genetic and environmental characterization of diseases and conditions relies on the large-scale mining of functional genomics data; hence, there is great desire to share data as broadly as possible. However, there is a scarcity of privacy studies focused on such data. A key first step in reducing private information leakage is to measure the amount of information leakage in functional genomics data, particularly in different data file types. To this end, we propose to to derive information-theoretic measures for private information leakage in different data types from functional genomics data. We will also develop various file formats to reduce this leakage during sharing. We will approach the privacy analysis under three aims. First, we will develop statistical metrics that can be used to quantify the sensitive information leakage from raw reads. We will systematically analyze how linking attacks can be instantiated using various genotyping methods such as single nucleotide variant and structural variant calling from raw reads, signal profiles, Hi-C interaction matrices, and gene expression matrices. Second, we will study different algorithms to implement privacy-preserving transformations to the functional genomics data in various forms. Particularly, we will create privacy-preserving file formats for raw sequence alignment maps, signal track files, three-dimensional interaction matrices, and gene expression quantification matrices that contain information from multiple individuals. This will allow us to study the sources of sensitive information leakages other than raw reads, for example signal profiles, splicing and isoform transcription, and abnormal three-dimensional genomic interactions. Third, we will investigate the reads that can be mapped to the microbiome in the raw human functional genomics datasets. We will use inferred microbial information to characterize private information about individuals, and then combine the microbial information with the information from human mapped reads to increase the re-identification accuracy in the linking attacks described in the second aim. We will use the tools to quantify the sensitive information and privacy-preserving file formats in the available datasets from large sequencing projects, such as the ENCODE, The Cancer Genome Atlas, 1,000 Genomes, gEUVADIS, and Genotype-Tissue Expression projects. Project Narrative: Sharing large-scale functional genomics data is critical for scientific discovery, but comes with important privacy concerns related to the possible misuse of such data. This proposal will quantify and manage the rieslkasted to releasing functional genomics datasets, based on integrating inferred genotypes from the raw sequence files, signal tracks, and microbiome mapped sequences. Finally, we will develop file formats, statistical methodologies, and related software for anonymization of functional genomics data that enable open sharing.",Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation,9970939,R01HG010749,"['3-Dimensional', 'Address', 'Algorithms', 'Assessment tool', 'Biology', 'ChIP-seq', 'Code', 'Computer software', 'Consent', 'DNA sequencing', 'Data', 'Data Files', 'Data Science', 'Data Set', 'Databases', 'Diet', 'Disease', 'Environment', 'Equilibrium', 'Extravasation', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Genotype-Tissue Expression Project', 'Glean', 'Human', 'Individual', 'Institutes', 'Laws', 'Learning', 'Letters', 'Life Style', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Mining', 'Motivation', 'Participant', 'Patients', 'Phenotype', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Protein Isoforms', 'Protocols documentation', 'Provider', 'Pythons', 'Quantitative Trait Loci', 'RNA Splicing', 'Research Personnel', 'Risk', 'Risk Assessment', 'Sampling', 'Sequence Alignment', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Smoker', 'Source', 'Structure', 'Techniques', 'The Cancer Genome Atlas', 'Tissues', 'Variant', 'base', 'clinically relevant', 'computerized data processing', 'data mining', 'data sharing', 'experimental study', 'file format', 'functional genomics', 'genome sequencing', 'genomic data', 'human tissue', 'interest', 'large datasets', 'microbial', 'microbiome', 'open data', 'privacy preservation', 'social', 'tool', 'transcriptome sequencing']",NHGRI,YALE UNIVERSITY,R01,2020,523409,0.009823301907482365
"Novel Statistical methods for DNA Sequencing Data, and applications to Autism. Summary One of the major problems in human genetics is understanding the genetic causes underlying complex phenotypes, including neuropsychiatric traits such as autism spectrum disorders and schizophrenia. Despite tremendous work over the past few decades, the underlying biological mechanisms are poorly understood in most cases. Recent advances in high-throughput, massively parallel genomic technologies have revolutionized the field of human genetics and promise to lead to important scientific advances. Despite this progress in data generation, it remains very challenging to analyze and interpret these data. The main focus of this proposal is the development of powerful statistical methods for the integration of whole-genome sequencing data with rich functional genomics data with the goal to improve the discovery of genes involved in autism spectrum disorders. We propose to integrate data from many different sources, including epigenetic data from projects such as ENCODE, Roadmap, and PsychENCODE, eQTL data from the GTEx, PsychENCODE and CommonMind consortia, data from large scale databases of genetic variation such as ExAC and gnomAD, in order to predict functional effects of genetic variants in non-coding genetic regions in a tissue and cell type specific manner, and generate functional maps across large number of tissues and cell types in the human body that we can then use to identify novel associations with autism in whole-genome sequencing studies. The proposed functional predictions and functional maps will be broadly available in the popular ANNOVAR database. We further propose to use these functional predictions in the analysis of almost 20,000 whole genomes from three large whole genome sequencing studies for autism. We believe that the proposed research is very timely and has the potential to substantially improve the analysis of non-coding genetic variation, and hence provide new insights into the biological mechanisms underlying risk to autism, and more broadly to other neuropsychiatric diseases. Narrative Autism Spectrum Disorders are common diseases with major impact on public health. Although coding variation has been extensively studied for its role in affecting risk to autism, the analysis of non-coding variation poses tremendous challenges. The proposed statistical methods and their applications to nearly 20,000 whole genomes from three large autism whole genome sequencing studies will improve our understanding of the biological mechanisms involved in autism with important implications for disease treatment strategies.","Novel Statistical methods for DNA Sequencing Data, and applications to Autism.",9923466,R01MH095797,"['Affect', 'Anterior', 'Biochemical', 'Biological', 'Biological Assay', 'Brain region', 'Chromatin', 'Code', 'Collection', 'Complex', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Encyclopedia of DNA Elements', 'Epigenetic Process', 'Generations', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human body', 'Individual', 'International', 'Lead', 'Maps', 'Measures', 'Methods', 'Molecular', 'Phenotype', 'Prefrontal Cortex', 'Public Health', 'Research', 'Risk', 'Role', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Source', 'Statistical Methods', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Variant', 'Work', 'autism spectrum disorder', 'cell type', 'cingulate cortex', 'data integration', 'design', 'epigenomics', 'exome', 'frontal lobe', 'functional genomics', 'gene discovery', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic data', 'histone modification', 'improved', 'insight', 'large scale data', 'large-scale database', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'software development', 'supervised learning', 'tool', 'trait', 'treatment strategy', 'whole genome']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,446831,-0.01270589159881854
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9850271,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Consumption', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computational suite', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2020,387500,-0.04309703353558224
"Privacy-preserving genomic medicine at scale 1 Project Summary  2  3 High-throughput sequencing, biomedical imaging, and electronic health record technologies are 4 generating health-related datasets of unprecedented scale. Integrative analysis of these  5 resources promises to reveal new biology and drive personal and precision medicine. Yet, the  6 sensitive nature of these data often requires that they be kept in isolated silos, limiting their 7 usefulness to science. The goal of this project is to develop innovative privacy-preserving  8 algorithms to enable data sharing and drive genomic medicine. Crucially, we will draw upon our  9 past success in secure genome analysis and algorithmic expertise in computational biology to 10 address the imminent need to perform complex integrative analyses securely and at scale. 11 Current privacy-preserving tools are prohibitively too costly to perform the complex 12 calculations required in genomic analysis. We previously leveraged the highly structured nature 13 of biological data and novel optimization strategies to implement efficient pipelines for secure 14 genome-wide association studies (GWAS) and drug interaction predictions which scaled to 15 millions of samples. In this project, we will further exploit the unique properties of biomedical data 16 to: (i) develop secure integrative analysis methods for genomic medicine; (ii) develop an easy-to- 17 use programming environment with advanced automated optimizations to facilitate the adoption 18 of privacy-preserving analyses; and (iii) promote the use of our privacy techniques to gain novel 19 biological insights through large-scale collaborative genetic studies of multi-ethnic cohorts. 20 With co-Iâs Amarasinghe (MIT) and Cho (Broad Institute), we aim to apply these tools to 21 realize the first multi-institution, multi-national secure genetic studies with our partners at the 22 Swiss Personalized Health Network, UK Biobank, Finnish FinnGen, All of Us, NIH NCBI, Broad 23 and Barcelona Supercomputing Center (Letters of Support). We will also use our privacy- 24 preserving approaches to study genomic origins of polygenic traits for disease as well as 25 neuroimaging and other clinical phenotypes. We will continue to actively integrate our methods 26 into community standards (MPEG-G, GA4GH). 27 Successful completion of these aims will result in computational methods and open-source, 28 easy-to-use, production-grade implementations that open the door to secure integration and 29 analysis of massive sets of sensitive genomic and clinical data. With input from our collaborations, 30 we will build these tools and apply them to better understand the molecular causes of human 31 health and its translation to the clinic. Project Narrative Combining genomic and health-related data from millions of patients will empower the development of clinically relevant measures of human health and disease risks. However, this task requires securely sharing sensitive data at an immense scale beyond what existing cryptographic platforms can achieve. Here we develop novel computational methods to enable biomedical data integration, analysis, and interpretation in a privacy-preserving and highly scalable manner.",Privacy-preserving genomic medicine at scale,9998648,R01HG010959,"['Address', 'Adoption', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biological', 'Biology', 'Clinic', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Data Analyses', 'Data Pooling', 'Data Security', 'Data Set', 'Disease', 'Drug Interactions', 'Electronic Health Record', 'Engineering', 'Environment', 'Genetic', 'Genetic study', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Institutes', 'Institution', 'Knowledge', 'Letters', 'Machine Learning', 'Mainstreaming', 'Measures', 'Medical Imaging', 'Medical Records', 'Medicine', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Patients', 'Performance', 'Pharmacology', 'Polygenic Traits', 'Privacy', 'Process', 'Production', 'Property', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Science', 'Secure', 'Security', 'Software Engineering', 'Software Tools', 'Standardization', 'Stream', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Time', 'Translations', 'United States National Institutes of Health', 'Work', 'analysis pipeline', 'base', 'biobank', 'bioimaging', 'clinical development', 'clinical phenotype', 'clinically relevant', 'cohort', 'computer framework', 'cost', 'cryptography', 'data analysis pipeline', 'data integration', 'data sharing', 'data warehouse', 'disorder risk', 'epidemiology study', 'experimental study', 'genome analysis', 'genome wide association study', 'genomic data', 'health data', 'innovation', 'insight', 'monomethoxypolyethylene glycol', 'neuroimaging', 'novel', 'open source', 'polygenic risk score', 'precision medicine', 'preservation', 'privacy preservation', 'statistics', 'success', 'task analysis', 'theories', 'tool']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,636185,0.018288727882210268
"Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration PROJECT SUMMARY The cell nucleus is a heterogeneous organelle that consists of nuclear bodies such as nuclear lamina, speckles, nucleoli and PML bodies. These structures continuously tether and tug chromatin at the small and large scales to synergistically orchestrate dynamic functions in distinct spatio-temporal compartments. A major obstacle to the production of navigable 4D reference maps and relating structure to function in the nucleus remains understanding how these different scales of organization influence each other. In particular, we have a poor understanding of the large-scale genome organization. Growing evidence suggests that such nuclear compartmentalization is causally connected with vital genome functions in human health and disease. However, the principles of this nuclear compartmentalization, its dynamics during changes in cell conditions, and its functional relevance are poorly understood. One lesson from Phase 1 4DN was the huge gap in throughput between imaging methods, that directly measure large-scale multi-landmark relationships, and genomic methods, that aim for whole genome high-resolution maps but are indirect measurements and provide limited information about large-scale compartments. For this 4DN UM1 Center application, we propose to meet these needs through the following Aims: (1) Generate multi-modal imaging and genomic datasets to reveal the structure, dynamics, and function of nuclear compartmentalization; (2) Develop and apply computational tools for data-driven genome structure modeling and integrative analysis of nuclear compartmentalization; (3) Develop an integrative analysis and visualization platform with navigable 4D reference maps of nuclear organization. The combined datasets and results of our proposed approaches will advance our understanding of nuclear compartmentalization, the interwoven connections among different nuclear components, and their functional significance. Our new integrative analysis tools and data-driven predictive models will produce more complete nuclear organization reference maps that integrate large-scale chromosome structure data from live and super-resolution microscopy with multi-modal genomic data including smaller scale chromatin interaction maps and predict functional relationships and dynamic responses. Our navigable reference maps will be publicly accessible through an analysis platform that provides interactive visualization of multiple data types, thus enabling investigators with diverse expertise to simultaneously explore their own data and related datasets/tools and promoting collaborations that will open new horizons into the role of the 4D nucleome in human health and disease. PROJECT NARRATIVE The proposed research is relevant to public health because it will enhance our understanding of nuclear genome organization and functions that are increasingly being linked to health and disease. Because we develop tools to disseminate this information and enable others to work with our data and their own data, we will also bring nuclear architecture to bear on a broad range of ongoing health related research. Thus, the proposed research is relevant to NIHâs mission that seeks to obtain fundamental knowledge that will help to improve human health.",Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration,10156141,UM1HG011593,"['Address', 'Architecture', 'Atlases', 'Binding', 'Biochemical', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Chromatin', 'Chromatin Loop', 'Chromatin Structure', 'Chromosome Structures', 'Chromosomes', 'Collaborations', 'Communities', 'Complement', 'Computing Methodologies', 'Cytology', 'DNA Replication Timing', 'Data', 'Data Set', 'Development', 'Disease', 'Formulation', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Interphase Chromosome', 'Intuition', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Methods', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nuclear', 'Nuclear Lamina', 'Nuclear Structure', 'Organelles', 'Outcome', 'Output', 'Phase', 'Population', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Structural Models', 'Structure', 'Technology', 'Three-Dimensional Imaging', 'United States National Institutes of Health', 'Ursidae Family', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'cell cycle genetics', 'cell type', 'computer framework', 'computerized tools', 'data exploration', 'data integration', 'data tools', 'experimental study', 'genome-wide', 'genomic data', 'histone modification', 'imaging modality', 'improved', 'insight', 'machine learning algorithm', 'mental function', 'multimodal data', 'multimodality', 'multiple data types', 'multiscale data', 'predictive modeling', 'response', 'spatiotemporal', 'structured data', 'tool', 'transcription factor', 'transcriptome sequencing', 'user-friendly', 'whole genome']",NHGRI,CARNEGIE-MELLON UNIVERSITY,UM1,2020,2075409,0.027527649332761188
"A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions Project summary There is a fundamental gap in our understanding of how mutations are preferentially targeted to the variable (V) regions of the Immunoglobulin (Ig) loci during somatic hypermutation (SHM). The persistence of this gap has limited our understanding of the mutagenic mechanisms involving activation-induced deaminase (AID) in the immune response and in the role of AID in mis-targeting mutations leading to B-cell lymphomas and other cancers. The long-term goal of the proposed research is to understand the global targeting of mutations in immunity that are required to protect us from infections. As high-throughput data from human antibody immune responses became available, it provided us with new opportunities to generate hypotheses to explain the underlying mechanisms of SHM. We now propose to generate further hypotheses using computational models applied to additional databases and to validate these hypotheses using cellular and animal experiments. Our objective is to understand what directs SHM across the many human Ig heavy chain V-regions. Our central hypothesis is that the V-region SHM process is highly dependent on a DNA sequence signature(s) that drives mutations in a largely deterministic fashion. This hypothesis is supported by our preliminary results using human in vivo data from a few human V region genes and has begun to be validated using independent databases and experiments in human B cell lines. The rationale is that evaluations of computational data based upon biological mechanisms, together with appropriate biological experiments, will reveal the key differences between IGHV regions (IGHV 3-23, 4-34, 1-18, 1-02, etc.) that lead to the dominance of each of those V regions in the responses to medically important antigens. Our hypothesis will be tested by pursuing two specific aims: 1) identify the extent to which a DNA signature determines the mutation process in four individual human IGHV genes that are important in disease responses; 2) examine the relationship between AID hotspots and PolÎ· hotspots across all the other human V region genes, thus rigorously defining a mutation targeting signature. Both aims will also entail studying human V region genes and modifications of them in human cell lines and in mice expressing a human V region to further confirm the signature and identify molecular mechanisms in vivo. Our approach is innovative because the computational models we are proposing will be mechanistically motivated focusing on the interaction between AID and PolÎ· hotspots, thus testing molecular mechanisms as opposed to classic statistical models using whole V region sequences that ignore the underlying biology. In addition, to focus on mechanisms we will leverage new high-throughput data from human V regions that have not undergone antigen selection. Our results will be highly relevant to human IgV repertoire analyses from immune responses that are currently hard to interpret and will help future vaccine and therapeutic antibody development, as well as help to understand mutations in human malignancies where AID plays a key role. Project narrative The proposed research is relevant to public health because understanding the targeting of AID-mediated mutations across many human heavy chain V-regions will make it possible to develop vaccines that will lead more rapidly to better and more broadly protective antibodies to infectious agents and reveal the risk factors in the development of B-cell malignancies and gastric and other solid tumors where AID is implicated.",A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions,10090262,R01AI132507,"['Affect', 'Alleles', 'Animal Experiments', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Autoimmunity', 'B lymphoid malignancy', 'B-Cell Development', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Biological', 'Biology', 'Cell Line', 'Chromatin', 'Complementarity Determining Regions', 'Computer Analysis', 'Computer Models', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Enzyme Activation', 'Evaluation', 'Event', 'Evolution', 'Family', 'Feedback', 'Frequencies', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Gene-Modified', 'Genes', 'Goals', 'HIV', 'Heavy-Chain Immunoglobulins', 'Human', 'Human Cell Line', 'Immune response', 'Immunity', 'Immunoglobulin Somatic Hypermutation', 'Immunoglobulins', 'Individual', 'Infection', 'Infectious Agent', 'Influenza', 'Influenza Hemagglutinin', 'Knock-in', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medical', 'Mismatch Repair', 'Molecular', 'Mus', 'Mutate', 'Mutation', 'Outcome', 'Pattern', 'Play', 'Polymerase', 'Process', 'Public Health', 'Research', 'Risk Factors', 'Role', 'Site', 'Solid Neoplasm', 'Statistical Models', 'Stomach', 'Structure of germinal center of lymph node', 'Techniques', 'Testing', 'Therapeutic antibodies', 'Time', 'Transgenic Mice', 'Vaccines', 'Validation', 'Variant', 'activation-induced cytidine deaminase', 'base', 'chromatin modification', 'density', 'experimental study', 'genetic variant', 'human data', 'in vivo', 'in vivo Model', 'innovation', 'neutralizing antibody', 'recruit', 'repair enzyme', 'response', 'spatial relationship', 'vaccine response']",NIAID,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,38074,0.009218605178767611
"A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions Project summary There is a fundamental gap in our understanding of how mutations are preferentially targeted to the variable (V) regions of the Immunoglobulin (Ig) loci during somatic hypermutation (SHM). The persistence of this gap has limited our understanding of the mutagenic mechanisms involving activation-induced deaminase (AID) in the immune response and in the role of AID in mis-targeting mutations leading to B-cell lymphomas and other cancers. The long-term goal of the proposed research is to understand the global targeting of mutations in immunity that are required to protect us from infections. As high-throughput data from human antibody immune responses became available, it provided us with new opportunities to generate hypotheses to explain the underlying mechanisms of SHM. We now propose to generate further hypotheses using computational models applied to additional databases and to validate these hypotheses using cellular and animal experiments. Our objective is to understand what directs SHM across the many human Ig heavy chain V-regions. Our central hypothesis is that the V-region SHM process is highly dependent on a DNA sequence signature(s) that drives mutations in a largely deterministic fashion. This hypothesis is supported by our preliminary results using human in vivo data from a few human V region genes and has begun to be validated using independent databases and experiments in human B cell lines. The rationale is that evaluations of computational data based upon biological mechanisms, together with appropriate biological experiments, will reveal the key differences between IGHV regions (IGHV 3-23, 4-34, 1-18, 1-02, etc.) that lead to the dominance of each of those V regions in the responses to medically important antigens. Our hypothesis will be tested by pursuing two specific aims: 1) identify the extent to which a DNA signature determines the mutation process in four individual human IGHV genes that are important in disease responses; 2) examine the relationship between AID hotspots and PolÎ· hotspots across all the other human V region genes, thus rigorously defining a mutation targeting signature. Both aims will also entail studying human V region genes and modifications of them in human cell lines and in mice expressing a human V region to further confirm the signature and identify molecular mechanisms in vivo. Our approach is innovative because the computational models we are proposing will be mechanistically motivated focusing on the interaction between AID and PolÎ· hotspots, thus testing molecular mechanisms as opposed to classic statistical models using whole V region sequences that ignore the underlying biology. In addition, to focus on mechanisms we will leverage new high-throughput data from human V regions that have not undergone antigen selection. Our results will be highly relevant to human IgV repertoire analyses from immune responses that are currently hard to interpret and will help future vaccine and therapeutic antibody development, as well as help to understand mutations in human malignancies where AID plays a key role. Project narrative The proposed research is relevant to public health because understanding the targeting of AID-mediated mutations across many human heavy chain V-regions will make it possible to develop vaccines that will lead more rapidly to better and more broadly protective antibodies to infectious agents and reveal the risk factors in the development of B-cell malignancies and gastric and other solid tumors where AID is implicated.",A combined computational and experimental approach to the evolution and role of the DNA sequence environment in targeting mutations to antibody V regions,9882227,R01AI132507,"['Affect', 'Alleles', 'Animal Experiments', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Autoimmunity', 'B lymphoid malignancy', 'B-Cell Development', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Biological', 'Biology', 'Cell Line', 'Chromatin', 'Complementarity Determining Regions', 'Computer Analysis', 'Computer Models', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Enzyme Activation', 'Evaluation', 'Event', 'Evolution', 'Family', 'Feedback', 'Frequencies', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Gene-Modified', 'Genes', 'Goals', 'HIV', 'Heavy-Chain Immunoglobulins', 'Human', 'Human Cell Line', 'Immune response', 'Immunity', 'Immunoglobulin Somatic Hypermutation', 'Immunoglobulins', 'Individual', 'Infection', 'Infectious Agent', 'Influenza', 'Influenza Hemagglutinin', 'Knock-in', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mediating', 'Medical', 'Mismatch Repair', 'Molecular', 'Mus', 'Mutate', 'Mutation', 'Outcome', 'Pattern', 'Play', 'Polymerase', 'Process', 'Public Health', 'Research', 'Risk Factors', 'Role', 'Site', 'Solid Neoplasm', 'Statistical Models', 'Stomach', 'Structure of germinal center of lymph node', 'Techniques', 'Testing', 'Therapeutic antibodies', 'Time', 'Transgenic Mice', 'Vaccines', 'Validation', 'Variant', 'activation-induced cytidine deaminase', 'base', 'chromatin modification', 'density', 'experimental study', 'genetic variant', 'human data', 'in vivo', 'in vivo Model', 'innovation', 'neutralizing antibody', 'recruit', 'repair enzyme', 'response', 'spatial relationship', 'vaccine response']",NIAID,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,591815,0.009218605178767611
